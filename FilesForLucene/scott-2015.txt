lable at ScienceDirect
Clinical Radiology 70 (2015) 1185e1191Contents lists avaiClinical Radiology
journal homepage: www.cl inicalradiologyonl ine.netRadiology reports: a quantifiable and objective
textual approach
J.A. Scott*, E.L. Palmer
Department of Radiology, Massachusetts General Hospital and Harvard Medical School, Boston, MA 02114, USAarticle information
Article history:
Received 9 March 2015
Received in revised form
20 April 2015
Accepted 5 June 2015* Guarantor and correspondent: J.A. Scott, D
White 427, 55 Fruit Street, Massachusetts Gen
02114, USA. Tel.: þ1 617 726 8350; fax: þ1 617 7
E-mail address: scott@helix.mgh.harvard.edu
http://dx.doi.org/10.1016/j.crad.2015.06.080
0009-9260/ 2015 The Royal College of Radiologists.AIM: To examine the feasibility of using automated lexical analysis in conjunction with
machine learning to create a means of objectively characterising radiology reports for quality
improvement.
MATERIALS AND METHODS: Twelve lexical parameters were quantified from the collected
reports of four radiologists. These included the number of different words used, number of
sentences, reading grade, readability, usage of the passive voice, and lexical metrics of
concreteness, ambivalence, complexity, passivity, embellishment, communication and cogni-
tion. Each radiologist was statistically compared to the mean of the group for each parameter
to determine outlying report characteristics. The reproducibility of these parameters in a given
radiologist’s reporting style was tested by using only these 12 parameters as input to a neural
network designed to establish the authorship of 60 unknown reports.
RESULTS: Significant differences in report characteristics were observed between radiolo-
gists, quantifying and characterising deviations of individuals from the group reporting style.
The 12 metrics employed in a neural network correctly identified the author in each of 60
unknown reports tested, indicating a robust parametric signature.
CONCLUSION: Automated and quantifiable methods can be used to analyse reporting style
and provide impartial and objective feedback as well as to detect and characterise significant
differences from the group. The parameters examined are sufficiently specific to identify the
authors of reports and can potentially be useful in quality improvement and residency training.
 2015 The Royal College of Radiologists. Published by Elsevier Ltd. All rights reserved.Introduction
The radiology report has long been a subject of conver-
sation among radiologists. Despite attempts at stand-
ardisation,1e4 clinical radiology reports retain substantial
variability, largely reflecting the personal biases of the au-
thors. There are many stories of colleagues anecdotally
identifying the author of a report simply by characteristicepartment of Radiology,
eral Hospital, Boston, MA
26 6165.
(J.A. Scott).
Published by Elsevier Ltd. All rightextual mannerisms. Such differences in style may not be
critical to report quality, but their analysis has generally
been based upon subjective assessments. A more objective
approach to report analysis could complement higher-level
subjective analyses by providing impartial and quantifiable
characterisation of the radiology report. The present study
presents an automated method involving the quantification
of the lexical properties of the report.
The primary goal was to improve dialogue among
reporting physicians regarding report quality. It is antici-
pated that an unbiased and quantifiable characterisation of
reports might be useful in providing non-confrontational
feedback to radiologists regarding their own reporting
styles and how these compare with their peers. In thets reserved.
J.A. Scott, E.L. Palmer / Clinical Radiology 70 (2015) 1185e11911186present study, reports from a group of four experienced
radiologists were examined. A randomly selected group of
reports was obtained from each radiologist and evaluated
using two standard lexical programs to determine the in-
dividual characteristics of the reports. The study was
limited to bone scintigraphy to ensure that differences be-
tween reporting physicians originated from reporting styles
rather than varying study complexity. The data were then
employed in a neural network to identify the author of
unknown reports to validate the specificity of the individual
report characterisations more effectively.
Materials and methods
This project was approved by the institutions review
board.
The database
Textual analysis was performed using data obtained from
Digitext Diction 7.0 (Digitext, Austin, TX, USA) andMicrosoft
Word 2010 (Microsoft Office Professional 2010, Microsoft,
Redmond, WA, USA). The Diction 7.0 program is a
commercially available software program that analyses
writing for a variety of stylistic elements. The intrinsic tools
of Microsoft Word measure simple textual parameters
related to reading difficulty, textual length, and passive
voice usage. The use of these programs is discussed in more
detail below.
Seventy-five consecutive bone scintigraphy reports
dating from January 2011 through March 2013 were
randomly selected for each of four reporting physicians.
Each of these was a staff physician with at least 20 years of
clinical experience. All four physicians were board certified
in nuclear medicine and three of the four in diagnostic
radiology. The database was limited to “staff only” dicta-
tions (those without resident involvement) so that the re-
ports reflected only the reporting style of staff physicians.
Because of the varying length of reports and the difficulty in
meaningfully characterising the occasional short “normal”
report, textual selections were constructed from five
different reports combined into a single textual body. This
process created 15 textual samples from the original 75
reports representing each of the four staff physicians
studied, each sample consisting of the text from five inde-
pendent reports. The report texts consisted of freeform
transcriptions and included only the body of the report and
the conclusion. All standard entries, such as dosages, clinical
information, and standard heading titles, were omitted so
that the text would reflect only the author’s personal
reporting style and not be influenced by standard verbiage
common to all reports.
Language processing
These 60 samples were processed using both Digitext
Diction and Microsoft Word to evaluate 12 independent
variables, described below. Each variable was quantified by
the appropriate software, resulting in a numeric value foreach report analysed. Microsoft Word was used to identify
four variables: the number of sentences in the text, the
percent usage of passive voice, the FlescheKincaid reading
grade level, and the Flesch reading ease.5 The
FlescheKinkaid reading grade level and the Flesch reading
ease are standard assessments used to quantify the reading
difficulty of a passage.
Digitext Diction software was used to analyse text for
eight linguistic parameters. Diction 7 is a computer-based
textual analysis program that evaluates the “tone” of a
verbal message by using dictionaries of words reflecting a
particular expressive content. Tone refers to “a tool people
use (sometimes unwittingly) to create distinct social im-
pressions via word choice”.6 The assumption made is that
“tone is the product of individual word choices that
cumulatively build up to produce patterned expectations
that tell an audience something important about the au-
thor’s outlook”.7 The program employs large dictionaries of
unique words that characterise certain tonal parameters
whose frequency in a text indicates the intensity of a
particular tone. The Diction analysis begins by defining
several different variables to describe the tone of a text,
each evaluating the use of language relating to a specific
area.8 The eight variables used were prospectively chosen
because of their anticipated likelihood of relevance to the
radiology report. These included: (a) a variable quantifying
the degree of “ambivalence” (words expressing hesitation
or uncertainty, implying a speaker’s inability or unwilling-
ness to commit to the verbalisation being made). Included
are hedges, statements of inexactness, confusion, restrained
possibility, and mystery; (b) a variable quantifying
“communication” (terms referring to social interaction,
including both modes and moods of intercourse, as well as
terms reflecting various social purposes); (c) a variable
quantifying “cognition” (words referring to cerebral pro-
cesses, both functional and imaginative); (d) a variable
quantifying “passivity” (words ranging from neutrality to
inactivity, including terms indicating compliance, docility,
and cessation). Note that passivity here refers to the use of
specific words rather than the grammatical passive voice;
(e) a variable quantifying “concreteness” (a large dictionary
with little thematic unity excepting words characterised by
tangibility and materiality); (f) a variable quantifying
“embellishment” (a selective ratio of adjectives to verbs
based on the conception that heavy modification slows
down a verbal passage by de-emphasising human and
material action); and (g) a variable quantifying
“complexity” (a measure of the average number of
characters-per-word). The eighth parameter that was
employed was the number of different words used in the
report (although this is not strictly a tonal parameter, it is
measured by the Diction software). The four variables ob-
tained fromMicrosoft Word and the eight variables derived
from Digitext Diction comprise the 12 variables examined.
Neural network processing
Having quantified these 12 parameters for each radiol-
ogist, it was of interest to determine how specific they were
Table 1
Values for each of the textual parameters are shown for each of the four
readers.
Variable Reader 1 Reader 2 Reader 3 Reader 4
Different words 19511.4 25318.2a 1175.6b 18913.8
Complexity 5.50.05 5.50.05 5.30.06 5.40.11
Reading grade 13.20.18a 11.90.26 11.70.27 8.70.4b
Readability 28.11.3 23.61.7 37.51.6 42.33
Number of sentences 291.3 38.32.1b 22.61.7 24.92
Passivity 60.99 7.41.02 11.51.2a 3.40.74
Embellishment 1.160.13 0.70.09 0.540.07 1.30.24
Communication 1.050.38 4.641 0.780.28 9.125.5
Cognition 4.41.1 3.30.53 0.740.23 3.150.68
Passive voice 1.070.67 5.31.18 4.530.88 9.471.34a
Concreteness 141.1 11.40.7 41.12.4b 12.21.3
Ambivalence 8.410.66 162.9 1.900.56 25.26.1
The numbers shown indicate the average value for the 15 textual
samples  the standard error of the mean. The number of different words
and number of sentences represent raw data. The other parameters were
derived from the textual software.
a p<0.05.
b p<0.01.
J.A. Scott, E.L. Palmer / Clinical Radiology 70 (2015) 1185e1191 1187to the reporting style of individual radiologists. In other
words, did they reproducibly characterise a radiologist’s
reporting style? In order to establish this, a neural network
was employed to predict the authorship of 60 unknown
reports. Twelve variables together with the known authors
of the texts were used to train a neural network whose
function would be to determine the most likely author of a
given unknown report from the set of four possible authors.
Neural network analysis was performed by using neural
network software (NeuroShell 2; Ward Systems Group,
Frederick, MD, USA) on a computer by using a 6800 cen-
tral processing unit (i7-3930K, 3.2 Ghz; Intel, Santa Clara,
CA, USA) with 32 GB of DDR3 RAM. The artificial neural
network consisted of the above 12 variables as input nodes
to a backpropagation network containing one intermediate
layer that consisted of five computer-derived or “hidden”
nodes, and four output nodes, each representing the nu-
merical likelihood that a particular author issued the report.
Networks were trained using a “round robin” or “leave one
out” method in which one case was omitted from network
training and the resulting network tested on this single case
left out during training. Each of the cases was left out once
during the training process such that all were tested by
networks created independently of the tested data.
In this manner, a total of 60 textual samples were used for
training, such that each individual text was tested by a
network trained on the other 59 samples. Each network was
trained until 500 consecutive iterations failed to minimise
the average error over all 59 training cases (the average error
is the sum of the squares of the differences between the
network predictions and the actual values for all cases). Once
the error of each network reached this level, training was
stopped. Each of the four output values indicated the likeli-
hood, expressed as a value between zero and one, which
when multiplied by 100, indicated the probability that a
particular reporting physician had dictated the given report.
These network-training parameters were selected based on
prior experience, as a compromise between “overtraining”
(memorisation by the network of irrelevant details in the
training cases, with resultant poor generalisation) and “un-
der-training” (insufficient learning of the interrelationships
between the input data, leading to poor predictions in new
cases). Each resulting networkwas then applied to the single
case that was left out of the training process, thus ensuring
independenceof the training and testing aspects of the study.
Statistics
Significant differences were evaluated by analysis of
variance (ANOVA) testing followed by a two-tailed t-test
using SPSS Statistics version 21 (2012, IBM, Armonk, NY,
USA). The Bonferroni method was applied to correct for
multiple comparisons. Data are shown as the
mean  standard error.
Results
Significant differences were found between the four
readers for all parameters except communication andcomplexity. These data are shown in Table 1. There were
significant outliers for many of the categories. Reader 1, for
example, showed the highest reading grade (p¼0.03
compared with all other readers). Reader 2 showed the
highest number of different words used (p¼0.017 compared
with all other readers) and the longest length of reports
(p¼0.003 compared with all other readers). Reader 3 was
highest in passivity (p¼0.044 compared with all other
readers) and concreteness (p<0.001 compared with all
other readers), while using the fewest different words
(p¼0.002 compared with all other readers). Reader 4 had
the lowest reading grade (p<0.001 compared with all other
readers) and the highest use of the passive voice (p¼0.044
compared with all other readers). Numerous other signifi-
cant differences existed between individual readers.
A parameter can best discriminate between different
readers if it shows a high interobserver variability and low
intra-observer variability (in other words an individual
reader is consistent but substantially differs from the other
readers). These data are shown in Fig 1 as the coefficients of
variation for each variable. The highest interobserver vari-
ability was found for communication, ambivalence,
passivity, and concreteness. The highest intra-observer
variability was evident in the use of the passive voice and
communication.
Fig 2 shows the average network predictions for the
reader of each of the four groups of scans. The y-axis rep-
resents the average network prediction that a given indi-
vidual dictated the report. The author of each of the 60
textual files was predicted correctly, the network assigning
the highest likelihood in each case to the actual reader.
Reader 4 showed themost unique style, while readers 1 and
2 showed the most similarity. The case in which the
network had the greatest difficulty (a report by reader 2),
the network still correctly predicted reader 2 with a 50%
likelihood, reader 1 with a 41% likelihood, reader 3 with a
6% likelihood, and reader 4 with a 3% likelihood.
Figure 1 Inter and Intra-reader variability is shown for the parame-
ters employed in the authorship analysis. For differentiating between
readers, high inter-reader variability and low intra-reader variability
are desirable in a given variable. The y-axis represents the coefficient
of variation, higher values indicating higher variability.
J.A. Scott, E.L. Palmer / Clinical Radiology 70 (2015) 1185e11911188Fig 3 shows the relative importance of the different
variables in the network analysis for the networks evalu-
ating authorship. The y-axis represents the relative impor-
tance of each parameter to the network’s prediction.
Different variables show varying degrees of importance in
the network determination. For example, identification of
authorship in the group studied was primarily determined
by reading grade, readability, and concreteness.Discussion
The appropriate composition and style of the diagnostic
report has been much discussed in the literature.2e4
Although there has been general academic agreement
concerning the essential qualities of a good report,4 varia-
tions in personal reporting style continue to reflect differ-
ences in training and opinion. Radiologists have historically
used an un-structured report in which the dictating physi-
cian describes imaging findings and their presumed clinicalFigure 2 The average network predictions for the authorship of the
reports of the four readers. The network predicted the correct reader
in all 60 cases. The y-axis represents the average relative likelihood of
a given reader issuing the report as predicted by the authorship
network. The x-axis shows the actual reader of the report.significance using their own personal style and terminol-
ogy. More recent approaches advocate more structured
reporting in an attempt to communicate with the referring
clinician more effectively. There is general agreement that a
good report is characterised by clarity, brevity, readability,
relevance to the clinical question, avoidance of hedging, and
providing a tailored list of differential diagnoses.9
Yet although radiologists may agree about the desirability
of these elements, they often differ regarding the means of
their implementation. Even reports of good quality contain
substantial stylistic differences.10
In this paper, a method of characterising radiology re-
ports using established methods of textual analysis was
evaluated.6e8 The quantified differences between the four
different readers (Fig 1) were sufficiently reproducible for a
machine-learning process to assign each of 60 unknown
reports to the correct author using only these variables.
Similar objective and quantitative methods could be used
for quality improvement, tracking resident performance
during training, and potentially to help identify the author
of a report when its authorship has fallen into question.
Computerised methods of quantifying authorial style
have been increasingly studied as computing power has
increased and the required technology becomemorewidely
accessible.11,12 Samples of texts from groups of known au-
thors are evaluated and compared, often with the goal of
identifying the writer of an unknown sample from among a
group of candidate authors. Multiple variables are generally
employed to enhance discrimination, usually emphasising
the use of “function words,” whose frequencies are not
expected to vary greatly with the topic of the text and
whose usage is often idiosyncratic and less consciously
controlled. In a similar manner, a number of reports of
sufficient length were evaluated tomeasure the style of four
different report authors across a range of 12 quantifiable
variables. These variable measurements were then used in a
machine-learning algorithm to determine the relative
similarity of an unknown report to each of these four
candidate authors. This analysis showed that the observed
differences reliably and reproducibly characterised each
individual (Fig 2).
This verification was accomplished using a neural
network to integrate the different textual parameters into a
numerical predictor of the likely authorship of a given
report. The network produced a simple percentage predic-
tion of the likelihood that a given individual (in this case,
which of the four possible authors) dictated a given report.
The network successfully integrated the several variables to
accomplish this, taking advantage of a high ratio of inter- to
intra-observer variability for several variables to discrimi-
nate between different readers with 100% accuracy. As Fig 3
demonstrates, concreteness, readability, and reading grade
were the most important network variables in determining
the author of the report. A neural network was chosen for
this analysis because of its ability to identify subtle and
often obscure inter-relationships between variables, re-
lationships that might be overlooked using other forms of
data analysis. Unfortunately, the complex mathematical
nature of neural networks makes the decision-making
Figure 3 The relative importance to the network of input parameters used to identify the author of the report. For example, readability was
highly important while embellishment and communication were less critical in making the predictions.
J.A. Scott, E.L. Palmer / Clinical Radiology 70 (2015) 1185e1191 1189process less amenable to simple verbal description. In other
words, although the network could correctly identify the
author of each report, the precise means bywhich it came to
this conclusion can only be generally described.
Analyses of radiology report quality have often employed
detailed metrics employing sophisticated evaluations, such
as whether the clinical question is appropriately responded
to and whether the patient’s clinical scenario is clarified by
the report.1,13 Although this provides a highly functional
analysis of the report, it is a complex and time-consuming
task, highly dependent upon experience, and subjective in
nature. The advantages of the analysis proposed here are its
relative simplicity, quantifiability, and objectivity, the latter
perhaps increasing the likelihood that the data will be
favourably received as feedback in quality-improvement
efforts Time-intensive subjective analyses will remain crit-
ical components of report evaluation, but their necessity
might be triaged using quicker computer-based methods,
such as that described here.
Although this type of analysis fails to consider more
complicated determinations, such as whether the conclu-
sion of the report adequately addresses the clinical ques-
tion, several aspects of the report measured here have been
shown to correlate with report quality as perceived by
referring physicians. For instance, Sierra et al.14 found strong
correlations between the FlescheKincaid readability of a
report and its clarity, as evaluated by referring practitioners.
They also found that perception of certainty in the report
was proportional to its readability, long and complex sen-
tences being viewed negatively by referring physicians.
Other workers have used word count, readability, and
definitive statements as goals in improving reporting
quality, emphasising the need for shorter, more simply
constructed, and more conclusive reports.10 It is thus likely
that many of the measures employed here, including
improved readability, lower reading grade, diminishedambivalence, shorter reports, and perhaps, lesser usage of
the passive voice would increase the perceived quality of
the reports.
The present analysis showed significant differences be-
tween the four readers studied (Table 1). For instance,
reader 3 produced short reports with high concreteness and
little ambivalence, implying a relatively high degree of
certainty. These reports would likely be perceived positively
by a referring clinician. On the other hand, reader 4 had the
highest ambivalence and the highest degree of embellish-
ment, suggesting a lower degree of certainty and potentially
resulting in a lower degree of satisfaction on the part of the
referring physician. Reader 2 produced the longest reports,
employed the largest variety of words and showed the
lowest readability, also less likely to be perceived positively
by the referrer. Such differences not only served to distin-
guish between the different readers, but provided quanti-
fiable and objective feedback to the radiologists.
Demonstrating an objective statistical difference between
one’s own reporting style and those of one’s peers may be
more likely to receive serious consideration regarding
change than would a subjective opinion voiced by a
colleague.
Excerpts from reports characterised by the software as
high and low in readability and ambivalence may serve as
examples. The following report shows both high readability
and low ambivalence:
“There is normal blood flow and soft tissue activity at the
level of both hips. The delayed images show a total hip
replacement in the right hip with no hardware compli-
cations. There are degenerative changes in the cervical
spine and lumbar spine. There is no evidence of loos-
ening or osteomyelitis in the right hip. There is no evi-
dence of RSD. The renal and soft tissue activities are
normal.”
J.A. Scott, E.L. Palmer / Clinical Radiology 70 (2015) 1185e11911190The following report measures low in readability:
“There are bilateral areas of increased activity in the
posterior seventh ribs. The aetiology of these foci is un-
clear. They may represent the residua of prior trauma
however radiographic correlation would be necessary to
confirm this. Metastatic disease would seem a less likely
aetiology but cannot be excluded. Mildly increased ac-
tivity in the lower lumbar spine at the L4e5 level is most
likely of osteoarthritic aetiology. Bilateral increased ac-
tivity at the medial compartments of both knees is
consistent with osteoarthritis. There is significant het-
erogeneous uptake in the cervical, and in the upper and
mid thoracic spine, most likely osteoarthritic in aeti-
ology. Radiographs would be necessary to confirm. There
is bilateral renal excretion of tracer. Although there is no
definite evidence of osseous metastatic disease, there are
several abnormal findings which are most likely post-
traumatic and degenerative. Radiographs would be use-
ful, as clinically indicated, to confirm the suspected
benign aetiology of abnormalities in the bilateral poste-
rior seventh ribs (likely traumatic) as well as the cervical
and upper thoracic spine (likely osteoarthritic).”
Finally, a report measuring high in ambivalence contrasts
the first example:
“Mildly high uptake at L5 bilaterally has an appearance
typically due to degenerative change but this limits
assessment for other processes. Areas of high uptake at
multiple joints including shoulders, elbows, wrists,
hands, hips knees, feet, and ankles are not specific but
are more suggestive of degenerative changes than met-
astatic disease. Focal sites of high uptake along the lower
SI joints with uptake relatively intense on the right;
although a common site for degenerative change, focal
asymmetric intensity on the right raises the potential for
this uptake being associated with a metastasis, particu-
larly given a history of back pain. Mildly high uptake at L5
bilaterally has an appearance most often due to degen-
erative change but uptake that can be due to this limits
assessment for metastatic disease. No new uptake ab-
normality that would strongly suggest skeletal metas-
tasis is shown.”
This methodology could have practical value in at least
three scenarios: first, the method provides a quantifiable
and unbiased measure of report characteristics that can
serve as feedback to the reporting radiologist. In addition to
receiving unbiased information as to their reporting style,
the physician can evaluate how their own style compares
with those of their peers and determinewhether alterations
in this style might be appropriate.
Second, this method can be used to measure and track
reporting styles in resident trainees, providing information
as to how their reporting styles might evolve to emulate
those of different mentors. It has been shown that resident
reporting skills increasewith training, despite the imperfect
nature of this process.15e17 Similarly, it has been shown that
the routine editing of trainee-generated reports improves
perceived report quality in terms of clarity, brevity,readability, and quality of the impression.18 This, or similar
methods, could be used to quantitatively track stylistic
changes in resident reports during training and perhaps to
examine the learning process insofar as it involves model-
ling the reports of staff mentors.
Finally, the identification of the author of a medical
report can be important when its authorship has been
called into question. In the computer age, a digital report is
several steps removed from the former practice of hand-
signing and physically delivering a report to the referring
physician. This digital distancing has been implicated in two
recent instances of fraud involved in radiology reporting.
The first involved a case in which a radiology technologist
forged mammography reports on 1289 women, resulting in
a number of cancers remaining undetected and an eventual
guilty plea on the part of the technologist.19 The second
involved a chiropractor who falsely attributed 487 radio-
logical reports to uninvolved radiologists, using image
analysis software operated by untrained technologists. The
chiropractor then issued the falsified reports bearing the cut
and pasted signatures of the unknowing radiologists.20 In
cases where report authorship has fallen into question,
whether by inadvertent digital error or deliberate intent,
differences in reporting style might be used to identify the
author of the report.
The present study is limited by the relatively small group
of reporting physicians studied, and by the use of only a
single imaging technique in the reports studied, and is
intended to be a pilot study. It seems likely that similar
results might be obtained using other imaging techniques,
although this has not been demonstrated here. Significant
improvements could likely be made by incorporating more
complex, machine-based analyses of linguistic structure.
The present method is also limited by its requirement for a
sufficient linguistic sample to permit meaningful analysis,
necessitating the concatenation of more than one report
text into a single sample. It would be unable to evaluate the
short, normal report. Finally, the accuracy of the report was
not addressed, as this was not the intent of the study. This
issue has been addressed elsewhere.
In conclusion, it is possible, using textual analysis and
artificial intelligence methods, to provide quantifiable and
objective feedback potentially useful for quality improve-
ment and resident training. These quantifiable data allow
impartial comparison of one’s own reporting style with
one’s peers and can uniquely characterise the reporting
styles of individual radiologists.References
1. Robert L, Cohen MD, Jennings GS. A new method of evaluating the
quality of radiology reports. Acad Radiol 2006;13:241e8.
2. Bosmans JML, Weyler JJ, Parizel PM. Structure and content of radiology
reports, a quantitative and qualitative study in eight medical centers.
Eur J Radiol 2009;72:354e8.
3. Hall FM. The radiology report of the future. Radiology 2009;251:313e6.
4. Kahn CE, Langlotz CP, Burnside ES, et al. Toward best practices in radi-
ology reporting. Radiology 2009;252:852e6.
5. Kincaid JP, Fishburne RP, Rogers RL, et al. Derivation of new readability
formulas (Automated Readability Index, Fog Count, and Flesch Reading Ease
J.A. Scott, E.L. Palmer / Clinical Radiology 70 (2015) 1185e1191 1191Formula) for Navy enlisted personnel. Research Branch report 8e75. Naval
Air Station Memphis: Chief of Naval Technical Training; 1975.
6. Hart RP. Systematic analysis of political discourse: the development of
DICTION. In: Sanders K, et al, editors. Political communication yearbook.
Carbondale, IL: Southern Illinois University Press; 1985. p. 97e134.
7. Hart RP, Childers JP, Lind CJ. Political tone: how leaders talk and why.
Chicago: University of Chicago Press; 2013.
8. Diction 7 reference manual. Austin, TX: Digitext Inc.; 2014.
9. Wallis A, McCoubrie P. The radiology reportdare we getting the mes-
sage across? Clin Radiol 2011;66:1015e22.
10. Pool F, Goergen S. Quality of the written radiology report: a review of
the literature. J Am Coll Radiol 2010;7:634e43.
11. Stamatatos E. A survey of modern authorship attribution methods. J Am
Soc Inf Sci Tech 2009;60:538e56.
12. Koppel M, Schler J, Argamon S. Computational methods in authorship
attribution. J Am Soc Inf Sci Tech 2009;60:9e26.
13. Wallis A, Edey A, Prothero D, et al. The Bristol radiology report assess-
ment tool (BRRAT): developing a workplace-based assessment tool for
radiology reporting skills. Clin Radiol 2013;68:1146e54.14. Sierra A, Bisesi MA, Rosenbaum TL, et al. Readability of the radiology
report. Invest Radiol 1992;27:236e9.
15. Mueller D, Georges A, Vaslow D. Cooperative learning as applied to
resident instruction in radiology reporting. Acad Radiol
2007;14:1577e83.
16. Williamson KB, Steele JL, Wilkin TD, et al. Assessing radiology resident
reporting skills. Radiology 2002;225:719e22.
17. Steele JL, Nyce JM, Williamson KB, et al. Learning to report. Acad Radiol
2002;9:817e20.
18. Coakley FV, Heinze SB, Shadbolt CL, et al. Routine editing of trainee-
generated radiology reports: effect on style quality. Acad Radiol
2003;10:289e94.
19. Brumback K. Former Ga. technician falsified mammogram reports.
Retrieved on August 17, 2014 from. 2014, http://bigstory.ap.org/article/
former-ga-technician-falsified-mammogram-reports html.
20. Millburg, S. Radiology fakery costs chiropractor $7 million. Retrieved on
August 17, 2014 from http://www.radiologydaily.com/daily/diagnostic-
imaging/radiology-fakery-costs-chiropractor-7-million/html.
