Chapter 4
Web Content Mining Using MicroGenres
VaÃÅclav SnaÃÅsÃåel, MilosÃå KudeÃålka, and ZdeneÃåk HoraÃÅk
Abstract. The size and growth of the current Web is still creating new challenges
to researchers. For example, one of these challenges is the improvement of user
familarity to a large number of Web pages. Today‚Äôs search engines provide tools that
allow users to refine their queries. One way is the refinement of a query based on the
analysis of web content. Possible outcomes are not only recommended collocations,
but also recommended page genres (e.g., discussion forums, etc.). It is proving to
be very useful to provide the details of page content when viewing the page. Not
only text snippets, but also parts of the page menu, for certain pages how many
posts are present in the discussion, what day the review was created, or what the
price is of a product sold on the page. Obtaining this information from unstructured
or semi-structured content is not straightforward. In this chapter the development
of methods capable of detecting and extracting information from Web pages will
be addressed. The concept of objects, called MicroGenre will be presented. Finally
we also present experiments with our own Pattrio method, which provides a way to
detect objects placed on Web pages.
4.1 Introduction
The Web is like a big city. It has its center, periphery, buildings and other facilities
with various purposes, and communications that connect one another. Also people
live in the city, and people move city over time. In order to study the city, one must
tackle many tasks. It would be a mistake, however, to study the city without people.
Certainly it could be achieved if one moves all the people away from the city, but it
would be difficult to find the purpose and meaning of many things. The comparison
VaÃÅclav SnaÃÅsÃåel ¬∑ MilosÃå KudeÃålka ¬∑ ZdeneÃåk HoraÃÅk
Faculty of Electrical Engineering and Computer Science,
VSÃåB‚ÄìTechnical University of Ostrava, Czech Republic
e-mail: 	% 
	%

	%
J.D. VelaÃÅsquez and L.C. Jain (Eds.): Advanced Techniques in Web Intelligence ‚Äì 1, SCI 311, pp. 79‚Äì111.
springerlink.com ¬© Springer-Verlag Berlin Heidelberg 2010
80 V. SnaÃÅsÃåel, M. KudeÃålka, and Z. HoraÃÅk
of the Web and a city is not self-serving. Many of the previous approaches and
methods for analyzing Web content resemble the analysis of an vacant city. For
example one can consider approaches dealing with the analysis of the text part of
a Web page regardless of its structure, visual appearance or development through
time. On the other hand one can consider approaches dealing with the structure of
the page with little or no emphasis on the content of particular parts of the Web
page.
It was only a matter of time when aspects based on human activity would per-
form an increasing role in the analysis of Web content. However the analysis of
Web content is very complex now. It embraces both technical aspects (starting with
HTML code, continuing through the overall structure of the page, finishing with the
visual representation of the page) and the human factor, which answers the ques-
tion: ‚ÄúHow do the people do it? How do they use it?‚Äù. Therefore, in the first part of
this chapter the areas of Web usability and Web design patterns will be addresed. In
our opinion, these two areas are not connected to the analysis of Web content very
often. Regardless of this, they can provide many interesting ideas that may help to
improved one‚Äôs understand of how to proceed with the analysis.
This chapter is organized as follows. In Sect. 4.2, a meaning of Web content
mining and typical tasks are explained. In Sect. 4.3, basic principles concerning
Web usability and Web design patterns are described. Sect. 4.4 is devoted to the
survey of recent approaches. In Sect. 4.5 the term MicroGenre as a building block
of Web page is defined. Our own Pattrio method, which focusses on the detection of
MicroGenres will presented. In Sect. 4.6, experiments related to the successfulness
of this method‚Äôs usability will be described. The last section of the chapter is devoted
to a summary and prospects for further research.
4.2 Web Content Mining Summary
The current World Wide Web is the result of interaction between authors of ideas and
users. This interaction is permanent and each of these groups takes part in the future
direction of the Web. One of the key elements of this progress is the view of the Web
from the opposing side - from the side of the data that results from this permanent
interaction. Automatically obtained data can be used for various tasks, especially
tasks connected with information retrieval. The approaches known from the field of
data mining (see Han and Kamber [30]) are applied in the Web environment during
the Web evolution and therefore new specific approaches arrise. These approaches
define the field of Web mining.
Web mining is the usage of data mining technology on the Web (see Han and
Chang [32]). Specifically, it is the case of finding and extracting information from
sources that relate to the user‚Äôs interaction with Web pages. In 2002 (see Han and
Chang [32]), several challenges had been formulated for developing an intelligent
Web: Web page complexity far exceeds the complexity of traditional text docu-
ments, the Web constitutes highly dynamic information, the Web serves as a broad
spectrum for user communities, and only a small portion of Web pages contain
4 Web Content Mining Using MicroGenres 81
truly relevant or useful information sources of any traditional text document col-
lection. Therefore, the following issues should be solved primarily: mining Web
search-engine data, analyzing Web link structures, classifying Web documents au-
tomatically, mining Web page semantic structures and page contents, mining Web
dynamics, building a multilayered, multidimensional Web, etc.
Most tasks are still relevant today. In [40], Kosala and Blockeel define three areas
of Web mining: Web content mining, Web structure mining and Web usage mining.
Web content mining: It describes the discovery of useful information from the
Web contents, data, and documents. The Web content data consists of unstruc-
tured data such as free texts, semi-structured data such as HTML documents, and
more structured data such as data in tables or database generated HTML pages.
Web structure mining: This attempts to discover the model underlying the linking
structures of the Web.
Web usage mining: This attempts to make sense of the data generated by the Web
user‚Äôs sessions or behaviors.
Our research focuses on Web content mining whose purpose is to analyze Web pages
in order to find out which useful information is included in the Web page (from the
user‚Äôs point of view). In this area, two sub-divisions can be found. The first one
is based on information retrieval and its purpose is to find information useful for
locating relevant Web pages in large collections (Web searching). The other one is
based on information extraction and its purpose (see Chang et al. [11]) is to find
structural information that can be, for example, saved in the database and to process
it accordingly (for example the name and price of products).
Liu and Chang [49] consider twelve topics on Web content mining. Following
this, there is a classification of mining tasks:
Structured Data Extraction: One of the reasons for the importance of this topic is
that structured data on the Web is often very important as they represent essential
information on their host pages (e.g., lists of products and services).
Unstructured Text Extraction: Most Web pages can be seen as text documents.
This research is closely related to text mining, information retrieval and natural
language processing.
Web Information Integration: Web sites may use different syntaxes to express sim-
ilar or related information. In order to make use of or extract information from
multiple sites to provide value added services, it is necessary to semantically
integrate information from multiple sources.
Building Concept Hierarchies: Because of the huge size of the Web, the organiza-
tion of information is obviously an important issue. A linear list of ranked pages
is usually produced by search engines. The standard method for information or-
ganization is through concept hierarchy or categorization. Popular techniques
include clustering methods.
Segmenting Web Pages & Detecting Noise: A typical Web page consists of many
blocks or areas (main content areas, navigation areas, advertisements, etc). It is
useful to separate these areas automatically for several practical applications.
82 V. SnaÃÅsÃåel, M. KudeÃålka, and Z. HoraÃÅk
Mining Web Opinion Sources: Companies usually conduct customer surveys or
engage external consultants to find such reviews about their products. There are
numerous Web sites and pages containing customer opinions (e.g., customer re-
views of products, discussion groups, and blogs).
Human factor plays a key role in all the aforementioned tasks. These are still the
same tasks on an abstract level, however as the requirements and technology evolve,
also the implementation environment changes. In order to successfully perform
these tasks the evolution of the Web and the way users work must be followed.
One must to adapt methods from the area of Web content mining. The field which
formalizes the interaction in the Web environment is the field of Web usability.
4.3 Web Usability Basics
Web usability is closely linked to User Centered Design (UCD, see [75]). In a wider
sense, UCD is a philosophy which results from the process of software system de-
velopment. In this system, the user is involved in the first stage of each phase. The
main difference from other approaches is that UCD tries to optimize the user inter-
face, so that it:
‚Ä¢ Corresponds to what users are used to.
‚Ä¢ Does not make the user change their way of working.
From the user‚Äôs point of view the result of the development process in the Internet
environment is a Web page. Each page is prepared for the user and it is dependent
on its quality if the user gains what he needs from interacting with it. The problem
is that the quality is very difficult to measure in advance. Users have to contribute to
the verification of Web page quality. If the Web page should be one of high quality, it
should fulfill certain principles. Nielsen defines usability as the following (see [55]):
Usability is a quality attribute that assesses how easy user interfaces are to use.
Usability is defined by five quality components:
1. Learnability: How easy is it for users to accomplish basic tasks the first time
they encounter the design?
2. Efficiency: Once users have learned the design, how quickly can they perform
the tasks?
3. Memorability: When users return to the design after a period of absence, how
easily can they reestablish proficiency?
4. Errors: How many errors do users make, how severe are these errors, and how
easily can they recover from the errors?
5. Satisfaction: How pleasant is it to use the design?
In the book [56], Nielsen and Loranger present many tests with users and many
important recommendations for Web page creators emerges from the results of these
tests. One of which is the definition of what Web page authors should accept and
avoid.
4 Web Content Mining Using MicroGenres 83
Standard: Eighty percent or more of Web sites use the same design approach.
Users strongly expect standard elements to work a certain way when they visit
a new site because that is how things almost always work (the standard can be
understood as a formal or universally acceptable norm regardless of the user
context.).
Convention: About 50 to 79 percent of Web sites use the same design approach.
Users expect conventional elements to work a certain way when they visit a new
site because that is how it usually works (the convention can be understood as a
custom in a specific context, e.g. in the discussion forum.).
Confusion: Withrespect to these elements, no single design approach dominates,
and even the most popular approach is used by less than half of Web sites. For
such design elements, users do not know what to expect when they visit a new
site.
Regarding the fact that the recommendations coming out of Web usability are ac-
knowledged by Web authors, one can, on the other hand, focus on certain features
(from the point of view of Web content mining) which result from standards and
conventions.
However, one of the problems is that recommendations in the field of Web usabil-
ity are not completely formalized. In [28], Graham concludes that recommendations
resulting from Web usability can be formalized with the use of patterns (for more
about patterns see section 4.3.1). One of the characteristics of the pattern which de-
scribes verified experience is the concise name, which characterize the solved task.
The methodology of using patterns for Web design is very thoroughly elaborated
in [74] (Van Duyne et al.). Classification of patterns and important recommendations
for developers that can be found link not only to Web usability.
Further interesting pieces of information related to the usability view on the Web
page which are stated in [72] (Tidwell). These pieces of information are related to
page layout and parts of page perception. The theory of grouping and alignment
was developed early in the 20th century. The Gestalt psychologist described several
layout properties . Some of which seem to be important for our visual systems (see
Figure 4.1):
Proximity occurs when elements are placed close together. These elements tend
to be perceived as a group. If things are close together viewers will associate
them with one another. This is the basis for a strong grouping of content and
controls on a user interface.
Similarity occurs when elements look similar to one another. These elements tend
to be perceived as a group. If two items are the same shape, size, color, or orien-
tation, then viewers will also associate them with each other.
Continuity occurs when the eye is forced to move through one element and con-
tinue through to another element. Our eyes want to see continuous lines and
curves formed by the alignment of smaller elements.
Closure occurs when elements are not completely enclosed in a space. If enough
of the information is indicated, these elements tend to be perceived as a group,
the missing elements are filled in automatically: One also expects to see
84 V. SnaÃÅsÃåel, M. KudeÃålka, and Z. HoraÃÅk
Fig. 4.1 Gestalt Principles (Proximity, Similarity, Continuity, Closure)
simple closed forms, like rectangles and blobs of whitespace that are not ex-
plicitly drawn for us.
In paper [23], Flieder and MoÃàdritscher propose a set of principles based on Gestalts
in a pattern language form. At the lowest level, these principles follow Gestalt prin-
ciples and describe the essential syntax, which establishes the rules for combin-
ing words, shapes, and images. These morphological elements have various visible
properties such as color, size, thickness, texture, orientation, and transparency.
As aforementioned, it is difficult to measure the quality of a Web page exactly.
However, Ivory et al. describe some characteristics which are unsuitable for Web
page quality evaluation (see [35]). Selected results are listed in Table 4.1.
Table 4.1 Selected Web page design metrics
Metric Description Significance
Word Count Total words on a page 0.150
Body Text % Percentage of words that are body vs. headers 0.824
Emphasized Body Text % Portion of body text that is emphasized 0.672
Text Positioning Count Changes in text position from flush left 0.244
Text Cluster Count Text areas highlighted (color, regions,...) 0.000
Link Count Total links on a page 0.000
Page Size Total bytes for the page and images 0.001
Graphics Count Total images on a page 0.002
Color Count Total colors employed 0.001
Font Count Total font face and size combinations employed 0.836
On the basis of these characteristics, the authors performed experiments with the
involvement of users, searching for links between users‚Äô evaluations of Web pages
and the characteristics. In paper [34], Ivory and Megraw work with these charac-
teristics as well as with Web design patterns and watch over time how the patterns
4 Web Content Mining Using MicroGenres 85
develop. The conclusions which emerged from these experiments with users and
through pattern evolution are linked to Web usability. Moreover they provide useful
technical basics for procedures linked to Web content mining.
Also eye-tracking is an important field in user interface design (see Goldberg et
al. [27]). It is about how a user‚Äôs goal influences the way they read and traverse a
Web page, which parts of a page users attend to first, how people react to adver-
tising, where they look first for common page elements, how they respond to text,
pictures, and much more. In [24], Gagneux et al. formulate an answer to the ques-
tion ‚ÄúIn which way does the visual organization of the Web pages help to lead the
visual exploration for an information retrieval?‚Äù. A specific goal can be described
by respecting two characteristics:
‚Ä¢ It must be compatible with the set of the designer‚Äôs intentions.
‚Ä¢ It must be compatible with the set of the user‚Äôs potentials.
As a result, methods of Web page analysis have to be based on the relations among
human perception, cognitive sciences, and biology. The Web page is usually split
up into two common structures:
‚Ä¢ The physical structure that expresses the organization as geometric blocks cre-
ated with homogeneous characteristics. This is the layout of the document.
‚Ä¢ The logical structure that expresses the semantic description of the physical or-
ganization, which deals with the human interpretation of each of the blocks (line,
section, title, paragraph. . .).
Based on these characteristics, good Web pages are characterized by a simple struc-
ture, a basic scan path, a low number of fixations, a heterogeneous distribution of
fixations and an understandable physical structure on a low level (during the pre-
attentive glance). In contrast, bad quality Web pages are characterized by many
fixations, a homogeneous distribution of fixations on images, a rather high density
of information and an absense of logical path on the physical structure.
Simply said, the more the developers keep to the mentioned standards, agree-
ments, principles and patterns, the more usable the Web page is. Then it can be
expected from the other side that the more usable the page is, the more synoptic it
is also for those who deal with algorithm design for its automatic analysis.
The persistent problem is the low level of formalization of methods, standards
and conventions. As mentioned above, one possible way to formalize such things is
the design patterns. Using this formalization, we can - at a certain level of granularity
- describe a Web page.
4.3.1 Web Design Pattern Basics
Design patterns and pattern languages came from the architectural of Christopher
Alexander and his colleagues. From the mid sixties to the mid seventies, Alexander
and his colleagues defined a new approach to architectural design. The new ap-
proach centered on the concept of pattern languages, and is described in a series of
books (see [1, 2]).
86 V. SnaÃÅsÃåel, M. KudeÃålka, and Z. HoraÃÅk
Alexander‚Äôs definition of pattern is as follows (see [1]): ‚ÄúEach pattern describes
a problem which occurs over and over again in our environment, and then describes
the core of the solution to that problem, in such a way that you can use this solution
a million times over, without ever doing it the same way twice‚Äù.
According to [72] patterns are structural and behavioral features improve the
applicability of software architecture, a user interface, a web site or something else
in some domain. They make things more usable and easier to understand. Patterns
are descriptions of the best practices within a given design domain. They capture
common and widely accepted solutions, their validity is empirically proved. Patterns
are not novel ideas, they are captured experiences and each of their implementations
is a little different.
In software architecture, design patterns are a mechanism for expressing object-
oriented design experience (see [25]). Design patterns identify, name, and abstract
common themes in object-oriented design. They provide a common vocabulary and
reduce system complexity by naming and defining abstractions. Design patterns can
be considered reusable micro-architecture of overall system architecture.
Interaction design patterns supply a solution to typical problems within the design
of the user interface. A typical example is the organization of user controls into lists
or tabs and so on. Interaction design patterns describe on a general level how to make
structures from information within user interface. They also tell which components
to use, how they should work together and how to work with them (see Tidwell [72]).
A good example is also the Web design patterns. These are design patterns related
to the web. A typical example of a Web design pattern can be the Forum pattern (see
Figure 4.2). This pattern is meant for designers who need to implement this element
on an independent web page or as part of another web page. The pattern describes
key solution features without implementation details.
Fig. 4.2 Sample of Forum pattern (www.welie.com)
4 Web Content Mining Using MicroGenres 87
Pattern Language
An important aspect about patterns is that they are usually related to each other and
occur in groups. Another key feature of patterns is that they can be worked within
a fashion similar to that of a dictionary, because each pattern has its name, which
characterizes its use. It is therefore possible to create so called pattern languages (see
Dearden and Finlay [16]), which are groups of patterns covering a certain domain.
When using the patterns, neither the author nor the user have to think about how
to describe what should be on a Web page in detail. He/she can use the dictionary
of pattern language. Pattern languages therefore provide a comprehensive language
with which we are able to communicate with the user about what is expected on a
Web page.
Borchers formally describes the pattern language in the following way (see [6]):
Definition 4.1. A Pattern Language is a directed acyclic graph PL = (P,R) with
nodes P = {p1, . . . , pn} and edges R = {r1, . . . ,rm}.
1. Each node pi ‚àà P is called a pattern.
2. For pi, p j ‚àà P : pi references p j if exists rk = (pi, p j) ‚àà R.
3. The set of edges leave a node pi, which is called its references.
4. The set of edges entered pi, is called its context.
The definition implies that there can be relations among the patterns, which can be
described on an even more detailed level. Relationships between patterns can be
defined by some levels. In [77], Welie uses a simple classification.
Aggregation: This is a form of a ‚Äúhas-a‚Äù relationship.
Specialization: This is a ‚Äúis-a‚Äù relationship, one pattern is a more specific version
of another pattern.
Association: This is not a ‚Äúhas-a‚Äù or ‚Äúis-a‚Äù relationship but simply a ‚Äúrelated-to‚Äù
relationship. A pattern may be associated with other patterns because they also
often occur in the larger context of the design problem, or because the patterns
are alternatives for the same kind of problems.
Patterns are simple descriptions of repeated problems and their solutions. They are
intended for developers and they do not contain technical details. That is why their
instances in real solutions are not stereotypes and in many cases they are difficult
to distinguish. Besides that they deal with different levels of detail (see Welie [78]).
It can be a level of a whole page, and also that, of a small part of a page, e.g. user
login. On the other hand, some parts of Web pages can be so basic that it is not
necessary to describe them in the form of patterns. However they can be interesting
from a page description point of view.
4.4 Recent Methods
Firstly, we will describe approaches and methods, which are used for the analy-
sis of Web page structure and Web page segmentation (see [48]). Methods can be
classified in several ways:
88 V. SnaÃÅsÃåel, M. KudeÃålka, and Z. HoraÃÅk
1. On the basis of role, which the DOM tree plays on DOM based and visual layout
based (see [7, 15]). The first category is especially focused on the detection of
interesting sub-trees in the DOM tree. The second category uses DOM only to
a limited extent (or they are not used at all) and in order to ensure the structure
of a page is from the point of view of external appearances. DOM is W3C‚Äôs
Document Object Model is a standard application programming interface to the
structure of documents ‚Äì see [76].
2. On the basis of human participation in the analysis (see [48]) there are super-
vised (wrapper induction, e.g. [3, 10]) and unsupervised (automatic extraction)
methods.
3. On a basis of the fact, if the analysis assumption is a known Web page struc-
ture (see e.g. [88, 90]). These are template-dependent and template-independent
methods.
Currently the trend is evolving towards automatic, template-independent and visual
layout based approaches. That is why in this text only selected approaches in this
field is mentioned.
Chen et al. describe in [12] the function-based Object Model (FOM). This ap-
proach is based on understanding authors‚Äô intentions, not on understanding semantic
Web pages. Every object on a Web page serves certain functions (basic and specific
Functions) which reflect the authors‚Äô intentions towards the purpose of an object.
The FOM model for Web page understanding is based on this consideration. FOM
includes two complementary parts: basic FOM (BFOM) based on the basic func-
tional properties of an object, and specific FOM (SFOM) based on the category of
an object. It is supposed that by combining BFOM and SFOM, a thorough under-
standing of the authors‚Äô intentions of creating a Web page can be determined.
In [81], Yang et al. present an approach of automatic analysis of the semantic
structure of HTML pages based on detecting visual similarities of content objects
on Web pages. Effective content organization is an essential factor of high quality
content services. The content is divided into categories and each holds records of re-
lated subtitles. Records in one category are normally organized in ways which have
a consistent visual layout style. Boundaries between different categories are marked
with a number of visual styles or separators. They define five types of page objects
(simple objects, container objects, group objects, list objects, and structured docu-
ments). They detect these objects and measure their similarity by fuzzy comparison
rules.
Kovacevic et al. present in [41] techniques based on a virtual screen defines a
coordinate system for specifying the position of objects inside Web pages. Authors
use a specific format of the page for rendering the page on the virtual screen. Based
on this representation, they use a number of heuristics for the recognition of the
header, footer, menu, and body of the page.
In the paper [50], Liu et al. describe a method to mine data records in a Web page
automatically. The algorithm is called MDR (Mining Data Records in Web pages).
It finds data records formed by tables and form related tags. A large majority of Web
data records are formed by them. The MDR method is based on two observations.
The first is that a group of data records that contain descriptions of a set of similar
4 Web Content Mining Using MicroGenres 89
objects are typically presented in a particular region of a page and are formatted
using similar HTML tags (sub-tree patterns). The second is that a group of similar
data records placed in a specific region is reflected in the tag tree by the fact that
they are under one parent node (forest-tree pattern). A Web page may have one data
region with data records or a few data regions. Different regions may have different
data records. MDR techniques work in three steps: (1) Building a HTML tag tree of
the page. (2) Mining data regions in the page using the tag tree and string compar-
ison. (3) Identifying data records from each data region. The experiments showed
good results in data region extraction in many domains (books, travel, software,
auctions, jobs, electronic products, shopping, and search engine results). There are
approaches following MDR (e.g. [69, 73]).
An improvement of the MDR method is presented in [85, 86]. It is called Depta
(Data Extraction based on Partial Tree Alignment). Specifically, the Depta method
uses visual cues to find data records. Visual information helps the system in two
ways: (1) It enables the system to identify gaps that separate data records, which
helps to segment data records correctly because the gap within a data record is typ-
ically smaller than that in between data records. (2) The proposed system identifies
data records by analyzing the HTML DOM tree. The visual information is obtained
after the HTML code is rendered by a Web browser; it also contains information
about the hierarchical structure of the tags. In this method, rather than analyzing the
HTML code, visual information is utilized to infer the structural relationship among
tags and to construct a tag tree.
VIPS (VIsion-based Page Segmentation, see Cai et al. [7, 8]) is an algorithm to
extract the content structure for a Web page. The algorithm makes full use of page
layout features and tries to partition the page at the semantic level. Each node in
the extracted tree content structure corresponds to a block of coherent content in
the original page. In the VIPS approach, a web page is understood as a finite set of
objects or sub-web-pages. All of these objects do not overlap. Each object can be
recursively viewed as a sub-web-page and has a subsidiary content structure. A page
contains a finite set of visual separators, including horizontal separators and vertical
separators. Adjacent visual blocks are separated by these separators. For each visual
block, the degree of coherence is defined to measure the extent of its coherence. The
vision-based content structure is more likely to provide a semantic partitioning of
the page. Every node, especially the leaf node, is more likely to convey a semantic
meaning for building a higher semantic via the hierarchy. In the VIPS algorithm,
the vision-based content structure of a page is deduced by combining the DOM
structure and the visual cues. The segmentation process work on DOM structure
and visual information, such as position, background color, font size, font weight,
etc. Some approaches follow VIPS, e.g. [80, 13, 91].
Takama and Mitsuhashi present in [71] a visual similarity comparison problem
and a visual feature model for web pages. The visual feature model should be as
general as possible and based on two principles: (1) Granularity of region segmen-
tation should be adjustable according to applications. (2) Positional relationships
among segmented regions should also be considered. The method of Web page lay-
out analysis is to segment a whole page image into several regions, each of which
90 V. SnaÃÅsÃåel, M. KudeÃålka, and Z. HoraÃÅk
is labeled as either of text, image or a mixture of both. In this method, a page image
is divided into several regions based on edge detection. The processing of layout
analysis can be summarized as follows: (1) Translates page images from RGB to
YCbCr color space. (2) Obtains an edge image from Y image. (3) Obtains an initial
region set by connecting neighboring edges. (4) Obtains the second region set by
merging small regions in the initial set. (5) Labels each region in it as either text or
image. (6) Merges neighboring regions in second region set. If the merged regions
have different content types, the resultant region has a label mixture. Experiments
on visual similarity comparison using this method showed positive results in the
search engine domain and corporate home pages.
A systematic HTML web page segmentation algorithm, specifically designed to
segment online journals, is described by Zou et al. in [93]. Based on the observation
of online journal articles, the geometric layout is the most important cue for se-
mantic organization. The paper introduces a tree structure model, zone tree, which
hierarchically groups the DOM nodes into zones based on the geometric layout of
the web page. Visually, a zone is a region on a web page that contains one or more
DOM nodes. The nodes are divided and then grouped into zones at different levels
depending on their geometric relationships. Similarly to VIPS, the important advan-
tage of the zone tree model is that it is HTML tag independent.
Remarks
1. There is one well known conclusion following the methods of segmentation
of Web page. A web page usually contains various contents such as advertise-
ments, navigation, interaction, etc., which are not fully related to the topic of
the whole Web page. Furthermore, a web page often contains multiple topics
that are not relevant to each other (see [8, 13, 22]). In this case, multi-topic Web
pages are divided into several blocks of homogeneous content.
2. This feature of Web pages can be used in different ways. Based on VIPS, in
[83], there is a method for how to improve a pseudo-relevant feedback in Web
information retrieval.
3. There is a set of approaches about how to find out those Web page parts which
are of a higher importance for the user in the semantic sense (e.g. [29, 41]).
The procedure is usually the following: the Web page is segmented into indi-
vidual parts and the quality of the content is evaluated for each segment. In the
technical sense it can be noise elimination and selection of text content.
The aforementioned methods have in common is that they analyze the page as a
whole and they are primarily aimed at the segmentation of content in a way, to
further explore the semantic content of each segment. From this perspective, all
these methods can be seen as a technical tool to support the methods listed below. A
common feature of the described methods is that the subjects of analysis (detection
and extraction) can be named concisely.
There are always one or more sections of a page that is semantically and struc-
turally independent, and often visually separated from other parts of the Web site.
An example might be a customer review, a forum, news, product features, product
4 Web Content Mining Using MicroGenres 91
catalog, a table, etc. In addition to these examples, the same logic of usage stands
also for other named parts of Web pages, but their analysis is not very significant.
Examples are the menu, advertisements, header, footer, etc. Usually, these objects
are called noise and there are methods that specialize in the removal of this noise
during the analysis process (e.g. Yi et al. [82]).
The following methods are divided into groups according to what is examined
and those that occur most frequently are described. But one can find others like
tourism and accommodation information (see Kiyavitskaya et al. [39]), book details
(see Zhai et al. [85] and Zhang et al. [87]), car technical detils (see [21]), etc.
For the named parts of the page the term MicroGenre is adopted, as will be ex-
plained in chapter 4.5.
Genre Detection
The aim of this group of methods is to assign the Web page to some type. This type
is either known in advance or it arises as a result of method application. The Genre
is the division of concrete forms of art using the criteria relevant to the given form
(e.g. film genre, music genre and literature genre). Genres were first introduced to
the information system field in the early nineties by Yates and Orlikowski (see [57]).
In document context, the Genre is a taxonomy that incorporates the style; form
and content of a document which is orthogonal to topic, with fuzzy classification of
multiple genres (see [5]). In the same paper, existing classifications are described.
As a result, 115 genres are identified e.g. Ads, Calendars, Classifieds, Comparisons,
Contact info, Coupons, Demographics, Documentaries, E-cards, FAQs, Homepages,
Hubs, Lists, News, Newsgroups, Reports, etc.
Regarding these classifications there are many approaches to genre identification
methods. In paper [65], classification problems are analyzed in terms of two broad
textual phenomena: genre hybridism and individualization. The aim of this paper is
to show that web pages need a zero to multi-genre classification scheme in addition
to the traditional single genre classification.
Kennedy and Shepherd [37] analyzes home page genres (personal homepage,
corporate home page or organization home page).
Chaker and Habib [9] propose a flexible approach for Web page genre catego-
rization. Flexibility means that the approach assigns a document to all predefined
genres with different weights.
In the paper [36], Kanaris and Stamatatos propose a low-level representation of
the style of Web pages based on n-grams. Experiments based on two benchmark
genre corpora were presented.
In the paper [61], Rehm analyzes academic Web pages in order to automati-
cally classify them into Web genres. The analysis of a 200 document sample illus-
trates a notion of Web genre hierarchy into which Web genre types and modules are
embedded.
Dong et al. [19] describe a set of experiments to examine the effect of various
attributes of web genres on the automatic identification of the genre of web pages.
92 V. SnaÃÅsÃåel, M. KudeÃålka, and Z. HoraÃÅk
Four different genres are used in the data set (FAQs, News, E-Shopping and Personal
Home Pages).
Rosso [63] explored the use of genre as a document descriptor in order to im-
prove the effectiveness of Web searching. The author of the paper formulates three
hypotheses: (1) Users of the system must possess sufficient knowledge of the genre.
(2) Searchers must be able to relate the genres to their information needs. (3) The
genre must be predictable by a machine applied algorithm because it is not typically
explicitly contained in the document.
KudeÃålka et al. [44] introduce a concept of MicroGenres. Under the notion of Mi-
croGenres, authors understand more or less independent Web page elements, which
have some specific purpose and content. Utilizing the term ‚ÄúMicroGenre‚Äù, the Web
page can be described as a collection of MicroGenres. In previous works, authors
have focused on Web design pattern detection (see [42]).
Table Extraction
Tables are an important element for structuring related data. The extraction of tables
from Web pages appears to be one of the key tasks for further retrieval of structured
data. Differentiation of tables emerges as a common problem for all approaches,
which represent page layout and those which contain structured data. Tables belong
to the field of domain independent objects; however, their extraction can serve well
for the detection of other domain dependent objects.
A survey of different approaches is in [84]. Generally, tables often have asso-
ciated text, including titles, captions, data sources, and footnotes or additional text
that elaborate on cells.
Lerman et al. [46] describe an approach to automatic Web table segmentation and
extraction of records. This approach is based on the common structure of many Web
sites, which present information as lists or tables. Two algorithms are presented that
use redundancies in the content of tables. The first one finds the segmentation by
solving a constraint satisfaction problem. The second one uses probabilistic infer-
ence to find the record segmentation.
In [21], an approach which requires aspects of table understanding is proposed,
but it relies especially on extraction ontology. This approach includes five steps:
(1) Location of table of interest using values specified in ontology. (2) Setting of
attribute-value pairs by individual strings recognized by ontology. (3) Adjustment
of attribute-value pairs. (4) Analysis of extraction patterns by the table layout, links
to sub-tables, and location of text surrounding the tables. (5) Using the recognized
extraction patterns, the system can infer a general mapping from the source to the
target.
An approach describing the transformation of arbitrary tables into explicit seman-
tics is presented in [58, 59]. The Tartar system (Transforming Arbitrary TAbles into
fRames) is based on a grounded cognitive table model. The presented method is in-
dependent of domain knowledge and document types, and enables query answering
over heterogeneous tables. HTML tables are analyzed along four aspects: Physical -
a description in terms of inter-cell relative location. Structural - the topology of cells
4 Web Content Mining Using MicroGenres 93
as an indicator of their navigational relationship. Functional - the purpose of areas
of the tables in terms of data access. Semantic - the meaning of text in the table and
the relationship between the interpretation of cell content, the meaning of structures
in the table, and the meaning of its reading.
In a paper [26], a different approach is presented . Authors use a tree represen-
tation of variation in the two-dimensional visual box model used by web browsers
for domain-independent information extraction from web tables. Authors refer to
the method as VENTex (Visualized Element Nodes Table extraction). The method
works in the following steps: (1) Table location as a task of identifying tables and
their constituent logical cells on web pages. (2) Table recognition as a task of iden-
tifying the spatial relationships between individual logical cells of a table. (3) Table
interpretation as a task of extracting and saving information from tables in a struc-
tured format that preserves the meta-information contained in all visual relationships
between the individual categories.
Kim and Lee [38] present an efficient method for Web page processing which
consists of two phases: area segmentation and structure analysis. The area segmen-
tation cleans up tables and segments them into attribute and value areas by check-
ing visual and semantic coherency. The hierarchical structure between attribute and
value areas is then analyzed and transformed into an XML representation using a
proposed table model.
Opinion Extraction
There are a wide area of methods which aim to summarize opinions of customers on
a product or on its specific features. In individual methods, language analysis (NLP)
is also used. Opinions of customers on product Web pages are the main source for
analysis, but there can also be discussions on thematic forums or individual reviews
in the form of articles (Customer Reviews, Reviews, Discussions, Blogs).
A survey of recent methods of opinion mining is ilustrated in [45]. In this pa-
per, three tasks specific to opinion mining are analyzed: development of linguistic
resources, sentiment classification, and opinion summarization.
Another survey is introduced in [14]. The survey is focused on legal blogs (a.k.a.
Blawgs) which is any given Weblog that focuses on substantive discussions of the
law, the legal profession, including law schools, and the process by which judicial
decisions are made. The top-level taxonomy presented shows a variety of topics for
the blogging sub-community (General legal blogs, Blogs categorized by legal spe-
cialty, Blogs categorized by law or legal event, Blogs categorized by jurisdictional
scope, Blogs categorized by author/publisher, Blogs categorized by number of con-
tributors, Miscellaneous blogs categorized by topic, Collections of legal blogs).
In [33], Hu and Liu propose a study of a problem of feature-based opinion sum-
marization of customer reviews of products sold online. The approach has two steps:
(1) Identify the features of the product on which customers have expressed opinions
and rank the features according to their frequencies. (2) For each feature positive or
negative opinions in customer reviews are identified.
94 V. SnaÃÅsÃåel, M. KudeÃålka, and Z. HoraÃÅk
A similar approach is described in [51]. Liu et al. focus on online customer re-
views and makes two contributions: (1) It proposes a framework for analyzing and
comparing consumer opinions of products. (2) A prototype system called Opin-
ion Observer is presented. Experimental results show that the technique is highly
effective.
Another similar approach is in [60]. Popescu and Etzioni introduce OPINE, a
review-mining system whose components include the use of relaxation labeling to
find the semantic orientation of words in the context of given product features. The
problem of review mining is decomposed into four main sub-tasks in the Opine sys-
tem: (1) Identify product features. (2) Identify opinions regarding product features.
(3) Determine the polarity of opinions. (4) Rank opinions based on their strength.
Ding et al. [18] propose a holistic lexicon-based approach to solving the prob-
lem by exploiting external evidences and linguistic conventions of natural language
expressions. This approach allows the handling oh opinion words that are context
dependent, which cause major difficulties for existing algorithms.
News Extraction
News extraction methods deal with the extraction of articles from news Web pages.
From the method point of view it is a special case of methods from the field of
text content extraction. One of the aims of these methods can be to find duplicities
published on different Web sites.
Reis et al. [62] present an approach based on the concept of tree-edit distance and
allows not only the extraction of relevant text from the pages of a given Web site,
but also the fetching of the entire Web site content and the identification of the pages
of interest. An algorithm is presented for determining the tree edit distance between
trees representing Web pages. In order to extract the news, this approach recognizes
common characteristics that are usually present in news portals (a home page that
presents some headlines; several sections of pages that provide the headlines divided
into areas of interest; and pages that actually present the news, containing the title,
author, date and body of the news). The goal of this method is to correctly extract
the news, and disregard the other pages.
In paper [88], there is a template-independent news extraction approach that sim-
ulates human beings. The approach is based on a stable visual consistency among
news pages across Web sites. The presented method is as following: (1) Representa-
tion of a DOM node with a block and basic visual features to eliminate the diversity
of tags. (2) Extended visual features are calculated as relative visual features to the
parent block. (3) Two classifiers are combined for a vision based wrapper for ex-
traction.
Han et al. [31] propose an effective and efficient algorithm to extract the news
article contents from the news pages without the analysis of news sites before ex-
traction. They calculate the relevance between the news title and each sentence in
the news page to detect the news article contents.
4 Web Content Mining Using MicroGenres 95
Discussion Extraction
Discussion as an object can be a good source for Opinion extraction. Schuth et
al. [68] describe techniques to collect, store, enrich and analyze comments on arti-
cles. An extraction method is based on similarity of comments and their recurring
features (name, date and time, e-mail, etc.).
A similar approach is in [47]. Limanto et al. present a system based on generated
wrappers.
Zhu et al. [92] propose a topic detection and tracking method for the discussion
threads. They design a framework, focusing on the very nature of discussion data,
including a thread/post activity validation step, a term pos-weighting strategy, and
a two-level decision framework considering not only the content similarity but also
the user activity information.
Product Details Extraction
Product details are specific objects on product Web pages. It is a basic characteristic,
which usually contains a picture, product name, price information, etc. The aim of
the methods is to extract as much information as possible. This information can be
saved onto a database and then one can implement operations within this database.
An extraction of information on product details is a typical task in supervised or
semi-supervised approaches.
The goal of Nie et al. [53, 54] is to explore a paradigm to enable web search at the
object level, extracting and integrating web information for objects relevant to a spe-
cific application domain. Authors introduce the overview of an object-level vertical
search engine ‚ÄúWindows Live Product Search‚Äù. By empirically studying Web pages
about the same type of objects across different Web sites, authors have found many
template-independent features. Therefore, they proposed template-independent
meta-data extraction techniques for the product objects (name, image, price, and
description). The purpose of this research is to provide intelligent search results
to the user. The same papers introduce an academic search engine Libra (the source
can be described as MicroGenre Author and publications) based on the same
idea.
A similar approach on extracting author meta-data is proposed in [89]. Zheng et
al. attempts to extract author meta-data from their home-pages. They have chosen
homepage domains as the source as it is more reliable, comprehensive and more up
to date than any other sources. This approach takes advantage of visual features.
Based on their observation, visual features are more stable than content-based fea-
tures for certain types of information which an author puts on the homepage. This
is understandable because when people design their homepages, they usually will
follow some hidden conventions (in our meaning web usability principles or/and
Web design patterns).
96 V. SnaÃÅsÃåel, M. KudeÃålka, and Z. HoraÃÅk
Technical Features Extraction
On product or review pages there is usually a list of the technical parameters of the
product. Methods of Product technical features mining deal with the extraction of
these parameters for selected products and the aim is mainly to be able to compare
similar products.
Schmidt et al. [66, 67] present a symbolic approach to extract domain-specific
technical features of products from large German corpora. The proposed methods
depend on manually added lists of technical measures. The method is a composition
of three steps: (1) Extracting product-features for the selected product-class to gen-
erate a template for concrete offers. (2) Extracting concrete offers and filling this
template, (3) Mapping the selected application to the constraints of technical fea-
tures and suggesting a set of adequate products. The experiment is presented on a
domain of camera technical features.
Wong and Lam describe a similar approach in [79] (experiments on camera and
MP3 player features). The approach is a two-phase framework for mining and sum-
marizing hot items in multiple auction Web sites. The objective of the first phase is
to extract the product features and product feature values of these items.
4.5 MicroGenre
In all sectors of the arts, the Genres are vague categories without fixed boundaries
and are especially formed by sets of conventions. Many artworks are cross-genre
and employ and combine these conventions. Probably the most deeply theoretically
studied Genres are the literary ones. It allows us to systematize the world of liter-
ature and consider it as a subject of scientific examination. One can find the term
MicroGenre in this field. For example, in [52] the MicroGenre is seen as part of
a combined text. This term has been introduced to identify the contribution of in-
serted Genres to the overall organization of text. The motivation of using the term
‚ÄúMicroGenre‚Äù is because it is used as a building block for the analytic descriptive
system. On the Web, MicroGenre can be used more technically and can provide a
means for an effective solution for Web pages. From our point of view the Web page
is structured similarly to a literacy text using parts which are relatively independent
and have their own purpose (see fig. 4.3). For these parts the term ‚ÄúMicroGenre‚Äù
was chosen.
Definition 4.2. (Web) MicroGenre is a part of a Web page,
1. whose purpose is general and repeats itself frequently
2. which can be named intelligibly and more or less unambiguously so that the
name is understandable for the Web page user (developer, designer, etc.)
3. which is detectable on a Web page using a computer algorithm
The MicroGenre can, but does not have to, strictly relate to the structure of a Web
page in a technical sense. E.g. it does not necessarily have to be represented by one
subtree in the DOM tree of the page or by one block in the sense of the visual layout
4 Web Content Mining Using MicroGenres 97
Fig. 4.3 Structure of Web page. Contemporary Web pages are often very complex, neverthe-
less they can usually be described using several MicroGenres. This kind of description can
be more flexible than the Genre description (which usually represents the whole page)
of the page. Instead it can be represented by one or more segments of a page, which
unite it.
Remark
MicroGenres are also contexts which encapsulate related information. In paper [43]
we show the way in which we extract snippets from individual MicroGenres. We
use these snippets in our Web application as additional information for Web page
description. The detection of MicroGenres can be considered in a similar way as to
a first step in using Web information extraction methods (see [11]).
4.5.1 Pattrio Method
It follows the previous description that in order to be able to speak about the Mi-
croGenre, this element has to be distinguishable to the user. From what attributes
should the user recognize, if and what MicroGenre there is or is in question? We
work with up to three levels of view:
1. The first view is purely semantic in the sense of the textual content of a page.
It does not always have to be a meaning in a sense of natural language such
98 V. SnaÃÅsÃåel, M. KudeÃålka, and Z. HoraÃÅk
as sentences or paragraphs with a meaningful content. Logically coherent data
blocks can still lack grammar (see [91]).
For example, Price information can only be a group of words and symbols
(‚Äôprice‚Äô, ‚Äôvat‚Äô, symbol $) of a data-type (price, number). For a similar approach
see [64].
2. The second view is visual in the sense of page perception as a whole. Here
individual segments of perception or groups of segments of the page are in
question. It is dependent on the use of colors, font and auxiliary elements (lines,
horizontal and vertical gaps between the segments etc.) Approaches based on
visual analysis of Web pages can be found in [7], [71].
3. The third view is a structural one in a technical sense. It is about the use of
special structures, such as tables, links, navigation trees, etc. There are ap-
proaches based on analysis of the DOM tree and special structures such as
tables [50], [59].
The first view is dependent on the user‚Äôs understanding of the text stated on a Web
page. The second and third views are independent of this user ability. However, it
can be expected that an Arabic or Chinese product page will be recognized also by
an English-speaking user who does not have a command of those languages. It is
determined by the fact that for the implementation of certain intentions there are
habitual procedures which provide very similar results regardless of the language.
On the other hand, if the user understands the page, he/she can focus more on the
semantic content of the MicroGenre. For example, in the case of Product main info,
the user can read what the product is in question, its price and on what conditions it
can be purchased.
Remark
In our previous work, we have focused on Web design pattern detection. The re-
search findings illustrated that it is useful to generalize the detection algorithm on
all objects that provide the benefits to the users. These objects can be out of the
scope to what is useful to the Web developers, but can be helpful in a similar way
to that of Genres. Therefore we have chosen the term ‚ÄúMicroGenre‚Äù for our further
research.
4.5.2 Analysis
In our approach, there are elements with semantic contents (words or simple phrases
and data types) and elements with importance for the structure of the web page
where the MicroGenre instance can be found (technical elements). The rules are in
the way that individual elements take part in the MicroGenre display. While defin-
ing these rules, we were inspired by the Gestalt principles. We formulated four rules
based on these principles. The first (proximity) defines the acceptable measurable
distances of individual elements from each other. The second (closure) defines the
way of creating independent closed segments containing elements. One or more
4 Web Content Mining Using MicroGenres 99
segments then create the MicroGenre instance on the web page. The third (similar-
ity) implies suggests that the MicroGenre includes more related segments that are
similar. The fourth (continuity) establishes that the MicroGenre contains more var-
ious segments that together create the Web pattern instance. The relations among
MicroGenres can be on various levels similar to the patterns in pattern languages
(especially association and aggregation).
Remark
Other objects in the Web page analysis can be used. E.g. Dujovne and VelaÃÅsquez
[20] introduce a methodology for discovering Web page key objects based in both
Web usage and content mining. Key objects could be any text, image or video
present in a web page, that are the most appealing objects to users.
Lightweight Formalization
For the purpose of the proposal of the detection algorithm, a lightweight formaliza-
tion was used. This formalization is a simple one and it is only a starting point for
the description of a MicroGenre.
Let us have a MicroGenre MG. Then let us mark the set of MicroGenres words
by W , the set of data types by D and the set of technical (HTML) elements by T . Let
us mark the set of MicroGenre rules by Ru. Let us mark the set of relations of the
MicroGenre MG to other MicroGenres by Re. So, the MicroGenre MG is defined as
MG = (W,D,T,Ru,Re). It is given that x is the Web page containing the instance of
this MicroGenre. Then the instance I(x) of the MicroGenre MG on the Web page x
is
I(x) = (W (x),D(x),T (x),Ru(x),Re(x)),
where W (x) ‚äÇ W , D(x) ‚äÇ D, T (x) ‚äÇ T , Ru(x) ‚äÇ Ru, Re(x) ‚äÇ Re, while these ele-
ments fulfill the rules Ru of the MicroGenre N.
Example (Discussion)
This formalization can be seen in a simplified example.
Words: {Main: re, reply, discussion, forum, author, question, answer, thread con-
tribution,, subject, sent; Complementary: date, name, post, topic}
Data types: {Main: date, time, first name; Complementary:}
Technical elements: {link, label, input}
Rules: {proximity: 16; closure: normal; similarity: high; continuity: low}
Associations: {Contains: Short Paragraphs; Uses: Date per Paragraph; Comple-
ments: Review and Comments}
The basic algorithm for the detection of MicroGenres then implements the pre-
processing of the code of the HTML page (only selected elements are preserved
‚Äì e.g. block elements such as tables, div, lines, etc., see Table 4.2), segmentation
and evaluation of rules and associations. The result for the page is the score of
100 V. SnaÃÅsÃåel, M. KudeÃålka, and Z. HoraÃÅk
Table 4.2 HTML tags - classification for analysis
Types Tags
Headings H1, H2, H3, H4, H5, H6
Text containers P, PRE, BLOCKQUOTE, ADDRESS
Lists UL, OL, LI, DL, DIR, MENU
Blocks DIV, CENTER, FORM, HR, TABLE, BR
Tables TR, TD, TH, DD, DT
Markups A, IMG
Forms LABEL, INPUT, OPTION
Fig. 4.4 MicroGenre detection process
MicroGenres that is present on the page. The score then says what the probability is
of expecting the MicroGenre instance on the page for the user. The entire process,
including MicroGenre detection, is displayed in Figure 4.4.
4.6 Experiments
10 MicroGenres was used for our experiments in this chapter (Price information,
Purchase possibility, Special offers, Hire sale, Technical features, Discussion, Re-
view, Second hand, Login, and Inquiry). The names of MicroGenres emerged from
the discussions between us and students that took part in our experiments. For an-
other experiment, especially in our research concerning social aspects of Web page
contents, more than 20 MicroGenres (see [44]) were prepared.
4 Web Content Mining Using MicroGenres 101
4.6.1 Accuracy of Pattrio Method
The aim of the experiment was to find out what information could be found using
this method on the web pages returned from the Google search engine. The Pattrio
method evaluated the relevance of each returned web page with respect to the query
and the user‚Äôs expectation. For this experiment three products were selected (we
typed the product id and keywords into the search engine) - Apple iPod Nano 1GB,
Canon EOS 20D, Star Wars Trilogy film.
The emphasis was placed on querying products that are common and are sold
in high quantities. One of the most requested queries is that of product information
which discusses the product and issues around purchasing it. Therefore our experi-
ment focused on two MicroGenres, the ‚ÄúDiscussion‚Äù and the ‚ÄúPurchasing possibil-
ity‚Äù. For each MicroGenre a simple query was guilt that consisted of the product
id and a keyword for the MicroGenre (Discussion and Purchase possibility). Six
groups of pages were provided after querying Google. From each result set only the
first 100 pages were used. As users pages with discussions and purchasing possi-
bilities were expected. First the pages for experiments were manually checked and
those that did not fit our requirements were removed - pages with broken links and
documents, were not evaluated (pdf, doc, xml). The remainder of the pages were
evaluated using a three-degree scale:
+ Page contains required object.
- Page does not contain required object.
? Unable to evaluate results.
The manual evaluation is not always precise. The reason is that these MicroGenres
do not consist of a precise formalization and are only a description of a common
experience of the user and the web designer. Therefore each page was evaluated
with a group of three students. Each student had to classify each page. If one of
them had a different mark for a page than the others the document were classified
as unable to evaluate the results (?). The results are shown in Figure 4.5.
Fig. 4.5 Manually evaluated pages
102 V. SnaÃÅsÃåel, M. KudeÃålka, and Z. HoraÃÅk
Then the sets of pages were processed again but through our method, which com-
putes the score of the MicroGenre presence on the page. Let the score (thresholds
are set up according to the results of our experiments) of MicroGenre be V then:
‚Ä¢ if V ‚â• 23 then MicroGenre is found (+)
‚Ä¢ if V ‚â§ 14 then MicroGenre is not found (-)
‚Ä¢ if 14 ‚â§V ‚â§ 23 then it is not possible to decide (?)
The results are shown in Figure 4.6. There are values in each column, whose manual
evaluation corresponds to the evaluation made by the Pattrio method.
Fig. 4.6 Evaluation by Pattrio method
Experiment Evaluation
For the evaluation of the accuracy of our method a formula was used which works
with the count of manual and automatically evaluated pages. The formula for com-
putation of retrieval accuracy (see [70]) was also used.
RA =
relevant pagesretrieved intopT returns
T
where T is the count of found pages.
In this experiment T was the count of pages in one set (returned by a search
engine) with the same value (e.g. pages with Canon EOS 20D with discussion and
value +). Only those pages which had the same value for the manual and for the
automatic evaluation were considered relevant.
Let Svi be the i-th set of pages manually evaluated with value v, where v ‚àà
{+,?,‚àí}. Let SMvi be a subset of Svi for which the manual evaluation and the eval-
uation using the Pattrio method have the same value. The overall method accuracy
for one particular set of pages can be computed according to the formula:
m(Svi ) = m
v
i =
|SMvi |‚à£‚à£Svi ‚à£‚à£
4 Web Content Mining Using MicroGenres 103
In Figure 4.7 the method accuracy is in percentage form for each set of pages evalu-
ated in our experiment (we worked with 6 groups of pages and each with 3 evaluated
sets).
Fig. 4.7 Method accuracy in percentage
For example the first value 61% expresses the method accuracy for the pages with
Canon EOS 20D product in which there was a discussion. The value was computed
according to the formula
m(S+1 ) = m
+
1 =
‚à£‚à£SM+1 ‚à£‚à£‚à£‚à£S+1 ‚à£‚à£ =
14
23
.= 0.609
where values 14 and 23 are from tables in Figures 4.5 and 4.6. The method accuracy
is between 13 and 100 percent.
For the overall evaluation of method accuracy in our experiment formula com-
puting was used with counts of manual evaluated pages for each set. The method
accuracy for the selected sets of pages is computed according to the formula:
M =
‚àë
[i,v]
|Svi |mvi
‚àë
[i,v]
‚à£‚à£Svi ‚à£‚à£
where mvi is the evaluation of one concrete set. In Table 4.3 there are result values
for the pages with discussions, purchase possibility and overall method accuracy
noted in percentages.
For example, the value 96% expresses the method accuracy in the situation when
the web page does not consist of a discussion. The value is computed according to
the formula:
M =
n‚àí1 m
‚àí
1 + n
‚àí
2 m
‚àí
2 + n
‚àí
3 m
‚àí
3
n‚àí1 + n
‚àí
2 + n
‚àí
3
.=
57 ¬∑1 + 70 ¬∑0.93+71 ¬∑0.96
57 + 70 + 71
.= 0.96
where each value is taken from the tables in Figures 4.5 and 4.7.
104 V. SnaÃÅsÃåel, M. KudeÃålka, and Z. HoraÃÅk
Table 4.3 Method accuracy
Microgenre + ? ‚Äì Total
Discussion 61% 50% 96% 83%
Purchase 84% 52% 84% 81%
Total 75% 51% 91% 82%
For this experiment a total of 572 web pages were used and which were manually
computed and evaluated. The manual evaluation was compared to the Pattrio method
evaluation. In the manual evaluation 169 pages were considered suitable (containing
MicroGenre), 348 were unsuitable (not containing MicroGenre) and 55 of which
were deemed hard to classify.
The method accuracy for Discussion is 83% and for Purchase possibility 81%.
Pages containing objects the method accuracy scored 75% and for pages that do not
contain objects 84%. Regarding the situations deemed hard to decide the method
accuracy is 52% and the overall method accuracy 82%.
4.6.2 Analysis by Nonnegative Matrix Factorization
The nonnegative matrix factorization (NMF) method for text mining is a technique
for clustering that identifies semantic features in a document collection and groups
the documents into clusters on the basis of shared semantic features [17]. A col-
lection of documents can be represented as a term-by-document matrix. Since each
vector component is given a positive value if the corresponding term is present in
the document and a zero value otherwise, the resulting term-by-document matrix
is always nonnegative. This data non-negativity is preserved by the NMF method
as a result of constraints that produce nonnegative lower rank factors that can be
interpreted as semantic features or patterns in the text collection.
NMF Method
With the standard vector space model a set of documents S can be expressed as an
m√ón matrix V , where m is the number of terms and n is the number of documents
in S. Each column Vj of V is an encoding of a document in S and each entry vi j of
vector Vj is the value of the i-th term with regard to the semantics of Vj, where i
ranges across the terms in the dictionary. The NMF problem is defined as finding
an approximation of V in terms of some metric (e.g., the norm) by factoring V into
the product WH of two reduced-dimensional matrices W and H [4]. Each column
of W is a basis vector. It contains an encoding of a semantic space or concept from
V and each column of H contains an encoding of the linear combination of the basis
vectors that approximates the corresponding column of V . Dimensions of W and H
are m√ó k and k√ó n, where k is the reduced rank. Usually k is chosen to be much
4 Web Content Mining Using MicroGenres 105
smaller than n. Finding the appropriate value of k depends on the application and is
also influenced by the nature of the collection itself.
Common approaches to NMF obtain an approximation of V by computing a
(W,H) pair to minimize the Frobenius norm of the difference V ‚àíWH. The ma-
trices W and H are not unique. Usually H is initialized to zero and W to a randomly
generated matrix where each Wi j > 0. These initial values are improved with itera-
tions of the algorithm.
GD-CLS Method
GD-CLS is a hybrid method that combines some of the better features of other meth-
ods. The multiplicative method, which is basically a version of the gradient descent
optimization scheme, is used at each iterative step to approximate the basis vector
matrix W. H, which is calculated using a constrained least squares (constrained least
squares - CLS) model as the metric.
Algorithm
1. Initialize W and H with nonnegative values, and scale the columns of W to unit
norm.
2. Iterate until convergence or after l iterations:
‚Ä¢ Wic = Wic
(VHT )ic
(WHHT )ic+Œµ
, for c and i [Œµ = 10‚àí9]
‚Ä¢ Rescale the columns of W to unit norm
‚Ä¢ Solve the constrained least squares problem where minHj{||Vj ‚àíWHj||22 +
Œª ||Hj||22 the subscript j denotes the j-th column, for j = 1, . . . ,m. Any neg-
ative values in Hj are set to zero. The parameter k is a regularization value
that is used to balance the reduction of the metric ||Vj ‚àíWHj||22 with the
enforcement of smoothness and sparsity in H.
For any given matrix V , matrix W has k columns or basis vectors that represent k
clusters, matrix H has n columns that represent n documents. A column vector in H
has k components, each of which denotes the contribution of the corresponding basis
vector to that column or document. The clustering of documents is then performed
based on the index of the highest value of k for each document. For document i
(i = 1, . . . ,n), if the maximum value is the j-th entry ( j = 1, . . . ,k), document i
is assigned to cluster j. We used the GD-CLS method for searching k = 2,3,4,5
clusters. Results are in Table 4.4.
One can see that in the table the results are readable. For example in the first row
is a vector which represents sale - price information, purchase possibility, special
offer, hire sale, technical features, and login. In the second row is a vector which de-
scribes information cluster - review, discussion, login, and inquiry. We set the limit
for a successful MicroGenre to 0.05. The method was unstable for 6 or more clus-
ters. In the table there are clusters-vectors with only one higher value (for example
row 5). These clusters do not have a positive information value because one expects
at least two MicroGenre on each page.
106 V. SnaÃÅsÃåel, M. KudeÃålka, and Z. HoraÃÅk
Table 4.4 Clustering by NMF
clusters price info purchase special offer hire sale features review discussion login second hand inquiry
1 / 2 0 0.01 0 0.01 0.03 0.29 0.27 0.337 0 0.06
2 / 2 0.33 0.25 0.19 0.06 0.07 0 0 0.05 0.03 0.01
1 / 3 0.35 0.26 0.21 0.07 0.07 0 0 0 0.04 0.01
2 / 3 0.02 0 0 0.01 0.01 0.41 0.46 0 0.02 0.06
3 / 3 0 0.08 0 0 0.08 0.04 0 0.76 0 0.04
1 / 4 0.42 0 0.41 0.04 0 0 0 0 0.13 0.01
2 / 4 0.01 0 0 0.02 0.02 0.42 0.47 0 0.01 0.06
3 / 4 0 0 0.01 0 0.05 0.04 0 0.85 0 0.05
4 / 4 0.26 0.49 0 0.09 0.15 0.01 0 0 0 0
1 / 5 0 0 0 0.02 0.02 0.42 0.47 0 0 0.06
2 / 5 0.25 0.50 0 0.09 0.16 0 0 0 0 0
3 / 5 0.31 0.01 0.57 0.06 0 0 0 0 0.04 0.01
4 / 5 0.62 0 0 0 0 0 0 0 0.36 0.02
5 / 5 0.01 0 0 0 0.04 0.04 0 0.85 0 0.05
4.7 Summary
In this chapter were discussed the development of methods from the field of Web
content mining, especially regarding their possible use for the search queries and
comprehensible description of the Web pages. These methods have this in com-
mon: each analyzed part of the Web page (object) can be named concisely. The
term MicroGenre was adopted for these objects. From an abstract point of view Mi-
croGenres can be seen as information granules, which simplify the appearence of
Web pages. Using this approach different methods can be applied from the field of
granular computing. Our experiments show that MicroGenres can be detected suc-
cessfully and in a relatively simple manner. After that, the page can be represented
in such a form that the standard mathematical methods can be applied. This has
been highlighted in the experiment applying a variant of the NMF method on an
extensive set of Web pages from a selling domain.
Our experiments have also shown that one of the qualities of MicroGenres is their
small language and cultural dependence. Our current research leads to the usage of
MicroGenres for cross-language searching.
References
1. Alexander, C.: A Pattern Language: Towns, Buildings, Construction. Oxford University
Press, New York (1977)
2. Alexander, C.: The Timeless Way of Building. Oxford University Press, Oxford (1979)
3. Baumgartner, R., Flesca, S., Gottlob, G.: Visual Web Information Extraction with Lixto.
In: Proc. of the 27th Int. Conference on Very Large Data Bases, pp. 119‚Äì128 (2001)
4. Shahnaz, F., Berry, M.W., Pauca, P.V., Plemmons, R.J.: Document clustering using non-
negative matrix factorization. Information Processing and Management 42(2), 373‚Äì386
(2006)
4 Web Content Mining Using MicroGenres 107
5. Boese, E.S., Howe, A.E.: Effects of web document evolution on genre classification. In:
Proceedings of the 14th ACM international Conference on information and Knowledge
Management, CIKM 2005, Bremen, Germany, October 31 - November 05, 2005, pp.
632‚Äì639. ACM, New York (2005)
6. Borchers, J.O.: A pattern approach to interaction design. AI & Society 15(4), 359‚Äì376
(2001)
7. Cai, D., Yu, S., Wen, J.-R., Ma, W.-Y.: Extracting Content Structure for Web Pages based
on Visual Representation. In: Zhou, X., Zhang, Y., Orlowska, M.E. (eds.) APWeb 2003.
LNCS, vol. 2642, pp. 406‚Äì417. Springer, Heidelberg (2003)
8. Cai, D., Yu, S., Wen, J.-R., Ma, W.-Y.: VIPS: a visionbased page segmentation algorithm,
Microsoft Technical Re-port, MSR-TR-2003-79 (2003)
9. Chaker, J., Ounelli, H.: Genre Categorization of Web Pages. In: ICDM Workshops 2007,
pp. 455‚Äì464 (2007)
10. Chang, C., Lui, S.: IEPAD: information extraction based on pattern discovery. In: Pro-
ceedings of the 10th international Conference on World Wide Web, WWW 2001, Hong
Kong, May 01-05, 2001, pp. 681‚Äì688. ACM, New York (2001)
11. Chang, C.H., Kayed, M., Girgis, M.R., Shaalan, K.F.: A Survey of Web Information
Extraction Systems. IEEE Transactions on Knowledge and Data Engineering 18(10),
1411‚Äì1428 (2006)
12. Chen, J., Zhou, B., Shi, J., Zhang, H., Fengwu, Q.: Function-based object model towards
Website adaptation. In: Proceedings of the 10th international conference on World Wide
Web, Hong Kong, pp. 587‚Äì596 (2001)
13. Chibane, I., Doan, B.L.: A web page topic segmentation algorithm based on visual crite-
ria and content layout. In: SIGIR 2007, pp. 817‚Äì818 (2007)
14. Conrad, J.G., Schilder, F.: Opinion mining in legal blogs. In: Proceedings of the 11th
international Conference on Artificial intelligence and Law, ICAIL 2007, Stanford, Cal-
ifornia, June 04-08, 2007, pp. 231‚Äì236. ACM, New York (2007)
15. Cosulschi, M., Constantinescu, N., Gabroveanu, M.: Classifcation and comparison of
information structures from a web page. The Annals of the University of Craiova 31,
109‚Äì121 (2004)
16. Dearden, A., Finlay, J.: Pattern Languages in HCI: A critical review. Human Computer
Interaction 21(1), 49‚Äì102 (2006)
17. Ding, C., Li, T., Peng, W., Park, H.: Orthogonal nonnegative matrix t-factorizations for
clustering. In: ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining (KDD 2006), pp. 126‚Äì135. ACM Press, New York (2006)
18. Ding, X., Liu, B., Yu, P.S.: A Holistic Lexicon-Based Approach to Opinion Mining. In:
Web Search and Web Data Mining, Palo Alto, California, USA, pp. 231‚Äì240 (2008)
19. Dong, L., Watters, C.R., Duffy, J., Shepherd, M.: An Examination of Genre Attributes
for Web Page Classification. In: HICSS 2008, p. 133 (2008)
20. Dujovne, L.E., VelaÃÅsquez, J.D.: Design and Implementation of a Methodology for Identi-
fying Website Keyobjects. In: VelaÃÅsquez, J.D., Rƒ±ÃÅos, S.A., Howlett, R.J., Jain, L.C. (eds.)
KES 2009. LNCS, vol. 5711, pp. 301‚Äì308. Springer, Heidelberg (2009)
21. Embley, D.E., Tao, C., Liddle, S.W.: Automating the extraction of data from HTML
tables with unknown structure. Data Knowl. Eng. 54(1), 3‚Äì28 (2005)
22. Fernandes, D., de Moura, E.S., Ribeiro-Neto, B.: Computing Block Importance for
Searching on Web Sites. In: ACM International Conference on Information and Knowl-
edge Management, Lisboa, Portugal, pp. 165‚Äì173 (2007)
23. Flieder, K., MoÃàdritscher, F.: Foundations of a pattern language based on Gestalt princi-
ples. In: CHI 2006 Extended Abstracts on Human Factors in Computing Systems, Mon-
treal, Quebec, Canada, April 22 - 27, pp. 773‚Äì778. ACM, New York (2006)
108 V. SnaÃÅsÃåel, M. KudeÃålka, and Z. HoraÃÅk
24. Gagneux, A., Eglin, V., Emptoz, H.: Quality Approach of Web Documents by an Evalu-
ation of Structure Relevance. In: Proceedings of WDA 2001, pp. 11‚Äì14 (2001)
25. Gamma, E., Helm, R., Johnson, R., Vlissides, J.: Design Patterns Elements of Reusable
Object-Oriented Software. Addison-Wesley, Reading (1995)
26. Gatterbauer, W., Bohunsky, P., Herzog, M., KruÃàpl, B., Pollak, B.: Towards domain-
independent information extraction from web tables. In: Proceedings of the 16th inter-
national Conference on World Wide Web, WWW 2007, Banff, Alberta, Canada, May
08‚Äì12, pp. 71‚Äì80. ACM, New York (2007)
27. Goldberg, J.H., Stimson, M.J., Lewenstein, M., Scott, N., Wichansky, A.M.: Eye tracking
in web search tasks: design implications. In: Proceedings of the 2002 Symposium on Eye
Tracking Research & Applications, ETRA 2002, New Orleans, Louisiana, March 25-27,
pp. 51‚Äì58. ACM, New York (2002)
28. Graham, L.: A pattern language for web usability. Addison-Wesley, Reading (2003)
29. Gupta, S., Kaiser, G., Neistadt, D.,, Grimm, P.: DOM-based Content Extraction of HTML
Documents. In: World Wide Web conference (WWW 2003), Budapest, Hungary, pp.
207‚Äì214 (2003)
30. Han, J., Kamber, M.: Data mining: concepts and techniques. Morgan Kaufmann Publish-
ers Inc., San Francisco (2000)
31. Han, H., Noro, T., Tokuda, T.: An Automatic Web News Article Contents Extraction
System Based on RSS Feeds. Journal of Web Engineering 8(3), 268‚Äì284 (2009)
32. Han, J., Chang, K.: Data Mining for Web Intelligence. Computer 35(11), 64‚Äì70 (2002)
33. Hu, M., Liu, B.: Mining and summarizing customer reviews. In: Proceedings of the Tenth
ACM SIGKDD international Conference on Knowledge Discovery and Data Mining,
KDD 2004, Seattle, WA, USA, August 22-25, pp. 168‚Äì177. ACM, New York (2004)
34. Ivory, M.Y., Megraw, R.: Evolution of Web Site Design Patterns. ACM Transactions on
Information Systems 23(4), 463‚Äì497 (2005)
35. Ivory, M.Y., Sinha, R.R., Hearst, M.A.: Empirically validated web page design metrics.
In: Proceedings of the SIGCHI conference on Human factors in computing systems,
Seattle, Washington, United States, March 2001, pp. 53‚Äì60 (2001)
36. Kanaris, I., Stamatatos, E.: Webpage Genre Identification Using Variable-Length Char-
acter n-Grams Tools with Artificial Intelligence, 2007. In: ICTAI 2007, pp. 3‚Äì10 (2007)
37. Kennedy, A., Shepherd, M.: Automatic identification of home pages on the web. In:
Annual Hawaii International Conference on System Sciences (HICSS 2005), pp. 236‚Äì
251 (2005)
38. Kim, Y.S., Lee, K.H.: Extracting logical structures from HTML tables. Computer Stan-
dards & Interfaces 30(5), 296‚Äì308 (2008)
39. Kiyavitskaya, N., Zeni, N., Mich, L., Cordy, J.R., Mylopoulos, J.: Text mining through
semi automatic semantic annotation. In: Reimer, U., Karagiannis, D. (eds.) PAKM 2006.
LNCS (LNAI), vol. 4333, pp. 143‚Äì154. Springer, Heidelberg (2006)
40. Kosala, K., Blockeel, H.: Web Mining Research: A Survey. SIGKDD Explorations 2(1),
1‚Äì15 (2000)
41. Kovacevic, M., Diligenti, M., Gori, M., Milutinovic, V.: Recognition of Common Areas
in a Web Page Using Visual Information: a possible application in a page classifica-
tion. In: Proceedings of the 2002 IEEE International Conference on Data Mining (ICDM
2002), p. 250 (2002)
42. KudeÃålka, M., SnaÃÅsÃåel, V., LehecÃåka, O., El-Qawasmeh, E.: Semantic Analysis of Web
Pages Using Web Patterns. In: Web Intelligence 2006, pp. 329‚Äì333 (2006)
4 Web Content Mining Using MicroGenres 109
43. KudeÃålka, M., SnaÃÅsÃåel, V., LehecÃåka, O., El-Qawasmeh, E., PokornyÃÅ, J.: Web Pages Re-
ordering and Clustering Based on Web Patterns. In: Geffert, V., KarhumaÃàki, J., Bertoni,
A., Preneel, B., NaÃÅvrat, P., BielikovaÃÅ, M. (eds.) SOFSEM 2008. LNCS, vol. 4910, pp.
731‚Äì742. Springer, Heidelberg (2008)
44. Kudelka, M., Snasel, V., Horak, Z., Abraham, A.: Social Aspects of Web Page Contents.
In: IEEE CASoN 2009, Fontainebleau, France, pp. 80‚Äì87 (2009)
45. Lee, D., Jeong, O., Lee, S.: Opinion mining of customer feedback data on the web. In:
Proceedings of the 2nd international Conference on Ubiquitous information Manage-
ment and Communication, ICUIMC 2008, Suwon, Korea, January 31 - February 01, pp.
230‚Äì235. ACM, New York (2008)
46. Lerman, K., Getoor, L., Minton, S., Knoblock, C.: Using the structure of Web sites for
automatic segmentation of tables. In: Proceedings of the 2004 ACM SIGMOD interna-
tional Conference on Management of Data, SIGMOD 2004, Paris, France, June 13-18,
pp. 119‚Äì130. ACM, New York (2004)
47. Limanto, H.Y., Giang, N.N., Trung, V.T., Zhang, J., He, Q., Huy, N.Q.: An information
extraction engine for web discussion forums. In: Special interest Tracks and Posters of
the 14th international Conference on World Wide Web, WWW 2005, Chiba, Japan, May
10-14, pp. 978‚Äì979. ACM, New York (2005)
48. Liu, B.: Web content mining (tutorial). In: Proceedings of the 14th International Confer-
ence on World Wide Web (2005)
49. Liu, B., Chang, K.C.-C.: Editorial: Special Issue on Web Content Mining. IGKDD Ex-
plor. Newsl. 6(2), 1‚Äì4 (2004)
50. Liu, B., Grossman, R., Zhai, Y.: Mining data records in Web pages. In: KDD 2003, pp.
601‚Äì606 (2003)
51. Liu, B., Hu, M., Cheng, J.: Opinion observer: analyzing and comparing opinions on the
Web. In: Proceedings of the 14th international Conference on World Wide Web, WWW
2005, Chiba, Japan, May 10-14, pp. 342‚Äì351. ACM, New York (2005)
52. Martin, J.R.: Text and clause: Fractal resonance. Text 15, 5‚Äì42 (1995)
53. Nie, Z., Wen, J.-R., Ma, W.-Y.: Object-level Vertical Search. In: CIDR 2007, Asilomar,
CA, pp. 235‚Äì246 (2007)
54. Nie, Z., Ma, Y., Shi, S., xWen, J.-R., Ma, W.-Y.: Web Object Retrieval. In: WWW 2007,
pp. 81‚Äì90 (2007)
55. Nielsen, J.: DesigningWeb Usability: The Practice of Simplicity. New Riders Publisher,
Indianapolis (2000)
56. Nielsen, J., Loranger, H.: Prioritizing Web Usability. New Riders Press, Berkeley (2006)
57. Yates, J., Orlikowski, W.J.: Genres of Organizational Communication: A Structura-
tional Approach to Studying Communication and Media. Academy of Management Re-
view 17(2), 299‚Äì326 (1992)
58. Pivk, A., Cimiano, P., Sure, Y.: From tables to frames. Journal of Web Semantics 3(2-3),
132‚Äì146 (2005)
59. Pivk, A., Cimiano, P., Sure, Y., Gams, M., Rajkovic, V., Studer, R.: Transforming arbi-
trary tables into logical form with TARTAR. Data Knowl. Eng. 60(3), 567‚Äì595 (2007)
60. Popescu, A., Etzioni, O.: Extracting product features and opinions from reviews. In:
Proceedings of the Conference on Human Language Technology and Empirical Methods
in Natural Language Processing, Vancouver, British Columbia, Canada, October 06 - 08,
pp. 339‚Äì346. Association for Computational Linguistics, Morristown (2005)
61. Rehm, G.: Towards Automatic Web Genre Identification. In: 35th Annual Hawaii Inter-
national Conference on System Sciences (HICSS 2002), vol. 4, p. 101 (2002)
62. Reis, D.C., Golgher, P.B., Silva, A.S., Laender, A.F.: Automatic web news extraction us-
ing tree edit distance. In: WWW 2004: Proceedings of the 13th international conference
on World Wide Web, pp. 502‚Äì511. ACM Press, New York (2004)
110 V. SnaÃÅsÃåel, M. KudeÃålka, and Z. HoraÃÅk
63. Rosso, M.A.: User-based identification of Web genres. JASIST (JASIS) 59(7), 1053‚Äì
1072 (2008/2009)
64. Santini, M.: Description of 3 feature sets for automatic identification of genres in web
pages (2006),
%&'
 ( 
(
(last accessed February 02, 2010)
65. Santini, M.: Characterizing Genres of Web Pages: Genre Hybridism and Individualiza-
tion. In: HICSS 2007, p. 71 (2007)
66. Schmidt, S., Stoyan, H.: Web-based Extraction of Technical Features of Products. GI
Jahrestagung (1), 246‚Äì250 (2005)
67. Schmidt, S., Mandl, S., Ludwig, B., Stoyan, H.: Product-advisory on the web: An infor-
mation extraction approach. In: Artificial Intelligence and Applications 2007, pp. 678‚Äì
683 (2007)
68. Schuth, A., Marx, M., de Rijke, M.: Extracting the discussion structure in comments on
news-articles. In: Proceedings of the 9th Annual ACM international Workshop on Web
information and Data Management, WIDM 2007, Lisbon, Portugal, November 09-09,
pp. 97‚Äì104. ACM, New York (2007)
69. Simon, K., Lausen, G.: ViPER: augmenting automatic information extraction with visual
perceptions. In: Proceedings of the 14th ACM international Conference on information
and Knowledge Management, CIKM 2005, Bremen, Germany, October 31 - November
05, pp. 381‚Äì388. ACM, New York (2005)
70. Su, Z., Zhang, H.J., Li, S., Ma, S.: Relevance feedback in content-based image retrieval:
Bayesian framework, feature subspaces, and progressive learning. IEEE Transactions on
Image Processing 12(8), 924‚Äì937 (2003)
71. Takama, Y., Mitsuhashi, N.: Visual Similarity Comparison for Web Page Retrieval. In:
Web Intelligence, pp. 301‚Äì304 (2005)
72. Tidwell, J.: Designing Interfaces: Patterns for Effective Interaction Design. O‚ÄôReilly Me-
dia, Inc., Sebastopol (2006)
73. Tseng, Y.-F., Kao, H.-K.: The Mining and Extraction of Primary Informative Blocks
and Data Objects from Systematic Web Pages. In: 2006 IEEE/WIC/ACM International
Conference on Web Intelligence (WI 2006 Main Conference Proceedings) (WI 2006),
pp. 370‚Äì373 (2006)
74. Van Duyne, D.K., Landay, J.A., Hong, J.I.: The Design of Sites: Patterns, Principles,
and Processes for Crafting a Customer-Centered Web Experience. Pearson Education,
London (2002)
75. Vredenburg, K., Isensee, S., Righi, C.: User-Centered Design: An Integrated Approach.
Prentice Hall, Upper Saddle River (2002)
76. W3C Document Object Model,  (last accessed February 02,
2010)
77. Van Welie, M., van der Veer, G.: Pattern Languages in Interaction Design: Structure
and Organization. In: Rauterberg, Menozzi, Wesson (eds.) Proceedings of Interact 2003,
ZuÃàrich, Switserland, September 1‚Äì5, pp. 527‚Äì534. IOS Press, Amsterdam (2003)
78. Van Welie, M.: Pattern in Interaction Design,  (last accessed
February 28, 2010)
79. Wong, T.-L.W., Lam, W.: Hot Item Mining and Summarization from Multiple Auction
Web Sites. In: ICDM 2005, New Orleans, Louisiana, USA, pp. 797‚Äì800 (2005)
80. Xiang, P., Yang, X., Shi, Y.: Effective Page Segmentation Combining Pattern Analysis
and Visual Separators for Browsing on Small Screens. In: Web Intelligence 2006, pp.
831‚Äì840 (2006)
4 Web Content Mining Using MicroGenres 111
81. Yang, Y., Chen, Y., Zhang, H.J.: HTML Page Analysis Based on Visual Cues. In: Inter-
national Conference on Document Analysis and Recognition, 2001, pp. 859‚Äì864 (2003)
82. Yi, L., Liu, B., Li, X.: Eliminating noisy information in Web pages for data mining.
In: International Conference on Knowledge Discovery and Data Mining, KDD 2003,
Washington, DC, USA, pp. 296‚Äì305 (2003)
83. Yu, S., Cai, D., Wen, J.-R., Ma, W.-Y.: Improving Pseudo-Relevance Feedback in Web
Information retrieval Using Web Page Segmentation. In: The Proceedings of Twelfth
World Wide Web conference (WWW 2003), Hungary, pp. 203‚Äì211 (2003)
84. Zanibbi, R., Blostein, D., Cordy, J.R.: A survey of table recognition: Models, observa-
tions, transformations, and inferences. International Journal on Document Analysis and
Recognition 7(1), 1‚Äì16 (2004)
85. Zhai, Y., Liu, B.: Web data extraction based on partial tree alignment. In: Proceedings of
the 14th international Conference on World Wide Web, WWW 2005, Chiba, Japan, May
10-14, pp. 76‚Äì85. ACM, New York (2005)
86. Zhai, Y., Liu, B.: Structured Data Extraction from the Web Based on Partial Tree Align-
ment. IEEE Transaction on Knowledge and Data Engineering 18(12), 1614‚Äì1628 (2006)
87. Zhang, R.Y., Lakshmanan, L.V.S., Zamar, R.H.: Extracting relational data from HTML
repositories. ACM SIGKDD Explorations Newsletter 6(2), 5‚Äì13 (2004)
88. Zheng, S., Song, R., Wen, J.-R.: Template-independent news extraction based on visual
consistency. In: Proceedings of AAAI‚Äì2007, pp. 1507‚Äì1511 (2007)
89. Zheng, S., Zhou, D., Li, J., Giles, C.L.: Extracting Author Meta-Data from Web Using
Visual Features. In: Data Mining Workshops, ICDM Workshops, pp. 33‚Äì40 (2007)
90. Zhu, J., Nie, Z., Wen, J., Zhang, B., Ma, W.: Simultaneous record detection and attribute
labeling in web data extraction. In: Proceedings of the 12th ACM SIGKDD international
Conference on Knowledge Discovery and Data Mining, KDD 2006, Philadelphia, PA,
USA, August 20 - 23, pp. 494‚Äì503. ACM, New York (2006)
91. Zhu, J., Zhang, B., Nie, Z., Wen, J.R., Hon, H.W.: Webpage understanding: an integrated
approach. In: Conference on Knowledge Discovery in Data, San Jose, California, USA,
pp. 903‚Äì912 (2007)
92. Zhu, M., Hu, W.: Topic Detection Tracking for Threaded Discussion Communities. In:
International Conferences on Web Intelligence and Intelligent Agent Technology, Syd-
ney, Australia, pp. 77‚Äì83 (2008)
93. Zou, J., Le, D., Thoma, G.R.: Combining DOM tree and geometric layout analysis for
online medical journal article segmentation. In: Proceedings of the 6th ACM/IEEE-CS
Joint Conference on Digital Libraries, JCDL 2006, Chapel Hill, NC, USA, June 11-15,
pp. 119‚Äì128. ACM, New York (2006)
