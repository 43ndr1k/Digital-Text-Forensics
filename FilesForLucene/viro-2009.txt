Peachnote: An Online Piano Community
Collecting Performance Data
Vladimir Viro
Ludwig-Maximilian-University Munich
Berliner Str. 36
80805 Munich, Germany
vladimir@viro.name
ABSTRACT
This  paper  presents  Peachnote,  a  system  designed  to 
collect  large  amounts  of  midi  piano  performance  data 
from  a  community  of  participating  piano  players. 
Peachnote  aims  at  collecting  at  least  10,000  hours  of 
performances.  It  also  creates  a  pool  of  users  who can 
provide  valuable  metadata  for  the  collected  data  (tags, 
ratings, listening behavior etc.), and can test and rate the 
output  of  various  algorithms  that  make  use  of  the 
collected  data.  The  community  nature  of  the  system 
makes  it  attractive  to  users,  making  large  scale  data 
acquisition cheap and thus possible. The software, which 
is a work in progress,  allows its users to share,  browse 
and  organize  their  performances,  give  and  receive 
feedback and motivation, measure up against others and 
compete with each other.  This paper  is an invitation to 
collaboration on the use of collected data.
1. INTRODUCTION
Enabling  computers  to  recognize  semantic  structure  in 
music  and  to  transform  it  meaningfully  requires  large 
amounts  of  training  data.  Existing  approaches  to  data 
collection  –  automated  audio  transcription  and  manual 
labeling – lack either required accuracy or  volume (for 
more  detailed  discussion,  see  Section  2).  This  lack  of 
adequate  and  accessible  data  sources  motivated  the 
creation of Peachnote. Its raison d'être is the collection of 
a  large  body of  accurate  data  that  compactly describes 
how music is being played. These data can be useful in a 
number of research areas in MIR (see Section 5).
Similar data acquisition projects can be found in artificial 
intelligence research (cf. OpenMind[3]). The complexity 
of our thinking of the world, the subject of general AI, is 
very  high,  in  a  sense  that  it  cannot  be  effectively 
described by compact models alone. Therefore, in order 
to understand it, one needs to collect the data that makes 
for its complexity. Music, though arguably less complex 
than the subject of general AI, still seems to be complex 
enough  to  evade  a  description  by  a  simple  model  1 
(cf.  Schmidhuber  [2]).  In  a  1991  interview  with  Otto 
Laske  [1],  Marvin  Minsky  made  the  case  for 
quantification  of  musical  common  sense  concepts. 
Peachnote  is  a  step  towards   a  musical  common sense 
data base envisioned by him.
In this paper we review existing sources of performance 
data  and  their  characteristics,  describe  Peachnote's 
technical  set-up, talk about  the ways piano players  and 
their  listeners  can  use  Peachnote,  and  discuss  possible 
novel uses of collected data in several research domains. 
Notes about the future of Peachnote,  a brief conclusion 
and acknowledgements conclude the paper. 
2. REVIEW OF EXISTING SOURCES OF 
PIANO PERFORMANCE DATA
An in-depth review of data acquisition systems and efforts 
can  be  found in  a  book chapter  “'Sense'  in  Expressive 
Music  Performance:  Data  Acquisition,  Computational 
Studies, and Models” by Goebl, Widmer et al. [5].
2.1 High-Volume but Imprecise Data Sources
First of all, there are plenty of music scores available  2, 
most of them as scanned images, some in MIDI format 3. 
Secondly,  huge libraries  of digital  audio recordings are 
accessible and can be analysed. Both sources,  however, 
have their shortcomings.
Music  scores  in  general  contain  too  little  information 
about  the  music  they  describe.  As  advanced  as  the 
modern  notation  is,  its  complexity  is  dwarfed  by  the 
complexity of the implicit convention transferred between 
generations of musicians and listeners [8]. 
Audio  recordings,  on  the  other  hand,  contain  a  lot  of 
information about every particular performance, but it is 
hard  to  automatically  extract  lexical  information  from 
1 However,  due  to  music's  perceived  fuzziness,  successful 
approximations of musical expressiveness with a small set of rules do 
exist (cf. KTH rule system [7]).
2  imslp.org – The Petrucci Music Library with more than 15,000 works 
and more than 29,000 scores in public domain, over 80 Gb in size.
3  kunstderfuge.org – more than 17,500 midi files
Permission to make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies 
are not made or distributed for profit or commercial advantage and that 
copies bear this notice and the full citation on the first page.
© 2009 International Society for Music Information Retrieval 
them with sufficient accuracy.  The problem of accurate 
recognition,  transcription  and  reperformance  is  being 
actively researched, but it is still far from being solved. 
(cf. MIREX: Multiple Fundamental Frequency Estimation 
&  Tracking  task).  The  fact  that  probably  the  most 
accurate  commercial  recognition  technology,  developed 
by  Zenph  Studios  1,  targets  only  one  instrument,  the 
piano, and has yet to be scaled out, supports this claim.
2.2 Accurate but Low-Volume Data Sources
There  are  two  ways  of  obtaining  accurate  and  usable 
performance data – manually annotating audio recordings 
and  recording  player  piano  performances.  The  first 
method can be  very accurate,  it  is,  however,  very time 
consuming  and  is  generally  used  to  analyze  only 
particular  pieces  and  techniques  [5,17].  The  second 
method, player piano recording, is more straightforward, 
but  to  the  author's  best  knowledge  the  research  driven 
data  collection  efforts  have  been  always  focused  on 
particular pieces as well. 
Probably the largest commercial effort of the second kind 
is the Yamaha's collection of music recordings for their 
Disklavier  player  pianos.  The  recordings,  however,  are 
expensive  (typically  ~45$  for  one  hour  of  music). 
Probably  the  largest  set  of  freely  available  MIDI 
recordings  can  be  found  at  Yamaha's  Piano-e-
Competition website 2 (currently approximately 165 hours 
of  performances,  or  46  Mb  of  MIDI  files  when 
compressed with bzip2 data compressor).
Peachnote  aims  at  collecting  at  least  two  orders  of 
magnitude more data than is available through the Piano-
e-Competition.
1 http://www.zenph.com
2 http://www.piano-e-competition.com
3. SYSTEM ARCHITECTURE
Peachnote  consists  of  three  main  components:  Client, 
Server  and Community.  The  client  runs on a computer 
connected  to  a  MIDI  keyboard  or  a  player  piano.  It 
handles in- and outgoing MIDI streams, the friend roster, 
text  messaging  and  configuration.  The  server  handles 
client  requests,  indexes  MIDI  streams,  partitions  them 
into  pieces  and  assembles  statistics.  The  community 
provides online access to MIDI streams, comments, user 
relationship management and statistics.
The  components  communicate  via  XMPP  –  an  open, 
extensible  and  well  established  IM  protocol  3.  A 
dedicated  XMPP server  instance  4 handles the message 
passing between clients and the server.
3.1 Cross-Platform Client
The  Peachnote  client  software  serves  three  main 
purposes:  sending  and  receiving  MIDI  data,  providing 
user  with an overview and  control  over  the client,  and 
collecting user feedback.
The MIDI data can be streamed from the MIDI keyboard 
to  the  server  and  then  rebroadcasted  to  other  clients. 
Incoming MIDI stream sent by the server can be received 
and  forwarded to a chosen MIDI port. It is important to 
note  that  all  transferred  MIDI  events  are  being 
timestamped,  so  network  latency  does  not  play  a  role 
when using a buffer.
The client can also function offline. In this case all data 
coming from the MIDI keyboard are stored locally. When 
the  connection  is  reestablished,  the  cache  content  is 
flushed to the server.
Users can see their usage statistics 
and  a  list  of  their  friends,  whose 
streams they follow. They can tag 
and  comment  on  MIDI  streams 
while  listening to  them. They can 
be asked by the system whether the 
music  is  well  described  by  given 
tags. These tags and comments are 
stored  on the  server  and  are  later 
available  on  the  community 
website.  Users  can  allow  either 
anybody  to  follow  them,  or  only 
those  whose  request  they  have 
accepted.  The  same  applies  to 
comments.
The  Peachnote  client  runs  on 
Windows,  Linux  and  Mac.  It  is 
available as a browser-based and as 
a standalone application. 
3 http://xmpp.org -  XMPP Standards Foundation
4 http://ejabberd.im - ejabberd XMPP IM server written in Erlang
System Architecture
The  in-browser  version  implements  only  the  basic 
functionality  (MIDI  data  exchange),  requires  no 
installation and uses a Java applet for MIDI and XMPP 
communication. The standalone version is written in C++, 
with GUI built with Qt4 1, XMPP functionality provided 
by the gloox library  2 and MIDI functionality  provided 
by the RtMidi  library  3.  The  GUI is  based  on Psi  4,  a 
feature-rich  open  source  instant-messaging  client.  The 
client is open source and is distributed under GPL.
3.2 Server
The server  stores  all  incoming MIDI data streams in a 
MySQL database. The indexing and partitioning of MIDI 
streams  is  handled  using  compression  based  methods. 
Scalability issues are addressed by using memcached 5 as 
a caching layer.
3.3 Community Website
The community website, Peachnote.com, provides MIDI-
stream  browsing,  commenting,  user  profiles,  music 
metadata, various ratings and data export functionality. It 
is  based  on  Drupal  content  management  system  6.  For 
more  detailed  account  of  the  website's  features  see 
Section 4.
3.4 Implementation Status
Peachnote is a work in progress and has not been publicly 
launched yet, although it has already been in private use 
for more than half a year.  The server is functional,  the 
cross-platform  client  supports  recording  and  streaming 
functionality.  Users  can  navigate  their  own  and  other 
users'  recordings  using the  client  or  the  website.  Near 
real-time  MIDI  broadcasting  is  possible.  The  social 
aspects  of  interaction,  such  as  friend  lists,  inter-user 
communication  and  notifications,  as  well  as  detailed 
practice  statistics,  however,  are  yet  to  be  implemented. 
These  features,  along  with  the  public  launch,  are 
scheduled for August-September 2009.
4.  PEACHNOTE FROM USER PERSPECTIVE
Though  built  with  data  acquisition  goal  in  mind, 
Peachnote is also a consumer service. In fact, its success 
in  data  acquisition  depends  on  its  success  with  its 
contributors – people playing piano and people listening 
to them. Since Peachnote has not been publicly launched 
1 Qt – a cross-platform application framework
2 gloox library by Jakob Schröter. http://camaya.net/gloox
3 RtMidi library by Gary P. Scavone.
  http://www.music.mcgill.ca/~gary/rtmidi/
4 Psi – a free instant messaging application designed for the Jabber IM 
network: http://psi-im.org/
5 Memcached – a high-performance, distributed memory object caching 
system:  http://www.danga.com/memcached
6 http://drupal.org
yet,  we  cannot  assess  its  attractiveness  directly  by 
measuring  the  growth  rate  of  its  user  base.  Still,  we 
believe  there  are  reasons  to  think  that  it  can  attract 
considerable audience due to several observations. First 
of  all,  people,  professionals  and  amateurs  alike,  enjoy 
expressing  themselves  and  sharing  their  creations  with 
others  7,  as can be easily seen on example of Youtube, 
Flickr  and  other  new  media  platforms  of  the  kind. 
Secondly,  owners  of  midi  pianos  and  keyboards  are 
currently  presented  with  the  only  practical  option  of 
passively consuming music provided for their instruments 
by specialty record labels. Sharing their own recordings is 
possible either by exchanging midi files or over Youtube, 
with both options being considerably less convenient than 
what  Peachnote  can  offer.  Satisfying  the  need  for  a 
convenient  platform  for  midi  piano  players,  of  whom 
there  are  not  a  few,  Peachnote  can  become a  reliable 
source of data needed in various applications.
4.1 Listeners
One  does  not  need  to  play  piano  in  order  to  use  the 
system. It can be viewed as a source of live music and a 
tool  for  discovering  new  music  and  performers. 
Interesting  artists  and  pieces  can  be  shared  with  other 
users.  And  of  course  one  can  communicate  with 
performers  and  with  other  listeners  (cf.  Audience 
Engagement Platform by Misnomer Dance Theater 8).
4.2 Piano Players
Piano  players  can  use  the  community as  their  musical 
blog.  They  can  organize  and  navigate  their  own 
performances by name of the piece, other tags, or music 
similarity. 
It is important to notice that the MIDI data are collected 
all  the time. Data collection is inclusive,  not  exclusive. 
That means that the system stores whole practice sessions, 
not only some chosen performances as it is normally the 
case with audio recordings. Users can decide whether to 
make their  whole stream public,  available  only to their 
friends, or private, as well as whether or not to license 
their  recordings  under  one  of  the  Creative  Commons 
licenses.  They  can  change  the  accessibility  of  single 
performances or time frames.
An important  part  of  blogging  experience  is  receiving 
feedback. Peachnote users can leave comments on whole 
performances or particular time frames. Another feature is 
discovering feedback  given to  other  performers  for  the 
same pieces one plays himself. One can learn from others' 
experience. 
Piano players can acquire audience and give live concerts. 
They can measure up against others by time and quality of 
7 cf. jamendo.com and also freesound.org by MTG (UPF) 
8 http://misnomer.org/aep
their  practicing.  The  corresponding  statistics  can  be 
viewed and compared on every level, from whole pieces 
to single passages.
An important  feature of the system from a pedagogical 
point of view is its ability to constantly provide feedback 
and motivation to students, either from their teachers and 
friends  or  in  form  of  system-generated  statistics  and 
ratings, making practicing less solitary and more engaging 
experience.
5.  POSSIBLE APPLICATIONS IN RESEARCH
The amount and nature  of data  collected  by Peachnote 
enables  several  new applications  of  piano  performance 
data.  Some of them depend on collected meta-data and 
user feedback, others rely on Peachnote's continuous data 
collection, and all profit from its scale.
5.1 Studying Development of Performances
Most,  if  not  all,  previous  MIR  studies  that  involved 
analysis  of  performance  data,  dealt  with  complete 
performances  [5].  These  were  so  to  say  snapshots  of 
musician's  work. The whole process of development of 
performances  was  being  left  unconsidered.  With 
Peachnote it becomes possible for the first time to study 
the process of creating a performance in its entirety, from 
the very beginning of work on new music. 
One  can  compare  this  point  of  view  with  that  of 
developmental  biology.  The  amount  and  nature  of 
collected  data  can  give  meaning  to  the  notion  of 
developmental musicology. (cf. Section 5.7)
5.2 Studying the Composition Process
Understanding music ultimately implies understanding the 
process  that  produced  it  [1].  With  composers  and 
improvisers using Peachnote we are able to better observe 
and study that process in vivo.
5.3 Extracting Musical Structure
The  nature  of  collected  data  lends  itself  to  the  use  in 
extracting  musical  structure.  People  often  learn  and 
practice  pieces  in  some meaningful  units,  like  musical 
phrases,  passages  or  movements.  Their  repeated 
performances  can  be  sensed  and  thereby  the  units 
discovered (cf. [9-13]).
5.4 Predicting High-Level Musical Properties
The system has several ways of eliciting feedback from 
users. A straightforward source is the ability of users to 
tag the music they are listening to. Another possibility is 
asking them whether a particular tag describes the music 
well. Yet another source of labels could be music scores 
in MusicXML format1 containing mood labels and other 
musical markings. With these data we can train models 
that  predict  properties  of  music  performances,  such  as 
emotional  expression,  or,  more  generally,  any  kind  of 
description a person can give while listening to music  
(cf. [14]). Not only can these models be trained from user 
labels,  they  can  also  be  validated  by  users,  who  can 
approve the accuracy of generated labels with one mouse 
click.
5.5 MIDI-to-score Transcription
The amount of information in a music score is normally 
much lower than in a MIDI recording of the same piece. 
Temporal  information  is  represented  in  a  highly 
discretized form using only a handful of rational numbers 
for  note's  lengths  and  onset  times.  Some  additional 
information is stored symbolically in form of general and 
local tempo markings, such as moderato, presto, etc. and 
accelerando,  ritenuto,  etc.  Information about  the note's 
volume is sometimes stored symbolically using modifiers 
such  as  forte ,  diminuendo,  etc.,  or  at  best  with  local 
modifiers applied to some time frame. 
With  enough  training  data  (MIDI  recordings  and 
corresponding scores in MIDI or MusicXML format), and 
with  user  feedback,  it  could  be  possible  to  train  a 
converter that, given a MIDI sequence, would convert it 
into an annotated music score.  Since the rules of music 
notation and exceptions to them can be fairly subtle,  it 
makes sense to apply machine learning techniques to a 
problem that otherwise could be served by a fixed set of 
rules as well. 
5.6 Performer Recognition
Development,  tuning  and  validation  of  performer 
recognition  algorithms  (cf.  [15,16])  can  profit  from  a 
large ground truth database,  which Peachnote is able to 
provide.  The  data  could  also  be  used  as  a  base  for  a 
corresponding  MIREX  task,  which  currently  would  be 
hard to hold due to scarcity of accurate ground truth data 
(since audio transcription isn't  yet  accurate  enough, see 
Section 2.1).
5.7 Mapping Performance Styles
We could also conduct performance analysis of the kind 
pioneered  by  the  Mazurka  Project  [17],  and  look  at 
similarities and relations among piano players. Assuming 
geographic diversity of the user base we could explore 
possible  manifestations  of  differences  in  performance 
traditions in different geographies and piano schools. One 
can  already  start  doing  this  with  data  from  Yamaha's 
Piano-e-Competition (cf. Section  2.2).
1 MusicXML at http://www.recordare.com/xml.html
5.8 Audio Transcription
One  of  the  big  problems  of  score  aided  audio 
transcription and performance parameter estimation is the 
deviation of the performance from the score [18].  When 
a pianist hits the wrong note, repeats or skips a portion of 
the score,  it  is  hard  to  recognize what happened  using 
only the score.  The data collected  by the system could 
allow the  creation  of  statistical  performer  models  that 
would include probabilities of all sorts of deviations from 
a given score.
5.9 Performance Rendering
One  could  reperform  musical  pieces  using  case  based 
reasoning,  or  patchworking,  by  combining  previously 
observed  performance  patterns.  The  patterns  could  be 
selected based on an algorithm or an interactive user input 
(cf. [6,19,20]). The results could be judged by the public.
5.10 Pedagogy
For  the  first  time  it  becomes  possible  to  connect  the 
quantity and the exact course of exercise with the playing 
quality, which can be measured based on user ratings. We 
can collect statistics on how people are improving, how 
intensive  and  how long they practice  particular  pieces, 
passages and playing techniques and what effect does it 
have. On the basis of these data we could formulate new 
or  quantitatively  support  or  disprove  existing  learning 
methods [21]. We could also estimate the amount of time 
and effort  people with different  skills need to invest to 
achieve particular goals, like mastering a particular piece, 
passage or playing technique (cf. [22]).
6. FUTURE WORK
The future of Peachnote will be determined by its users. 
The interests, preferences and ideas of piano players, their 
listeners and the scientific community will determine its 
development  path.  Such  things  are  inherently  hard  to 
predict, therefore we can only hope that the interest will 
be  there  and  assure  that  the  design  of  the  system  is 
flexible enough to meet the emerging demands.
An independent topic however is the broadening of user 
base. A Facebook application and integration with other 
social networks suggest themselves. Social networks are 
especially effective for this purpose because the potential 
users  already  form tightly  knit  communities  there.  An 
application, if attractive, can spread very quickly.
An  important  technical  extension  would  be  enabling 
audio streaming from the client. This would have many 
applications interesting to both users and researchers. For 
example, one could compare audio outputs from different 
keyboards and player pianos and finding the best mapping 
between MIDI streams produced on different instruments. 
Another interesting extension would be server side audio 
rendering of  received  MIDI streams. That  would allow 
users without good MIDI rendering devices to listen to 
performances of other Peachnote members. 
MIDI  streams  will  have  to  be  mapped  for  specific 
instruments – for example,  the dynamic range of a key 
onset on a Disklavier is 100, not 127, therefore streams to 
and from Disklaviers have to be normalized to account 
for  this  difference.  Controlling  the  client  with  special 
keyboard gestures could be implemented.
It  would  be  interesting  to  integrate  Peachnote  with 
Noteflight  1,  an  in-browser  score  editing  software  
(cf.  Section  5.5),  with  Musipedia  Melody  Search  
Web-service  2 [23],  as  well  as  with  the  International 
Music Score Library Project.
7. CONCLUSION
This paper  presented Peachnote,  a  community of piano 
players that serves as a source of piano performance data, 
corresponding  meta-information,  and  as  a  testbed  for 
algorithms that use these data. 
Music gives a lot of pleasure to a lot of people,  and it 
makes sense to try using some of the energy it releases for 
research  related  to  it.  Engaging  musicians  and  music 
lovers  in  music  research  projects  can  offer  benefits  of 
scale and of immediate human evaluation and feedback, 
that  are  very  much  needed  when  dealing  with  such  a 
complex human activity.  We just  have to provide ways 
for  people to enjoy music more,  ways that  at  the same 
time  facilitate  music  research.  The  constraint  of 
unobtrusiveness will be outweighed by benefits of scale 
and feedback.
8. ACKNOWLEDGMENTS
We would  like  to  thank  Evgeny Zinovyev  for  helping 
developing the GUI of the client, Yushen Han for the OS 
X  build  of  the  client,  and  Tzvetan  Ivanov  for  helpful 
comments on the manuscript.
9. REFERENCES
[1] M. Minsky and O. Laske. “A Conversation with 
Marvin  Minsky,”  AI  Magazine,  North  America, 
1315 09 1992. Available at 
http://www.aaai.org/ojs/index.php/aimagazine/article/view/1009/927
[2] J.  Schmidhuber.  “Simple  Algorithmic  Theory  of 
Subjective  Beauty,  Novelty,  Surprise, 
Interestingness,  Attention,  Curiosity,  Creativity, 
Art,  Science,  Music,  Jokes,”  Journal  of  SICE, 
48(1):21-32, 2009 
1 http://www.noteflight.com
2 http://www.musipedia.org
[3] P.  Singh.  “The  public  acquisition  of  commonsense 
knowledge,” Proceedings of AAAI Spring Symposium 
on  Acquiring  (and  Using)  Linguistic  (and  World) 
Knowledge  for  Information  Access.  Palo  Alto,  CA. 
2002.
[4] D.  Lenat,  “CYC:  A  Large-Scale  Investment  in 
Knowledge  Infrastructure,” Communications  of  
the ACM, Nov 1995, Vol. 38, No. 11.
[5] W. Goebl., S. Dixon, G. De Poli, A. Friberg,  A. 
Bresin, G.  Widmer. “"Sense" in Expressive Music 
Performance:  Data  Acquisition,  Computational 
Studies,  and  Models”,  in  P.  Polotti  and  D. 
Rocchesso (Eds.), Sound to Sense, Sense to Sound 
--  A  State  of  the  Art  in  Sound  and  Music  
Computing, Logos Verlag, 2007.
[6] R. de Mantaras,  J.  Arcos.  “AI and Music:  From 
Composition  to  Expressive  Performance”,  AI 
Magazine, 23(3), 2002.
[7] A. Friberg, R. Bresin, J. Sundberg. “Overview of 
the  KTH rule  system for  musical  performance,” 
Advances in Cognitive Psychology, Special Issue  
on Music Performance, 2006.
[8] M.S. Lew,  N.  Sebe,  C.  Djeraba,  and  R.  Jain. 
“Contentbased  multimedia  information  retrieval: 
State  of  the  art  and  challenges,”  ACM  Trans.  
Multimedia  Computing,  Communications,  and  
Applications, 2(1): 1–19, 2006.
[9] R.  Dannenberg,  N.  Hu.  “Discovering  musical 
structure  in  audio  recordings,”  In  Music  and 
Artifical  Intelligence:  Second  International  
Conference, Edinburgh, 2002.
[10] G.  Peeters.  “Sequence  representation  of  music  
structure using higher-order similarity matrix and 
maximum-likelihood  approach,”  Proc.  ISMIR, 
Vienna, Austria, 2007.
[11] C.  Rhodes,  M.  Casey.  “Algorithms  for  
determining  and  labelling  approximate  
hierarchical  self-similarity,”  Proc.  ISMIR, 
Vienna, Austria, 2007.
[12] M.A. Casey, R.C. Veltkamp, M. Goto, M. Leman, 
C.  Rhodes,  M.  Slaney.  “Contentbased  music 
information retrieval: Current directions and future 
challenges,” Proceedings of the IEEE 96(4), 668-
696. 2008.
[13] J.  de  Nooijer,  F.  Wiering,  A.  Volk,  H.J.M. 
Tabachneck-Schijf  (2008).  “Cognition-based 
segmentation  for  music  information  retrieval 
systems,” in:  C.  Tsougras,  R.  Parncutt  (eds.), 
Proceedings  of  the  fourth  Conference  on  
Interdisciplinary Musicology (CIM08)
[14] J.-J.  Aucouturier.  “Sounds  like  teen  spirit: 
Computational insights into the grounding of everyday 
musical  terms,”  In  J.  W.  Minett  &  W.  S.-Y.  Wang 
(Eds.), Language, evolution and the brain,  frontiers in 
linguistics  series  (in  preparation).  Taiwan:  Academia 
Sinica Press.  2008.
[15] S.T. Madsen, G. Widmer. “Exploring similarities 
in  Music  Performances  with  an  Evolutionary 
Algorithm,”  in  Proceedings  of  The  18th 
International  FLAIRS  Conference,  Clearwater 
Beach, Florida, May 16-18, 2005. AAAI Press.
[16] E.  Stamatatos,  G.  Widmer.  “Automatic 
identification  of  music  performers  with  learning 
ensembles,” Artificial Intelligence, 165 (1), 37–56. 
2005.
[17] C.  Sapp. “Hybrid numeric/rank similarity metrics 
for musical performance analysis”,   Proc. ISMIR, 
Philadelphia, USA, 2008.
[18] R. Fiebrink, P. Cook, S. Smallwood, D. Trueman, 
G.  Wang.  “Laptop  Orchestras  and  Machine 
Learning  in  Real-time  Music  Performance”, 
Workshop  on  Computational  Creativity  Support, 
Boston, MA, USA, April, 2009  
[19] M. Riedl, B. O’Neill. “Computer as Audience: A 
Strategy  for  Artificial  Intelligence  Support  of 
Human Creativity”,  Workshop on Computational  
Creativity  Support,  Boston,  MA,  USA,  April, 
2009  
[20] R.  Dannenberg,  C.  Raphael.  “Music  score 
alignment  and  computer  accompaniment,” 
Communications of the ACM, 49, 8  (Aug. 2006), 
39-43.
[21] R.A.  Duke,  A.L.  Simmons,  and  C.D.  Cash.
“It's Not How Much; It's How: Characteristics of 
Practice  Behavior  and Retention of  Performance 
Skills,”  Journal of Research in Music Education, 
January 1, 2009; 56(4): 310 - 321. 
[22] A.C.  Lehmann,  J.W.  Davidson,  “Taking  an 
Acquired  Skills  Perspective  on  Music 
Performance”, MENC  Handbook  of  Musical  
Cognition  and  Development,  2006  -  Oxford 
University Press, USA 
[23] S.T.  Madsen, R. Typke,  G. Widmer. “Automatic 
Reduction  of  MIDI  Files  Preserving  Relevant 
Musical  Content”,  In  Proceedings  of  the  6th 
International  Workshop on Adaptive  Multimedia  
Retrieval (AMR'08), Berlin, June 2008
