AUTHORSHIP ATTRIBUTION USING FUNCTION WORDS ADJACENCY NETWORKS
Santiago Segarra, Mark Eisen, and Alejandro Ribeiro
Department of Electrical and Systems Engineering, University of Pennsylvania
ABSTRACT
We present an authorship attribution method based on relational data
between function words. These are content independent words that
help define grammatical relationships. As relational structures we use
normalized word adjacency networks. We interpret these networks as
Markov chains and compare them using entropy measures. We illustrate
the accuracy of the method developed through a series of numerical
experiments including comparisons with frequency based methods. We
show that accuracy increases when combining relational and frequency
based data, indicating that both sources of information encode different
aspects of authorial styles.
Index Terms— Authorship attribution, word adjacency network,
Markov chain, relative entropy
1. INTRODUCTION
The goal of authorship attribution is to match a text of unknown or dis-
puted authorship to one of a group of potential candidates. More gen-
erally, it can be seen as the search for a compact representation of an
author’s writing style, or stylometric fingerprint. Applications of this
study range from forensics to questions of plagiarism in the works of
both published authors as well as students. With recent developments
in computational efficiency and information processing, authorship at-
tribution studies are of both increasing interest and accuracy [1, 2]. The
study of authorship attribution, sometimes called stylometry, has its be-
ginnings in works published over a century ago [3] which proposed dis-
tinguishing authors by looking at word lengths. This was later improved
upon by [4] to consider average sentence length as a determinant.
These two rudimentary ideas have improved since. A significant de-
velopment came with the introduction of the influential idea of analyz-
ing function words as a way to characterize authors’ styles [5]. Function
words are words like prepositions, conjunctions, and pronouns which on
their own carry little meaning but instead help define grammatical rela-
tionships between words. The study of function words is beneficial as
they primarily inform about syntax rather than content. Since [5], func-
tion words appeared in a number of papers such as [6] where principal
component analysis was performed regarding the frequencies of the most
common words in a text. A similar look at commonly appearing words
was done in [7]. Attention has also been given to analyzing features
other than appearances of high-frequency words. Examples of these are
the stylometric techniques in [8] and the use of vocabulary richness as a
stylometric marker [9–11] – see also [12] for a critique. Furher exam-
ples are word stability, the extent to which a word can be replaced by an
equivalent [13], or syntactical markers like part-of-speech taggers [14].
Frequency based feature analysis have been expanded into the appli-
cation of Markov chains in stylometry. Studies done in [15] and [16] use
letter based Markov chains to model texts. Although this approach gen-
erates positive results, there is little intuitive reasoning behind the notion
that an author’s style can be better modeled by his usage of individual
letters rather than words. Research in [17] has looked at using word
based Markov chains but the author did not focus on function words and
Supported by NSF CAREER CCF-0952867 and NSF CCF-1217963.
had to introduce smoothing techniques to account for transitions not en-
countered in the training sets, deteriorating the accuracy.
In this paper, we focus on function words but instead of using their
frequency distribution as an author signature [5] we propose the use of
the relational structure of function words. In order to classify the au-
thorship of a text we compute an asymmetric network of function word
adjacencies capturing how likely it is to find a particular function word
within the next few words conditional on the occurrence of another given
word (Section 3). The resulting matrices can be interpreted as transition
probabilities of a Markov chain. The similarity of different texts is es-
timated by the relative entropy of these transition probabilities (Section
3.1). We test the proposed methodology in authorship attribution prob-
lems including texts from up to 18 different authors using training sets
consisting of between 1 and 6 known texts per author. Estimation accu-
racy in the order of at least 90% is observed in most cases (Section 4).
We further demonstrate that our classifier performs better that classifiers
based in word frequencies (Section 4.1). Perhaps more important, nu-
merical experiments show that classifiers based on word frequencies en-
code different stylometric fingerprints than the classifiers proposed here
and can then be combined for increased attribution correctness.
2. PROBLEM FORMULATION
We are given a set of n authors A = {a1, a2, ..., an}, a set of m
known texts T = {t1, t2, ..., tm} and a set of k unknown texts U =
{u1, u2, ..., uk}. We are also given an authorship attribution function
rT : T → A mapping every known text in T to its corresponding author
in A, i.e. rT (t) ∈ A is the author of text t for all t ∈ T . We further
assume rT to be surjective, this implies that for every author ai ∈ A
there is at least one text tj ∈ T with rT (tj) = ai. Denote as T (i) ⊂ T
the subset of known texts written by author ai, i.e.
T (i) = {t | t ∈ T, rT (t) = ai}. (1)
According to the above discussion, it must be that |T (i)| > 0 for all i
and {T (i)}ni=1 must be a partition of T . In Section 3, we use the texts
contained in T (i) to generate a relational profile for author ai. There
exists an unknown attribution function rU : U → A which assigns
each text u ∈ U to its actual author rU (u) ∈ A. Our objective is to
approximate this unknown function with an estimator r̂U built with the
information provided by the attribution function rT . In particular, we
construct word adjacency networks (WAN) for the known texts t ∈ T
and unknown texts u ∈ U . We attribute texts by comparing the WANs
of the unknown texts u ∈ U to the WANs of the known texts t ∈ T .
In constructing WANs the concepts of sentence, proximity, and func-
tion words are important. Every text consists of a sequence of sentences,
where a sentence is defined as an indexed sequence of words between
two stopper symbols. We think of these symbols as grammatical sen-
tence delimiters, but this is not required. For a given sentence we define
a directed proximity between two words parametric on a discount factor
α ∈ (0, 1) and a window length D. If we denote as i(ω) the position of
word ω within its sentence the directed proximity d(ω1, ω2) from word
ω1 to word ω2 when 0 < i(ω2)− i(ω1) ≤ D is defined as
d(ω1, ω2) := α
i(ω2)−i(ω1). (2)
Common Function Words
the and a of to
in that with but it
Table 1. 10 most common function words found in the texts
In every sentence there are two kind of words: function and non-function
words [18]. Function words are words that express primarily a grammat-
ical relationship. Examples of function words include articles, preposi-
tions, and pronouns. The 10 most common function words are listed in
Table 1. We exclude gender specific pronouns (“he” and “she”) as well
as pronouns that depend on narration type (“I” and “you”) from the set
of function words to avoid biased similarity between texts written using
the same grammatical person – see Section 3 for details. The concepts of
sentence, proximity, and function words are illustrated in the following
example.
Example 1 Define the set of stopper symbols as {. ; }, let the parameter
α = 0.8, the window D = 4, and consider the text
“A swarm in May is worth a load of hay; a swarm in June is worth
a silver spoon; but a swarm in July is not worth a fly.”
The text is composed of three sentences separated by the delimiter { ; }.
We then divide the text into its three constituent sentences and highlight
the function words
a swarm in May is worth a load of hay
a swarm in June is worth a silver spoon
but a swarm in July is not worth a fly
The directed proximity from the first “a” to “swarm” in the first sentence
is α1 = 0.8 and the directed proximity from the first “a” to “in” is
α2 = 0.64. The directed proximity to “worth” or “load” is 0 because
the indices of these words differ in more than D = 4.
Define the relative accuracy as the fraction of unknown texts that are
correctly attributed. With I denoting the indicator function we can write
the estimation accuracy ρ as
ρ(r̂U ) =
1
k
∑
u∈U
I {r̂U (u) = rU (u)} , (3)
We use ρ(r̂U ) to gauge performance of the classifier in Section 4.
3. WORD ADJACENCY NETWORK
As relational structures we construct WANs for each text. These
weighted and directed networks contain function words as nodes.
The weight of a given edge represents the likelihood of finding the
words connected by this edge close to each other in the text. Formally,
from a given text t we construct the network Wt = (F,Qt) where
F = {f1, f2, ..., ff} is the set of nodes composed by a collection of
function words common to all WANs and Qt : F × F → R+ is a
similarity measure between pairs of nodes. We choose F as the set
of the f most common function words among the texts analyzed. The
choice of the number |F | of function words is discussed in Section 4.1.
In order to calculate the similarity function Qt we first divide the
text t into sentences sht where h ranges from 1 to the total number of
sentences. We denote by sht (e) the word in the e-th position within sen-
tence h of text t. In this way, we define
Qt(fi, fj) =
∑
h,e
I{sht (e) = fi}
D∑
d=1
αd I{sht (e+ d) = fj}, (4)
for all fi, fj ∈ F , where α ∈ (0, 1) is the discount factor that decreases
the assigned weight as the words are found further apart from each other
and D is the window limit to consider that two words are related. The
similarity measure in (4) is the sum of the directed proximities from fi
to fj defined in (2) for all appearances of fi when the words are found
at most D positions apart. Since in general Qt(fi, fj) 6= Qt(fj , fi) the
WANs generated are directed.
Example 2 Consider the same text and parameters of Example 1. There
are four function words yielding the set F = {a, in, of, but}. The matrix
representation of the similarity function Qt is
Q =

a in of but
a 0 3× 0.82 0.82 0
in 2× 0.84 0 0 0
of 0 0 0 0
but 0.8 0.83 0 0
. (5)
The total similarity score from “a” to “in” is obtained by summing up
the three 0.82 proximity scores that appear in each sentence. Although
the word “a” appears twice in every sentence, Q(a, a) = 0 because its
appearances are more than D = 4 words apart.
Using text WANs, we generate a network Wc for every author ac ∈ A
as Wc = (F,Qc) where
Qc =
∑
t∈T (c)
Qt. (6)
Similarities in Qc depend on the amount and length of the texts written
by author ac. This is undesirable since we want to be able to compare
relational structures among different authors. Hence, we normalize the
similarity measures as
Q̂c(fi, fj) =
Qc(fi, fj)∑
j Qc(fi, fj)
, (7)
for all fi, fj ∈ F . In this way we achieve normalized networks P̂c =
(F, Q̂c) for each author ac. In (7) we assume |F | small enough or texts
long enough to guarantee a non zero denominator.
Our claim is that every author ac has an inherent relational structure
Pc that serves as an authorial fingerprint and can be used towards the
solution of authorship attribution problems. P̂c estimates Pc with the
available known texts written by author ac.
3.1. Network Similarity
The normalized networks P̂c can be interpreted as discrete time Markov
chains (MC) since the similarities out of every node sum up to 1. Thus,
the normalized similarity between words fi and fj is a measure of the
probability of finding fj in the words following an encounter of fi. In a
similar manner, we can build a MC Pu for each unknown text u ∈ U .
Since every MC has the same state space F we use the relative en-
tropy H(P1, P2) as a dissimilarity measure between the chains P1 and
P2. The relative entropy is given by
H(P1, P2) =
∑
i,j
π(fi)P1(fi, fj) log
P1(fi, fj)
P2(fi, fj)
, (8)
where π is the limiting distribution on P1. The choice ofH as a measure
of dissimilarity is not arbitrary. In fact, if we denote as w1 a realization
of the MC P1, H(P1, P2) is proportional to the logarithm of the ratio
between the probability that w1 is a realization of P1 and the probability
that w1 is a realization of P2. In particular, when H(P1, P2) is null, the
ratio is 1 meaning that a given realization of P1 has the same probability
5 6 1 T 2 3 4 M 11 7 8 9 10
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
Texts
R
el
at
iv
e 
E
nt
ro
py
Fig. 1. Clustering of Twain’s (T) and Melville’s (M) relational profiles
and the 11 unknown texts. Two clusters, blue and red, emerge corre-
sponding to both authors with perfect accuracy.
Author Texts Author Texts
Shakespeare, W. 10 Twain, M. 9
Austen, J. 7 Allen, G. 7
Cooper, J.F. 6 Dickens, C. 6
Marlowe, C. 8 Bacon, F. 6
Beaumont, F. & Fletcher, J. 7 Hawthorne, N. 6
Abbott, J. 7 James, H. 8
Alger, H. 7 Jonson, B. 6
Alcott, L.M. 7 Aldrich, T.B. 7
Garland, H 8 Melville, H. 8
Table 2. Authors and total number of texts per author considered for the
second numerical experiment, Table 3. See details in [20].
of being observed in both MCs [19]. Using (8) we generate the attribu-
tion function r̂U (u) by assigning the text u to the author with the most
similar relational structure
r̂U (u) = ap, where p = argmin
c
H(Pu, P̂c). (9)
We evaluate this classifier in the next section after the following remark.
Remark 1 In (9) we assume that the unknown texts are long enough
for the corresponding MC to be ergodic. This ensures that the limiting
distribution π is well defined. If this is not achieved, we replace π(fi)
with the expected fraction of time a randomly initialized walk spends in
state fi. The random initial function word is drawn from a distribution
proportional to the word frequencies in the text.
4. NUMERICAL RESULTS
In this section we fix α = 0.8,D = 10 and the set of sentence delimiters
to be { . ( ) ? ! ; : }. Moreover, we consider state spaces of 10 function
words except in Section 4.1 where we vary the number of function words
considered.
To illustrate the method developed, we begin by solving an author-
ship attribution problem with two candidate authors: Mark Twain and
Herman Melville. For each author we have 3 known texts. We are given
11 unknown texts where the first 6 belong to Twain and the other 5 were
written by Melville [20]. Every text in this simulation belongs to a dif-
ferent book and corresponds to a 10,000 words extract, i.e. around 25
pages of a paper back mid size edition. With the method here developed,
the 11 unknown texts are attributed with perfect accuracy. An intuitive
reason of why this works is depicted in Fig. 1. In this figure, we plot
Known texts per author Rnd.
1 2 3 4 5 6 Attr.
N
um
be
ro
fA
ut
ho
rs
2 1.00 1.00 1.00 1.00 1.00 1.00 .50
3 .87 1.00 1.00 1.00 1.00 1.00 .33
4 .76 .92 1.00 1.00 1.00 1.00 .25
5 .74 .90 1.00 1.00 1.00 1.00 .20
6 .74 .82 .93 .90 .93 1.00 .17
7 .63 .85 .94 .92 .94 1.00 .14
8 .67 .86 .94 .93 .95 1.00 .13
9 .60 .83 .92 .90 .95 .92 .11
10 .55 .77 .90 .91 .95 .92 .10
11 .53 .74 .89 .86 .88 .85 .09
12 .57 .76 .90 .87 .89 .87 .08
13 .60 .78 .91 .88 .90 .88 .08
14 .59 .78 .90 .86 .87 .88 .07
15 .61 .77 .89 .87 .88 .88 .07
16 .57 .73 .85 .84 .88 .89 .06
17 .58 .74 .85 .85 .89 .90 .06
18 .54 .69 .79 .83 .88 .86 .06
Table 3. Accuracy for different number of candidate authors and number
of known texts per author. Expected accuracy of random attribution is
also informed. Accuracy decreases with increasing number of authors
and decreasing number of training texts per author.
the average linkage hierarchical clustering dendrogram [21] of the au-
thor profiles (T and M) and the eleven unknown texts. Relative entropy
(8) is used as a dissimilarity measure. Two different clusters arise, corre-
sponding to the two authors. This means that in average two texts by the
same author are not further apart than 0.06 but two texts from different
authors are at a distance greater than 0.09.
The second numerical experiment varies the number of authors, see
Table 2, as well as the number of known texts per author. The corpus
of texts analyzed can be found in [20]. The text lengths vary from just
over 4,000 to 100,000 words each. Texts longer than this were truncated
to this maximum word count. The accuracy obtained can be observed in
Table 3. E.g. focus on the 92% accuracy of the attribution with 4 authors
and 2 known texts per author. To understand the source of this accuracy
value, consider the first four authors in Table 2, these are Shakespeare,
Twain, Austen, and Allen. Take 2 of their texts as known. In this way,
there are 8+7+5+5 = 25 unknown texts to attribute among these four
authors. The accuracy of 92% indicates that 23 out of the 25 texts were
correctly attributed by our method. The expected accuracy of random
attribution is also informed in the last column of the table. The consistent
difference between the accuracy of the proposed method and the one
corresponding to random attribution is an indicator that the relational
data in the MCs capture stylistic features of the authors.
The attribution between two authors in the first row of Table 3 is
done between Shakespeare and Twain, who lived more than two cen-
turies apart. Perfect accuracy is achieved with one known text from each
author. This hints that little information is needed to distinguish between
authors with marked differences in writing styles. Moreover, based on 2
known texts per author, the method can distinguish with maximum accu-
racy between three authors, these are Shakespeare, Twain, and Austen.
For 3 known texts, the perfect attribution holds for 5 authors and for 6
known texts the method can correctly attribute the texts among 8 authors.
The accuracy is deteriorated by increasing the number of candidate au-
thors. For example, if we fix the known texts per author to be 3, then by
increasing the number of candidates authors from 4 to 16 the accuracy is
reduced from 100% to 85%. Furthermore, the accuracy increases when
the number of known texts per author is increased. E.g., fixing the num-
ber of authors as 8, if we go from 1 training text to 6, we increase the
accuracy from 67% to 100%. In columns with higher number of known
texts, the accuracy deteriorates with the incorporation of more authors
2 4 6 8 10 12 14
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Function Words
A
cc
ur
ac
y
 
 
WAN
l1 freq
SVM freq
WAN + l1 freq
SVM(200)
WAN + SVM(200)
Fig. 2. Accuracy as a function of function words for a problem with 8
authors and 4 known texts per author [20]. The accuracy of the WAN
method developed is higher than that of a frequency based SVM and a
frequency based l1 norm for this range of function words. The accuracy
of the combined methods is higher than that of their constituent meth-
ods indicating that relational and frequency based information encode
different stylistic features.
at a lower rate. This indicates that when considering more known texts,
a more reliable relational profile is built for each author, improving the
accuracy of the attributions.
4.1. Comparison and Combination with Existing Methods
The method developed correctly attributes both anonymous texts in the
Authorship Simulations proposed in [22] where 20 books are considered
from 8 different authors. Furthermore, we present Fig. 2 where we
depict the accuracy of a number of methods as a function of the amount
of function words considered. This experiment is done for a pool of
8 authors with 4 known texts per author [20]. The l1 method consists
in generating normalized frequency patterns of function words for each
author from the known texts. A given unknown text is attributed to the
author with minimal l1 distance to the frequency vector of such text. The
support vector machine (SVM) method is a refinement of the l1 method.
In the former, we start by applying a lineal, one-against-all SVM filter,
i.e. we undertake a binary attribution between an author and every other
author considered together. If the text is attributed to the single author,
this author passes the filter. The l1 method is used to decide in case
multiple candidates pass the filter. The method developed in this paper
outperforms the l1 and the SVM based methods for this range of function
words; see Fig 2. In fact, the SVM method based on 200 function words
achieves the same accuracy obtained by the WAN method here described
with 11 words. For low number of function words, almost 70% of the
texts are correctly classified by the WAN method when focusing on the
relation between the words the, and, and a.
When normalizing the word adjacency networks (7), purely rela-
tional information is retained. It has been critiqued that in methods that
combine frequency with relational data [22], the source of the attribu-
tion accuracy is unclear [23]. By relying exclusively on relational data
we settle this dispute. However, frequency data is useful and a more ac-
curate method would be one that combines both sources of information.
We present two ways of combining them.
The first one is by measuring dissimilarities between texts as the
sum of the relative entropy measure (8) and a frequency based measure,
e.g. the l1 norm of the frequency vectors. Fig. 2 shows that the combina-
tion achieves a higher accuracy than both methods considered separately.
For example, for 8 function words the WAN and the l1 methods have ac-
curacies of 68% and 57% respectively while their combination has an
accuracy of 79%.
Another possible combination is to apply a frequency SVM filter as
0 0.02 0.04 0.06 0.08 0.1 0.12 0.14
0
50
100
150
200
250
300
350
400
Relative Entropy
B
irt
h 
D
at
e 
D
iff
er
en
ce
 (
Y
ea
rs
)
Fig. 3. Relative entropy between author profiles as a function of the
year difference between birth dates for 9 authors born between 1564 and
1915 [20]. The increasing tendency of the scattered points indicates that
relational structures encode literature style features common to contem-
porary authors.
the one described for the pure SVM method based on a high number of
function words, e.g. 200, and then choose among the candidates that
pass the filter using a relational measure such as relative entropy (8).
Figure 2 shows that this combined method achieves the highest accuracy
among the methods considered with a maximum of 93% for 11 func-
tion words. Both combined methods show that the mixture of relational
and frequency data yields higher accuracy than both information sources
considered separately. This indicates that frequency and relational data
encode different stylistic features.
The accuracy decreases when shorter texts are considered. We re-
peat the experiment in Fig. 2 with 500 word extracts of the previously
utilized texts. The best WAN accuracy is 36% and is achieved for a net-
work with 6 function words. This is approximately three times the ex-
pected accuracy of random attribution among 8 authors. The best SVM
accuracy is also 36%. However, the accuracy when 200 function words
are considered is 25%. Nevertheless, when considering the WAN+SVM
method proposed, the combined accuracies of 25% and 36% yield a to-
tal accuracy of 43%. This reinforces the idea that both methods rely on
complementary information.
4.2. Temporal Profiling
In Fig. 3 we depict the relative entropy, i.e. the dissimilarity between
authorial styles given by our method, as a function of the year differ-
ence between the birth dates of the authors. The positive correlation
observed hints that the historical period has a direct influence on the
authorial style, even when considering content independent data as the
use of function words. Therefore, we can use this authorship attribution
method to estimate the period of time when the author of a given text
lived. Profiling studies can be expanded to consider other characteristics
such as gender and nationality.
5. CONCLUSION
An authorship attribution method based on relational data between func-
tion words was developed. Normalized word adjacency networks were
used as relational structures. These networks were interpreted as Markov
chains in order to facilitate their comparison using entropy measures.
The accuracy of the method developed for long texts was presented us-
ing an ad-hoc corpus, comparisons with existing methods and an ap-
plication in temporal profiling. Further, it was shown that an increase
in accuracy can be achieved through combination with frequency based
methods. Thus, unveiling the fact that relational and frequency based
methods capture different aspects of stylometric information.
6. REFERENCES
[1] P. Juola, “Authorship attribution,” Foundations and Trends in In-
formation Retrieval, vol. 1, pp. 233–334, 2006.
[2] E. Stamatatos, “A survey of modern authorship attribution meth-
ods,” Journal of the American Society for Information Science and
Technology, vol. 60, pp. 538–556, March 2009.
[3] T. C. Mendenhall, “The characteristic curves of composition,” Sci-
ence, vol. 9, pp. 237–246, 1887.
[4] G. U. Yule, “On sentence-length as a statistical characteristic of
style in prose: With application to two cases of disputed author-
ship,” Biometrika, vol. 30, pp. 363–390, 1939.
[5] F. Mosteller and D. Wallace, “Inference and disputed authorship:
The federalist,” Addison-Wesley, 1964.
[6] J. F. Burrows, “an ocean where each kind...: Statistical analysis
and some major determinants of literary style,” Computers and the
Humanities, vol. 23, pp. 309–321, 1989.
[7] D. I. Holmes and R. S. Forsyth, “The federalist revisited: New
directions in authorship attribution,” Literary and Linguistic Com-
puting, vol. 10, pp. 111–127, 1995.
[8] R. S. Forsyth and D. I. Holmes, “Feature-finding for test classifi-
cation,” Literary and Linguistic Computing, vol. 11, pp. 163–174,
1996.
[9] G. U. Yule, “The statistical study of literary vocabulary,” CUP
Archive, 1944.
[10] D. I. Holmes, “Vocabulary richness and the prophetic voice,” Lit-
erary and Linguistic Computing, vol. 6, pp. 259–268, 1991.
[11] F. J. Tweedie and R. H. Baayen., “How variable may a constant
be? measures of lexical richness in perspective,” Computers and
the Humanities, vol. 32, pp. 323–352, 1998.
[12] D.L. Hoover, “Another perspective on vocabulary richness,” Com-
puters and the Humanities, vol. 37, pp. 151–178, 2003.
[13] M. Koppel, N. Akiva, and I. Dagan, “Feature instability as a crite-
rion for selecting potential style markers,” Journal of the American
Society for Information Science and Technology, vol. 57, pp. 1519–
1525, September 2006.
[14] D. Cutting, J. Kupiec, J. Pedersen, and P. Sibun, “A practical part-
of-speech tagger,” Proceedings of the third conference on Applied
Natural Language Processing, pp. 133–140, 1992.
[15] D. V. Khemelev and F.J. Tweedie, “Using markov chains for iden-
tification of writers,” Literary and linguistic computing, vol. 16,
pp. 299–307, 2001.
[16] O. V. Kukushkina, A. A. Polikarpov, and D. V. Khmelev, “Using
literal and grammatical statistics for authorship attribution,” Prob-
lems of Information Transmission, vol. 37, pp. 172–184, 2001.
[17] C. Sanderson and S. Guenter, “Short text authorship attribution
via sequence kernels, markov chains and author unmasking: An
investigation,” Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing, vol. 37, pp. 482–491,
2006.
[18] R. Quirk, S. Greenbaum, G. Leech, and J. Svartvik, “A compre-
hensive grammar of the english language,” Longman, 1985.
[19] G. Kesidis and J. Walrand, “Relative entropy between markov tran-
sition rate matrices,” IEEE Trans. Information Theory, vol. 39, pp.
1056–1057, May 1993.
[20] S. Segarra, M. Eisen, and A. Ribeiro, “Compila-
tion of texts used for the numerical experiments,”
https://fling.seas.upenn.edu/ maeisen/wiki/index.php?n=Main.
TextAttribution.
[21] D. Müllner, “Modern hierarchical, agglomerative clustering algo-
rithms,” arXiv:1109.2378, September 2011.
[22] D.L. Hoover, “Frequent collocations and authorial style,” Literary
and Linguistic Computing, vol. 18, 2003.
[23] S. Argamon and S. Levitan, “Measuring the usefulness of func-
tion words for authorship attribution,” Proceedings of the 2005
ACH/ALLC Conference, 2005.
