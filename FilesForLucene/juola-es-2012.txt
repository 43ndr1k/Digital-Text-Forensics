Full Terms & Conditions of access and use can be found at
http://www.tandfonline.com/action/journalInformation?journalCode=nest20
Download by: [University of Aegean] Date: 20 August 2016, At: 05:37
English Studies
ISSN: 0013-838X (Print) 1744-4217 (Online) Journal homepage: http://www.tandfonline.com/loi/nest20
Large-Scale Experiments in Authorship Attribution
Patrick Juola
To cite this article: Patrick Juola (2012) Large-Scale Experiments in Authorship Attribution,
English Studies, 93:3, 275-283, DOI: 10.1080/0013838X.2012.668792
To link to this article:  http://dx.doi.org/10.1080/0013838X.2012.668792
Published online: 22 May 2012.
Submit your article to this journal 
Article views: 209
View related articles 
Large-Scale Experiments in Authorship
Attribution
Patrick Juola
Despite the importance (and recent research attention) of authorship attribution as a
scholarly problem, the proposed solutions are at best a collection of ad-hoc and mutually
incompatible methods and at worst simply a muddle. While most methods are better
than chance, there is little understanding of which ones are substantially better or, more
importantly, of why some methods outperform. This paper describes some large-scale
experiments directly comparing hundreds or in some cases millions of different methods
to determine whether there are significant and reliable performance differences. The
results of these experiments are shown in the hope of finding useful best practices for
further experimentation.
1. Introduction
Authorship attribution is an important problem that has received substantial research
attention in recent years. This attention, however, has not created a consensus on the
best way to approach it. Quite the opposite; there are probably more ways proposed
today than there have ever been before. More annoyingly, most of the proposed
methods ‘‘work’’ if one sets the bar low enough. But, as the old joke has it, a hundred
fools are not equal to one wise man—and similarly, a hundred unreliable analyses are
not likely to yield a gold standard. In this paper, we survey the state of the art and
present the results of some large-scale experiments in search of that elusive and
probably illusionary gold standard.
2. Problem Statement
Stylometric authorship attribution—assessing the author of a document by statistical
analysis of its contents—has its origins in the nineteenth century,1 but has
experienced tremendous resurgence since the work of Frederick Mosteller and David
L. Wallace and the beginnings of the corpus revolution. With the exponential growth
of digital-only texts and the increasing need to validate or test the legitimacy of
Patrick Juola is affiliated with Duquesne University, USA. Email: juola@mathcs.duq.edu
1De Morgan; Mendenhall.
English Studies
Vol. 93, No. 3, May 2012, 275–283
ISSN 0013-838X (print)/ISSN 1744-4217 (online)  2012 Taylor & Francis
http://dx.doi.org/10.1080/0013838X.2012.668792
questioned digital documents, there is an obvious need for proven techniques and
best practices with good track records.
Unfortunately, the current state of affairs does not present this. Instead, as several
recent surveys have shown,2 we have a collection of thousands or millions of different
techniques, most of which work (in the sense of better than chance) but there are no
clear front-runners. Indeed, Joseph Rudman suggested more than a decade ago that
more than one thousand different feature sets had already been proposed.3
The basic question is one of reliability. We can agree that generally, a person who
writes of ‘‘lorries’’ is not the same as one who writes of ‘‘trucks’’, or that a person who
uses words like ‘‘contrapuntal’’ probably has more than a grammar school education.
Indeed, similar observations can claim to be the basis for the entire field, as evidenced
by the suggestion of Augustus de Morgan, that a correspondent interested in the
authorship of the Gospels should ‘‘try to balance in your own mind the question
whether the latter [text] does not deal in longer words than the former [text]. It has
always run in my head that a little expenditure of money would settle questions of
authorship this way. [. . .]Some of these days spurious writings will be detected by this
test.’’4 This idea is at least superficially plausible, in that authors with large
vocabularies may typically use longer words. Unfortunately, studies, including those
of M. W. A. Smith,5 have shown that average word length is neither stable within a
single author, nor does it distinguish between authors. In Smith’s words, cited by
David I. Holmes, ‘‘Mendenhall’s method now appears to be so unreliable that any
serious student of authorship should discard it.’’6
What is left, after we discard the ‘‘unreliable’’? And what is the ‘‘unreliable’’ to
discard? What do other methods look like?
3. Some Typical Examples
The most commonly cited stylometric study is probably that of Mosteller and
Wallace, who examined the frequency of appearance of approximately thirty function
words within the collection of documents known as The Federalist Papers.7 Using a
form of Bayesian analysis, they were able to show significant differences among the
various authors in their use of these words and hence infer the probabilities that each
document had been written by each author—that is, infer authorship. Another classic
experiment in this field is the study of the Oz books by José Nilo G. Binongo, where
he applied principal component analysis (PCA) to the frequencies of the fifty most
frequent words in these books and was able to demonstrate (via the first two principal
2Koppel, Schler, and Zigdon; Juola, ‘‘Authorship Attribution’’; Argamon et al.; Koppel, Schler, and Argamon;
Stamatatos; Jockers and Witten.
3Rudman.
4De Morgan.
5Smith.
6Holmes.
7Mosteller and Wallace.
276 P. Juola
components) a clear visual separation between the books written by Baum and those
written later by Thompson.8
These studies, while excellent in themselves, also illustrate the issue at hand; the
results cannot be directly compared, as they use not only different corpora, but also
different pre-processing, different feature sets, different numbers of features, and
different classification technologies. It is not clear that we can expect the same
accuracy from other analyses using these techniques in other settings, and
furthermore, that we can expect much worse results when using, for example,
Mosteller/Wallace’s list of (English) function words to analyze documents in
French.
We can, however, discern a prototypical or ur-experiment in this setting. A
‘‘typical’’ authorship attribution experiment might run as follows. After selecting a
questioned document of interest and collecting sample documents from each of
the candidate authors under consideration, one performs the following steps:
. pre-process the documents to convert them into canonical form (‘‘canonicize’’);
. extract features or events from each document;
. cull the features/events to a reasonable number depending upon accuracy
considerations as well as performance measures (e.g. PCA requires creating and
inverting a document-by-feature matrix, which can be huge);
. apply a classification technique to determine which candidate author wrote
the questioned document. More sophisticated analysis can determine
probabilities and confidence measures, or possibly decide that ‘‘none of the
above’’ wrote it.
4. JGAAP and the AAAC
This ur-experiment lends itself easily to formalization, and the JGAAP (Java
Graphical Authorship Attribution Program) system does just that.9 JGAAP is a
modular, Java-based program in which each of the phases above is instantiated in an
abstract class which can be instantiated in any of multiple ways.10 JGAAP itself
provides dozens of pre-built classes to cover many proposed methods or components
for authorship attribution. These classes can in turn be combined as discussed below
to allow a huge number of potential experiments.
In addition, JGAAP is distributed with a standard corpus derived from our earlier
AAAC (Ad-hoc Authorship Attribution Competition) corpus,11 a collection of
problems designed to cover a range of authorship attribution problems includ-
ing different languages, language families, genres, sizes, and number of attributes.
Originally posed as a test corpus for a TREC-like competition (Text Retrieval
8Binongo.
9Juola, ‘‘Authorship Attribution,’’ ‘‘20,000 Ways.’’
10Juola et al.; Juola, Sofko, and Brennan.
11Juola, ‘‘Ad-hoc Authorship Attribution Competition.’’
Large-Scale Experiments in Authorship Attribution 277
Conference), this corpus has also served as a base for other comparative experiments.
Most notably for this paper, John Noecker Jr and Patrick Juola used this corpus to
show that despite the high overhead associated with state of the art Support Vector
Machines, they did not have higher accuracy than simpler and faster classifiers such
as linear discriminant analysis (LDA) or simple nearest neighbour classifiers.12
5. Early Large-Scale Experiments
As of January 2012 the JGAAP source code (available at www.evllabs.com for free
download) contains nine different canonicizers, creating at least five hundred
different possible canonicization methods. Similarly, it contains more than thirty-
five different event set generators,13 several different flexible cullers, and more than
fifty different classification methods. Simple math yields more than 50063566650
different analyses—more than five million potential stylometric algorithms.
Having this many algorithms enables massive, large-scale testing. For example,
suppose one is interested in authorship attribution in a novel and written in a
potentially less studied language. Of the thirteen AAAC problems, eight are in
English, but five involved other languages (French [twice], Dutch, Latin, and Serbian/
Slavonic). Juola analyzed 281 separate experiments of the AAAC and discovered that
the better and more accurately a method performed on the English problems, the
better it also did on non-English data.14 (More formally, the correlation was 0.6680,
p5 0.001, r2¼ 0.4462.) This finding strongly suggests that the best place to start
looking for authorship methods in obscure languages is with the ones we know work
well for well-studied ones—but this result can only be inferred when we have looked
at many methods, the good as well as the bad and the ugly.
But this only postpones the inevitable question of which methods are the good
ones. Fortunately, the same large-scale research scheme can be applied to this
question. Juola describes the results of more than twenty thousand different analyses
of the AAAC corpus, but fundamentally fails to find any magic bullet.15 Essentially,
the ‘‘average’’ performance of any component across all configurations was roughly
the same. That is, if you focus, for example, on questions like ‘‘does it improve or
worsen performance if you strip all the punctuation out of a document?’’ the answer
seems to be ‘‘it doesn’t really do anything’’. In some situations, it helps, while in other
situations, it does not.
This seems surprising given previous publications showing that some specific
methods (e.g. word length) are highly unreliable. If word length is so unreliable, but
12Noecker; Noecker and Juola.
13This number is a little vaguer, as some of the generators are parameterizable and may themselves yield many
different options. For example, the same generator can create word 2-grams, word 3-grams, word 4-grams . . .
and word 61-grams if anyone were fool enough to want those. Thirty-five will do as a basis for calculations.
14Juola, ‘‘Mixture-of-experts.’’
15Juola, ‘‘20,000 Ways.’’
278 P. Juola
function words are as useful as the findings in Binongo suggest, why is there no clear-
cut difference between the two on large-scale tests?
6. Vescovi’s Results
A more detailed analysis was presented by Darren M. Vescovi. Vescovi analyzed ten
thousand randomly chosen JGAAP analyses of AAAC problem A.16 This problem
consisted specifically of essays written by students in a first-year English class at
Duquesne University. Unlike the previous study, Vescovi applied a more sophisti-
cated statistical analysis (logistic regression), and for the first time was able to show
significant differences among the accuracies of individual JGAAP components. For
instance:
. Stripping punctuation and/or numbers from the essays reduced accuracy.
. Normalizing case (i.e. converting all characters to lower case) increased accuracy.
. Using only length-limited words as event sets (e.g. all words with two or three
letters in them) uniformly performed poorly.
. The best performance was obtained by simple character frequency sets.
. Word length performed surprisingly well (about in the middle of the event sets
studied), better than many other feature sets that have been recommended in the
literature.
. As suggested by Noecker and Juola, nearest neighbour algorithms (find the
document closest in style to the unknown document and infer the unknown
document was written by the same author) worked better than many more
complex inference mechanisms, and in fact, topped the charts.
For the first time, we were also able to see what might be termed ‘‘clinical’’ or
practical differences between and among different methods. For example, a rather
complex analytic technique called ‘‘Markov Chain Analysis’’ was only about a quarter
as accurate as the nearest neighbour algorithm using the simple Euclidean distance
formula. Nearest neighbour with Euclidean distance, in turn, was only three-fifths as
accurate as another, less commonly used distance called ‘‘normalized cosine
distance’’ or ‘‘dot-product distance’’.
7. Best Practices
The real lesson from these experiments, however, seems to be that good components
are not good in isolation, and it is the combination of compatible components that
make actual best practices. It is a simple matter to run large-scale experiments and
equally simple to see which of the twenty thousand or so tested methods performs
best and to what degree.
16Vescovi.
Large-Scale Experiments in Authorship Attribution 279
In as yet unpublished results, we have done this on what we believe to be the
largest single collection of authorship attribution experiments. Using JGAAP, we
have analyzed the AAAC corpus a total of 3,169,296 times using a combination of
canonicizers, event sets, event cullers, and distance-based methods (ignoring non-
distance-based analysis methods like Markov Chain Analysis). For reasons of
space, we obviously cannot present the full results, or even a substantial fraction
of them.
We can, however, present the top method. The best method was an eight-way tie
among essentially identical methods:
. canonicizers: unify case, strip alphanumeric, punctuation separator
. event set: characters
. event culler: [none]
. analysis method: nearest neighbour
. distance: Manhattan or nominal Kolomogorov-Smirnoff (KS) distance.17
These individual components also tended to be well represented among good-
performing analyses. For example, forty of the top fifty methods used Manhattan/KS
distance, forty-one of the top fifty used unify case, and in stark contrast to the
methods of studies like Mosteller and Wallace or Binongo none of them used words
as the features of interest or eliminated punctuation.
It thus seems that large-scale competitive testing is a useful way to look at the
authorship attribution task, as it has (at least in this instance) shown that the best
results are coming from a most unlikely and understudied combination.
There are several potential explanations (and confounds) for this apparently
nonsensical finding. Examination of the corpus itself, especially problem A (also
studied by Vescovi) shows that there is in fact a huge amount of variation in the
usage of punctuation, especially quotes. Students vary in the amount of quotations
used, in the amount of scare quotes used, and, most importantly, in the style of
quotes used. The differences among ‘example’, ‘‘example’’, and ‘ ‘example’ ’ are
salient, as are the differences among ASCII apostrophes, backticks, primes, and single
quotes. Other examples include the differences among ASCII double quotes, ditto
marks, opening and closing double quotes. Greg Baker gives many examples of such
subtle variations.18
Much of this difference, then, is due to differences in curation rather than
authorship—a ‘‘proper’’ editor would have normalized these differences. In this case,
we may need to look to other methods.
17This distance is a formalization of having to walk along the grid of a Manhattan-like city instead of cutting
through blocks. If you are at 20th Street and 3rd Avenue, and want to go to 23rd Street and 7th Avenue, you are
only five blocks away ‘‘as the crow flies,’’ but you will have to walk seven blocks, three along 20th Street and then
four along 7th Avenue, to get there.
18Baker.
280 P. Juola
8. Mixture of Experts
The fundamental problem, of course, is that we have still not found the magic bullet
that solves the entire authorship problem. Even the top-scoring method studied may
not be accurate enough to use in a court setting. Is there some way to boost
performance using these flawed and unreliable methods?
At least in theory, there is. The key insight is that, while these methods are
unreliable, they do work—in the sense that they are right more often than they are
wrong. By combining analyses in a technique called ‘‘mixture of experts’’, we may be
able to boost accuracy over the levels of the individual experiments. The math is
simple. As an analogy, consider a coin biased to land heads 55 per cent of the time.
Any individual coin flip is hard to predict, but over time (and enough trials), heads
are extremely likely to win out in the long run. With only fifteen flips, the odds of
getting a majority (eightþ) heads is about 65 per cent, much better than the chance
of getting heads on a single flip. With fifty-one flips, the odds are better than three to
one (76 per cent) of getting a majority of heads. Five hundred and one flips would
show a majority of heads more than 98 per cent of the time.
Similarly, if each analysis could only get the right answer 55 per cent of the time,
running fifty-one analyses and picking the most common answer would get the right
answer three-quarters of the time, and 501 analyses would be right almost all the time.
There are, of course, a few caveats here, most notably that the coin flips are independent;
that is, that the outcome of one coin flip does not depend on the output of any other.
Can this method be used for authorship attribution? Results, unfortunately, are
mixed. We showed that reanalysis of the top-performing results presented in the AAAC
will, in fact, perform better—for example, the majority vote of the top five performers
was more accurate than any of the top five individually.19 On the other hand, large-scale
experiments using the JGAAP set-up have not been able to reproduce this success. In
2011, for example, we have analyzed three disputed Elizabethan plays, ostensibly by
Thomas Kyd, using more than 250 different JGAAP-based methods.20 Unfortunately,
the results were not positive, and in fact were not even negative. The hoped for
distribution of one dominant author arising from the votes of 250 independent analyses
did not emerge. We attribute this to the lack of independence. Any two analyses of, for
example, the distribution of punctuation in two documents will produce similar results
regardless of the specifics of the analysis. The number of different and independent
analyses is, therefore, more limited than the JGAAP combinatorics suggest.
9. Conclusions
Despite the limitations of the previous section, we are placed to make formal
recommendations about best practices in distance-based authorship attribution.
19Juola, ‘‘Mixture-of-experts.’’
20Juola, ‘‘Fishing the Ocean.’’
Large-Scale Experiments in Authorship Attribution 281
Specifically, our results show clearly that for this corpus, and arguably for most self-
curated electronic documents, punctuation-based features, evaluated using the
Manhattan distance on their frequency, should be considered as the best practices
among those tested. This should be of particular interest to those with a forensic
application in mind.
From a research perspective, though, this suggests that more work should be done
on the theory of authorship attribution. Techniques such as mixture-of-experts can
be used to help boost performance when the analyses are truly independent, but
despite the huge number of different-in-detail analyses available, there are relatively
few independent things that can be analyzed. What may be needed is a better theory
of linguistic variation, perhaps akin to the principles and parameters approach that
has been successful in describing differences between languages. We know of
theoretical syntactic parameters such as head directionality and pro-dropping. We
know that English is ‘‘head-initial’’ while Japanese is ‘‘head-final’’ (as is German). We
also know that there are atheoretical differences (such as vocabulary) between
individual users of language (one person may call it a ‘‘couch’’, another a ‘‘sofa’’). Are
there interesting theoretically motivated parameters that could describe these micro-
variations?
Even if this suggested principles and parameters approach fails, such a search for
such parameters is likely to give us a much better understanding of what type of
variation in language occurs and how to measure it. This should help improve
authorship attribution at the practical level as well.
Acknowledgments
This material is based upon work supported by the National Science Foundation
under Grant Numbers OCI-0721667 and OCI-1032683. Any opinions, findings, and
conclusions or recommendations expressed in this material are those of the author(s)
and does not necessarily reflect the views of the National Science Foundation. The
error mine, the thanks theirs.
References
Argamon, Shlomo, Moshe Koppel, James W. Pennebaker, and Jonathan Schler. ‘‘Automatically
Profiling the Author of an Anonymous Text.’’ CACM 52, no. 2 (2009): 119–23.
Baker, Greg. ‘‘Commonly Confused Characters.’’ [cited 6 January 2012] Available from http://
www.cs.sfu.ca-/_ggbaker/reference/characters/.
Binongo, José Nilo G. ‘‘Who Wrote the 15th Book of Oz? An Application of Multivariate Analysis
to Authorship Attribution.’’ Chance 16, no. 2 (2003): 9–17.
De Morgan, Augustus. ‘‘Letter to Rev. Heald 18/08/1851.’’ In Memoirs of Augustus de Morgan by his
Wife Sophia Elizabeth de Morgan with Selections from his Letters, edited by Sophia Elizabeth
de Morgan. London: Longmans, 1851.
Holmes, David I. ‘‘Authorship Attribution.’’ Computers and the Humanities 28, no. 2 (1994): 87–
106.
282 P. Juola
Jockers, Matthew L., and Danuela M. Witten. ‘‘A Comparative Study of Machine Learning
Methods for Authorship Attribution.’’ Literary and Linguistic Computing 25, no. 2 (2010):
215–23.
Juola, Patrick. ‘‘Ad-hoc Authorship Attribution Competition.’’ In Proceedings of the 2004
Joint International Conference of the Association for Literary and Linguistic Computing and
the Association for Computers and the Humanities (ALLC/ACH 2004). Göteborg, 2004.
———. ‘‘Authorship Attribution.’’ Foundations and Trends in Information Retrieval 1, no. 3 (2006):
233–334.
———. ‘‘Authorship Attribution: What Mixture-of-experts Says We Don’t Yet Know.’’ In
Proceedings of the American Association for Corpus Linguistics. Provo, Utah, 2008.
———. ‘‘20,000 Ways Not to do Authorship Attribution and a Few that Work.’’ In Proceedings of
2009 Biennial Conference of the International Association of Forensic Linguists (IAFL-09).
Amsterdam, 2009.
———. ‘‘Fishing the Ocean: Lessons from Large-scale Experiments in Stylometry.’’ Talk given at
School for Advanced Studies, 29 March, 2011.
Juola, Patrick, John Noecker Jr., Mike Ryan, and Sandy Speer. ‘‘JGAAP 4.0—A Revised Authorship
Attribution Tool.’’ In Proceedings of Digital Humanities 2009. College Park, Md., 2009.
Koppel, Moshe, Jonathan Schler, and Shlomo Argamon. ‘‘Computational Methods in Authorship
Attribution.’’ Journal of the American Society for Information Science and Technology 60, no. 1
(2009): 9–26.
Koppel, Moshe, Jonathan Schler, and Kfir Zigdon. ‘‘Determining an Author’s Native Language by
Mining a Text for Errors.’’ In Proceedings of the 11th ACMSIGKDD International Conference
on Knowledge Discovery and Data Mining. New York: ACM, 2005.
Mendenhall, T. C. ‘‘The Characteristic Curves of Composition.’’ Science 9, no. 214 (1887): 237–49.
Mosteller, Frederick, and David L. Wallace. Inference and Disputed Authorship: The Federalist.
Reading, Mass.: Addison-Wesley, 1964.
Noecker, John Jr. ‘‘An Empirical Study of Linear Separability on Authorship Attribution Feature
Spaces.’’ In Proceedings of the 2008 Colloquium for Digital Humanities in Computer Science.
Chicago, 2008.
Noecker, John, Jr., and Patrick Juola. ‘‘Cosine Distance Nearest-neighbor Classification for
Authorship Attribution.’’ In Proceedings of Digital Humanities 2009. College Park, Md., 2009.
Rudman, Joseph. ‘‘The State of Authorship Attribution Studies: Some Problems and Solutions.’’
Computers and the Humanities 31, no. 4 (1998): 351–65.
Smith, M. W. A. ‘‘Recent Experiences and New Developments of Methods for the Determination of
Authorship.’’ Association of Literary and Linguistic Computing Bulletin 11 (1983): 73–82.
Stamatatos, Efstathios. ‘‘A Survey of Modern Authorship Attribution Methods.’’ Journal of the
American Society for Information Science and Technology 60, no. 3 (2009): 538–56.
Vescovi, Darren M. Best Practices in Authorship Attribution of English Essays. M.A. thesis, Duquesne
University, 2011.
Large-Scale Experiments in Authorship Attribution 283
