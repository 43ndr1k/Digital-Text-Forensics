1
3
4
5
6
7
8
9
1 1
12
13
14
15
16
17
18
19
2 0
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
Expert Systems with Applications xxx (2015) xxx–xxx
ESWA 9827 No. of Pages 12, Model 5G
9 February 2015Contents lists available at ScienceDirect
Expert Systems with Applications
journal homepage: www.elsevier .com/locate /eswaEnsemble learning methods for pay-per-click campaign managementhttp://dx.doi.org/10.1016/j.eswa.2015.01.047
0957-4174/ 2015 Elsevier Ltd. All rights reserved.
⇑ Corresponding author at: Department of Business Information Technology,
Virginia Polytechnic Institute and State University, Blacksburg, VA, USA. Tel.: +1 540
231 6596.
E-mail addresses: michael.king@vt.edu (M.A. King), abra@vt.edu (A.S. Abra-
hams), cliff.ragsdale@vt.edu (C.T. Ragsdale).
Please cite this article in press as: King, M. A., et al. Ensemble learning methods for pay-per-click campaign management. Expert Systems with Appl
(2015), http://dx.doi.org/10.1016/j.eswa.2015.01.047Michael A. King ⇑, Alan S. Abrahams, Cliff T. Ragsdale
Pamplin College of Business, Virginia Polytechnic Institute and State University, Blacksburg, VA, USA
a r t i c l e i n f o a b s t r a c t21
22
23
24
25
26
27
28Article history:
Available online xxxx
Keywords:
Sponsored search
Pay-per-click advertising
Classification
Ensemble modeling29
30
31
32
33
34
35
36Sponsored search advertising has become a successful channel for advertisers as well as a profitable busi-
ness model for the leading commercial search engines. There is an extensive sponsored search research
stream regarding the classification and prediction of performance metrics such as clickthrough rate,
impression rate, average results page position and conversion rate. However, there is limited research
on the application of advanced data mining techniques, such as ensemble learning, to pay per click cam-
paign classification. This research presents an in-depth analysis of sponsored search advertising cam-
paigns by comparing the classification results from four base classification models (Naïve Bayes,
logistic regression, decision trees, and Support Vector Machines) with four popular ensemble learning
techniques (Voting, Boot Strap Aggregation, Stacked Generalization, and MetaCost). The goal of our
research is to determine whether ensemble learning techniques can predict profitable pay-per-click cam-
paigns and hence increase the profitability of the overall portfolio of campaigns when compared to stan-
dard classifiers. We found that the ensemble learning methods were superior classifiers based on a profit
per campaign evaluation criterion. This paper extends the research on applied ensemble methods with
respect to sponsored search advertising.
 2015 Elsevier Ltd. All rights reserved.3758
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
771. Introduction
Search engines are an indispensable tool for interacting with
the World Wide Web (WWW) and have long provided user value,
from the earliest universal resource locator (URL) directories to
present day highly optimized query results. Companies competing
in the search engine industry have slowly monetized their early
search innovations and have created sustainable business models
by providing the business community with an advertising channel
called sponsored search. Internet advertising is nearing a $42.78
billion dollar industry as reported by the Interactive Advertising
Bureau (IAB Internet Revenue Report, 2013). The three search
engine industry leaders, Google, Yahoo!, and Bing, who hold a
combined market share of over 96% (comScore, 2014), each offer
a competitive sponsored search platform. Search engine providers
are acutely aware of user search behavior and the associated mar-
keting value of the page location of search results (Haans, Raassens,
et al., 2013). Search engines allow any individual or company to
submit a URL for advertising purposes so it can be indexed and
then made available for retrieval. Search engines call this78
79
80
81
82submission process organic search and provide the service free.
However, the probability of a search engine listing a specific URL
for an advertiser’s landing page in the top display section is quite
low, even with a search engine optimized landing page (Jansen &
Mullen, 2008). The statistics in Exhibit 1 are often quoted as sup-
port for sponsored search.
In contrast to organic search, sponsored search advertising is
more complex, but offers the potential of high return on invest-
ment (Moran & Hunt, 2009). In sponsored search, advertisers first
bid on keywords offered by the search engines and after a keyword
is acquired, their advertisements associated with the keyword are
displayed using proprietary ranking algorithms. Ad rankings typi-
cally take into account relevance to users’ search and keyword
bid amount offered by advertiser (Fain & Pedersen, 2005; Jansen
& Mullen, 2008). The three predominant advertisement billing
schemes used by search engines are pay-per-impression (PPM),
pay-per-click (PPC), and pay-per-action (PPA) (Mahdian & Tomak,
2008; Moran & Hunt, 2009). When using a PPM billing scheme,
advertisers are charged each time their ad is displayed, regardless
of whether the user clicks on the ad. Under PPC billing, the adver-
tiser is charged only when their ad or URL is clicked on, and with
PPA billing, the advertiser pays only when a user action such as a
sign-up or purchase occurs.
The research conducted for this article analyzed a large data set
of PPC advertisements placed on Google. Our data set was providedications
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
93% of directed traffic to websites is referred by search engines. Forrester Research, 2006
99% of Internet searchers do not look past the first 30 search results. Forrester Research, 2006
97% of Internet searchers do not look past the top three results. Forrester Research, 2006
65% of online revenue is generated by holders of the top three results. Forrester Research, 2006
Approximately 131 billion searches performed each month, globally. comScore, 2010
Exhibit 1. Search engine usage statistics.
2 M.A. King et al. / Expert Systems with Applications xxx (2015) xxx–xxx
ESWA 9827 No. of Pages 12, Model 5G
9 February 2015by an industry leading Internet marketing company (NAICS
518210) specializing in PPC campaign services, that managed a
campaign portfolio containing 8499 PPC campaigns for a multi-
billion dollar home security provider (NAICS 561612). The Internet
marketing company sells its online marketing services to its clients
and assumes all PPC related costs, while the advertising clients pay
on a PPA basis when purchases are made.
The objective for this study was to construct a set of classifica-
tion models, using a combination of data and text mining tech-
niques and ensemble learning techniques, that are capable of
classifying a new PPC advertisement campaign as either sufficiently
profitable or not. Profit is based on whether clicks per acquisition
are lower than the breakeven threshold for the advertised item with
an overall objective of maximizing total profit of the full portfolio of
initiated campaigns. An ‘‘acquisition’’ refers to the event of the
advertiser successfully selling a multi-year home security contract
to the user who clicked on the advertisement.
The remainder of this paper is organized as follows. Section 2
provides a literature review of related work. Section 3 discusses
our specific research questions and contributions. Methodology
and research design implementations are described in Section 4.
Section 5 provides a detailed discussion of the research results
while Section 6 presents managerial implications, research limita-
tions, and conclusion.155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
1872. Related work
This section describes related work in the fields of sponsored
search, advertisement content modeling, and data mining.
2.1. Sponsored search and search engine marketing
Advertisers are anxious to improve the success of sponsored
search listings (D’Avanzo et al., 2011). Various authors have stud-
ied prediction of sponsored search campaign success from text fea-
tures created from keywords or advertisement text. For example,
Jansen and Schuster (2011) investigate whether keywords associ-
ated with different phases of the buying funnel (Consumer Aware-
ness, Research, Decision, and Purchase) have different success
rates. A number of authors have attempted to determine whether
semantic features of keywords impact sponsored search success
(Rutz & Bucklin, 2007; Rutz, Trusov, et al., 2011; Shaparenko,
Çetin, et al., 2009).
2.2. Advertisement content modeling
Textual content, including that found in advertisements, can be
analyzed within a data or text mining context based on the
numeric representations of stylometric, sentiment, and semantic
features of the text (Abrahams, Coupey, et al., 2013; Aggarwal &
Zhai, 2012; Haans et al., 2013; Nielsen, Shapiro, et al., 2010).
Stylometrics describes the readability and stylistic variations of
a portion of text using numerous metrics such as characters per
word, syllables per word, words per sentence, number of word rep-
etitions, and Flesch Reading Ease (Sidorov, Velasquez, et al., 2014).
Tweedie, Singh, et al. (1996) and Ghose and Ipeirotis (2011)Please cite this article in press as: King, M. A., et al. Ensemble learning methods
(2015), http://dx.doi.org/10.1016/j.eswa.2015.01.047describe the value of stylometric modeling of text using several
machine learning techniques.
Sentiment content refers to the emotional or affective commu-
nication embedded in text. Feldman (2013) provides an overview
of business applications of sentiment analysis in areas such as con-
sumer reviews, financial market blogs, political campaigns, and
social media advertising. When studying the characteristics of an
advertisement, the representation and interpretation of its senti-
ment content is important because advertisements not only deliver
objective descriptions about branded products or services, but also
can induce measureable emotional reactions from current or
potential customers. Heath and Nairn (2005) proposes that adver-
tisements that approach readers’ feelings rather than knowledge
can be processed with low attention and can result in increased
buying behavior.
Semantics refers to the meaning of the text, such as the catego-
ries of items referred to in the text. Stone, Dunphy, et al., 1966
describe a semantic tagging method that extracts word senses
from text and classifies the words into concept categories.
Abrahams et al., 2013 illustrates how this type of sematic tagging
of advertisement content can be useful for audience targeting.
2.3. Data mining
Numerous data mining and machine learning techniques have
been used to create models that predict important sponsored
search performance metrics such as clickthrough rate, conversion
rate, and bounce rate. Logistic regression, Support Vector Machines
(SVM) and Bayesian models, as well as other techniques, have been
applied to these predictive problems. Search engines are primarily
concerned with modeling the click-through rate for both new and
ongoing ads, because revenue depends on their ability to rank PPC
ads relevant to searchers with as high a click-through rate as pos-
sible. In contrast, advertisers are more likely to focus on the con-
version rates associated with their PPC campaigns.
Richardson, Richardson, et al. (2007) argues that modeling
click-through rates for ads with known run times is a straight-
forward process, while modeling the click-through rate for a new
ad presents unique challenges. Because of the rapid growth in
the inventory of new PPC ads, along with the fast turnover of these
ads, the authors indicate that it has become increasingly difficult to
estimate plausible click-through rates based on historical data. The
authors present a logistic regression model fit using a feature set
created from the actual ad text and numeric attributes (e.g., land-
ing page, keywords, title, body, clicks and views). The authors dis-
cuss how their base logistic regression model was more accurate
than an ensemble of boosted regression trees.
Wang, Hao, et al. (2012) add to the research stream of ensemble
learning within a sponsored search context by introducing an
ensemble for click-through prediction. Wang et al., were team
members who participated in the 2012 KDD Cup (http://www.kdd-
cup2012.org/) and built an ensemble consisting of four base classi-
fiers: maximum likelihood estimation, online Bayesian probit
regression, Support Vector Machines, and latent factor modeling.
Feature creation is a reoccurring research theme within the context
of sponsored search because the unit of analysis, the PPC advertise-
ment, typically provides relatively few independent variables. Thefor pay-per-click campaign management. Expert Systems with Applications
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
M.A. King et al. / Expert Systems with Applications xxx (2015) xxx–xxx 3
ESWA 9827 No. of Pages 12, Model 5G
9 February 2015authors’ main contribution is their novel approach for combining
the prediction results from the four base classifiers by using a rank-
ing-based ensemble algorithm.
Ciaramita, Murdock, et al. (2008) maintain that commercial
search engines must be able to accurately predict if an ad is likely
to be clicked. These ongoing predictions help determine an esti-
mated click-through rate, which in turn, drives revenue for the
search engine. Using click stream data from a commercial search
engine, the author created three variants of the perceptron algo-
rithm: a binary perceptron, a ranking perceptron, and a multilayer
perceptron. Based on different feature sets developed from key-
word query logs and numeric representations of text attributes,
the authors show that the multilayer perceptron outperforms
both the binary and ranking perceptrons. Adding to the feature
creation research stream, the authors provide a discussion of fea-
ture creation from several novel numeric representations of ad
text.
Graepel et al. (2010) present a new online machine learning
algorithm for predicting click-through rates for Microsoft’s com-
mercial search engine Bing, called adPredictor. At the time of their
publication, adPredictor was supporting 100% of Bing’s sponsored
search traffic. AdPredictor is based on a general linear regression
model with a probit link function that predicts a binary outcome.
The authors indicate that their primary reasons for implementing
adPredictor were for speed and the scaling improvements.
Ghose and Yang (2008) take an innovative research approach by
developing a predictive model from the perspective of the adver-
tiser. The fundamental issue that Ghose and Yang attempt to
address is how sponsored search conversion rates, order values,
and profits compare to the same performance metrics for organic
search. The authors argue that predicting the conversion rate of a
specific advertisement is quite useful for most retail advertisers
because it gives the advertiser the ability to better plan campaign
budgets, improve ad content, select a better range of keywords,
and improve the pairing of ad with the keyword, all in advance.
Using a data set containing paid search advertising information
from a major retail chain, the authors developed a complex hierar-
chical Bayesian Network model for conversion rate prediction.
Their results show a strong positive overall association between
the advertisement conversion rate and the feature set, retailer
name, brand name, and keyword length. Ghose and Yang (2009)
also provide an in-depth discussion of variable sets and metrics
as related to sponsored search modeling.
Sculley, Malkin, et al. (2009), Becker et al. (2009), Attenberg,
Pandey, et al. (2009) discuss sponsored search performance met-
rics such as the bounce rate, landing page impressions, and naviga-
tional trails. Each of the authors modified several classification
models that enhanced the performance of the related metrics.294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
3103. Research contributions
The objective of our research is the development of a more effec-
tive method of classifying PPC campaign success, thus maximizing
Total Campaign Portfolio Profitability. Sophisticated data mining
techniques, such as ensemble learning, are needed to assist
campaign managers with predicting PPC campaign success
(Abrahams, Barkhi, et al., 2014). Sponsored search advertising can
be profitable, having a clear and measurable return on investment.
However, to be successful, advertisers must understand the main
assumption of search engine marketing; that is, a high level of rel-
evant traffic plus a good conversion rate equals more sales, where
quality advertisement content drives the level of relevant traffic
(Moran & Hunt, 2009). We define a PPC advertisement as including
a title, a description text, and the URL. We hypothesized that
ensemble learning techniques could achieve higher classificationPlease cite this article in press as: King, M. A., et al. Ensemble learning methods
(2015), http://dx.doi.org/10.1016/j.eswa.2015.01.047accuracies and more profitable campaigns, when compared to stan-
dard classification methods.
Ensemble analysis is currently quite popular; however, numer-
ous researchers continue to rely on conventional data sets from the
University of California Machine Learning Repository as well as
other data repositories. We make a significant research contribu-
tion by using actual data provided to us based on a collaborative
relationship with an industry leading PPC campaign management
company. PPC campaign data set features are relatively restricted
due to the length limit imposed on PPC ads by Google. We make
additional research contributions by outlining the creation of
derived features based on the ad text related to the title and ad
content.
We preprocessed the textual content of each PPC advertisement
by tokenizing them into stylometric, sentiment, and semantic fea-
tures. These text preprocessing techniques are a more sophisti-
cated approach for representing data, than the standard bag of
words model (Witten, Frank, et al., 2011) and are discussed in
detail in Section 4. We then analyzed the feature set, against a ser-
ies of four base classifiers and ten ensemble algorithms. We believe
this is the first work to use advertisement content modeling tech-
niques to extract a broad set of stylistic, sentiment, and semantic
attributes from PPC marketing campaigns, and to then apply
ensemble learning techniques to PPC campaign success prediction
using these attributes. We assess the level and robustness of profit
produced by the portfolio of advertising campaigns chosen by each
classifier.4. Methodology
The case study method of theory building is widely accepted
(Benbasat, Goldstein, et al., 1987; Eisenhardt, 1989; Yin, 2009).
This research follows a design consistent with earlier studies of
sponsored search advertisements (Rutz et al., 2011), and adheres
to the guidelines of content analysis research suggest by
Neuendorf (2002).
4.1. Data set
The data set contains 8499 PPC campaign marketing records
over a six week period obtained from a major PPC campaign man-
agement company who administers Google ads on the behalf of a
large home security systems and service advertiser. The target
class, defined as a successful (i.e. profitable) campaign, contains
1094 records, which represents 12.87% of the data set. We devel-
oped 271 numeric features from the stylistic, sentiment, and
semantic analysis of the keywords available to the advertiser, the
advertising text, and the campaign intention parameters. Exhibit
2 illustrates a partial record and feature set.
4.1.1. Dependent variables
Our data set contains four traditional sponsored search depen-
dent variables: ad impressions, the clickthrough rate, the average
page position, and the purchase rate. An impression is a metric
with roots from traditional advertising that counts the times a spe-
cific advertisement is shown. The clickthrough rate is the number
of clicks received for an ad divided by the number of impressions
provided. The average position metric indicates the average ranked
order of the advertisement in relation to other competing ads dis-
played on a search engine’s results page. The purchase rate, also
known as the conversion rate or acquisition rate, represents the
percentage of clicks that result in a purchase. However, we use
only one of these dependent variables for this research paper.
We derived a categorical dependent variable by using the purchase
rate. We coded this dependent variable with a label of Success if thefor pay-per-click campaign management. Expert Systems with Applications
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
Exhibit 2. Data set example.
4 M.A. King et al. / Expert Systems with Applications xxx (2015) xxx–xxx
ESWA 9827 No. of Pages 12, Model 5G
9 February 2015purchase rate for a campaign was above 0.5%, otherwise Fail. We
chose 0.5% as the breakeven purchase rate, because we deter-
mined, for the target class, that at least one purchase was neces-
sary for every 200 paid clicks in order for the campaign to be
profitable. In other words, the break even ratio is the average
cost-per-click divided by the marginal profit, which we found to
be approximately $1:$200.358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
378
379
380
381
382
383
384
385
386
387
388
3904.1.2. Independent variables
An example of a paid search ad is illustrated in Exhibit 3. See
Appendix A for an example of sponsored search listings. Our data
set contains four complex independent variables: the advertise-
ment title, the advertisement textual content, the available key-
words for bidding, and a set of campaign intentions developed by
the campaign management company. The advertisement is the
text displayed in the user’s web browser after a keyword query.
The advertisement contains three sections, a title headline,
descriptive text and a display URL. Google limits the length of an
entire ad to 70 characters.
We tokenized the textual content of each advertisement in the
data set for content modeling. The Affective Norms for English
Words [ANEW] (Bradley & Lang, 2010), and AFFIN lexicons
(Nielsen, 2011) and SentiStrength sentiment application
(Thelwall, Buckley, et al., 2010, 2012) were used for deriving the
sentiment content in the advertisements contained in the data
set. In each case, sentiment scores using each method (ANEW,
AFINN, SentiStrength) were summed for the advertisement text.
We extracted the semantic content of each PPC advertisement
using General Inquirer, which includes approximately 11,780 word
entries from the Harvard-IV-4 and the Lasswell lexicons (Stone
et al., 1966). We labeled each word in each PPC advertisement with
one or more tags; each tag corresponding to one category defined
by the General Inquirer lexicon. The counts of the tags were used as
the numeric representation for the campaign. After stylometric,
sentiment and semantic text preprocessing, and feature reduction,
we obtained 271 content related features.
Bid words are the keywords supplied by a search engine that
advertisers bid on in an auction setting. The bid amount deter-
mines the ranked position of the ad on the search results page
and the eventual price per click charged to an advertiser by the
search engine company. Google displays the advertisement if theExhibit 3. Structure of a sponsored ad.
Please cite this article in press as: King, M. A., et al. Ensemble learning methods
(2015), http://dx.doi.org/10.1016/j.eswa.2015.01.047bid amount is among the highest and the associated landing page
the most relevant based on Google’s propriety Page Rank algo-
rithm. Each bid word set is represented as tokens for content
modeling.
In a similar fashion as traditional impression-based advertising,
PPC campaign managers seek to segment their market and target
customer groups using specific campaigns with the hope of match-
ing it with customer motivation. Our data set contains nine cam-
paign intentions (e.g., time targeting, device targeting, brand
targeting, and place targeting) that we coded numerically.
4.2. Evaluation criteria
A common practice in classification analysis is to assume that
the misclassification costs are symmetric and that the classes con-
tained in a data set are equally balanced. However, researchers
making these assumptions should use caution as classification
techniques have numerous evaluations metrics that may lead to
very different performance conclusions given a specific research
context (Moraes, Valiati, et al., 2013). We base this concern on
the observation that many real world data sets are imbalanced
with respect to the target class and exhibit asymmetric misclassi-
fication costs. Weiss and Provost (2003) ran a large empirical study
with imbalanced data sets and concluded that relying solely on
overall accuracy led to biased decisions with respect to the target
class.
Although we evaluate several traditional confusion matrix met-
rics, shown in Exhibit 4, for comparing the performance of our base
and ensembles models, the main goal of our research is assessing
the total profitability of the full portfolio of selected PPC marketing
campaigns, applying ensemble learning techniques. The true posi-
tive rate (TP) represents the number of true instances correctly
classified. The true negative rate (TN) represents the false instances
correctly classified. The false positive rate (FP) and false negative
rate (FN) indicate the converse. Due to the search engine marketing
context of our research, we suggest two specific financial metrics,
Profit per PPC Campaign and Total Campaign Portfolio Profit, as
more applicable performance metrics for our analysis (Moran &
Hunt, 2009). We define Profit per PPC Campaign as:
ðCampaign Purchases  Revenue per ConversionÞ  Campaign SpendExhibit 4. Performance metrics.
for pay-per-click campaign management. Expert Systems with Applications
391
392
393
394
395
396
397
398
399
400
401
402
403
404
405
406
407
408
409
410
411
412
413
414
415
416
417
418
419
421
422
423
424
425
426
427
428
429
430
431
432
433
435
436
437
438
439
440
441
442
443
444
445
446
447
448
449
450
451
452
453
454
455
456
457
458
459
460
461
462
463
464
465
466
467
468
470
471
472
473
474
475
476
477
478
479
480
481
M.A. King et al. / Expert Systems with Applications xxx (2015) xxx–xxx 5
ESWA 9827 No. of Pages 12, Model 5G
9 February 2015Campaign Spend is the sum of the cost-per-click, for all clicks in the
campaign. We define Total Campaign Portfolio Profit as the sum of
the profit or loss for all PPC campaigns. As discussed later in detail,
we calculate all performance metrics, including the profit analysis,
from 30 different unseen hold out partitions.
4.3. Base classifiers
We utilized four popular classifier algorithms based on their
ability to predict a categorical dependent variable, their extensive
research streams, and their diversity of classification mechanisms,
e.g., probabilistic, statistical, structural or logical (Kotsiantis, 2007).
We feel that our classifier selection process helps reduce model
bias and facilitates the comparative assessments of model
performance.
4.3.1. Naïve Bayes
The Naïve Bayes technique is a simple probabilistic classifier
patterned after the Bayes Theorem. Despite its independence
assumption simplification, the Naïve Bayes classifier often exhibits
excellent predictive accuracy with the additional benefit of being
very computationally efficient (Domingos & Pazzani, 1997).
Researchers have frequently applied the Naïve Bayes algorithm
to classification applications related to text mining (Li, Liu, et al.,
2013). In a PPC context, the independent variables are the attri-
butes we derived. The naïve independence assumption states that
attributes A ¼ fa1; a2; a3; . . . ang representing a campaign ppci that
is classified as label Cj are all statistically independent. In our bin-
ary class environment where C0 represents Fail and C1 represents
Success, we calculated the predicted class Cj for campaign ppci as
the maximization of
PðCjjppciÞ ¼ argmax
PðCjÞ
Qn
i¼1PðaijCjÞ
PðC0Þ
Qn
i¼1PðaijC0Þ þ PðC1Þ
Qn
i¼1PðaijC1Þ
 482
483
484
485
486
487
488
489
490
491
492
493
494
495
496
497
498
499
500
501
502
503
504
505
506
507
508
509
510
511
5124.3.2. Logistic regression
Logistic regression is a popular and powerful statistics-based
classification technique used in numerous business applications
(Richardson et al., 2007). Logistic regression is similar to multiple
linear regression, but is appropriate for situations involving a bin-
ary dependent variable. The logistic response function ensures the
estimated value of the dependent variable will always fall between
zero and one, making it a suitable technique for binary classifica-
tion problems. The logistic response function estimates the proba-
bility of an observation A ¼ fa1; a2; a3; . . . ang belonging to class C1
as follows:
pðyÞ ¼ 1
1þ e b0þb1X1þb2X2þbnXnð Þ
The parameters for the logistic regression function are typically
determined using maximum likelihood estimation.
4.3.3. Classification trees
Classification trees (Breiman, Friedman, et al., 1984; Quinlan,
1986) are one of the most intuitive and transparent classification
algorithms, when compared to other supervised learning tech-
niques, which helps explain its popularity in application as well
as its extensive research stream. Classification trees require few
model assumptions, no domain knowledge and minimal parameter
settings, which makes the flexible technique attractive in numer-
ous business applications. The goal of a classification tree is to
recursively partition a training data set, typically using binary
splits, into increasingly homogenous subsets. The level of homoge-
neity is based on the concept of minimizing membership diversity
within each new partition (Shmueli, Patel, et al., 2010).Please cite this article in press as: King, M. A., et al. Ensemble learning methods
(2015), http://dx.doi.org/10.1016/j.eswa.2015.01.0474.3.4. Support Vector Machines
Support Vector Machines are a supervised classification algo-
rithm that is growing in popularity due to strong theoretical sup-
port provided by Statistical Learning Theory and its superior
accuracy in numerous applications. SVM have been used exten-
sively in text mining applications and research (Taboada, Brooke,
et al., 2011; Miner, Delen, et al., 2012; Sebastiani, 2002). Vapnik,
Boser and Guyon developed SVM in the early 1990s, and subse-
quently patented their algorithm (Vapnik, 1995). The main idea
behind Support Vector Machines is classifying by applying a linear
hyperplane regardless of whether the data is linear separable. The
algorithm transforms the original data inputs into a higher dimen-
sion feature space using one of several popular kernel functions.
The main theoretical advantage of using a SVM is that the problem
can be written as a constrained quadratic optimization problem,
which assures the algorithm finds a global maximum (Burges,
1998). The optimization problem can be expressed by
max
Xn
i¼1
ai  :5
X
ij
aiajcicjKðAi;AjÞ
where Ai is a PPC campaign attribute vector such that Ai ¼
fai; a2; a3; . . . ang, Aj is a testing instance, ci represents the known
classes, cj represents the testing instance class, ai and aj are param-
eters for the model, and K denotes the incorporated kernel function.
We used the linear kernel, which is the default kernel setting in
RapidMiner v5.3.013
4.4. Ensemble learning
Ensemble learning, involves building a classification model
from a diverse set of base classifiers where accuracy can come to
the forefront and the errors tend to cancel out. Exhibit 5 provides
a classic conceptual illustration of classification error cancelation
by averaging decision tree hyperplanes. The first pane shows that
the two classes are separated by a diagonal decision boundary.
The second pane shows the classification errors for three separate
decision trees. The last pane shows the error cancelation effect.
Ensembles, also known as meta-classifiers, are generally more
accurate than a standalone classifier when the ensemble contains
a diverse set of base models, created from a moderately indepen-
dent set of training partitions, and then combined by an algebraic
method (Kim, 2009; Kuncheva, 2004). Our research compares the
classification accuracy of four well-known ensemble methods,
Voting, Boot Strap Aggregation, Stacked Generalization, and
MetaCost, with the four previously discussed individual classifiers.
We selected these ensembles based on their extensive research
streams as well as being representative of common themes synthe-
sized from several taxonomies reflected in the literature (Kuncheva,
2004; Kantardzic, 2011; Witten et al., 2011; Rokach, 2009).
4.4.1. Voting
Voting is the simplest ensemble learning algorithm for combin-
ing several different types of classification models. Because of this
simplicity, researchers commonly use Voting as a base line mea-
sure when comparing the performance of multiple ensemble mod-
els (Sigletos, Paliouras, et al., 2005). For a standard two-class
problem, Voting determines the final classification result based
on a majority vote with respect to the class label. For a multi-class
problem, plurality Voting determines the class label. In the base
case, Voting gives each classification model equal weighting, and
represents a special case of algebraic combination rules. Weighted
Voting, a more general Voting rule, allows a modeler to place more
weight on models that have higher accuracy, based on prior
domain knowledge, which lessens the chance of selecting an inac-
curate model (Major & Ragsdale, 2001). For a continuous depen-for pay-per-click campaign management. Expert Systems with Applications
513
514
515
516
517
518
519
520
521
522
523
524
525
526
527
528
529
530
531
532
533
534
535
536
537
538
539
540
541
542
543
544
545
546
547
548
549
550
551
552
553
554
555
556
557
558
559
560
561
562
563
564
565
566
567
568
569
570
571
572
573
574
575
576
577
578
579
580
581
582
583
584
585
586
587
588
589
590
591
592
593
594
595
596
597
598
599
600
601
602
603
604
605
606
607
608
609
610
611
Exhibit 5. Decision tree averaging (after Dietterich, 1997).
6 M.A. King et al. / Expert Systems with Applications xxx (2015) xxx–xxx
ESWA 9827 No. of Pages 12, Model 5G
9 February 2015dent variable, additional algebraic combination rules are applied;
such as the maximum rule, the minimum rule, and the product
rule (Kantardzic, 2011).
4.4.2. Boot Strap Aggregation
Boot Strap Aggregation, abbreviated as Bagging, was proposed
by Breiman (1996) and has become one of the most extensively
researched and applied ensemble learning techniques to date due
to its accuracy, simplicity and computation efficiency (Polikar,
2012). The basic process behind Bagging is to take a training data
set and create a set of new training data partitions, each containing
the same number of observations, by randomly sampling with
replacement from the original data set. The next step is to train a
set of classifiers using the training data set replicates and then to
combine the predicted class of each observation from each model
by majority vote. Several research streams indicate that the Bag-
ging ensemble achieves higher average accuracy when it contains
unstable classifiers, such as decision trees and artificial neural net-
works (Kim, 2009).
4.4.3. Stacked Generalization
Stacked Generalization (or Stacking), created by Wolpert
(1992), is a hierarchical structured ensemble that contains a set
of level 0 classifiers and one level 1 classifier, where the level 1
classifier acts as an arbiter by combining the results of each level
0 classifier. In Stacking, unlike Bagging or other ensemble tech-
niques, the level 0 set of classifiers may contain different types of
algorithms, e.g., decision trees, logistic regression, Naïve Bayes,
and Support Vector Machines. Stacking is a popular ensemble tech-
nique; however, there is a lack of consensus on how to choose a
level 1 classifier (Wang et al., 2011). Stacking is very similar to Vot-
ing, but instead being restricted to a simple majority mechanism
for combination, Stacking allows the researcher to embed any clas-
sifier as the level 1 classifier.
Stacking first divides the data into two disjoint partitions where
the training partition contains 67% of the observations data and the
testing partition contains the remainder. Next, each level 0 classi-
fier is trained on the training partition. Next, the level 0 classifiers
are applied to the testing partition to output a predicted class.
Lastly, to train the level 1 classifier, the predicted classes from each
level 0 classifiers are used as inputs along with the correct class
labels from the test set as outputs.
4.4.4. MetaCost
Most classification algorithms make the implicit assumptions
that the data set is balanced with respect to the class label and,
most importantly, that the cost of misclassification errors are sym-
metric (Elkan, 2001). However, the assumption of symmetric mis-
classification cost is seldom true when working with realistic data
sets because the class label of interest usually represents a small
portion of the data set, and has a relatively high cost of
misclassification.Please cite this article in press as: King, M. A., et al. Ensemble learning methods
(2015), http://dx.doi.org/10.1016/j.eswa.2015.01.047MetaCost is an ensemble wrapping technique that allows a
researcher to embed a cost agnostic classifier within a cost minimi-
zation procedure, which makes the base classifier cost sensitive by
assigning a higher cost to false negatives than to false positives.
The wrapper does not make any changes to the embedded classi-
fier and does not require any functional knowledge of the embed-
ded classifier. The MetaCost ensemble is quite flexible as it is
applicable to multi-class data sets as well as to arbitrary cost
matrices (Sammut & Webb, 2011). In his seminal paper,
Domingos (1999), noted that it is common to see large misclassifi-
cation cost reductions when using MetaCost when compared to
cost agnostic classifiers. In a sense, MetaCost is a modified form
of Bootstrap Aggregation where the known class labels are rela-
beled based on minimizing a conditional risk function. After the
training data sets are re-labeled, a single new classifier is trained
(Witten et al., 2011).
In addition to the ability to make a classifier cost sensitive,
MetaCost has the added benefit of interpretability, because once
the ensemble work of class relabeling is completed, it trains a sin-
gle classifier. MetaCost requires a longer runtime and thus higher
computational utilization when compared to other common
ensembles. In addition, MetaCost assumes that misclassification
costs are known in advance and are the same for each observation
(Witten et al., 2011).4.5. Experimental design
We constructed 14 different classification models, utilizing Rap-
idMiner v5.3.013 as our software platform. RapidMiner is an open
source data mining workbench, with both a community edition
and commercial version, which contains an extensive set of ensem-
ble operators. We created four standalone classification models, as
previously discussed, to establish baseline performance metrics.
Then we created one Voting ensemble containing all base models,
four Bagging ensembles, one for each base model, one Stacking
ensemble containing all base models and four MetaCost ensem-
bles, one for each base model. Exhibit 6 provides a conceptual dia-
gram of the process logic for our complete experimental procedure.
To implement the full experiment, we followed a repeated mea-
sures experimental design where the same subject is measured
multiple times under different conditions. In our current research
context, the subjects are the training and testing partitions and
the conditions represent the 14 different classification model con-
figurations. The primary advantage of using a repeated measures
design is that the structure gives us the ability to control variations
of the training and testing partitions across all 14 models, which
typically creates a smaller standard error and thus a more powerful
t test statistic (Ott & Longnecker, 2001). Using a stratified random
sampling approach, which we replicated using 30 different random
seeds, we created 30 different training and testing partitions, using
the generally accepted partitioning ratio of 67% for training and
33% for testing (Witten et al., 2011; Kantardzic, 2011). Every modelfor pay-per-click campaign management. Expert Systems with Applications
612
613
614
615
616
617
618
619
620
621
622
623
624
625
626
627
628
629
630
631
632
633
634
635
636
637
638
639
640
641
642
643
644
645
646
647
648
649
650
651
652
653
654
655
656
657
658
659
660
661
662
663
664
665
666
667
668
669
670
671
672
673
674
675
676
677
678
679
680
681
682
683
684
685
686
687
Exhibit 6. Conceptual ensemble experiment.
Training/Testing 
Partitions 1 2 3 … 14
1 m 1,1 m 1,2 m 1,3 … m 1,14
2 m 2,1 m 2,2 m 2,3 … m 2,14
3 m 3,1 m 3,2 m 3,3 … m 3,14
… … … … …
30 m 30,1 m 30,2 m 30,3 … m 30,14
Base Model or Ensemble
Exhibit 7. Repeated measure experimental design.
M.A. King et al. / Expert Systems with Applications xxx (2015) xxx–xxx 7
ESWA 9827 No. of Pages 12, Model 5G
9 February 2015was tested on the same 30 random training and test set pairs,
allowing us to use the matched pair t test method to determine
whether differences were statistically significant. Each of the 30
training and testing partitions, considered a replication, was used
as input across the 14 classification models as illustrated in Exhibit
7.
We selected the default parameter settings for most RapidMin-
er operators; however, we did make several exceptions. We
selected the Laplace Correction parameter for the Naïve Bayes
operator. We increased the ensemble sizes of both the Bagging
and MetaCost ensembles to 25 members (Kim, 2009; King et al.,
2014). We controlled our random seed values for partitioning the
data set with the simple process of enumerating them 1 through
30. The MetaCost ensemble requires a parameter input of false
negative and false positive misclassification costs. We derived
these values based on our revenue per acquisition and cost per
acquisition. Estimating the actual values used for these two mis-
classification cost are important, however the size of the ratio of
the two costs has the most effect on MetaCost accuracy (Kim,
Choi, et al., 2012).
5. Results and evaluation
In this research study, there were two advertisement campaign
classes, Success and Failure. As previously discussed, we defined an
advertisement campaign as a Success where the Purchase Rate was
above 0.5%. As is common with most binary classification research,
a discussion of overall accuracy, precision, recall and model lift will
follow. Moreover, we also present results describing model profit,
as this metric is the most applicable for our research due to asym-
metric classification costs.
Exhibit 8 shows the average model accuracies, as previously
defined in Exhibit 4. Naïve Bayes base model and the BaggingPlease cite this article in press as: King, M. A., et al. Ensemble learning methods
(2015), http://dx.doi.org/10.1016/j.eswa.2015.01.047and MetaCost ensemble models performed poorly as compared
to the remaining base and ensemble models. We did not expect
the degree of performance difference to be this large, as the Naïve
Bayes classification typically performs quite well with large feature
sets. There are several possible reasons why the Naïve Bayes clas-
sifiers performed so poorly. Researchers commonly use the Naïve
Bayes classifier for baseline classification analysis, due to its com-
putational speed and simplicity. The Voting ensemble is imple-
mented with the same reasoning. One possible reason for poor
performance of the Naïve Bayes classifier is the conditional inde-
pendence that the Naïve Bayes classifier assumes is severely inap-
propriate for our data set. When features have abnormally high
levels of multicollinearity, the Naïve Bayes algorithm performs
poorly. An additional possible reason for the poor performance is
that our hypothesis space may not be linearly separable.
A final possible reason that Naïve Bayes models perform poorly
when measured in terms of overall campaign portfolio profit is that
Naïve Bayes models have very high recall and low precision as
illustrated in Exhibit 10. The Naïve Bayes models are costly in
terms of ad spend due to the large number of false positives, i.e.
failed campaigns that Naïve Bayes invests in as a result of their
low precision. In addition, while the Naïve Bayes models have high
recall across campaigns in general, it should be noted that cam-
paigns have vastly different profitability, and the resulting distri-
bution of campaign profit has a strong right skew. This means
there are a small number of extremely high profit campaigns. Evi-
dently, our Naïve Bayes models do not select the highest profit
campaigns, and leave a ‘‘lot of money on the table.’’ However, the
MetaCost Naïve Bayes model clearly compensates to a degree for
this weakness, as illustrated in Exhibit 11. Therefore, in terms of
Recall, while the Naïve Bayes models are correct more often than
other classifiers, they are unfortunately incorrect ‘‘when it counts’’,
suffering from false negatives on the highest profit campaigns.
While the remaining 11 models have similar overall accuracies,
the majority of accuracy differences are statistically significant at
the .05, and the .01 significance level, based on a matched pair t
test with the Bonferroni correction (Bonferroni, 1936) depicted
by shading, as detailed in Exhibit 9. Given the larger number of
pair-wise model accuracy comparisons, the experiment-wise error
rate (Familywise Error Rate) is elevated. Thus, we applied the Bon-
ferroni correction to determine whether each pair-wise model
comparison was actually statistically significant. We divided both
alpha levels of 0.05 and 0.01 by 14 (the number of models being
compared) to determine our significance thresholds. It is interest-
ing to note that the base logistic regression model achieved thefor pay-per-click campaign management. Expert Systems with Applications
688
689
690
691
692
693
694
695
696
697
698
699
700
701
702
703
Exhibit 8. Overall model accuracy.
Naïve Bayes
Bagging Naïve 
Bayes
MetaCost Naïve 
Bayes Voting Decision Trees  Stacking  SVM Bagging SVM
Bagging Decision 
Trees
MetaCost Logistic 
Regression
Bagging Logistic 
Regression
MetaCost 
Decision Trees MetaCost SVM
 Logistic 
Regression 
Naïve Bayes
Bagging Naïve Bayes 0.1722  
MetaCost Naïve Bayes 0.0000 0.0000  
Voting 0.0000 0.0000 0.0000  
Decision Trees 0.0000 0.0000 0.0000 0.0348  
 Stacking 0.0000 0.0000 0.0000 0.0050 0.0152  
 SVM 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000  
Bagging SVM 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0833  
Bagging Decision Trees 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0755 0.4068  
MetaCost Logistic Regression 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0271 0.0698 0.1927  
Bagging Logistic Regression 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0030 0.0005 0.0231 0.0094  
MetaCost Decision Trees 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0009 0.0000 0.0017 0.0003 0.3211  
MetaCost SVM 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0011 0.0000 0.0043 0.0001 0.2214 0.4441  
 Logistic Regression 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0007 0.0000 0.0044 0.0003 0.0038 0.3172 0.3052  
Exhibit 9. Overall accuracy: P-values <.05 and .01 level, Bonferroni adjusted.
Exhibit 10. Model precision vs recall.
8 M.A. King et al. / Expert Systems with Applications xxx (2015) xxx–xxx
ESWA 9827 No. of Pages 12, Model 5G
9 February 2015highest overall accuracy, although not statistically different from
the MetaCost Decision Trees and MetaCost Support Vector
Machines ensemble models. As a reminder, overall model accuracy
is not an appropriate evaluation metric for classification when
there are asymmetric costs associated with misclassification error.
Relying on overall accuracy in a classification context as this can be
misleading; however, the metric can provide a comparative base-
line for additional analysis.Please cite this article in press as: King, M. A., et al. Ensemble learning methods
(2015), http://dx.doi.org/10.1016/j.eswa.2015.01.047The classification evaluation metrics of precision and recall can
provide more granular performance details when required. Preci-
sion and recall are calculated from the results contained in a con-
fusion matrix and were previously defined in Exhibit 4. Precision is
a measure of accuracy provided a specific class for an instance was
predicted. In our research, precision describes the percentage of
correct Success predictions. As shown in Exhibit 10, once again
the base model logistic regression has the highest precision. Thefor pay-per-click campaign management. Expert Systems with Applications
704
705
706
707
708
709
710
711
712
713
714
715
716
717
718
719
720
721
722
723
724
725
726
727
728
729
730
731
732
733
734
735
736
737
738
739
740
741
742
743
744
745
746
747
748
749
750
751
Exhibit 11. Model profit per campaign portfollio.
M.A. King et al. / Expert Systems with Applications xxx (2015) xxx–xxx 9
ESWA 9827 No. of Pages 12, Model 5G
9 February 2015ranking of model precision is very similar to overall accuracy per-
formance. Recall is a measure of accuracy or ability of a classifica-
tion model to select instances from a specific class. Recall is also
known as the True Positive Rate. In our research, recall describes
the percentage of Success cases identified as such by the classifier.
Exhibit 10 displays the Recall metrics for our research. Note the
tradeoff between our precision and recall metrics. This tradeoff
pattern is found in most classification research. The harmonic
mean F, also shown in Exhibit 10, might be a more appropriate
metric in other research contexts because it emphasizes the impor-
tance of small values, whereas outliers have a more significant
influence on the arithmetic mean. However, the harmonic mean,
which is essentially the same over the majority of our models,
did not provide much insight in this research.
Although calculating the overall model accuracy, precision, and
recall evaluation metrics are helpful and can provide some
research direction, our primary goal is to discover which classifica-
tion methods predict the most campaign profit. As previously dis-
cussed, we define Profit per PPC Campaign as the number of
purchases from a campaign multiplied by the revenue per conver-
sion minus the campaign costs. The Total Campaign Portfolio Profit
is the sum of profits from all campaigns selected by the classifier. A
profit evaluation is much more applicable to our research because
the main business objective is maximizing the overall campaignBaseline Model 
Profit
Base Model Naïve 
Bayes
Bagging Naïve 
Bayes
MetaCost Naïve 
Bayes Voting
Base Model 
Decision Trees
MetaCost 
Decision Trees
Baggin
T
Baseline Model Profit
Naïve Bayes 0.0000  
Bagging Naïve Bayes 0.0000 0.0113  
MetaCost Naïve Bayes 0.0000 0.0000 0.0000  
Voting 0.0000 0.0000 0.0000 0.0000  
Decision Trees 0.0000 0.0000 0.0000 0.0000 0.2442  
MetaCost Decision Trees 0.0000 0.0000 0.0000 0.0000 0.3048 0.4305  
Bagging Decision Trees 0.0000 0.0000 0.0000 0.0000 0.1024 0.1628 0.1147  
Bagging Logistic Regression 0.0000 0.0000 0.0000 0.0000 0.0789 0.1169 0.1183
Base Model SVM 0.0000 0.0000 0.0000 0.0000 0.0084 0.0445 0.0318
Bagging SVM 0.0000 0.0000 0.0000 0.0000 0.0102 0.0424 0.0410
Logistic Regression 0.0000 0.0000 0.0000 0.0000 0.0267 0.0406 0.0309
MetaCost SVM 0.0000 0.0000 0.0000 0.0000 0.0133 0.0324 0.0139
Stacking 0.0000 0.0000 0.0000 0.0000 0.0130 0.0411 0.0281
MetaCost Logistic Regression 0.0000 0.0000 0.0000 0.0000 0.0009 0.0012 0.0001
Optimum Model Profit 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000
Exhibit 12. Model profits: P-values <.05
Please cite this article in press as: King, M. A., et al. Ensemble learning methods
(2015), http://dx.doi.org/10.1016/j.eswa.2015.01.047portfolio profit and not overall model accuracy. In numerous busi-
ness applications such as fraud detection, medical diagnosis and
marketing campaigns such as ours, the primary focus for the
researcher, is in fact, the minority class. In these applications, not
only are the data distributions skewed, but also the misclassifi-
cation costs associated with confusion matrix elements. A majority
of classification algorithms assume equal misclassification costs
and ignore the different types of misclassification errors. One prac-
tical method to minimize this problem is to utilize a cost sensitive
classification algorithm (Viaene & Dedene, 2005). As discussed pre-
viously, cost sensitive classification techniques, such as MetaCost,
include misclassification costs during model training and create a
classifier that has the lowest cost associated with its confusion
matrix. In our case, the MetaCost ensemble assigns higher cost to
false negatives than to false positive which improves the classifier
performance with respect to the positive class of Success.
Exhibit 11 summarizes the campaign portfolio profit, based on
30 replications, for each base and ensemble model. Not surprising,
the Naïve Bayes models produced the lowest average profits,
which follows the same overall accuracy pattern seen in Exhibit
8. The three Naïve Bayes model profits also show a much larger
variation when compared to the other base and ensemble models.
However, the ranking of the remaining models are quite different
when compared to the ranking in Exhibit 8. The MetaCost Logisticg Decision 
rees
Bagging Logistic 
Regression  Base Model SVM Bagging SVM
 Base Model 
Logistic 
Regression MetaCost SVM  Stacking 
MetaCost Logistic 
Regression
Optimum Model 
Profit
0.3041  
0.1483 0.2606  
0.1563 0.2366 0.4579  
0.1271 0.0225 0.4517 0.4712  
0.1126 0.1722 0.4331 0.4629 0.4957  
0.1117 0.2230 0.3728 0.4221 0.4722 0.4740  
0.0039 0.0042 0.0281 0.0350 0.0216 0.0194 0.0526  
0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000  
and .01 level, Bonferroni adjusted.
for pay-per-click campaign management. Expert Systems with Applications
752
753
754
755
756
757
758
759
760
761
762
763
764
765
766
767
768
769
770
771
772
773
774
775
776
777
778
779
780
781
782
783
784
785
786
787
788
789
790
791
792
793
794
795
796
797
798
799
800
801
802
803
804
805
806
807
808
809
810
811
812
813
814
815
816
817
818
819
820
821
822
823
824
825
826
827
828
829
830
831
832
833
834
835
836
837
Exhibit 13. Optimum and baseline campaign portfolio profits.
10 M.A. King et al. / Expert Systems with Applications xxx (2015) xxx–xxx
ESWA 9827 No. of Pages 12, Model 5G
9 February 2015Regression ensemble performed the best with an average cam-
paign portfolio profit of $14,962, considerably higher than the sec-
ond and third place ensembles.
As detailed in Exhibit 12, the optimal model profit results are
statistically different, at both the .05 and .01 significance level,
from all models, using the Bonferroni correction. Although it was
not surprising that the MetaCost ensemble models and the logistic
regression models performed well, what is interesting is the Stack-
ing ensemble ranked second in profitability. Stacking is not known
for strong performance when applied in a text mining context and
since this research is based on a hybrid data set containing both
standard numeric attributes and numeric attributes created from
text attributes that produce a very sparse data set, its excellent
performance was unexpected. We also note that all of the SVM
models performed well, as expected, due to its strong text classifi-
cation capabilities (Joachims, 2002).
Exhibit 11 illustrates how using a different evaluation metric,
such as campaign portfolio profit, can provide valuable, as well
as conflicting results in comparison to more traditional measures
of classification accuracy. Our model results are an excellent exam-
ple of why researchers should use several evaluation metrics when
comparing classifier models. Classifier performance metrics such
as profits or costs are of critical importance, but are typically less
robust when compared to a more stable metric such as overall
accuracy. Pay-per-click campaigns can have multiple goals. As an
example, a risk averse campaign analyst may wish to choose a
model with a combination of stable overall accuracy and a mini-
mum level of campaign profit, while a risk seeking campaign ana-
lyst may choose to the classifier that produces the highest
campaign profit.
From a technical standpoint, our PPC campaigns have unequal
profits. We have very few high profit campaigns and many lower
profit campaigns. A classifier with a lower overall accuracy may
still achieve a higher profit when compared to a classifier with a
higher overall accuracy, because a higher overall accuracy classifier
may yield its few false negatives on high profit campaigns, while a
lower overall accuracy classifier may yield slightly more false neg-
atives with respect to predominately low profit campaigns. The
lower overall accuracy classifier may be correct when it matters
and thus correctly predict the very high profit campaigns. Whereas
Exhibit 9 shows that the majority of the models statistically differ-
ent in terms of overall accuracy, Exhibit 12, with statistical signif-
icance after Bonferroni correction depicted by shading, displays a
contrasting pattern of non-significant differences in campaign
profits.Please cite this article in press as: King, M. A., et al. Ensemble learning methods
(2015), http://dx.doi.org/10.1016/j.eswa.2015.01.0476. Conclusion and future work
Our experimental results support our hypothesis that applying
ensemble learning techniques in PPC marketing campaigns can
achieve higher profits for our home security service and equipment
provider. We introduced the evaluation metric of Total Campaign
Portfolio Profit and illustrated how relying on overall model accu-
racy can be misleading. As additional metrics for model compari-
son, Exhibit 13 shows box plots of optimum and baseline
campaign portfolio profits. Optimum campaign portfolio profit
represents the sum of the scenarios where only profitable PPC
campaigns were identified and deployed. The baseline campaign
portfolio profit represents the sum of the scenarios where all PPC
campaigns whether profitable or not were deployed. As can be
seen, the two means of +$115,057.23 and $97,622.12, convey a
high level of variance which is a result of the marketing risk asso-
ciated the PPC campaigns. Thus, any positive profit advantage
achieved through improved classification techniques is beneficial.
We are encouraged that all of our models outperformed the base-
line campaign portfolio profit, and that the top 11 models pro-
duced positive profits. However, none of our model profits came
close to the optimum campaign model profit, although all models
performed consistently well, with the exception of the Naïve Bayes
models. Given the difficulty of feature creation due the limited
amount of words contained in an advertisement, we feel that our
ensemble framework for sponsored search advertising makes a
valuable contribution to industry.
One interesting observation we noted from Exhibit 10 is Voting
(typically used for a baseline ensemble performance) has the high-
est F metric based on the harmonic mean between precision and
recall. Different configurations of the Voting ensemble could be a
possible area for future research. Additionally, using each of the
remaining dependent variables discussed in Section 4.1.1 and
applying the same experiment method introduced in this article
could be useful for the business community. Naïve Bayes classifiers
typically perform well, making our results somewhat unexpected
and a subject for future research. Additionally, feature selection
methods and ensemble learning techniques could be compared.7. Uncited references
Abernethy and Franke (1996), Adeva, García, Atxa, et al. (2014),
Jansen, Sobel, et al. (2011), Kim, Street, et al. (2005), and Zhang,
2000.for pay-per-click campaign management. Expert Systems with Applications
8389
841
842
843
844
845
846
847
848
849
850
851
852
853
854
855
856
857
858
859
860
861
862
863
864
865
866
867
868
869
870
871
M.A. King et al. / Expert Systems with Applications xxx (2015) xxx–xxx 11
ESWA 9827 No. of Pages 12, Model 5G
9 February 2015Appendix A. Paid search examples872
873
874
875
876
877
878
879
880
881
882
883
884
885
886
887
888
889
890
891
892
893
894
895
896
897
898
899
900
901References
http://www.iab.net/research/industry_data_and_landscape/adrevenuereport. (Last
accessed September, 2014).
http://www.comscore.com/Insights/Press-Releases/2014/4/comScore-Releases-
March-2014-U.S.-Search-Engine-Rankings. (Last accessed September, 2014).
Abernethy, A. M., & Franke, G. R. (1996). The information content of advertising: A
meta-analysis. Journal of Advertising, 25(2), 1–17.
Abrahams, A. S., Coupey, E., et al. (2013). Audience targeting by B-to-B
advertisement classification: A neural network approach. Expert Systems with
Applications, 40(8), 2777–2791.
Abrahams, A. S., Barkhi, R., et al. (2014). Converting browsers into recurring
customers: An analysis of the determinants of sponsored search success for
monthly subscription services. Information Technology and Management, 15(3),
177–197.
Adeva, J. J., García, J. M., Atxa, P., et al. (2014). Automatic text classification to
support systematic reviews in medicine. Expert Systems with Applications, 41(4,
Part 1), 1498–1508.
Aggarwal, C. C., & Zhai, C. (2012). Mining text data. New York: Springer.
Attenberg, J., Pandey, S., et al. (2009). Modeling and predicting user behavior in
sponsored search. Kdd-09: 15th ACM SIGKDD conference on knowledge discovery
and data mining. Paris, France: Association of Computing Machinery.
Benbasat, I., Goldstein, D. K., et al. (1987). The case research strategy in studies of
information systems. MIS Quarterly, 11(3), 369–386.
Bradley, M. M., & Lang, P. J. (2010). Affective norms for English words (ANEW):
Instruction manual and affective ratings. Technical Report C-2, The Center for
Research in Psychophysiology, University of Florida.
Breiman, L., Friedman, J. H., et al. (1984). Classification and regression trees. Belmont,
Calif.: Wadsworth International Group.
Breiman, L. (1996). Bagging predictors. Machine Learning, 24(2), 123–140.Please cite this article in press as: King, M. A., et al. Ensemble learning methods
(2015), http://dx.doi.org/10.1016/j.eswa.2015.01.047Bonferroni, C. E. (1936). Teoria statistica delle classi e calcolo delle probabilità.
Pubblicazioni del R Istituto Superiore di Scienze Economiche e Commerciali di
Firenze, 8(1936), 3–62.
Burges, C. C. (1998). A tutorial on support vector machines for pattern recognition.
Data Mining and Knowledge Discovery, 2(2), 121–167.
Ciaramita, M., Murdock, V., et al. (2008). Online learning from click data for
sponsored search. Proceedings of the 17th international conference on World Wide
Web (pp. 227–236). Beijing, China: ACM.
D’Avanzo, E., Kuflik, T., et al. (2011). Online advertising using linguistic knowledge
information technology and innovation trends in organizations. A. D’Atri, M.
Ferrara, J. F. George, & P. Spagnoletti (Eds.), Physica-Verlag HD (pp. 143–150).
Dietterich, T. G. (1997). Machine-learning research – Four current directions. AI
Magazine, 18(4), 97–136.
Domingos, P., & Pazzani, M. (1997). Beyond independence: Conditions for the
optimality of the simple bayesian classifier. Machine Learning, 29, 103–130.
Domingos, P. (1999). MetaCost: A general method for making classifiers cost-
sensitive. Proceedings of the fifth ACM SIGKDD international conference on
knowledge discovery and data mining (pp. 155–164). San Diego, California,
USA: ACM.
Eisenhardt, K. M. (1989). Building theories from case study research. The Academy of
Management Review, 14(4), 532–550.
Elkan, C. (2001). The foundations of cost-sensitive learning. In: Proceedings of the
seventeenth international conference on artificial intelligence (pp. 973–978).
Seattle, Washington, USA. August 2001.
Fain, D. C. J., & Pedersen, O. (2005). Sponsored search: A brief history. Bulletin of the
American Society for Information Science and Technology, 32(2), 12.
Feldman, R. (2013). Techniques and applications for sentiment analysis.
Communications of the ACM, 56(4), 82–89.
Ghose, A., & Yang, S. (2008). Comparing performance metrics in organic search with
sponsored search advertising. Proceedings of the 2nd international workshop onfor pay-per-click campaign management. Expert Systems with Applications
902
903
904
905
906
907
908
909
910
911
912
913
914
915
916
917
918
919
920
921
922
923
924
925
926
927
928
929
930
931
932
933
934
935
936
937
938
939
940
941
942
943
944
945
946
947
948
949
950
951
952
953
954
955
956
957
958
959
960
961
962
963
964
965
966
967
968
969
970
971
972
973
974
975
976
977
978
979
980
981
982
983
984
985
986
987
988
989
990
991
992
993
994
995
996
997
998
999
1000
1001
1002
1003
1004
1005
1006
1007
1008
1009
1010
1011
1012
1013
1014
1015
1016
1017
1018
1019
1020
1021
1022
1023
1024
1025
1026
1027
1028
1029
1030
1031
1032
1033
12 M.A. King et al. / Expert Systems with Applications xxx (2015) xxx–xxx
ESWA 9827 No. of Pages 12, Model 5G
9 February 2015data mining and audience intelligence for advertising (pp. 18–26). Las Vegas,
Nevada: ACM.
Ghose, A., & Yang, S. (2009). An empirical analysis of search engine advertising:
Sponsored search in electronic markets. Management Science, 55(10),
1605–1622.
Ghose, A., & Ipeirotis, P. G. (2011). Estimating the helpfulness and economic impact
of product reviews: Mining text and reviewer characteristics. IEEE Transactions
on Knowledge and Data Engineering, 23(10), 1498–1512.
Graepel, T., Candela, J., et al. (2010). Web-scale bayesian click-through rate
prediction for sponsored search advertising in Microsoft’s Bing search engine.
In: 27th international conference on machine learning. Haifa, Israel.
Haans, H., Raassens, N., et al. (2013). Search engine advertisements: The impact of
advertising statements on click-through and conversion rates. Marketing Letters,
24(2), 151–163.
Heath, R., & Nairn, A. (2005). Measuring affective advertising: implications of low
attention processing on recall. Journal of Advertising Research, 45(2), 269–281.
Jansen, B., & Schuster, S. (2011). Bidding on the buying funnel for sponsored search
campaigns. Journal of Electronic Commerce Research, 12(1), 1.
Jansen, B. J., & Mullen, T. (2008). Sponsored search: An overview of the concept,
history, and technology. International Journal of Electronic Business, 6(2),
114–131.
Jansen, B. J., Sobel, K., et al. (2011). The brand effect of key phrases and
advertisements in sponsored search. International Journal of Electronic
Commerce, 16(1), 77–106.
Joachims, T. (2002). Learning to classify text using support vector machines. SciTech
book news. Portland: Ringgold Inc. 26.
Kantardzic, M. (2011). Data mining: concepts, models, methods, and algorithms.
Hoboken, N.J.: John Wiley: IEEE Press.
Kim, J., Choi, K., et al. (2012). Classification cost: An empirical comparison among
traditional classifier, cost-sensitive classifier, and MetaCost. Expert Systems with
Applications, 39(4), 4013–4019.
Kim, Y., Street, W. N., et al. (2005). Customer targeting: A neural network approach
guided by genetic algorithms. Management Science, 51(2), 264–276.
Kim, Y. S. (2009). Boosting and measuring the performance of ensembles for a
successful database marketing. Expert Systems with Applications, 36(2),
2161–2176.
King, M. A., Abrahams, A. S., et al. (2014). Ensemble methods for advanced skier
days prediction. Expert Systems with Applications, 41(4, Part 1), 1176–1188.
Kotsiantis, S. B. (2007). Supervised machine learning: A review of classification
techniques. Informatica, 31(3), 249–268.
Kuncheva, L. I. (2004). Combining pattern classifiers methods and algorithms. Wiley.
Li, D., Liu, L. H., et al. (2013). Research of text categorization on WEKA. 2013 Third
international conference on intelligent system design and engineering applications.
New York: IEEE.
Mahdian, M., & Tomak, K. (2008). Pay-per-action model for on-line advertising.
International Journal of Electronic Commerce, 13(2), 113–128.
Major, R. L., & Ragsdale, C. T. (2001). Aggregating expert predictions in a networked
environment. Computers & Operations Research, 28(12), 1231–1244.
Miner, G., Delen, D., et al. (2012). Practical text mining and statistical analysis for non-
structured text data applications. Waltham, MA: Academic Press.
Moraes, R., Valiati, J. F., et al. (2013). Document-level sentiment classification: An
empirical comparison between SVM and ANN. Expert Systems with Applications,
40(2), 621–633.
Moran, M., & Hunt, B. (2009). Search engine marketing, Inc.: Driving search traffic to
your company’s website. IBM Press/Pearson.
Neuendorf, K. A. (2002). The content analysis guidebook. Thousand Oaks, Calif.: Sage
Publications.
Nielsen, F. Å. (2011). A new ANEW: Evaluation of a word list for sentiment analysis
in microblogs. from <http://arxiv.org/abs/1103.2903>.
Nielsen, J. H., Shapiro, S. A., et al. (2010). Emotionally and semantic onsets:
Exploring orienting attention responses in advertising. Journal of Marketing
Research, 47(6), 1138–1150.
Ott, R. L., & Longnecker, M. (2001). An introduction to statistical methods and data
analysis. Pacific Grove, Calif.: Duxbury-Thomson Learning.
Polikar, R. (2012). Ensemble learning. In C. Zhang & Y. Ma (Eds.), Ensemble machine
learning (pp. 1–34). New York: Springer US.1034
Please cite this article in press as: King, M. A., et al. Ensemble learning methods
(2015), http://dx.doi.org/10.1016/j.eswa.2015.01.047Quinlan, J. (1986). Introduction of decision trees. Machine Learning, 81–106.
Richardson, M., Dominowska, E., et al. (2007). Predicting clicks: Estimating the click-
through rate for new ads. Proceedings of the 16th international conference on World
Wide Web. Banff, Alberta, Canada: ACM (pp. 521–530). Banff, Alberta, Canada:
ACM.
Rokach, L. (2009). Taxonomy for characterizing ensemble methods in classification
tasks: A review and annotated bibliography. Computational Statistics & Data
Analysis, 53(12), 4046–4072.
Rutz, O. J., & Bucklin, R. E. (2007). A model of individual keyword performance in
paid search advertising. from <http://ssrn.com/abstract=1024765> or <http://
dx.doi.org/10.2139/ssrn.1024765>.
Rutz, O. J., Trusov, M., et al. (2011). Modeling indirect effects of paid search
advertising: Which keywords lead to more future visits? Marketing Science,
30(4), 646–665.
Sammut, C., & Webb, G. I. (2011). Encyclopedia of machine learning. New York:
Springer.
Sculley, D., Malkin, R. G., et al. (2009). Predicting bounce rates in sponsored search
advertisements. Proceedings of the 15th ACM SIGKDD international conference on
knowledge discovery and data mining. Paris, France: ACM (pp. 1325–1334). Paris,
France: ACM.
Sebastiani, F. (2002). Machine learning in automated text categorization. ACM
Computing Surveys, 34(1), 1–47.
Shaparenko, B., Çetin, Ö., et al. (2009). Data-driven text features for sponsored search
click prediction. Proceedings of the Third international workshop on data mining
and audience intelligence for advertising. Paris, France: ACM (pp. 46–54). Paris,
France: ACM.
Shmueli, G., Patel, N. R., et al. (2010). Data mining for business intelligence: Concepts,
techniques, and applications in microsoft office excel with XLMiner. Hoboken, N.J.:
Wiley.
Sidorov, G., Velasquez, F., et al. (2014). Syntactic N-grams as machine learning
features for natural language processing. Expert Systems with Applications, 41(3),
853–860.
Sigletos, G., Paliouras, G., et al. (2005). Combining information extraction systems
using voting and stacked generalization. Journal of Machine Learning Research, 6,
1751–1782.
Stone, P., Dunphy, D., et al. (1966). The general inquirer: A computer approach to
content analysis. Cambridge, MA.: The MIT Press.
Taboada, M., Brooke, J., et al. (2011). Lexicon-based methods for sentiment analysis.
Journal of Computational Linguistics, 37(2), 267–307.
Thelwall, M., Buckley, K., et al. (2012). Sentiment strength detection for the social
web. Journal of the American Society for Information Science and Technology,
63(1), 163–173.
Thelwall, M., Buckley, K., et al. (2010). Sentiment strength detection in short
informal text. Journal of the American Society for Information Science and
Technology, 61(12), 2544–2558.
Tweedie, F. J., Singh, S., et al. (1996). Neural network applications in stylometry: The
federalist papers. Computers and the Humanities, 30(1), 1–10.
Vapnik, V. N. (1995). The nature of statistical learning theory. New York: Springer-
Verlag.
Viaene, S., & Dedene, G. (2005). Cost-sensitive learning and decision making
revisited. European Journal of Operational Research, 166(1), 212–220.
Wang, G., Hao, J., et al. (2011). A comparative assessment of ensemble learning for
credit scoring. Expert Systems with Applications, 38(1), 223–230.
Wang, X., Lin, S., et al. (2012). Click-through prediction for sponsored search
advertising with hybrid models. 2012 KDD Cup, Beijing, China.
Weiss, G., & Provost, F. (2003). Learning when training data are costly: The effect of
class distribution on tree induction. The Journal of Artificial Intelligence Research,
3(19), 315–354.
Witten, I. H., Frank, E., et al. (2011). Data mining: Practical machine learning tools and
techniques. Elsevier.
Wolpert, D. H. (1992). Stacked generalization. Neural networks, 5(2), 241–259.
Yin, R. K. (2009). Case study research: Design and methods. Los Angeles, Calif.: Sage
Publications.
Zhang, G. P. (2000). Neural networks for classification: A survey. IEEE Transactions
on Systems, Man, and Cybernetics, Part C: Applications and Reviews, 30(4),
451–462.for pay-per-click campaign management. Expert Systems with Applications
