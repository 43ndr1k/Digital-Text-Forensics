Intelligent Data Analysis 14 (2010) 555–571 555
DOI 10.3233/IDA-2010-0439
IOS Press
Identifying violin performers by their
expressive trends
Miguel Molina-Solanaa,∗, Josep Lluı́s Arcosb and Emilia Gomezc
aDepartment of Computer Science and Artificial Intelligence, Universidad de Granada, Granada, Spain
bArtificial Intelligence Research Institute (IIIA), Spanish National Research Council (CSIC),
Bellaterra, Spain
cMusic Technology Group, Universitat Pompeu Fabra, Barcelona, Spain
Abstract. Understanding the way performers use expressive resources of a given instrument to communicate with the audience
is a challenging problem in the sound and music computing field. Working directly with commercial recordings is a good
opportunity for tackling this implicit knowledge and studying well-known performers. The huge amount of information to be
analyzed suggests the use of automatic techniques, which have to deal with imprecise analysis and manage the information
in a broader perspective. This work presents a new approach, Trend-based modeling, for identifying professional performers
in commercial recordings. Concretely, starting from automatically extracted descriptors provided by state-of-the-art tools, our
approach performs a qualitative analysis of the detected trends for a given set of melodic patterns. The feasibility of our
approach is shown for a dataset of monophonic violin recordings from 23 well-known performers.
1. Introduction
The advances in digital sound synthesis and computational power capabilities have allowed to provide
real-time control of synthesized sounds. Expressive control becomes then a relevant area of research and
a key challenge in the sound and music computing field [26].
According to Serra et al. [23], music performance is a complex activity that involves complementary
facets from different areas such as acoustics, psychology and creativity. In this sense, the research in this
field has a multidisciplinary character, ranging from studies that try to understand expressive performance
to attempts at modeling performance aspects in a formal, quantitative and predictive way. One of the
relevant research questions in this area is the modeling and identification of a given player or a playing
style by analyzing a set of performances.
In this interdisciplinary research field, musical expressivity can be approached from different perspec-
tives. One of them is the musicological analysis of music and the study of the different stylistic schools.
This approach provides a valuable understanding about musical expressivity.
Another perspective related to the present article is the computational modeling of music performance
analyzing music recordings. This analysis can be performed on a set of pieces specifically recorded for
the intended study and related to the performance aspect we want to analyze, where specific expressive
resources are emphasized. An alternative approach is to directly use commercial recordings for the
∗Corresponding author: Miguel Molina-Solana, Department of Computer Science and AI, ETSIIT, c/. Daniel Saucedo
Aranda, s/n 18071 Granada, Spain. Tel.: +34 958 240806; Fax: +34 958 243317; E-mail: miguelmolina@ugr.es.
1088-467X/10/$27.50  2010 – IOS Press and the authors. All rights reserved
556 M. Molina-Solana et al. / Identifying violin performers by their expressive trends
analysis of expressivity. This approach has several advantages: there are many recordings available (and
some performers may have several ones); the performances are ‘real’ and gather the decisions taken by
the performers without any external influence.
Nevertheless, working with commercial recordings has a drawback: these recordings do not come
from a controlled scenario and the sound analysis may become more difficult. Our claim is that the
advantages of working with commercial recordings overcome the drawbacks. Specifically, if a sufficient
amount of data is available, we can take advantage of data-intensive techniques that both soft computing
and machine learning fields provide.
In this paper, we focus on the task of identifying violinists from their playing style using descriptors
that are automatically extracted from commercial audio recordings by means of state-of-the-art tools.
First, since we consider audio excerpts from quite different sources, we assume a high heterogeneity in
the recording conditions. Second, as state-of-the-art audio transcription and feature extraction tools are
not 100% precise, we assume a partial accuracy in both the melodic transcription and feature extraction.
In this research we are only dealing with monophonic violin recordings.
Taking into account these constraints, our proposal identifies violin performers through capturing their
general expressive footprint with the following three stage process: (1) a higher-level abstraction of the
automatic transcription that focuses on the melodic contour; (2) melodic segments tagged according
to the E. Narmour’s Implication/Realization (IR) model [15]; and (3) the characterization of the way
melodic patterns are played as a set of frequency distributions.
The rest of the paper is organized as follows: Section 2 describes related work on the field of expressive
music performance and performer’s identification. In Section 3 we present the data collection being
used. In Section 4, we describe the proposed Trend-Based model and the developed system, including
data gathering, representation of recordings and distance measurement. In Section 5, some experiments
to validate our approach are proposed and their results are described. The paper concludes with final
considerations and pointing out future work in Section 6.
2. Related work
It is a clear fact that, in music performances, musicians do not play exactly what is written in the score.
They deviate from the score and enrich the performance with tempo, dynamics and tuning deviations.
There is an extensive literature on the analysis and modeling of music performances, focusing on different
instruments and using different methodologies. Juslin et al. [11] propose a model that characterizes this
variability in terms of four different sources: generative rules related to the musical structure; the
emotional expression decided by the performer; random variations; and movement principles. In our
research we are interested in capturing the variations due to the emotional expression and the performer’s
analysis of a piece.
Lopez de Mantaras et al. [3] studied the expressiveness of some computer music systems based on
artificial intelligence techniques and related it with the expressiveness of human-performed music. Re-
search on expressive music performance has traditionally focused on the analysis of score representation.
Nevertheless, the advances on audio content analysis and description (see [6] for a recent overview) allow
the study of music performance by directly analyzing audio recordings of professional musicians. These
performer’s variations have been modeled by applying machine learning techniques.
Related to the piano, Dovey [4] outlined an attempt to use inductive logic programming to determine
various interpretative rules that pianist Sergei Rachmaninoff may have used during his pianoforte per-
formances in an augmented piano. The goal was to determine general rules (in the form of universal
M. Molina-Solana et al. / Identifying violin performers by their expressive trends 557
predicates), concerning duration, tempo and key pressure, that underlie the way Rachmaninoff played
the songs.
Relevant work on automatic piano performer identification has been carried out by the group led by
Gerhard Widmer. Their research is based on acquiring a performance model via inductive learning and
data mining techniques applied on a huge corpus of precisely manually measured performances. To cite
some results, in [27] they applied an inductive rule learning algorithm to find general rules of music
performance, while in [24] they propose a set of simple features for representing stylistic characteristics
of piano music performers. In our approach, we do not require the availability of an annotated corpus. An
interesting contribution is the paper by Goebl [5], where he focused on finding a ‘standard performance’
by exploring the consensus among different performers.
In a recent work, Saunders et al. [22] represent pianists’ performances as strings with information
related to changes in tempo and loudness. Finally, they use Support Vector Machines to identify the
performer of new recordings.
Hong [10] investigated expressive timing and dynamics in recorded cello following an empirically
perspective. He took the work by Todd [25] as a departure point, extending and testing it with some
commercial recordings by famous cellists. Sapp’s work [21] should also be cited as an interesting
proposal for representing musical performances by means of scape plots based on tempo and loudness
correlation.
Ramirez et al. [19] focused on saxophone recordings and studied deviations of parameters such as pitch,
timing, amplitude and timbre, both at an inter-note level and at an intra-note level. They further extended
the system to identify performers in Celtic violin recordings [20] and to the analysis of ornaments in
bassoon recordings [18].
An alternative approach to inductive learning is the use of case-based reasoning (CBR) techniques.
CBR is a lazy learning technique that, instead of building a general model from the existing examples,
uses directly the examples to construct a solution for new problems. Examples of systems that deal with
music performance using a CBR approach are SaxEx and TempoExpress systems. SaxEx [1] is a system
capable of generating high-quality expressive solo performances of jazz saxophone ballads based on
examples of human performers. TempoExpress [9] performs expressivity-aware tempo transformations
of saxophone jazz recordings.
3. Musical data
We have chosen to work with Sonatas and Partitas for solo violin from J.S. Bach [13]. Sonatas and
Partitas for solo Violin by J.S. Bach is a six work collection (three Sonatas and three Partitas) composed
by the German musician. It is a well-known collection that almost every violinist plays during their
artistic life. All of them have been recorded many times by several players. The reason of using this
work collection is twofold: 1) we have the opportunity of testing our model with existing commercial
recordings of the best known violin performers, and 2) we can constrain our research on monophonic
music.
In our experiments, we have extended the musical collection presented in [17]. We analyzed music
recordings from 23 different professional performers. Because these audio files were not recorded for
our study, we have not interfered at all with players’ style at the performance [12]. For the experimental
results presented in this paper, we used three different movements: the Second and the Sixth movements
of Partita No. 1 (both a Double) and the Fifth movement of Partita No. 3 (Bourrée). These three
movements are quite interesting for initial experiments because most of the notes are eighth notes,
558 M. Molina-Solana et al. / Identifying violin performers by their expressive trends
Table 1
Performers analyzed in the experiments
Ara Malikian Jacqueline Ross Rachel Podger
Arthur Grumiaux James Ehnes Sergiu Luca
Brian Brooks Jascha Heifetz Shlomo Mintz
Christian Tetzlaff Josef Suk Sigiswald Kuijken
Garrett Fischbach Julia Fischer Susanna Yoko Henkel
George Enescu Lucy van Dael Tanya Anisimova
Itzhak Perlman Mela Tenenbaum Yehudi Menuhin
Jaap Schroder Nathan Milstein
leading us to acquire a model based on many homogeneous segments. The scores of the analyzed pieces
are not provided to the system.
Table 1 shows the list of the 23 professional violinists used in our study. From the list, it can be stated
that there is a variety of performing styles. Moreover, some of them are from the beginning of the last
century while others are modern performers currently active. Violinists like B. Brooks, R. Podger, and
C. Tetzlaff play in a Modern Baroque style; whereas violinists like S. Luca, S. Mintz, and J. Ross are
well-known for their use of détaché.
We also included two recordings that are clearly different from the others: G. Fischbach and T.
Anisimova. G. Fischbach plays with a sustained articulation. T. Anisimova is a cellist and, then, the
performance is clearly very different from the others.
4. Trend-based modeling
Because we are using state-of-the-art feature extraction tools and we do not manually process the
information we collect, it is not feasible to learn a deep model about each violin performer, i.e. to learn
rules mapping specific musical concepts to expressive resources. From one side, we cannot perform
a confident musical analysis from the (approximate) score provided by the extraction tools. On the
other side, without a high precision in identifying note attacks, we know that the accuracy of expressive
features is not guaranteed.
The alternative is to take a more global perspective of a performer by trying to capture their essential
(and recurrent) expressive decisions. For instance, playing in a romantic articulation will affect note
durations and will produce characteristic energy envelopes. Additionally, the use of ritardando produces
local changes in note durations and energy strengths.
These expressive decisions are somehow related to the musical analysis each violinist performs of
the written score. Nevertheless, we do not have the real score (that one on the music sheet), but only
an approximated score derived from the audio. We propose to use a melodic contour segmentation
as a way to capture the musical structure of the piece. As most of automatic melody segmentation
approaches, we will perform note grouping according to a human perception model. Different levels of
abstraction have been proposed for melodic contour analysis. In [7], the advantages and drawbacks of
using different abstraction levels have been presented and the Implication/Realization (IR) model [16]
has demonstrated the best performance because its capacity of summarizing melodic and rhythmic
information. Specifically, the IR model proposes some basic melodic patterns that will be used as an
approximation of the local structure of a piece. Moreover, IR model allows a melodic analysis in terms
of melodic intervals and relative durations, i.e. precision on score extraction is not critical. Furthermore,
in Section 5.3 we show how Narmour’s model outperforms the results of a simple contour-based model.
M. Molina-Solana et al. / Identifying violin performers by their expressive trends 559
Then, our approach for dealing with the identification of violin performers is based on the acquisition
of trend models that characterize each particular performer to be identified. Specifically, a trend model
characterizes, for a certain audio descriptor, the relationships a given performer is establishing among
groups of neighbor musical events. We perform a qualitative analysis of the variations of the audio
descriptors. Moreover, as we will describe in the next subsection, we analyze these qualitative variations
with a local perspective.
We use two trend models in this paper: energy and duration. The trend model for the energy descriptor
relates, qualitatively, the energy variation for a given set of consecutive notes, and it is related to dynamics.
On the other hand, the trend model for duration indicates, also qualitatively, how note durations change
for note sequences. Duration is related to articulation and timing. Notice that trend models are not trying
to characterize the audio descriptors with respect to an expected global behavior.
Given a musical recording of a piece as input, the trend analysis is performed by aggregating the
qualitative variations on their small melody segments. Thus, previously to build trend models, input
streams are broken down into segments.
Our system has been designed in a modular way with the intention of creating an easy extendable
framework. We have three different types of modules in the system: 1) the audio feature extraction and
segmentation modules; 2) the trend analysis modules; and 3) the identification modules. Moreover, the
system may work in two different modes: training and testing. Modules from (1) and (2) are used in both
modes. Modules from (3) are only used in the testing mode. Figure 1 shows a diagram of the system
modules. On top, audio files in .wav format as input.
At the training stage, the goal of the system is to characterize performers by extracting expressive
features and constructing trend models. Next, at the identification stage, the system analyzes the input
performance and looks for the most similar previously learned model. The training process is composed
of three main steps: 1) the extraction of audio descriptors and the division of a performance in segments;
2) the tagging of segments according to IR patterns; and 3) the computation of probabilistic distributions
for each IR pattern and descriptor (trend generation).
4.1. Feature extraction and segmentation
The first step consists on extracting audio features. Our research is not focused on developing new
methods for extracting audio features, so that we employ existing techniques. At the moment, we
consider fundamental frequency and energy, as these are the main low-level audio features related to
melody. These features are then used to identify note boundaries and to generate melodic segments. The
current version of our system uses the fundamental frequency estimation algorithm proposed in [2]. This
module provides a vector with instantaneous fundamental frequency and energy values computed every
0.01 seconds.
We have developed a post-processing module for determining the possible notes played by the per-
formers. This module first converts fundamental frequencies into quantized pitch values, and then a pitch
correction procedure is applied in order to eliminate noise and sudden changes. This correction is made
by assigning to each sample the value given by the mode of their neighbors around a certain window of
size σ. When some notes are played together we only take into account the one with more energy.
With this process, a smooth vector of pitches is obtained. By knowing on which frames pitches are
changing (i.e. consecutive values are different), a note-by-note segmentation of the whole recording is
performed. For each note we collect its pitch, duration and energy.
We assume that there might be some errors in this automatic segmentation, given the heterogeneity
of recording conditions. Our approach for dealing with this problem consists of using a more abstract
560 M. Molina-Solana et al. / Identifying violin performers by their expressive trends
Fig. 1. Architecture of the system.
representation that the real notes, but still close to the melody. That is, instead of focusing on the absolute
notes, we are interested in modeling the melodic surface. We use the Implication/Realization model to
perform melodic segmentation.
4.1.1. The implication/realization model
The Implication/Realization (IR) Model proposed by Eugene Narmour [15,16] is based on a perceptual
and cognitive approach for analyzing the structure of a musical piece. Gratchen et al. [8] showed in
MIREX’05 that the IR model is suitable for assessing melodic similarity. Since our goal is to characterize
expressive trends, we analyze the way different audio descriptors change in the different IR patterns.
IR model tries to explicitly describe the patterns of expectations generated in the listener with respect
to the continuation of the melody. It follows the approach introduced by Meyer [14] that applies the
principles of Gestalt Theory to melody perception. The model describes both the continuation implied
by particular melodic intervals, and the degree to which this expected continuation is actually realized
by the following interval.
Gestalt theory states that perceptual elements are grouped together to form a single perceived whole
M. Molina-Solana et al. / Identifying violin performers by their expressive trends 561
Table 2
Characterization of the ten IR structures we are able to identify; in the second
column, ‘S’ denotes small, ‘L’ large, and ‘0’ a unison interval
Structure Interval sizes Same direction? PID satisfied? PRD satisfied?
P S S yes yes yes
D 0 0 yes yes yes
ID S S (eq) no yes no
IP S S no yes no
VP S L yes no yes
R L S no yes yes
IR L S yes yes no
VR L L no no yes
RP L L yes no no
RR S L no no no
Fig. 2. Ten basic structures of the IR model.
(called ‘gestalt’). This grouping follows some principles: proximity (two elements are perceived as a
whole when they are perceptually close), similarity (two elements are perceived as a whole when they
have similar perceptual features, e.g. color in visual perception), and good continuation (two elements
are perceived as a whole if one is a ‘natural’ continuation of the other).
Narmour claims that similar principles hold for the perception of melodic sequences. In IR, these
principles take the form of implications and involve two main principles: registral direction (PRD) and
intervallic difference (PID). The PRD principle states that small intervals create the expectation of a
following interval in the same registral direction (for instance, a small upward interval generates an
expectation of another upward interval), and large intervals create the expectation of a change in the
registral direction (for instance, a large upward interval generates an expectation of a downward interval).
The PID principle states that a small (five semitones or less) interval creates an expectation of a following
similarly-sized interval (plus or minus two semitones), and a large interval (seven semitones or more)
creates an expectation of a following smaller interval.
Based on these two principles, melodic patterns can be identified that either satisfy or violate the
implication as predicted by the principles. Such patterns are called structures and labeled to denote
characteristics in terms of registral direction and intervallic difference. The ten basic structures are
shown in Fig. 2. For example, the P structure (‘Process’) is defined as two or more consecutive small
intervals (of similar size) satisfying both the registral direction principle and the intervallic difference
principle. Similarly, the IP (‘Intervallic Process’) structure satisfies intervallic difference, but violates
registral direction.
Some additional structures are retrospective counterparts of the basic structures. In general, the
retrospective variant of a structure has the same registral form and intervallic proportions, but the
intervals are smaller or larger. For example, an initial large interval does not give rise to a P structure
(rather to an R, IR, or VR, see Fig. 2), but when is followed by another large interval in the same
registral direction, the pattern is a pair of similarly sized intervals with the same registral direction and
it is identified as a retrospective P structure (denoted as RP). An analogous analysis can be performed
with the R structure resulting in a new retrospective pattern (denoted as RR). Table 2 summarizes the
characteristics of all these IR structures.
562 M. Molina-Solana et al. / Identifying violin performers by their expressive trends
Fig. 3. IR analysis of the beginning of the Sixth Movement (Double) of Partita No. 1.
We have developed an algorithm to automate the annotation of melodies with their corresponding IR
analyses. The algorithm implements most of the ‘innate’ processes mentioned before and is able to
detect the ten different 3-note patterns described in Table 2: P, D, ID, IP, VP, R, IR, VR, RP and RR. The
algorithm uses a sliding window of three notes that moves in steps of one note. For each window, we
calculate the size and direction of the two intervals within it, and the IR structure which matches with
this 3-note group is selected. In Fig. 3 the IR analysis of the beginning of the Sixth Movement of Partita
No. 1 can be found as an example of how we tag each 3-note group.
4.2. Modeling trends
A trend model is represented by a set of discrete frequency distributions for a given audio descriptor
(e.g. energy). Each of these frequency distributions represents the way a given IR pattern is played
against that certain audio descriptor. Since we are tagging the audio segments with ten different IR
patterns, each trend model is represented by ten different frequency distributions.
To generate trend models for a particular performer and audio descriptor, we use the sequences of values
extracted from the notes identified in each segment. From these sequences, a qualitative transformation
is first performed to the sequences in the following way: each value is compared to the mean value of the
fragment and is transformed into a qualitative value where + means ‘the descriptor value is higher than
the mean’, and − means ‘the descriptor value is lower than the mean’. Being s the size of the segment
and n the number of different qualitative values, there are ns possible resulting shapes. In the current
approach, since we are segmenting the melodies in groups of three notes and using two qualitative values,
eight (23) different shapes may arise. We note these possibilities as: − − −, − − +, − + −, − + +,
+ −−, + − +, + + − and + + +.
Next, a histogram per IR pattern with these eight qualitative shapes is constructed by calculating the
percentage of occurrence of each shape. These histograms can be understood as discrete probability
distributions. Thus, trend models capture statistical information of how a certain performer tends to play.
Combining trend models from different audio descriptors, we improve each performer characterization.
Since our goal is the identification of violin performers, the collection of trend models acquired for
each performer is used as the patterns to compare with when a new audio recording is presented to the
system.
4.2.1. Current trends
We have generated trend models for both duration and energy descriptors as they are the low-level
descriptors more closely related with the melody. Note durations are computed as the number of samples
between pitch changes. The mean duration of each note (in samples) is obtained by dividing the total
number of samples for the whole fragment by the number of recognized notes in it (obviously taking into
account different figures). Because we have computed both the real and the expected (mean) duration
M. Molina-Solana et al. / Identifying violin performers by their expressive trends 563
Fig. 4. Frequency distribution of duration deviations for the P pattern in the Sixth movement of Partita No. 1. Only four
performers are shown.
for each note we can say whether a note is longer than it should be, or opposite, if it is shorter. So that,
the binary vector with qualitative deviations is built and the trend model for the duration descriptor is
obtained by matching with the identified IR structures.
Figure 4 shows, for the duration descriptor, the frequency distributions of the eight shapes in P patterns
(ascending or descending sequences of small intervals) and for four violin performers (Ara Malikian,
Arthur Grumiaux, Brian Brooks, and Christian Tetzlaff).
We observe that the way different professional performers are playing is not equally distributed. For
instance, A. Malikian has a higher propensity to extend the durations while an opposite behavior can
be observed for A. Grumiaux (see his values for the two left qualitative shapes). It should be noticed
that more exact ways of obtaining this measure could be used, as well as taking into account the attack
and release times, as other researchers do [19]. This would lead us to a more complex process that
we definitively want to avoid by now. Hopefully, we guess that our approach does not need so precise
information to identify a given performer.
We have also acquired trend models for the energy descriptor in an analogous way. The energy average
for each fragment is calculated and, given the energy of each note, qualitative deviations are computed.
Next, from these qualitative values, the trend models are constructed by calculating the frequencies of
the eight shapes for each IR pattern.
4.3. Classifying new performances
A nearest neighbor classifier is used to predict the performer of new recordings. Trend models acquired
in the training stage, as described in the previous section, are used as class patterns, i.e. each trained
performance is considered a different solution class. When a new recording is presented to the system,
the feature extraction process is performed and its trend model is created. This trend model is compared
with the previously acquired models. The classifier outputs a ranked list of performer candidates where
564 M. Molina-Solana et al. / Identifying violin performers by their expressive trends
distances determine the order, with 1 being the most likely performer relative to the results of the training
phase.
4.3.1. Distance measure
The distance dij between two trend models i and j (i.e. the distance between two performances), is
defined as the weighted sum of distances between the frequency distributions of IR patterns:
dij =
∑
n∈N
wnijdist(ni, nj) (1)
where N is the set of the different IR patterns considered; dist(ni, nj) measures the distance between
two frequency distributions (see (3) below); and wnij are the weights assigned to each IR pattern. Weights
have been introduced for balancing the importance of the IR patterns with respect to the number of times
they appear. Frequent patterns are considered more informative due to the fact that they come from more
representative samples. Weights are defined as the mean of cardinalities of respective histograms for a
given pattern n:
wnij = (N
n
i + N
n
j )/2 (2)
Mean value is used instead of just one of the cardinalities to assure a symmetric distance measure in
which wnij is equal to w
n
ji. Cardinalities could be different because recognized notes can vary from one
performance to another one, even though the score is supposed to be the same.
Finally, distance between two frequency distributions is calculated by measuring the absolute distances
between the respective patterns:
dist(s, r) =
∑
k∈K
|sk − rk| (3)
where s and r are two frequency distributions for the same IR pattern; and K is the set of all possible
values they can take (in our case |K| = 8).
When both audio descriptors are considered, we simply aggregate the individual corresponding dis-
tances.
5. Experiments
The goal of the experiments was to assess the feasibility of our approach. We are aware that additional
features could be extracted from the recordings. But combining two basic features such as energy and
duration we were interested in measuring the robustness of an identification model that only captures
some basic expressive trends.
Experiments consisted in training the system with one movement and, then, testing the acquired trend
models with a different movement. We used three different movements in the experiments: the Second
and Sixth movements of Partita No. 1 (both a Double) and the Fifth movement of Partita No. 3 (Bourrée).
Each experiment was performed using three different configurations of trend models. Specifically,
the performance of experiments was compared with only using duration-based trend models, only using
energy-based trend models, and using both trend models. Finally, we compared the results with only
using a contour-based model.
M. Molina-Solana et al. / Identifying violin performers by their expressive trends 565
Table 3
Number of different IR structures identified in each of the three movements
studied. The values are the mean of the 23 recordings
P D ID IP VP R IR VR RP RR
P.1 No.2 287 0 187 222 42 89 56 0 113 100
P.1 No.6 257 0 156 157 26 56 28 0 66 64
P.3 No.5 256 0 125 145 18 44 23 0 52 55
Table 4
Success rate (%) in all experiments taking into account three different ranking posi-
tions proposed for the correct performer: 1st, 3rd, and 10th
set-1 set-2 set-3
1st 3rd 10th 1st 3rd 10th 1st 3rd 10th
duration 34.8 60.9 95.7 21.7 43.5 91.3 10.5 26.3 68.4
energy 21.7 69.6 91.3 30.4 47.8 91.3 15.8 31.6 73.7
both 52.2 65.2 91.3 34.8 47.8 95.7 15.8 26.3 68.4
5.1. Acquiring trend models
The generation of trend models starts with the segmentation of the recordings at two levels: note events
and IR patterns. At the low level, the pitch estimation algorithm segments a recording with a sequence
of possible notes. Comparing the number of notes automatically detected with the real scores, the pitch
estimator is generating more segments than the notes in the score (sometimes doubling the number of
notes). This result is not surprising because we are not explicitly dealing with expressive resources like
vibratos or portamentos. Thus, some IR patterns will contain these expressive resources. Nevertheless,
because we are not explicitly working with notes, these expressive patterns will also contribute to the
characterization of performing styles. For instance, a portamento is usually captured as a sequence of
notes where one (or some) have a short duration.
Table 3 details the number (in mean) of IR segments detected for each movement. Observe that there
are two IR structures (D and VR) that are not present in the recordings. The most recurrent IR structure
is the P pattern followed by the ID pattern. This result is coherent with the fact that the three movements
contain a lot of arpeggios.
Finally, trend models of performers are built from the way these 710 to 1.000 IR structures are
performed in terms of energy and duration.
In order to better understand what the different trend models are capturing, we obtained the distances
dij between all of them (see Eq. (1)) and applied a hierarchical clustering algorithm (using a complete
linkage method). Figure 5 shows the dendrogram representation of the hierarchical clustering for the
Sixth movement of Partita No. 1. It is interesting to remark that some performing styles are captured
by these trend models. For instance, the most dissimilar recordings (G. Fischbach and T. Anisimova)
are clearly far from the rest; violinists playing in a Modern Baroque style (B. Brooks, R. Podger, and
C. Tetzlaff) are clustered together; violinists using détaché (S. Luca, S. Mintz, and J. Ross) also appear
close to each other; and the usage of expressive resources such as portamento, vibrato, or ritardando
presents a relationship with the clustering result.
5.2. Results
We performed two different types of experiments. The first experiment was focused on assessing the
performance of the system by using two movements from the same piece. Specifically, we used the
566 M. Molina-Solana et al. / Identifying violin performers by their expressive trends
Fig. 5. Hierarchical clustering for the Sixth movement of Partita No. 1.
Second and the Sixth movements of Partita No. 1. In the following, we will call set-1 the experiment
where 23 instances of the second movement were used for training and 23 from the sixth for testing.
Analogously, we will call set-2 the experiment where the sixth movement was used for training and the
second for testing.
The second type of experiments was focused on assessing the performance of the system by using
two movements from different pieces. Specifically, we used the second movement of Partita No. 1 for
training and the fifth movement of Partita No. 3 for testing. We will refer to this test as set-3.
For each input recording, the system result is a ranked list of performers sorted from the most similar
to the furthest one to the input. The highest accuracy is achieved when the correct performed is the first
of the list. Otherwise, a last position for the correct performer represents the worst accuracy.
A complete view of the results is shown in Fig. 6 and summarized in Table 4. Figure 6 shows the
percentage of input recordings identified at each position. It provides a picture of the system accuracy
using the length of the proposed ranking as a threshold. Table 4 summarizes the performance of the
system for the three experimental sets and the three trend models. The three columns of each experiment
show, respectively, the percentage of performers identified in the first position, at least in the third
position, and at least in the tenth position.
Regarding the experiments with movements from the same Partita (experiments set-1 and set-2), the
correct performer was mostly identified in the first half of the list, i.e. at most in the 12th position.
The correct performer is predicted, in the worst case, 20% of times as the first candidate, clearly
outperforming the random classifier (whose success rate is 4.3%). Additionally, using the four top
candidates the accuracy reaches the 50% of success.
Regarding experiment set-3, the most difficult scenario, the 90% of identification accuracy was
overcame at position 15. The 50% of success for the trend models based on only one feature (duration
or energy) is achieved by selecting seven performers. Combining both features the accuracy overcomes
60%. The results are promising, especially comparing with a random classification where the success
rate is clearly outperformed.
M. Molina-Solana et al. / Identifying violin performers by their expressive trends 567
Fig. 6. Accumulated success rate by position of the correct performer in all the performed experiments.
568 M. Molina-Solana et al. / Identifying violin performers by their expressive trends
Fig. 7. Classifier output in matrix form for set-1 where both (duration and energy) trend models were used.
Figure 7 presents a matrix that summarizes the classifier output for set-1 using both duration and
energy trend models. The figure details the information given by the ‘duration+energy’ curve in Fig. 6a.
Specifically, it shows, for each input recording (row), the sorted list of predicted performers as squares.
Ranking values are mapped onto a gray scale. The black color indicates the first performer proposed
and the gray degradation is used to draw all the performers predicted until the correct one. Notice that
the success in the first position means a black square in the diagonal. The matrix is not supposed to
be symmetric and each column can have the same color several times because a predicted performer
can occur in the same position for several inputs. For instance, we can see that Garret Fischbach’s
performance (gar) for Sixth Movement is very different from the rest of performers’ Second Movement
performances: all values correspond to distance positions. On the other hand, Christian Tetzlaff’s (chr)
and Rachel Podger’s (rac) performances are quite similar to most of Second Movement performances
since there are many squares in their columns.
Finally, Fig. 8 shows in which position the correct performer is ranked for each performer in the test
set. This Figure complements the former two ones. The results came from set-1 using both trend models
(‘duration+energy’ curve in Fig. 6a). Twelve right identifications were achieved at first position (52%
of success rate). The rest was correctly identified in positions 2 to 4 except three performers. Nathan
Milstein was identified at position 6. Finally, Sergiu Luca and Shlomo Mintz were not clearly identified.
After a detailed analysis of the distances among all performers, we observed that these two musicians
are not clearly distinguished when using a nearest neighbor classifier. Their performances, with respect
to duration and energy, are close to multiple other performers.
5.3. Analyzing a contour-based approach
In order to validate the IR approach, we performed experiments for assessing the performance of a
contour-based segmentation. The contour segmentation provides nine classes regarding the direction of
the two existing intervals within each 3-note group: two ascending intervals, two descending, one up
M. Molina-Solana et al. / Identifying violin performers by their expressive trends 569
Fig. 8. Correct performer position for each performance in set-1. Both trend models are used.
Fig. 9. Comparison of results by using Narmour segmentation and simple contour segmentation for set-1, set-2 and set-3. Both
trend models (duration and energy) were used.
and one down, two unisons, and so on. Figure 9 provides a comparison between the classification results
using IR trends versus contour trends. The contour-based segmentation also outperforms the random
classifier. Nevertheless, the IR-based models present better performance than the contour-based models.
The results are not surprising because IR models are constructed with a finer interval analysis. Moreover,
we have to stress that the contour-based results profit from the rhythmic regularity (most of the notes are
eights), i.e. in the general case results will be worse.
570 M. Molina-Solana et al. / Identifying violin performers by their expressive trends
6. Conclusions
This work focuses on the task of identifying violinists from their playing style by building trend-
based models that capture expressive tendencies. Trend models are acquired by using state-of-the-art
audio feature extraction tools and automatically segmenting the obtained melodies using IR patterns.
Performers were characterized by a set of frequency distributions, capturing their personal style with
respect to a collection of melodic patterns (IR patterns). We have shown that, without a great analysis
accuracy, our proposal is quite robust.
The experiments were concentrated on identifying violinists and using note durations and energies as
descriptors. We tested the system with 23 different professional performers and different recordings.
Results obtained show that the proposed model is capable of learning performance patterns that are useful
for distinguishing performers. The results clearly outperform a random classifier and, probably, it would
be quite hard for human listeners to achieve such recognition rates. In order to assure the robustness of
the system, other sets of works should be used for learning and testing.
Our current experiments have been constrained to monophonic audio. We would like to extend the
method in order to deal with polyphonic recordings in an appropriate way.
We have presented a qualitative analysis using only two qualitative values. We want to keep our model
at this qualitative level but we plan to extend the model with the use of fuzzy sets. This improvement
will allow us to use the capabilities of fuzzy theory for a better assessment in the similarity measure.
Combining information from different music features has been demonstrated to improve results. We
are currently working on increasing the number of descriptors. Since the predictability of a given
descriptor varies depending on the performers, we are also interested in discovering relations among the
descriptors. Finally, the use of hierarchical classifiers or ensemble methods is a possible way to improve
the identification accuracy.
Acknowledgements
The authors want to thank Agustin Martorell for his valuable comments regarding the musicologist
meaning of the expressive resources used by the violinists in the recordings. Miguel Molina-Solana
is supported by the Spanish Ministry of Education and Science (project TIN2006-15041-C04-01 and
FPU grant AP2007-02119). Josep Lluı́s Arcos is supported by the Spanish project MID-CBR (TIN2006-
15140-C03-01), EU-FEDER funds, and by the Catalan Government (2005-SGR-00093). Emilia Gomez is
supported by EU-eContent-Plus project VARIAZIONI1 (ECP-2005-CULT-038264).
References
[1] Josep Lluı́s Arcos, Ramon López de Mántaras and Xavier Serra, Saxex: a case-based reasoning system for generating
expressive musical performances, Journal of New Music Research 27(3) (1998), 194–210.
[2] Arturo Camacho, SWIPE: A Sawtooth Waveform Inspired Pitch Estimator for Speech and Music, PhD thesis, University
of Florida, USA, 2007.
[3] Ramon Lopez de Mantaras and Josep Lluis Arcos, AI and music: from composition to expressive performance, AI
Magazine 23(3) (2002), 43–57.
1http://www.variazioniproject.org
M. Molina-Solana et al. / Identifying violin performers by their expressive trends 571
[4] Matthew J. Dovey, Analysis of Rachmaninoff’s piano performances using inductive logic programming, In Proc. of the
8th European Conference on Machine Learning (ECML95), pages 279–282, London, UK, 1995. Springer-Verlag.
[5] Werner Goebl, Analysis of piano performance: Towards a common performance standard? In Proc. of the Society for
Music Perception and Cognition Conference (SMPC99), Evanston, USA, 1999.
[6] F. Gouyon, P. Herrera, E. Gómez, P. Cano, J. Bonada, A. Loscos, X. Amatriain and X. Serra, Content Processing of
Music Audio Signals, (Chapter 3), Logos Verlag Berlin GmbH, Berlin, 2008, pp. 83–160.
[7] Maarten Grachten, Josep Lluis Arcos and Ramon Lopez de Mantaras, Melodic similarity: Looking for a good abstraction
level, pages 210–215, 2004.
[8] Maarten Grachten, Josep Lluis Arcos and Ramon Lopez de Mantaras, Melody retrieval using the Implication/Realization
Model. In 6th International Conference On Music Information Retrieval (ISMIR 2005). (First prize of the MIREX
Symbolic Melodic Similarity Contest), 2005.
[9] Maarten Grachten, Josep Lluis Arcos and Ramon Lopez de Mantaras. A case based approach to expressivity-aware
tempo transformation, Machine Learning 65(2-3) (2006), 411–437.
[10] Ju-Lee Hong, Investigating expressive timing and dynamics in recorded cello, Psychology of Music 31(3) (2003),
340–352.
[11] Patrik N. Juslin, Anders Friberg and Roberto Bresin, Toward a computational model of expression in performance: The
GERM model, Musicae Scientiae, special issue: 63–122, 2002.
[12] Patrik N. Juslin and John A. Sloboda, Music and Emotion: Theory and Research, Oxford University Press, 2001.
[13] Joel Lester, Bach’s Works for Solo Violin: Style, Structure, Performance, Oxford University Press, 1999.
[14] Leonard Meyer, Emotion and Meaning in Music, Chicago, IL: Univ. Chicago Press, 1956.
[15] Eugene Narmour, The Analysis and Cognition of Basic Melodic Structures: The Implication Realization Model, Chicago,
IL: Univ. Chicago Press, 1990.
[16] Eugene Narmour, The Analysis and Cognition of Melodic Complexity: The Implication Realization Model, Chicago, IL:
Univ. Chicago Press, 1992.
[17] Montserrat Puiggros, Comparative analysis of expressivity in recorded violin performances. Study of the sonatas and
partitas for solo violin by J. S. Bach. Master’s thesis, Universitat Pompeu Fabra, Barcelona, Spain, 2007.
[18] Montserrat Puiggros, Emilia Gómez, Rafael Ramirez, Xavier Serra and Roberto Bresin, Automatic characterization of
ornamentation from bassoon recordings for expressive synthesis, In Proc. of 9th International Conference on Music
Perception and Cognition (ICMPC 2006), Bologna, Italy, 2006.
[19] Rafael Ramirez, Esteban Maestre, Antonio Pertusa, Emilia Gómez and Xavier Serra, Performance-based interpreter
identification in saxophone audio recordings, IEEE Trans. on Circuits and Systems for Video Technology 17(3) (2007),
356–364.
[20] Rafael Ramirez, Alfonso Perez, Stefan Kersten and Esteban Maestre, Performer identification in celtic violin recordings.
In Proc. of 9th International Conference on Music Information Retrieval (ISMIR 2008), pages 483–488, Philadelphia,
USA, 2008.
[21] Craig Sapp, Comparative analysis of multiple musical performances, In Proc. of 8th International Conference on Music
Information Retrieval (ISMIR 2007), pages 497–500, Vienna, Austria, 2007.
[22] Craig Saunders, David Hardoon, John Shawe-Taylor and Gerhard Widmer, Using string kernels to identify famous
performers from their playing style, Intelligent Data Analysis 12(4) (2008), 425–440.
[23] Xavier Serra, Gerhard Widmer and Marc Leman, A roadmap for sound and music computing, The S2S2 Consortium,
2007.
[24] Efstathios Stamatatos and Gerhard Widmer, Automatic identification of music performers with learning ensembles,
Artificial Intelligence 165(1) (2005), 37–56.
[25] Neil P. Todd, The dynamics of dynamics: A model of musical expression, Journal of the Acoustical Society of America
91(6) (1992), 3540–3550.
[26] Vesa Valimaki, Rudolf Rabenstein, Davide Rocchesso, Xavier Serra and Julius O. Smith, Signal processing for sound
synthesis: Computer-generated sounds and music for all, IEEE Signal Processing Magazine 24(2) (2007), 8–10.
[27] Gerhard Widmer, Simon Dixon, Werner Goebl, Elias Pampalk and Asmir Tobudic, In search of the Horowitz factor, AI
Magazine 24(3) (2003), 111–130.
