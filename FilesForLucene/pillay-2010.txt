1569329097  
 
APWG eCrime Researchers Summit 2010 
1 

Abstract—Extracting useful information from user generated 
text on the web is an important ongoing research in natural 
language processing, machine learning, and data mining. Online 
tools like emails, news groups, blogs, and web forums provide an 
effective communication platform for millions of users around the 
globe and also provide an added advantage of anonymity. 
Millions of people post information on different web forums daily. 
The possibility of exchanging sensitive information between 
anonymous users on these web forums cannot be ruled out. This 
document proposes a two stage approach for combining 
unsupervised and supervised learning approaches for performing 
authorship attribution on web forum posts. During the first stage, 
the approach focuses on using clustering techniques to make an 
effort to group the data sets into stylistically similar clusters. The 
second stage involves using the resulting clusters from stage one 
as features to train different machine learning classifiers. This 
two stage approach is an effort towards reducing the complexity 
of the classification task and boosting the prediction accuracy. 
 
 
Index Terms—Authorship attribution, clustering, machine 
learning classifiers, stylometry, and text categorization 
 
I. INTRODUCTION 
UTHORSHIP attribution can be defined as a text 
classification task where the objective is to determine the 
author of an anonymous text by applying machine learning and 
other Natural Language Processing (NLP) [1]. The idea is that 
an author might leave a unique pattern in his/her posts and 
authorship analysis can be used to identify those unique 
characteristics to classify authors. This classification problem 
mainly focuses on two aspects of text: the content of the post 
and the writing style of the author. Some of the major 
challenges are that web forum posts are generally short in 
length and the writing styles are less formal. A common 
problem with authorship attribution is that sufficient 
benchmark data is not available for comparing and evaluating 
the accuracy of results.  
Web forums can provide an effective means of 
communication for criminal activities among anonymous 
users. Screen names or aliases are used by people to 
 
Manuscript received November 08, 2010. This work was supported by the 
Department of Computer and Information Sciences at University of Alabama 
at Birmingham.  
S. R. Pillay is a graduate student in the Department of Computer and 
Information Sciences at the University of Alabama at Birmingham, 1300 
University Boulevard, 139 Campbell Hall, Birmingham, AL 35294 USA (e-
mail: rsangita@uab.edu).  
T. Solorio is an assistant professor in the Department of Computer and 
Information Sciences at the University of Alabama at Birmingham, 1300 
University Boulevard, 113 Campbell Hall, Birmingham, AL 35294 USA (e-
mail: solorio@uab.edu).  
978-1-4244-7761-6/10/$26.00 ©2010 IEEE. 
communicate with each other on these forums. In addition, 
some users may not include real identification information 
with their account.  Moreover, a user can setup multiple 
accounts on these online forums. For example, suppose a user 
who was involved in cyber-bullying other users, or in 
conducting illegal activities was previously identified and 
banned from the forum. The same user can open up a new user 
account with false information and start using the forum again. 
Therefore, this anonymity poses several challenges to the law 
enforcement community in tracing back the identity of the 
anonymous users. As an effort towards addressing the 
problem, this paper proposes to combine unsupervised and 
supervised learning algorithms to perform authorship 
attribution. During the training phase, clustering will be used 
to divide the large data sets into stylistically similar clusters. 
Later, a classification model will be trained using a variety of 
lexical features, syntactical features and clustering solutions.  
This paper focuses on identifying different author-
specific characteristics from web forum posts and then using 
those to train different machine learning algorithms for 
classifying authors. We use stylometry (identification of 
unique patterns), statistical language models, clustering, and 
machine learning algorithms such as Bayesian Networks, 
Decision Trees (C4.5) and Naïve Bayes for classification. The 
rest of the paper reviews previous efforts related to this area, 
conducts experiments on different datasets using the proposed 
approach, analyzes the classification results, and finally 
discusses ideas for future work.  
 
II. BACKGROUND 
A. Types of Authorship Analysis 
Apart from authorship identification, the authorship 
analysis task can be categorized as follows: 
1) Verifying whether a given piece of text was produced by a 
particular author [2].  
2) Finding similarities between two texts for detecting 
plagiarism [3]. 
3) Extracting certain characteristics and creating a profile for 
an author based on the work produced by that author [4]. 
Authorship analysis has a variety of applications in 
cybercrime for detecting similarities between different 
messages and linking it to known suspects [5]. Authorship 
analysis can also be used to identify authors of spam messages 
and malicious programs [6]. Authorship attribution also has 
applications in finding anonymous authors that defame people 
on online forums using multiple accounts. 
B. Related Work 
Authorship attribution using statistical methods has a 
history starting from the 19th century [7]. The first 
Authorship Attribution of Web Forum Posts 
Sangita R. Pillay and Thamar Solorio 
A 
1569329097  
 
APWG eCrime Researchers Summit 2010 
2 
revolutionary work was done by Mendenhall in 1887 on plays 
of Shakespeare. A detailed study was done by Mosteller and 
Wallace on the authorship of “The Federalist Papers1
In the field of stylometry, the linguistic behavior of a 
language is studied to determine the author of a text. The 
efficiency of various stylometric procedures using both the 
lexical and the syntactic features extracted from an author’s 
work was evaluated by Baayen et al. [9].  Lexical 
characteristics like word frequencies, word length, sentence 
length, and character frequencies were used as features for 
performing the statistical analysis. Holmes suggested the 
importance of stylometry towards authorship attribution and 
emphasized that stylometry techniques can be successfully 
used along with traditional procedures [10]. 
” using a 
Bayesian statistical analysis and was one of the most 
influential works on authorship attribution [8]. For performing 
the authorship analysis, frequencies of a small set of common 
words were used as features that produced major 
discriminating results. Traditional authorship attribution work 
was completely controlled by human experts and all work was 
done manually. The study done by Mosteller and Wallace 
initiated usage of computer assisted methodologies for solving 
authorship attribution tasks. 
A statistical n-gram (character level) language model 
was proposed by Peng et al. for performing authorship 
attribution [11]. A language model estimates the probabilities 
of the n-gram words occurring in the dataset, which can be 
used to predict next words occurring in the sentences. 
Traditionally, language models were used for speech 
recognition but recently have been widely used for machine 
learning, information retrieval and bio-informatics. They 
perform classification on Greek, English, and Chinese text 
separately. The authors ranged from 8-10 in their experimental 
datasets. Their approach achieved an overall accuracy of 90% 
on Greek text, 98% on English text, and 94% on Chinese text. 
They analyze the effect of context length (choose n to be 2 or 
3), training size (larger training corpus produces better 
accuracies on test instances), and smoothing technique (Good 
Turing approach performed better overall) on the accuracies 
achieved.  
A relevant work which included combining standard 
data retrieval techniques with meta- learning schemes was used 
to perform authorship attribution on thousands of blogs [12]. 
Their corpus consists of 18,000 blogs where each blog consists 
of several posts written by an author. The test data set 
contained 10,000 blogs with 500 words each while the rest 
were used for training and validation. The cosine similarity 
function was used to match the test snippet with one of the 
candidate authors.  Only 33.3% of all test cases were 
classified, out of which 88.2% were classified correctly. The 
remaining 66.7% of test cases were not classified. 
One of the authorship attribution works towards web 
forum messages focused on multilingual content [5]. A multi-
lingual model was built and then applied to Arabic and English 
web forum messages linked to known extremist groups. Their 
 
1 “The Federalist Papers” is a series of political essays written by John Jay, 
Alexander Hamilton, and James Madison, out of which some were claimed to 
be written by both Madison and Hamilton. 
extracted text consisted of 20 authors per language with 20 
messages per author. For conducting experiments, the dataset 
was divided into smaller datasets where each dataset contained 
5 authors with 20 messages each. They used lexical, syntactic, 
structural, and content-specific features for their experiments. 
Support Vector Machines and Decision Trees (C4.5) were 
used to perform the classification. For their dataset, Support 
Vector Machines performed better than the Decision Trees. 
Support Vector Machines produced an accuracy of 97% on 
English text and 94.83% on Arabic text. On the other hand, 
Decision Trees were able to classify 90.10% and 71.93% of 
instances correctly for English and Arabic text respectively. 
A recent approach involved using Probabilistic 
Context-Free Grammars (PCFG) for predicting the author of a 
document [13]. Both lexical and syntactic characteristics were 
utilized to capture an author’s writing style. The corpus 
included data from different genre like poetry, football, 
business, travel, and cricket. The number of authors ranged 
from 3-6 for all the datasets. The approach involved 
implementing a PCFG for all authors independently and then 
later building a training model using the grammar for 
performing the classification. This model was combined with 
other models (bag-of-words Maximum Entropy classifier and 
n-gram language models) that captured lexical features as well. 
This collective model performed better for most of the 
datasets. For the cricket dataset, 95% of the total instances 
were correctly classified and was recorded the highest among 
the other datasets. They concluded that their variants of the 
PCFG always performed better or equal to that of the best 
baseline. 
We employ a two-stage methodology that combines 
clustering algorithm and machine learning techniques for 
performing authorship attribution. Our approach uses 
stylometric features (both lexical and syntactical) as well as 
statistical language models for generating the feature set. 
Additionally, we use the information from the discovered 
clusters as meta-features for identification of authors. The 
corpus includes data from a specific online web forum 
pertaining to an academic environment. We experiment with 
large number of candidate authors that range from 5 to 100.  
 
TABLE I 
STATISTICS ON TOTAL NUMBER OF AUTHORS AND POSTS PER DATASET 
Dataset 
Number of 
Authors 
Total 
Number of 
posts 
Average 
Number of 
Posts per 
Author 
Standard 
Deviation 
1 5 87 17.4 5.12 
2 10 544 54.4 15.95 
3 15 2441 162.7 34.77 
4 30 3380 112.6 68.40 
5 50 5464 109.2 62.29 
6 100 16145 161.4 344.47 
 
III. DATASETS 
The data was extracted from the forums of the 
‘Chronicle of Higher Education’ portal using standard web-
1569329097  
 
APWG eCrime Researchers Summit 2010 
3 
crawling techniques. The next step involved cleaning and 
organizing of the extracted data. This stage involved parsing of 
the html files for removal of html tags/notations, retrieval of 
information such as the author and the post itself, and finally, 
storage of the data in a format that was suitable for subsequent 
stages. 
We created six datasets from different threads of the 
web forum for performing our clustering and classification 
experiments. The number of authors for the datasets range 
from 5 to 100. All the posts belonging to a dataset share a 
common discussion topic. The distribution of posts per author 
for all the datasets is not equally balanced, since acquiring real 
world data with a balanced distribution of text between all 
authors is not always feasible. The text acquired from the 
forums clearly showed that some authors produced less text 
compared to others. The statistics on the total number of posts 
and authors per dataset is shown in Table I.  
 
 
 
Fig. 1 Block diagram illustrating our approach for performing authorship 
attribution on web forum posts 
 
Contraction and punctuation handling were 
performed on datasets only during extracting certain features 
(features numbered 12 and 13 listed in section IV (A)). 
Contraction word is formed when two words are joined 
together to produce a shortened version. Contraction handling 
separated these words to their original form using a predefined 
contraction word list. During punctuation handling, we 
separated all the punctuation characters from the words with a 
space character. Additionally, we pre-processed all the posts to 
remove all quotations from other authors. On web forums, 
people post their replies to different threads and often they 
include specific questions from other users to whom their 
replies are directed. Since these quotations do not belong to 
the author of the post, we removed them from all the posts. At 
the end, a single marker was left behind to indicate that the 
post contained quotations from other authors.  
 
IV. OUR APPROACH 
The thrust of this research is the adaptation of natural 
language processing and corpus-based approaches for 
performing authorship attribution on web forum posts. The 
research entails extracting structured information from 
unstructured text, analyzing the linguistic structure in text, 
extracting discriminating characteristics from text, building 
training and testing models, and performing author 
identification.  
Most of the authorship attribution problems involve 
dealing with a small number of classes. However, the number 
of posts and authors extracted from the web forum is 
significantly large and hence in our authorship attribution 
problem we need to deal with hundreds of potential authors. 
For our experiments, we chose the maximum number of 
authors to be 100. Initially, our approach focuses on using 
clustering techniques to fragment the datasets into similar 
subsets. We want to use clustering first for identifying relevant 
features that can then be used for performing classification 
using machine learning algorithms. Fig.1. shows the block 
diagram illustrating the approach. 
A. Features Extracted 
The features generated to build a feature vector 
matrix are as follows: 
 
1) Total number of words in the post. 
2) Average number of words per sentence. 
3) Binary feature indicating use of quotations. 
4) Binary feature indicating use of signature. 
5) Frequency of words containing all capitalized letters 
normalized by total number of words. 
6) Total number of special characters over total number of 
alphabetic characters. 
7) Total number of ‘Beginning of Sentence’ (BOS) words 
having first letter capitalized normalized over total 
number of sentences. 
8) Total number of digits normalized over total number of 
words. 
9) Total number of punctuations (!?.;:,) per sentence. 
10) Frequency of contractions normalized over total number 
of words. 
11) Frequency of two or more non-alphanumeric characters 
occurring together normalized over total number of 
tokens. 
12) Perplexity values from 3-gram language models trained on 
words. 
13) Part-of-Speech (POS) tags (POS tag represents 
grammatical role function of a word in the sentence). 
14) Cluster identifier (CID). 
1569329097  
 
APWG eCrime Researchers Summit 2010 
4 
Most of the features extracted are based on stylistic 
characteristics of the author’s writings. The basic assumption 
is that authors unconsciously follow a specific pattern and are 
consistent in their choices. Hence, the idea is that stylistic 
features will be able to capture the unique style of an author. 
The estimation of frequency of words containing all 
capitalized letters will capture information on whether an 
individual has a tendency to use capitalized letters often. 
Feature number 7 may capture whether an author follows the 
usual rule of capitalizing the first letter of the BOS word. The 
idea behind using feature 11 was to identify authors who tend 
to use consecutive non-alphanumeric characters like question 
marks (?), periods (.), and exclamation marks (!) in their posts. 
This particular feature is targeted to capture character 
emoticons { :-), ;), =), :-D, and others}. 
Perplexity values were computed using a 3-gram 
language model trained on words. The SRI Language 
Modeling toolkit was used both for training the language 
models and computing the perplexity values on the test data 
[14]. Perplexity measures are used as a way to evaluate 
language models and indicate how confused the language 
model gets on unseen text. The main motivation was that 
language models may capture author’s distinct writing choices 
and when applied to text with similar style will produce lower 
perplexity value that can act as a discriminative feature during 
the classification. The idea is to generate a language model for 
each author using the training data and then apply it on the test 
instances. The language model that produces the lowest 
perplexity score on a test instance indicates to be the closest 
match for the test data.  
We follow a profile-based approach to generate 
training files where each author is represented by one file in 
the training dataset. We grouped all the writings of an author 
into a single file. We trained the language model on this file 
and then applied it on the test instances, which resulted in 
perplexity scores. We used the Leave-One-Out-Cross-
Validation (LOOCV) technique for the generation of the 
training files. For example, assume the dataset contained 5 
authors with 10 posts each. Then for each of the post, 5 
training files were generated. To create the training file for 
post ‘P1’ written by author ‘A1’, we grouped all the text 
produced by author ‘A1’ except the text from post ‘P1’ into a 
single file. The remaining four training files for the post ‘P1’ 
were generated using the text produced by the other four 
authors. We use these five training files to generate language 
models for the post ‘P1’. These language models were then 
applied on the test post ‘P1’ to generate perplexity values. This 
entire procedure was repeated for all the posts. Since the 
amount of text available for each author on these forums is 
little, the LOOCV approach was chosen to take maximum 
advantage of the text available. 
Additionally, some authors chose to include signature 
information with all of their posts that were mostly unique to 
the author. Since we generate training files for training 
language models by grouping all the posts, the signature 
information will repeat as many times as the number of posts 
and hence, to avoid overly optimistic results, the signature 
information was removed during the data pre-processing step.  
POS tags (represented by feature number 13 from 
section IV (A)) were extracted from the data using an efficient 
statistical POS tagger known as Trigrams'n'Tags (TnT) [15]. 
The tagger is an implementation of the Viterbi algorithm for a 
second order Markov model. The tag set of the language 
model used for the tagging process contains 159 tags. The bag-
of-words approach was employed such that each POS tag 
represented a feature in the feature vector. For each 
instance/post, the corresponding feature vectors consisted of 
the frequency counts of those features/tags. 
B. Clustering 
Clustering is an unsupervised algorithm that divides 
the data instances into similar groups using feature vectors 
describing the instances. The goal behind clustering is to 
maximize intra-cluster similarity and minimize the inter-cluster 
similarity. Resulting clusters can explain the characteristics of 
the underlying data distribution. In an ideal case, each cluster 
will contain all posts belonging to one author. The clustering 
algorithm associates a unique cluster identifier (represented by 
feature number 14 from section IV (A)) with each of the 
clusters. This resulting identifier can be used as a 
discriminative feature during the classification stage.  
During the training phase, we created subsets of posts 
written by a group of authors and extracted a series of lexical 
and syntactical features to generate a feature vector. The 
feature vector was then sent to a clustering module for 
generating a k-way clustering solution, where ‘k’ is the number 
of desired clusters. We analyzed the characteristics of the 
solution clusters to see how well the posts belonging to each 
author were grouped.  
C. Classification using Machine Learning Algorithms 
During the second stage, we performed classification 
on our test datasets using different machine learning 
algorithms. Initially, the features 1-13 described in section IV 
(A) were used to train the model for performing classification 
and the resulting accuracy was recorded. With clustering, each 
post gets assigned to a cluster that is identified by an 
associated cluster identifier (feature number 14 from section 
IV (A)). We used all the features from 1-14 including the 
cluster identifier to build a new classification model and then 
we classified the test instances using different machine 
learning algorithms.  
 
TABLE II 
ENTROPY AND PURITY VALUES FOR ALL DATASETS 
The identifiers ‘E’ and ‘P’ above stand for entropy and purity respectively. 
Subscript description: PPL = perplexity values, POS = parts-of-speech tags, 
STYLISTIC = features 1-11, and ALL = features 1-13 from section IV (A). 
Data EPPL+ 
STYLISTIC 
PPPL+ 
STYLISTIC 
EPOS+ 
STYLISTIC 
PPOS+ 
STYLISTIC EALL PALL Set 
1 0.836 0.368 0.71 0.437 0.815 0.414 
2 0.874 0.25 0.67  0.39 0.866 0.265 
3 0.832 0.207 0.613 0.333 0.835 0.204 
4 0.484 0.523 0.499 0.425 0.475 0.54 
5 0.841 0.102 0.655 0.211 0.836 0.106 
6 0.642 0.183 0.441 0.403 0.644 0.178 
1569329097  
 
APWG eCrime Researchers Summit 2010 
5 
   
  TABLE III 
CLASSIFICATION RESULTS FOR DATASET 1 
5 Authors (87 instances) 
Classifiers 
CPPL + STYLISTIC 
(%) 
CPPL + STYLISTIC + CID 
(%) 
CPOS + STYLISTIC 
(%) 
CPOS + STYLISTIC + CID 
(%) 
CALL - CID 
(%) 
CALL 
(%) 
BayesNet 90.80 90.80 90.80 89.65 90.80 90.80 
Naïve Bayes 68.96 67.81 48.27 48.27 47.12 47.12 
Decision 
Trees (C4.5) 77.01 77.01 80.45 80.45 80.45 80.45 
 
TABLE IV 
CLASSIFICATION RESULTS FOR DATASET 2   
10 Authors (544 instances) 
Classifiers 
CPPL + STYLISTIC 
(%) 
CPPL + STYLISTIC + CID 
(%) 
CPOS + STYLISTIC 
(%) 
CPOS + STYLISTIC + CID 
(%) 
CALL - CID 
(%) 
CALL 
(%) 
BayesNet 59.55 59.55 61.62 59.92 54.41 54.59 
Naïve Bayes 49.63 49.81 38.60 38.78 38.97 38.60 
Decision 
Trees (C4.5) 63.05 63.23 63.23 63.60 63.23 63.23 
 
TABLE V 
CLASSIFICATION RESULTS FOR DATASET 3  
15 Authors (2441 instances) 
Classifiers 
CPPL + STYLISTIC 
(%) 
CPPL + STYLISTIC + CID 
(%) 
CPOS + STYLISTIC 
(%) 
CPOS + STYLISTIC + CID 
(%) 
CALL - CID 
(%) 
CALL 
(%) 
BayesNet 70.25 71.65 56.65 62.35 60.09 60.79 
Naïve Bayes 54.15 54.03 33.42 33.83 35.19 35.19 
Decision 
Trees (C4.5) 67.92 68 61.24 63.21 65.05 65.05 
 
TABLE VI 
CLASSIFICATION RESULTS FOR DATASET 4 
30 Authors (3380 instances) 
Classifiers 
CPPL + STYLISTIC 
(%) 
CPPL + STYLISTIC + CID 
(%) 
CPOS + STYLISTIC 
(%) 
CPOS + STYLISTIC + CID 
(%) 
CALL - CID 
(%) 
CALL 
(%) 
BayesNet 61.18 66.12 64.02 68.46 60.68 63.87 
Naïve Bayes 55.82 57.13 51.15 51.77 53.52 53.60 
Decision 
Trees (C4.5) 73.72 73.78 65.41 67.39 72.18 72.24 
 
TABLE VII 
CLASSIFICATION RESULTS FOR DATASET 5 
50 Authors (5464 instances) 
Classifiers 
CPPL + STYLISTIC 
(%) 
CPPL + STYLISTIC + CID 
(%) 
CPOS + STYLISTIC 
(%) 
CPOS + STYLISTIC + CID 
(%) 
CALL - CID 
(%) 
CALL 
(%) 
BayesNet 37.73 37.73 37.11 44.96 37.11 37.11 
Naïve Bayes 14.95 14.98 11.51 12.35 11.01 10.96 
Decision 
Trees (C4.5) 35.35 35.34 33.49 37.42 33.47 33.54 
 
TABLE VIII 
CLASSIFICATION RESULTS FOR DATASET 6 
100 Authors (16145 instances) 
Classifiers 
CPPL + STYLISTIC 
(%) 
CPPL + STYLISTIC + CID 
(%) 
CPOS + STYLISTIC 
(%) 
CPOS + STYLISTIC + CID 
(%) 
CALL - CID 
(%) 
CALL 
(%) 
BayesNet 24.81 24.81 41.25 49.19 24.15 24.15 
Naïve Bayes 02.81 02.86 13.58 14.61 04.60 04.60 
Decision 
Trees (C4.5) 45.70 45.69 44.70 51.22 45.30 45.28 
‘C’ in all above tables stands for classification model and the accuracy in all tables is depicted in terms of percentage values. 
Subscript description: PPL = perplexity values, STYLISTIC = features 1-11 from section IV (A), CID = cluster identifier, 
POS = parts-of-speech tags, ALL = features 1-14 from section IV (A), + = inclusion of a feature, - = exclusion of a feature. 
 
1569329097  
 
APWG eCrime Researchers Summit 2010 
6 
V. RESULTS 
A. Clustering Results 
As discussed earlier, during the first stage the focus 
was on performing graph-based clustering on different datasets 
and then analyzing the clustering solutions. We used the 
CLUTO software package for clustering the datasets into 
similar groups [16]. The discovered clusters gave information 
about how different classes/authors were distributed in each 
one of the clusters. The measure used to evaluate the quality of 
clustering results is based on entropy and purity values 
produced by the clustering methods.  
The entropy measure shows how various authors of 
posts are distributed within each cluster. This metric can be 
seen as the error distribution among the clusters generated by 
the algorithm. The purity measure on the other hand estimates 
the extent to which each cluster contains posts from one class. 
Small entropy values and large purity values indicate good 
clustering solutions. Although clustering is an unsupervised 
approach, to measure the quality of clustering using entropy 
and purity metrics, labeled datasets are required. 
Table II shows entropy and purity values generated 
by the graph clustering method on six different datasets. The 
capital letters ‘E’ and ‘P’ stand for entropy and purity 
respectively. The subscript ‘PPL + STYLISTIC’ represents 
that clustering was performed on the datasets using features 1-
12 along with the perplexity values from section IV (A). The 
subscript ‘POS + STYLISTIC’ represents that clustering was 
performed on the datasets using features 1-11 and the POS 
tags (feature number 13). Finally, the subscript ‘ALL’ 
represents that clustering was performed on the datasets using 
features 1-13.  
From Table II, it can be seen that the purity of the 
clusters is high while the entropy of the clusters is low for the 
datasets 1, 2, 3, 5, and 6 when the clustering model contained 
POS tags as features excluding the perplexity values. For 
dataset 4, the model that was clustered on the combination of 
all the features produced high purity and low entropy value.  
For the graph method, the desired k-way clustering 
solution is computed by modeling the posts using a nearest 
neighbor graph where each post becomes a vertex and each 
post is connected to its most similar other post. Here, ‘k’ is the 
number of desired clusters. Then the graph is divided into k-
clusters using a min-cut graph partitioning algorithm. 
For most of the clustering algorithms, we need to 
specify the desired number of clusters in advance. The criteria 
for selecting the optimum number of clusters depends on the 
number of classes and instances present in the datasets. 
Increasing the number of clusters will reduce the entropy in the 
resulting clusters. For our approach, ‘k’ was equivalent to the 
number of authors in the dataset. With the assumption that all 
authors have a distinct style that is discriminative enough to 
cluster all posts belonging to one author in the same cluster. 
B. Classification Results 
We performed classification with the help of a data 
mining software toolkit named Weka [17]. Weka provides a 
compilation of machine learning algorithms and other tools for 
data preprocessing, analyzing, and visualization. The different 
classifiers used for these experiments were Bayesian 
Networks, Naïve Bayes, and Decision Trees (C4.5). Bayesian 
Networks are probabilistic graphical models and are used to 
model complex and uncertain systems. Decision Trees are 
used to generate a pruned or un-pruned decision tree and 
visualize the steps taken to reach the classification decision. 
Decision Trees provide a powerful means to express or 
identify structures in the text. Decision Trees recursively 
classifies test instances until they converge that is they are 
classified as perfectly as possible [18]. Several other classifiers 
were tried as well, but the above mentioned performed best on 
an average across all datasets. Support Vector Machines 
performed worst on all of the datasets probably because we did 
not perform any feature selection. Training a Neural Networks 
on the other hand was time consuming due to the large number 
of posts and authors.  
We used 10-fold cross validation for performing the 
classification, so that we could take maximum advantage of the 
datasets available to us. Initially, we divided the data set 
randomly into 10 subsets of equal size. The classification 
model was trained on nine subsets and then tested on the left 
out subset. This entire process was repeated 10 times and 
every time there was no overlap between the training and 
testing data. This technique helped us to use all our posts for 
testing. The resulting classification information was evaluated 
for its accuracy. This entire process was repeated on all six 
datasets and the results were analyzed.  
Tables III, IV, V, VI, VII, and VIII depict 
classification results for six datasets using different machine 
learning classifiers. These tables give the percentage of 
instances that were correctly classified. The tables also give 
information regarding the accuracy achieved using different 
algorithms with features including and not including cluster 
identifiers. The symbol ‘C’ indicates classification model and 
the subscripts give information on the features used to train the 
classifiers. The subscript ‘PPL’ represents perplexity values 
(feature number 12 from section IV (A)), ‘POS’ represents 
POS tags (feature number 13 from section IV(A)), 
‘STYLISTIC’ represents features 1-11 from section IV (A), 
‘CID’ represents cluster identifier (feature number 14 from 
section IV (A)), and finally, the subscript ‘ALL’ represents 
features 1-14 from section IV (A).  
From the tables it can be seen that the machine 
learning algorithms were able to classify authors with a 
believable rate of accuracy. With the variance in number of 
authors and instances the resulting accuracy decreases but is 
still encouraging. The numbers in boldface signify the cases 
where the models incorporating clustering solutions gave 
slightly better classification results than its counterpart. This 
fact can be seen from Tables IV, V, VI, VII, and VIII. For 
dataset 1, the inclusion of clustering solutions (CID) during 
machine learning did not make any difference in the 
classification results (Table III). 
For dataset 1, most of the instances were correctly 
classified with an accuracy of 90.80% (Table III). The 
machine learning algorithm that gave these results is Bayesian 
Networks. For dataset 2, which consists of 544 instances with 
1569329097  
 
APWG eCrime Researchers Summit 2010 
7 
10 authors, the highest accuracy attained is 63.60% that is 
achieved using the Decision Trees (C4.5) shown in Table IV. 
From Table V, it can be seen that the highest accuracy 
achieved for dataset 3 is 71.65% realized using the Bayesian 
Networks.  
For dataset 4, the percentage of instances correctly 
classified is 73.78% achieved using the Decision Trees 
classifier (Table VI).  Table VII shows that the highest 
accuracy achieved for dataset 5 containing 50 authors is 
44.96% realized using the Bayesian Networks algorithm. The 
highest accuracy achieved for dataset 6 consisting of 16,145 
instances and 100 authors is 51.22%.  
From Tables IV, VII and VIII, it is clear that the 
classification model CPOS+STYLISTIC+CID performed well compared 
to its counterparts. Tables V and VI depict that higher 
classification results were obtained using the classification 
model CPPL+STYLISTIC+CID. A general inference is that the 
classification model (CALL and CALL-CID) that combined all 
features did not give better classification results than the other 
models (CPPL+STYLISTIC, CPPL+STYLISTIC+CID, CPOS+STYLISTIC, and 
CPOS+STYLISTIC+CID) for any of the datasets. From the tables it is 
also observed that as the number of authors and instances 
increases, the resulting classification model that incorporates 
cluster identifier performs significantly better than the ones 
that do not include it. It can also be observed that Bayesian 
Networks and Decision Trees (C4.5) performed better than 
Naïve Bayes for our datasets.  
It can be said that the features implemented were able 
to capture style markers pertaining to individual authors to 
some extent. Situations were instances were incorrectly 
classified, it is probable that the two authors share similar 
writing styles and hence, also possible that these two screen 
names belong to the same individual. We intend to address this 
problem in the near future. 
VI. CONCLUSION 
The results from the clustering and the classification 
are encouraging and demonstrate that our approach was 
effective for short, informal, and contemporary text. It was 
observed that inclusion of clustering solutions as a feature for 
performing classification gained better accuracy in most of the 
cases. Most of the features used can be extracted from texts 
belonging to different domains and our approach can be 
adapted in that case.  
This is an ongoing research with continuous efforts 
and currently the focus is on evaluation and analysis of 
clustering results on different datasets containing a large 
number of authors. For our future work we intend to take a 
multi-modal meta-features approach. Rather than clustering all 
features together we want to perform clustering from different 
perspectives.  
 
 
 
 
 
 
 
Authorship attribution has a wide range of 
applications in intelligence, criminal law, civil law, computer 
forensics, and literary works. In addition, little work has been 
done related to authorship attribution of web forum posts. In 
this work on authorship attribution, we focus on making an 
effort to research and solve the problem using unsupervised 
and supervised learning algorithms.  
 
REFERENCES 
 
[1] Y. Zhao and J. Zobel, “Searching with Style: Authorship Attribution in 
Classic Literature”. 
[2] M. Koppel and J. Schler, “Authorship verification as a one-class 
classification problem,” in Proceedings of the 21st International 
Conference on Machine Learning. NewYork: ACM Press, 2004, pp. 62. 
[3] S. Meyer zu Eissen et al., “Plagiarism detection without reference 
collections,” Advances in data analysis. Berlin, Germany: Springer, 
2007, pp. 359–366. 
[4] M. Koppel et al., “Automatically categorizing written texts by author 
gender,” in Literary and Linguistic Computing, Vol. 17, No. 4, 2002, 
pp. 401-412. 
[5] A. Abbasi and H. Chen, “Applying Authorship Analysis to Extremist-
Group Web Forum Messages,” IEEE Intelligent Systems and Their 
Applications, 20 (5): 67-75, 2005.  
[6] G. Frantzeskou et al., “Effective identification of source code authors 
using byte-level information,” in Proceedings of the 28th International 
Conference on Software Engineering. NewYork: ACM Press, 2006, pp. 
893–896. 
[7] E. Stamatatos, “A Survey of Modern Authorship Attribution Methods,” 
in Journal of the American Society for Information Science and 
Technology. Wiley, 60(3), 2009, pp. 538-556.  
[8] F. Mosteller and D. L. Wallace, “Inference and disputed authorship: The 
Federalist”. Reading, MA: Addison-Wesley, 1964. 
[9] R. Baayen et al., “Outside the cave of shadows: Using syntactic 
annotation to enhance authorship attribution,” in Literary and 
Linguistic Computing, 11(3), 1996, pp. 121–131. 
[10] D. I. Holmes, “The evolution of stylometry in humanities scholarship,” 
in Literary and Linguistic Computing, 13(3), 1998, pp. 111–117.  
[11] F. Peng et al., “Language independent authorship attribution using 
character level language models,” in Proceedings of the 10th 
Conference of the European Chapter of the Association for 
Computational Linguistics. Morristown, NJ, 2003, pp. 267–274. 
[12] M. Koppel et al., “Authorship attribution with thousands of candidate 
authors,” in Proceedings of the 29th ACM SIGIR. NewYork: ACM 
Press, 2006, pp. 659–660.   
[13] S. Raghavan et al., “Authorship Attribution Using Probabilistic 
Context-Free Grammars,” in Proceedings of the 48th Annual Meeting of 
the Association for Computational Linguistics (ACL), 2010. 
[14] A. Stolcke, “SRILM -- An Extensible Language Modeling Toolkit,” 
in Proceedings of the International Conference on 
Spoken Language Processing. Denver, vol. 2, pp. 901-904, 2002. 
[15] T. Brants, “TnT -- Statistical Part-of-Speech Tagging,” in Proceedings 
of the Sixth Applied Natural Language Processing ANLP-2000. Seattle, 
WA, 2000. 
[16] G. Karypis, “Cluto: Software for Clustering High Dimensional Data 
Sets,” www.cs.umn.edu~karypis, 2005.  
[17] M. Hall et al., “The WEKA Data Mining Software: An Update,” 
SIGKDD Explorations, Volume 11, Issue 1, 2009. 
[18] I. H. Witten and E. Frank, “Data Mining: Practical Machine Learning 
Tools and Techniques”, Second edition, Morgan Kaufmann, San 
Francisco, CA, 2005. 
 
 
 
 
 
 
