 
1 
 
T√çTULO: ADAPTACI√ìN DEL M√âTODO DE REDUCCI√ìN NO LINEAL LLE PARA LA 
SELECCI√ìN DE ATRIBUTOS EN WEKA 
Autores: Isabel Cristina P√©rez-Verona1, isabelcristina@uclv.cu 
     Reinier Millo-S√°nchez2, rmillo@uclv.cu 
RESUMEN 
El problema de selecci√≥n de atributos consiste en encontrar un subconjunto que describa los 
objetos del dominio a trav√©s de la redimensi√≥n de los datos. Entre los algoritmos de embebido 
de datos que realizan reducci√≥n de dimensi√≥n no lineal, destaca LLE por su efectivo mecanismo 
de preservar las estructuras locales de vecindad. En el presente trabajo se describe una 
adaptaci√≥n de este m√©todo para la selecci√≥n de atributos, el cual fue programado como plugin 
para utilizarse en la herramienta para el aprendizaje autom√°tico WEKA. 
Palabras clave: reducci√≥n de dimensi√≥n no lineal, selecci√≥n de rasgos, detecci√≥n de ort√≥logos, 
WEKA 
ABSTRACT 
The goal in feature selection is to find a subset describing domain objects by resizing the data. 
Among the data embedded algorithms that perform non-linear dimension reduction, LLE stands 
by its effective mechanism to preserve local neighborhood structures. Here, we describe an 
adaptation of LLE for feature selection. We programmed the method as a plug-in tool for the 
machine learning software WEKA. 
Keywords: manifold learning, feature selection, orthologous detection, WEKA 
INTRODUCCI√ìN 
El aprendizaje automatizado (machine learning, ML) es una rama de la inteligencia artificial, en 
gran parte inspirada en el razonamiento humano, que comprende el aprendizaje a partir de 
experiencia. Esta rama comprende los problemas de clasificaci√≥n, asociaci√≥n, agrupamiento 
(clustering), y selecci√≥n de rasgos (Cong, P√©rez, & Morell, 2015; Sammut & Webb, 2011).  
La selecci√≥n de rasgos (variables, rasgos, caracter√≠sticas) es una etapa fundamental de cualquier 
proceso de modelado, la cual consiste en seleccionar un subconjunto de rasgos relevantes para 
usar en la construcci√≥n de un modelo (Han, Kamber, & Pei, 2011; Za√Øane, 1999). Dado a que la 
selecci√≥n de rasgos es aplicable a disimiles situaciones reales, es dif√≠cil llegar a un consenso en 
cuanto a cu√°l es la mejor selecci√≥n posible. Esto posibilita que existan m√∫ltiples algoritmos de 
este tipo. Un m√©todo de selecci√≥n de rasgos t√≠picamente incluye un criterio de evaluaci√≥n para 
comparar el subconjunto de caracter√≠sticas, un procedimiento de b√∫squeda y un criterio de 
parada, t√≠picamente un umbral de relevancia o la dimensi√≥n final del espacio de caracter√≠sticas. 
Si bien la manera de abordar el problema puede cambiar en dependencia del m√©todo, todos se 
basan en 3 principios fundamentales: la reducci√≥n de la dimensi√≥n de los datos, mantener estable 
la tasa de aciertos de manera que no disminuya significativamente y que la distribuci√≥n de la 
clase resultante sea lo m√°s semejante posible a la distribuci√≥n de la clase original, dado todos 
los atributos.  
 
2 
 
En el presente art√≠culo se describe la adaptaci√≥n el m√©todo de embebido no lineal Locally Lineal 
Embedding (LLE) para la selecci√≥n de atributos en WEKA. En las secciones 0 y 1 se introduce 
brevemente el aprendizaje de distancias y se explica el funcionamiento del m√©todo original LLE. 
La descripci√≥n detallada del dise√±o de la experimentaci√≥n, y las modificaciones realizadas al 
algoritmo paso a paso pueden encontrarse en la secci√≥n 0. Finalmente, en la secci√≥n 0 se 
analizan los resultados obtenidos y se da pie a futuras l√≠neas de trabajo.  
DESARROLLO 
La necesidad de v√≠as concretas para medir la distancia o semejanza entre datos es un factor 
clave en los m√©todos de selecci√≥n de rasgos. El uso de t√©cnicas de aprendizaje de distancia para 
automatizar el aprendizaje de la m√©trica facilita dicha tarea. El aprendizaje de m√©tricas de 
distancia cl√°sico (metric learning) tiene como principio la adaptaci√≥n de una m√©trica de distancia 
a una aplicaci√≥n espec√≠fica, utilizando para ello informaci√≥n de conjunto de entrenamiento, como 
por ejemplo: modificar la m√©trica de distancia de un k-NN para implementar la distancia de 
Mahalanobis, donde el objetivo final es inducir una m√©trica de distancia m√°s potente a partir de 
los datos conocidos (Bellet, Habrard, & Sebban, 2013a; Garakani, Liu, Sha, & Liang, 2014)‚ïë. 
Los m√©todos de aprendizaje de m√©tricas de distancia pueden clasificarse en dos grupos 
fundamentales: aprendizaje supervisado de m√©tricas de distancia (supervised metric learning) y 
aprendizaje no supervisado de m√©tricas de distancia (unsupervised metric learning) (Bellet, 
Habrard, & Sebban, 2013a), si bien algunos autores tambi√©n consideran la clasificaci√≥n de 
m√©todos semi-supervisados de aprendizaje de distancias(L Yang & Jin, 2006).  
 
Figura 0.1 Clasificaci√≥n de algunos m√©todos de metric learning (Kulis, 2013; Liu Yang, 2006) 
 
3 
 
Los algoritmos no supervisados de aprendizaje de m√©tricas de distancias no conocen las 
etiquetas de las instancias de entrenamiento individuales, en su lugar utilizan informaci√≥n 
adicional (o criterios que son asumidos) en forma de conjuntos de restricciones  ùì¢, ùìì, ùì°. Esta 
peculiaridad resulta particularmente √∫til en una variedad de aplicaciones donde los datos 
etiquetados son dif√≠ciles de obtener, mientras que resulta relativamente f√°cil obtener informaci√≥n 
adicional (Bellet et al., 2013b). 
Existen m√©todos en el embebido no lineal que trabajan con un espectro sparse este es el caso 
de m√©todos LLE ( Ziegelmeier, Kirby, & Peterson, 2012;Liu et al., 2013), un m√©todo que est√° 
fundamentalmente orientado a la preservaci√≥n de las estructuras locales de los datos. Las 
matrices dispersas permiten el procesamiento de grandes vol√∫menes de informaci√≥n de 
conjuntos de datos no densos. El aprovechamiento de este conocimiento permite reducir el costo 
computacional de las operaciones que se pueden realizar sobre estas matrices, as√≠ como el costo 
espacial para el almacenamiento de la informaci√≥n (Millo S√°nchez et al., 2015).  
LOCALLY LINEAR EMBEDDING 
Entre los algoritmos de embebido de datos que realizan reducci√≥n de dimensi√≥n no lineal, 
destaca LLE (Locally Linear Embedding),  por su efectivo mecanismo de preservar las estructuras 
locales de vecindad. En los m√©todos hasta el momento como MDS o Isomap, se depend√≠a del 
c√°lculo de las distancias (en l√≠nea recta) entre los puntos de los datos, LLE, en su lugar recupera 
una estructura no lineal de las localidades del manifold.  
Este algoritmo, visualiza el espacio manifold como una colecci√≥n de parches, o puntos de 
coordenadas que se solapan entre s√≠. Si el manifold es lo suficientemente uniforme y las 
vecindades son peque√±as, entonces el algoritmo considera estas zonas como lineales. El 
objetivo es identificar cada uno de esos parches y caracterizar la geometr√≠a interna en ellos, para 
luego construir las vecindades (Cayton, 2005). Se asume que estas regiones locales se 
superponen una con otra de manera tal que las reconstrucciones locales se combinaran en una 
local.  
En un paso inicial, el algoritmo representa los puntos ùë•ùëñ del espacio de alta dimensionalidad como 
una combinaci√≥n lineal ùê∞ùëñ de sus ùëò vecinos m√°s cercanos en el espacio (ùë•ùëñùëó) a trav√©s de una 
reconstrucci√≥n de pesos, y recrea esa representaci√≥n en el plano reducido. En otras palabras, el 
algoritmo crea un hiperplano entre ùë•ùëñ y sus vecinos m√°s cercanos, asumiendo que el manifold es 
linealmente local. Estas caracter√≠sticas hacen el manifold invariante a operaciones de 
modificaci√≥n como el escalado, traslaciones, rotaciones, y reconstrucciones invariantes; lo que 
permite que un mapeo lineal de este hiperplano a la representaci√≥n reducida preserve la 
reconstrucci√≥n de los pesos en ese espacio. Un esquema detallado de los pasos del LLE pueden 
verse en la Figura 0.2. 
 
 
 
 
 
 
4 
 
 
Locally Linear Embedding 
 
Figura 0.2 Pasos del algoritmo LLE 
Fuente: (Cayton, 2005) 
El algoritmo est√°ndar comprende 3 etapas: la b√∫squeda de los vecinos m√°s cercanos, a partir 
de lo cual construye una matriz de pesos que contiene la soluci√≥n a la ecuaci√≥n ùëò √ó ùëò para cada 
uno de las vecindades locales y finalmente un paso de descomposici√≥n parcial de vectores 
propios. La complejidad total del algoritmo viene dada por la ecuaci√≥n 0.1 donde ùëÅ es el n√∫mero 
de puntos de datos de entrenamiento, ùê∑ la dimensi√≥n de entrada, ùëò el n√∫mero de vecinos m√°s 
cercanos y ùëë la dimensi√≥n de salida 
ùëÇùê∑ log ùëòùëÅ log ùëÅ + ùëÇùê∑ùëÅùëò3 + ùëÇùëëùëÅ2 0.1 
DESCRIPCI√ìN DE LA EXPERIMENTACI√ìN 
En la implementaci√≥n de esta variante de LLE se emplea como base el primer paso del algoritmo 
original (Ver figura Figura 0.1), para luego realizar la selecci√≥n de rasgos a partir de la matriz de 
reconstrucci√≥n de pesos mediante un m√©todo de comparaci√≥n. La comparaci√≥n se puede realizar 
mediante la norma de las diferencias absolutas  o la similitud coseno (Barkman & others, 1958; 
Sidorov, Gelbukh, G√≥mez-Adorno, & Pinto, 2014; Sidorov, Velasquez, Stamatatos, Gelbukh, & 
Chanona-Hern√°ndez, 2013; Singhal, 2001). En el algoritmo propuesto se calcula una matriz de 
reconstrucci√≥n empleando los atributos del subconjunto y se compara con la matriz de 
reconstrucci√≥n global para ver cu√°l subconjunto de atributos produce diferencias m√°s 
significativas. 
La selecci√≥n de rasgos o atributos en el WEKA se puede realizar de dos formas diferentes, 
mediante ranking como realiza el algoritmo de an√°lisis de componentes principales PCA o 
mediante subconjuntos como realiza CfsSubsetEval. La implementaci√≥n de LLE para realizar 
selecci√≥n de atributos en el WEKA descrita en este art√≠culo realiza la reducci√≥n de dimensiones 
por agrupaci√≥n de subconjuntos. 
 
5 
 
Para definir un nuevo algoritmo para la selecci√≥n de rasgos o atributos en WEKA, se agreg√≥ una 
clase llamada LocallyLinearEmbedding en el paquete weka.attributeSelection. Esta clase 
hereda de la clase UnsupervisedSubsetEvaluator e implementa la interfaz OptionHandler. 
Para implementar el algoritmo se empleron los m√©todos:  
ÔÇ∑ public double evaluateSubset(BitSet arg0) 
ÔÇ∑ public void buildEvaluator(Instances instances) 
El m√©todo buildEvaluator, calcula una matriz de reconstrucci√≥n de pesos empleando todos los 
atributos de la base de casos, matriz que se denomin√≥ matriz de reconstrucci√≥n global. 
Posteriormente se ejecuta el m√©todo evaluateSubset, para evaluar eval√∫an cada uno de los 
subconjuntos de atributos para determinar cu√°l produce mejores resultados.  
B√öSQUEDA DE LOS N VECINOS PARA CADA INSTANCIA. 
Al igual que en el algoritmo original LLE es necesario realizar la b√∫squeda de los N vecinos m√°s 
cercanos a cada una de las instancias. Para establecer este par√°metro N se emplean 
anotaciones de Java junto a los m√©todos: 
ÔÇ∑ public Enumeration listOptions() 
ÔÇ∑ public void setOptions(String[] options) 
ÔÇ∑ public String[] getOptions() 
Para hallar los N vecinos m√°s cercanos se implement√≥ un m√©todo que permitiese el c√°lculo de 
los N vecinos de cada instancia a partir de un subconjunto de atributos: private Instances 
getKNN(Instance point, int knn_size, BitSet attr) Donde point es la instancia a la cual se le 
calculan los N vecinos m√°s cercanos, knn_size es la cantidad de vecinos que se desean calcular 
y attr es el conjunto de atributos que se emplear√°n para el c√°lculo de los vecinos. 
Para el c√°lculo de los vecinos se emple√≥ la clase EuclideanDistance de WEKA que implementa 
el c√°lculo de la distancia euclidiana, para dos instancias, siendo posible indicar el conjunto de 
atributos que se emplea para el c√°lculo de la distancia y se utilizaron √°rboles balanceados de 
Java (TreeSet) para obtener los N vecinos m√°s cercanos al objeto. 
C√ÅLCULO DE LA MATRIZ DE RECONSTRUCCI√ìN DE LOS PESOS 
El c√°lculo de la matriz de reconstrucci√≥n de los pesos se implement√≥ como est√° definido en el 
algoritmo del LLE. Para manejar la alta dimensonalidad de los datos se decidio utilizar matrices 
sparse. Las matrices dispersas permiten el procesamiento de grandes vol√∫menes de informaci√≥n 
de conjuntos de datos no densos. El aprovechamiento de este conocimiento permite reducir el 
costo computacional de las operaciones que se pueden realizar sobre estas matrices, as√≠ como 
el costo espacial para el almacenamiento de la informaci√≥n (Millo S√°nchez et al., 2015). Dado a 
que WEKA no dispone de una implementaci√≥n de matrices sparses, se realiz√≥ una 
implementaci√≥n de matrices sparse que emplea √°rboles balanceados para optimizar el empleo 
de la memoria y permitir el acceso a la informaci√≥n de la matriz con una complejidad temporal 
de ùëÇ (ùëôùëúùëîùëõ ùëôùëúùëîùëö) donde n y m son la cantidad de filas y columnas de la matriz. Esta 
implementaci√≥n de matriz sparse permite operaciones b√°sicas como son la multiplicaci√≥n de 
matrices, multiplicaci√≥n por un escalar, suma de matrices, divisi√≥n por un escalar, traspuesta de 
una matriz, entre otras.se analizo. Si bien se consultaron varias bibliotecas de Java para el 
 
6 
 
trabajo con matrices, en las bibliotecas consultadas la implementaci√≥n de matrices sparse se 
realiza empleando arreglos din√°micos, siendo en ocasiones muy lenta la implementaci√≥n de 
operaciones b√°sicas con matrices, como es el caso de la multiplicaci√≥n, muy empleada en la 
implementaci√≥n del algoritmo, por lo cual se decidi√≥ optar por la representaci√≥n de √°rboles 
balanceados. 
SELECCI√ìN DE ATRIBUTOS A PARTIR DE LAS MATRICES DE RECONSTRUCCI√ìN DE 
PESOS 
Para realizar la selecci√≥n de rasgos o atributos se realiz√≥ una comparaci√≥n entre la matriz de 
reconstrucci√≥n de pesos de un conjunto de atributos y la matriz de reconstrucci√≥n de pesos global 
que emplea todos los atributos. La idea tras esta comparaci√≥n es hallar la diferencia absoluta 
entre ambas matrices, restando la matriz de reconstrucci√≥n del conjunto de atributos de la matriz 
de reconstrucci√≥n global, hallando el valor absoluto de los valores calculados. Una vez calculada 
la nueva matriz se promedian las diferencias por cada instancia obteni√©ndose un vector con las 
diferencias promedios para cada una de las instancias. Posterior a esto se toma como la 
diferencia entre ambas matrices de reconstrucci√≥n el valor medio de estas diferencias promedios. 
RESULTADOS 
El m√©todo realiza una selecci√≥n de atributos menos exhaustiva que m√©todos tradicionales como 
el CfsSubsetEval, al considerar m√°s atributos, La idea de tras del m√©todo es construir la matriz 
de pesos y hallar una matriz que sea similar  ella, por lo cual trata de eliminar la menor cantidad 
de atributos manteniendo la similitud con la matriz de pesos. En la experimentaci√≥n se utiliz√≥ 
k=3. 
Tabla 1 Descripci√≥n de las bases de datos 
nombre instancias atributos 
anneal 789 38 
car 1728 6 
diabetes 768 8 
ionosphere 351 34 
mfeat-fourier 2000 649 
wine 178 13 
zoo 101 18 
 
Para validar los resultados de la selecci√≥n de atributos del m√©todo propuesto, se emplearon 7 
datasets del UCI Repository, los cuales se describen en la tabla Tabla 1. Para realizar la selecci√≥n 
de atributos se utiliz√≥ GreedySteepwise como m√©todo de b√∫squeda. 
 
7 
 
 
Una vez seleccionados los atributos con el nuevo m√©todo, se realizaron pruebas de clasificaci√≥n 
sobre los datasets empleando los m√©todos J48 y IBK. Se emplearon los par√°metros por defecto 
del WEKA, variando solo en el m√©todo IBK la cantidad de vecinos a 3. La validaci√≥n de la 
clasificaci√≥n se realiz√≥ utilizando validaci√≥n cruzada en 10 partes. Los resultados de la 
clasificaci√≥n (¬°Error! No se encuentra el origen de la referencia.) muestran que el m√©todo 
propuesto obtiene resultados similares a los m√©todos cl√°sicos de aprendizaje, y en algunos casos 
un mejor desempe√±o se aprecia en las figuras Figura 0.3 y Figura 0.4.  
 
Figura 0.3 Porcentaje de clasificaci√≥n correcta con el m√©todo IBK 
 
 
Figura 0.4 Porcentaje de clasificaci√≥n correcta con el m√©todo J48 
 
 
60
65
70
75
80
85
90
95
100
anneal car diabetes ionosphere mfeat-fourier wine zoo
IBK CfsSubsetEval LLE (Norma) LLE (Cosine Similarity)
60
70
80
90
100
anneal car diabetes ionosphere mfeat-fourier wine zoo
J48 CfsSubsetEval LLE (Normal) LLE (Cosine Similarity)
 
8 
 
CONCLUSIONES 
Originalmente el m√©todo LLE, es un m√©todo de reducci√≥n de dimensi√≥n no lineal, que se alcanza 
buenos resultados preservando las vecindades locales. El m√©todo LLE modificado para la 
selecci√≥n de atributos, se enfoca en preservar eliminar la menor cantidad de atributos 
manteniendo la similitud con la matriz de pesos que constituyen las soluciones halladas por LLE. 
Si bien, esta modificaci√≥n para la selecci√≥n de rasgos es menos exhaustiva que m√©todos 
tradicionales; esta, no afecta los resultados de la clasificaci√≥n. El m√©todo propuesto obtiene 
resultados similares a los m√©todos cl√°sicos de aprendizaje, y en algunos casos un mejor 
desempe√±o, tal como puede apreciarse en Figura 0.3 y Figura 0.4.  
  
 
9 
 
REFERENCIAS BIBLIOGR√ÅFICAS 
Barkman, J. J., & others. (1958). Phytosociology and ecology of cryptogamic epiphytes, 
including a taxonomic survey and description of their vegetation units in Europe. 
Phytosociology and Ecology of Cryptogamic Epiphytes, Including a Taxonomic Survey and 
Description of Their Vegetation Units in Europe. 
Bellet, A., Habrard, A., & Sebban, M. (2013b). A Survey on Metric Learning for Feature Vectors 
and Structured Data. arXiv Preprint arXiv:1306.6709, 57. 
http://doi.org/10.1073/pnas.0809777106 
Bellet, A., Habrard, A., & Sebban, M. (2013a). A Survey on Metric Learning for Feature Vectors 
and Structured Data. arXiv Preprint arXiv:1306.6709, 57. 
http://doi.org/10.1073/pnas.0809777106 
Cayton, L. (2005). Algorithms for manifold learning. 
Cong, B. N., P√©rez, J. L. R., & Morell, C. (2015). Aprendizaje supervisado de funciones de 
distancia: estado del arte Supervised distance metric learning: state of the art. Revista 
Cubana de Ciencias Inform{√°}ticas, 9(2). 
Garakani, B., Liu, K., Sha, F., & Liang, Y. (2014). Metric Learning. 
Han, J., Kamber, M., & Pei, J. (2011). Data mining: concepts and techniques: concepts and 
techniques. Elsevier. 
Kulis, B. (2013). Metric Learning‚ÄØ: A Survey By Brian Kulis, 5(4), 287‚Äì364. 
http://doi.org/10.1561/2200000019 
Liu, X., Tosun, D., Weiner, M. W., Schuff, N., Initiative, A. D. N., & others. (2013). Locally linear 
embedding (LLE) for MRI based Alzheimer‚Äôs disease classification. NeuroImage, 83, 148‚Äì
157. 
Millo S√°nchez, R., P√©rez Verona, I. C., Vargas, J. A., Hern√°ndez, T. R., Central, U., Abreu, M., 
‚Ä¶ Clara, S. (2015). Matrices dispersas en Java para el procesamiento de grandes 
vol√∫menes de datos. Revista Cubana de Ciencias Inform√°ticas, 1‚Äì8. 
Sammut, C., & Webb, G. I. (2011). Encyclopedia of machine learning. Springer Science & 
Business Media. 
Sidorov, G., Gelbukh, A., G√≥mez-Adorno, H., & Pinto, D. (2014). Soft similarity and soft cosine 
measure: Similarity of features in vector space model. Computaci{√≥}n Y Sistemas, 18(3), 
491‚Äì504. 
Meyer, C. D. (2000). Matrix analysis and applied linear algebra. Siam. 
Watrous, J. (2011). Theory of quantum information. University of Waterloo Fall. 
Sidorov, G., Velasquez, F., Stamatatos, E., Gelbukh, A., & Chanona-Hern√°ndez, L. (2013). 
Syntactic dependency-based n-grams as classification features. In Advances in 
Computational Intelligence (pp. 1‚Äì11). Springer. 
Singhal, A. (2001). Modern information retrieval: A brief overview. IEEE Data Eng. Bull., 24(4), 
35‚Äì43. 
 
10 
 
Yang, L. (2006). Distance Metric Learning‚ÄØ: A Comprehensive Survey, 1‚Äì51. 
Yang, L., & Jin, R. (2006). Distance metric learning: A comprehensive survey. Unpublished 
Manuscript, 1‚Äì51. http://doi.org/10.1073/pnas.0809777106 
Za√Øane, O. R. (1999). Introduction to data mining. 
Ziegelmeier, L., Kirby, M., & Peterson, C. (2012). Locally Linear Embedding Clustering 
Algorithm for Natural Imagery. arXiv Preprint arXiv:1202.4387. 
