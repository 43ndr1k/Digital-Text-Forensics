AUTOMATIC DISCRIMINATION OF GENRES: THE ROLE OF ADJECTIVES AND 
ADVERBS AS SUGGESTED BY LINGUISTICS AND PSYCHOLOGY 
by 
ROBERT JOHN RITTMAN 
A Dissertation submitted to the 
Graduate School – New Brunswick 
Rutgers, The State University of New Jersey 
in partial fulfillment of the requirements 
for the degree of 
Doctor of Philosophy 
Graduate Program in Communication, Information and Library Studies 
written under the direction of 
Dr. Paul B. Kantor 
and approved by 
Dr. Paul B. Kantor 
Dr. Nina Wacholder 
Dr. Michael Lesk 
Dr. Kenneth Safir 
New Brunswick, New Jersey 
May, 2007 
© 2007
Robert John Rittman 
ALL RIGHTS RESERVED 
ii 
 
ABSTRACT OF THE DISSERTATION 
AUTOMATIC DISCRIMINATION OF GENRES: THE ROLE OF ADJECTIVES AND 
ADVERBS AS SUGGESTED BY LINGUISTICS AND PSYCHOLOGY 
by ROBERT JOHN RITTMAN 
Dissertation Director: 
Dr. Paul B. Kantor 
 
Traditionally, information retrieval systems have been designed and evaluated 
based on the assumption that information is relevant if it is about the topic of the 
expressed information need. The tremendous growth in the scope and size of information 
resources, particularly in the World Wide Web, has motivated research on selecting 
information using non-topical criteria. Non-topical criteria, such as genre, have been 
recognized for many years. Genre is a socially constructed classification of texts based on 
external criteria of use, such as academic writing versus news reportage versus fiction.
The proximate goal of this exploratory research is to support information users 
when non-topical criteria are important. Specifically, we assess the feasibility of 
automatically classifying documents by genre using only adjectives and adverbs as 
discriminating features. We distinguish our work by our choice of features and our 
motivation for choosing these features. In contrast to ad hoc methods that use pure 
machine learning, or pure statistical approaches, we base our a priori selection of features 
on an understanding of the role of adjectives and adverbs in language using insights from 
the diverse fields of linguistics and psychology. Our systematic study of adjectives and 
adverbs results in more than 300 genre discrimination tests.  
iii 
 
Our findings demonstrate that we can build more parsimonious classifiers whose 
power is equal to or greater than that of classifiers which use statistical or machine 
learning rules to select features ad hoc from much larger and more diverse sets of 
features. We find that representing documents as vectors of the frequency of fewer than 
20 adverbs, known as speaker-oriented adverbs, is effective for discriminating documents 
by genre. In some cases, performance improves more than 90% over the most rigorous 
baseline using a measure we call accuracy gain.
More generally, we (a) present a model for the systematic study of many complex 
features, (b) demonstrate the principle of selecting features motivated from other 
disciplines, and (c) calculate a new performance measure (accuracy gain) that compares 
results across studies. These contributions can be generalized to other classification 
problems, including tasks defined by other scientific perspectives and disciplines.  
 
iv 
 
Acknowledgements 
This dissertation owes a great deal to my faculty committee. In particular, it 
would not have been possible without the scholarly advice, personal guidance, and 
persistent involvement of my advisor, Dr. Paul B. Kantor. Prior to this dissertation 
research, Paul Kantor gave me the opportunity to work with him on a number of research 
projects. During this time, he trained me to think like a scientist. As my thesis advisor, 
Paul proved to be a trusted friend and counselor as well. 
Dr. Nina Wacholder trained me to think about the role of linguistic features in 
corpora. In particular, Nina stimulated my early interest in the role of adjectives as 
discriminators of types of texts. Nina also directed my earliest dissertation work, helping 
me to form the general framework of my thesis. Nina proved to be a patient, sympathetic, 
and encouraging teacher. 
Dr. Kenneth Safir stimulated my interest in speaker-oriented adverbs, and how 
they might be useful as discriminators of genre. Ken also helped me to bridge the 
disciplinary gap between information science and linguistics. Ken proved to be an 
effective and approachable teacher. 
Dr. Michael Lesk motivated me to develop a way to interpret our results in terms 
of previous research. This led ultimately to the development of our new performance 
measure, accuracy gain.
I also want to thank all my colleagues at Dr. Kantor’s Alexandria Project 
Laboratory (APLab) for providing a fertile and enjoyable work environment. I especially 
wish to acknowledge Bing Bai for creating a number of scripts that enabled me to process 
the texts used in this dissertation. Dr. Ying Sun was indispensable in providing scientific 
v
criticism of my work, and in improving my knowledge of machine learning and statistical 
analysis. More importantly, both Bing and Ying proved to be valuable friends and 
confidants during critical times.  
I would also like to acknowledge two additional lab mates for many hours of 
fruitful collaboration and productive criticism; namely, Aleksandra Sarcevic, a fellow 
Ph.D. student, and Kwong Bor Ng, Associate Professor, Queens College, City University 
of New York. In addition, I owe special thanks to administrative assistant, Shelee Saal, 
for her support and encouragement throughout my tenure at APLab. 
I also owe a debt of gratitude to Tomek Strzalkowski, Associate Professor, 
University at Albany, SUNY, for providing funding that supported me throughout most 
of my doctoral studies through the HITIQA project (High-Quality Interactive Question 
Answering) sponsored by the Advanced Research Development Activity (ARDA).  
Finally, I want to acknowledge Frances Housten, Librarian, Cranford Public 
Library. Fran located many rare journal articles for me. Fran was an effective sounding-
board, general editor, and close friend and confidant. 
 
vi 
 
Table of Contents 
Abstract ……………………………………………………………………. …….. ii 
 
Acknowledgements ……………………………………………………………...... iv 
 
List of Tables ……………………………………………………………………… ix 
 
List of Figures …………………………………………………………………...... xi 
 
Chapter 1: Introduction …………………………………………………………. 1 
 
Chapter 2: Genre and the Computational Study of Text Characteristics …… 7
2.1 Definition of Genre ………………………………………………….... 8 
 2.2 Textual Variation by Genre …………………………………………… 9 
 2.3 Automatic Genre Classification Research .……………………………. 13 
 
Chapter 3: Adjectives and Adverbs as Discriminating Features of Genre ….. 22
3.1 Introduction …………………………………………………………… 22 
 3.2 Adjectives as a Grammatical Class …………………………………… 23 
 3.3 Adjectives and Subjectivity in Documents …………………………… 23 
 3.4 Adjectives and Human Personality: Trait Adjectives …………..…….. 26 
 3.5 Summary of Adjective Research ……………………………………… 29 
 3.6 Variation of Adverbs in Text …………………………………………. 30 
 3.7 Adverbs as a Grammatical Class ……………………………………… 31 
 3.8 Lexical Semantics of Adverbs ………………………………………… 32 
 3.8.1 Speaker-Oriented, Subject-Oriented, & Manner Adverbs ...... 32 
 3.8.2 Speaker- and Subject-Oriented Adverb Sub-Classes ……….. 34 
 3.9 Summary of Adverbs Research……………………………………….. 38 
 3.10 Summary of Adjectives and Adverbs Research  .…………………......38 
 
Chapter 4: Research Problem ………………………………………………...... 39 
 4.1 Exploratory Hypotheses …………………………………………….... 39 
 4.2 Evidence Leading to the Hypotheses………………………………… 40 
 4.3 Potential Significance …………………………………………………. 40 
 
Chapter 5: Methodology, Experimental Design, and Evaluation Measures …. 42
5.1 Corpus …………………………………………………………………. 43 
 5.2 Genre Categories …………………………………………………….... 44 
 5.3 Discriminating Features  ……………………………............................  45 
 5.3.1 Count versus Vector   …………………………...................... 46 
 5.3.1.1 Word Count (Type and Token) …………………… 46 
 5.3.1.2 Word Vector (Type and Token) ………………….. 46 
 5.3.1.3 Count versus Vector in IR Parlance ………………. 47 
 5.4 Experimental Design …………………………………………………... 48 
vii 
 
5.4.1 Methods, Problems and Models …………………………….. 48 
 5.4.2 Learning Method: Discriminant Analysis .………………….. 49 
 5.5 Evaluation Measures ………………………………………………….. 50 
 5.5.1 Classification Accuracy ……………………………………... 50 
 5.5.2 Calculation of Rigorous Baseline and Accuracy Gain ……… 51 
 
Chapter 6: Data Preparation ………………………………………………….... 54 
 6.1 Extraction of Linguistic Features from Corpus ………………………. 55 
 6.1.1 Extraction of Speaker-Oriented Adverbs from Sentence-Initial  
 Position ………………………………………………………….... 56 
 6.1.2 Extraction of Trait Adjectives, Trait Adverbs, and Subjective  
 Adjectives .. ……………………………………………………….. 59  
 
Chapter 7: Results ………………………………………………………………. 62 
 7.1 Accuracy Gain is Better Performance Measure than Accuracy …........ 62 
 7.2 Results Overview ……………………………………………………..  63 
 7.3 Adjective & Adverb Features vs. Other Classes ……………………...  64 
 7.3.1 Relative Performance of Methods by Model Sets ………….  66 
 7.3.2 Universal Methods ………………………………………….  68 
 7.4 Vector versus count …………………………………………………… 71 
 7.4.1 Speaker-Oriented Adverbs in Vector Models ........................ 71  
 7.4.2 Sentence Position of Speaker-Oriented Adverbs …………..... 74 
 7.4.3 Model Performance of Speaker-Oriented Adverbs Combined with  
 Trait Adjectives and Trait Adverbs ……………………………….. 75 
 7.4.4 Most Efficient Vector Models ……………………………..... 76 
 7.5 Salience of Words: Salience Score ……………………………………. 78 
 7.6 Comparison of Results with Previous Classification Research ………. 83 
 
Chapter 8: Conclusions …………………………………………………………. 88
8.1 Summary of Results …………………………………………………..  88 
 8.2 Conclusions ……………………………………………………………  90 
 8.3 Limitations of the Study and Future Work …………………………… 93 
 
References ………………………………………………………………………... 95 
 
Appendix 1.1: Four Genres Returned by Google Relevant to Moby Dick …… 101
Appendix 1.2: Glossary of Terms ……………………………………………….. 102 
 
Appendix 1.3: Abbreviations ……………………………………………………. 106
Appendix 3.1: Sample Word Lists in BNC2 ……………………………………. 108 
 
Appendix 4.1: CLAWS4 POS Tags in BNC2 ………………………………….. 111 
 
Appendix 5.1: BNC2 Genre Categories ………………………………………… 113 
viii 
 
Appendix 5.2: Methods and Models …………………………………………….. 116 
 
Appendix 6.1: Experimental Scripts ……………………………………………. 124 
 
Appendix 6.2: Variables in SPSS Table ………………… ………………………128 
 
Appendix 7.1: Results Table (324 Tests / Models) ………………….………… 136
Appendix 7.2: Number of Unique Words in Vector Models ………………….. 139 
 
Appendix 7.3: Relative Performance of Methods Ranked in Decreasing Order of 
Accuracy Gain by Model Sets ……………………………………….…………. 143
Curriculum Vita ………………………………………………………………….. 146 
 
ix 
 
List of Tables 
2.1: Feature Categories Used for Automatic Genre Classification ………………... 14 
2.2: Summary of Selected Genre Classification Research, 1994-2006 …………… 15 
3.1: Big Five Factors Derived from 100 Trait Adjectives ……………………….  28 
3.2: Three Grammatical Functions of Adverbs …………………………………….31 
3.3: Sentence Position by Adverb Class …………………………………………... 33 
3.4: Subclasses of Adverbs …………………………………. ……………………  35 
3.5: Paraphrase of Speaker-Oriented Adverb Classes ……………………………  35 
5.3: Standard One-Model Performance Matrix …………………………………… 51 
5.4: Standard Performance Measures …………………………………………….. 51 
6.1: Training-Testing Sets …………………………………………………………. 55 
6.2: Thirty Speaker-Oriented Adverbs ………………………………….….……..  56 
6.3: Aggregated Frequency of Thirty Speaker-Oriented Adverbs in BNC2 ……...  57 
6.4: Examples of Speaker-Oriented Adverbs in Sentence-Initial Position ……….. 57 
6.5: Twenty-One Speaker-Oriented Adverbs not Listed by Ernst (2002) ………… 58 
6.6: Possible Alternative Patterns of Speaker-Oriented Adverbs ………………… 58 
6.7: Word Classes Selected for Experiments ……………………………………. 60 
6.8: Word Classes Used in Vector Experiments …………...……………………. 61 
7.1: Ranked Performance of Best Models of Various Adjective and Adverb Features 
…………...………………………………………………………………………… 68 
7.2: Methods effective for all problems ………………………………………….. 69  
7.3: Best Accuracy Gain by Method for Matched Pairs for Speaker-Oriented Adverbs 
Compared to Models Including Trait Adjectives and Trait Adverbs……………... 76 
x
7.4: Number of Unique Words in Vector Models ……………………………….. 77 
7.5: Relative Efficiency of Vector Models ………………………………………. 78 
7.6: Words Ranked in Decreasing Order by Salience Score…………………….. 80 
7.7: Comparison with Published Results ………………………………………. .  84 
 
xi 
 
List of Figures 
2.1: Representative Variation of Occurrence of Nouns and Personal Pronouns by Genre 
…………………………………………………………………………………….. 10 
3.1: Representative Variation of Occurrence of Adjectives and Adverbs by Genre in 
BNC2 …………………………………………………........................................... 22 
5.1: Count of Types and Tokens for Words Belonging to a Given Class ………… 46 
5.2: Vector of Types and Tokens ………………………………………………….. 47 
5.3: Calculation of Accuracy Gain (AG) ………………………………………….. 53 
6.1: Unix Script Used to Extract Speaker-Oriented Adverbs from Sentence-Initial 
Position ………………………………………………………………………...... 56 
6.2: Error Analysis: Extraction of Speaker-Oriented Adverb from Sentence-Initial 
Position ………………………………………………………………………….. 58 
7.1: Relation Between Accuracy and Accuracy Gain  …………………………… 62 
1
Chapter 1: Introduction 
This study is exploratory in nature rather than confirmatory. It is guided by two 
exploratory hypotheses which we set forth in Chapter 4.  
We begin our study by discussing the problem of relevance as it relates to 
information retrieval in order to introduce the problem of automatic genre classification. 
This framework springs from the information science perspective of finding relevant 
information. But in a larger way, we argue that our contributions are extensible to other 
classification problems as defined by other scientific perspectives and disciplines.    
Information retrieval (IR) and relevance are key ideas that motivate information 
science research (Saracevic, 1999). According to Saracevic (1996, p. 3), people 
understand relevance intuitively and use it every day for “determining a degree of 
appropriateness or effectiveness [of information] to ‘the matter at hand.’” This intuition 
has been traditionally applied to the design and evaluation of IR systems, based on the 
assumptions that (a) the goal of IR systems is not to inform but simply to find relevant 
information (see Salton, 1989; van Rijsbergen, 1979), and that (b) relevance can be 
understood as a topical match between a query and a document (see Schamber, 
Eisenberg, & Nilan, 1990). A document is relevant to an information need if its content is 
about the topic (or subject) of the expressed information need. This seemingly 
straightforward proposition, however, is more complex than it first appears. Belew (2000) 
discusses one example of this complexity as found in the Web environment.  
 
2
1.1 The Problem of Relevance and Information Retrieval 
Belew (2000) views the modern field of IR in the context of language and 
communication. Seeking information is a cognitive process that involves (a) 
representation of an information need as a query, a kind of language production, and (b) 
recognition of a representation of the information that satisfies that need, a kind of 
language comprehension. Unlike direct person-to-person communication, however, 
seeking information in the Web is (a) mediated by the search engine, and (b) separated by 
time between production by the author and the request by the information seeker. In 
addition, Web information is often fragmented and lacking context. For example, familiar 
markers that characterize physical objects, such as newspaper headings, are often missing 
in Web surrogates. In this context, relevance judgments may be difficult to make. 
Conversely, an entire document offers more contextual clues about the retrieved 
information, such as document structure, bibliographical references, and writing style. 
Belew thus identifies the tension between topical and non-topical requirements of 
information retrieval. Topical relevance judgments are often conditioned by non-topical 
criteria, and these requirements are not limited to the Web environment. The same can be 
said of library card catalogs and databases. In general, one can say that relevance is 
multidimensional (Saracevic, 1996; Schamber et al., 1990). Relevance involves (a) a 
relationship (such as a query to retrieved documents), (b) intentions (a purpose or reason 
to use information), (c) context (unique users in unique situations), (d) inference 
(judgments and interpretations of information), and (e) interaction (interactive exchange 
of information between users and systems) (Saracevic, 1996).  
 
3
1.2 Is Genre Detection Important for Information Retrieval? 
While relevance decisions are based primarily on topic (see Maglaughlin & 
Sonnenwald, 2002) it is clear that this process is complex, and that users consider a range 
of non-topical criteria. Relevance decisions can be influenced by the unique task or 
problem, and the specific kind of information that is needed to resolve that problem. 
Some users may need information that has certain properties, such as detail, objectivity, 
or multiple viewpoints (see Tang et al., 2003). 1 Other users may require information in a 
specific genre.  
In a study of user criteria in making relevance judgments, Maglaughlin and 
Sonnenwald (2002, p. 333) reported that some users are influenced by the “form or type 
of artifact.” One subject affirmed that “this one looks really relevant because it’s a 
dissertation [italics added].” Another subject complained that, “You have all these books, 
I can’t read them all, but if I get a review essay [italics added], I can get the key content.” 
In these situations, relevance decisions were strongly motivated by genre. For this study, 
we adopt the analysis by Lee (2001) in which he defines genre as a socially constructed 
classification of texts based on the external criteria of use, such as the intended audience, 
the speaker’s purpose, and type of activity (see also Kessler, Nunberg, & Schutze, 1997; 
Wolters & Kirsten, 1999). Thus, a news report is intended to inform, an editorial or 
opinion piece is intended to persuade, and a novel is intended to entertain.  
A simple information retrieval example illustrates the problem of relevance and 
genre in real-world information-seeking. Suppose that a user of Google needs 
information about the novel, Moby Dick, but requires that it be in the form of academic 
 
1 The problem of automatically classifying texts by qualitative properties has been studied by Sun (2005). 
4
writing. A search for “Moby Dick” retrieved 18,800 documents. 2 Occurring within the 
top 10 documents are specimens from at least three different written genres: academic,
popular, and fiction. Two news documents occur within the top 70 documents (see results 
reported in Appendix 1.1). Classifying the 18,800 documents by genre would reduce the 
effort for this user of accessing relevant information about Moby Dick. As this example 
illustrates, non-topical property discrimination, such as genre, is increasingly important 
because of the enormous growth in the scope and size of information collections. This is 
a problem not only in the Web, but in library collections and other information sources as 
well.  
1.3 Automatic Genre Discrimination 
By automatic discrimination we mean the use of computer systems to 
differentiate documents by recognizing distinguishing features in text, and to classify 
documents into categories (such as genre) using these features. Previous studies in 
automatic genre classification used a variety of discriminating features, such as 
frequencies of parts of speech, stop words (e.g., a, and, in, the, and that), most frequent 
words in a corpus, punctuation, and various ratios such as words per sentence or 
characters per word (see Finn & Kushmerick, 2006; Karlgren & Cutting, 1994; Kessler et 
al., 1997; Lee & Myaeng, 2002; Lim, Lee, & Kim, 2005; Ng, Rieh, & Kantor, 2000; 
Stamatatos, Fakotakis, & Kokkinakis, 2000; Wolters & Kirsten, 1999). Features are often 
selected ad hoc using a pure statistical approach or a pure machine learning approach. In 
the present work, we select features a priori based on insights about characteristics of 
 
2 Search limited to text documents, performed on March 7, 2006, using the search terms, "’Moby Dick’ 
filetype:txt”.  
5
adjectives and adverbs from research in other disciplines, namely, linguistics and 
psychology.  
We propose that features selected this way are generalizable across a range of 
classification problems. The general principle is based on the capacity to exploit human 
ability to think in terms of underlying causes by consulting scientific literature. In this 
instance, the literature suggests that adjectives and adverbs will vary by genre because of 
their unique patterns of usage in text. Our research studies this variation and the impact 
on genre. While other features may be useful as well, we focus on the unique contribution 
of adjectives and adverbs to automatic genre classification.  
We use the machine learning method called discriminant analysis to classify 
documents by genre (specifically the statistical package provided by SPSS, Version 
11.0). Discriminant analysis selects a sample of observed groups (training data), makes 
the groups as statistically distinct as possible, and makes predictions about the 
membership of unseen cases (testing data) (see Eisenbeis & Avery, 1972). Discriminant 
analysis has been used for automatic genre classification by Karlgren and Cutting (1994), 
Ng et al. (2000), and Stamatatos et al. (2000). Discriminant analysis is appropriate for our 
data because like these authors, we also represent genres as groups that can be 
distinguished statistically, and these distinctions can be “learned” using computer 
systems.  
In Chapter 2, we review literature related to automatic genre classification and the 
computational study of text characteristics. We show how it is difficult to compare results 
across genre classification studies. In Chapter 3, we present research from other 
perspectives and disciplines about the characteristics of adjectives and adverbs, and 
6
discuss why these insights can be exploited in a systematic way for automatic genre 
discrimination. In Chapter 4, we discuss our research problem which is guided by two 
exploratory hypotheses. In Chapter 5 and Chapter 6, we present our methodology and 
data preparation, respectively. In these chapters we also define a new measure for 
comparing classification results across studies, which we call accuracy gain. We also 
show how to systematically compare a large set of features from a narrow selection of 
grammatical classes for multiple classification problems.  
Chapter 7 reports our experimental results and analysis. Chapter 8 concludes with 
a summary of results, conclusions, limitations of our study, and recommendations for 
future research. In short, we recommend that researchers (a) replicate our model for 
comparing large feature sets, (b) use accuracy gain (AG) to compare performance across 
research studies, and (c) use features motivated by research from other disciplines that 
can be generalized to other classification problems. 
We also provide a glossary of terms as defined in this study (Appendix 1.2), and a 
list of abbreviations (Appendix 1.3).  
 
7
Chapter 2: Genre and the Computational Study of Text 
Characteristics 
In this chapter, we introduce the method of corpus linguistics, define genre, and 
discuss how characteristics of text can vary by genre. We also review literature related to 
automatic genre discrimination, and show how it is difficult to compare results across 
studies.  
We adopt the analytical perspective of corpus linguistics. Corpus linguistics is the 
study of textual characteristics in large collections of real-world documents through 
statistical and machine learning methods. A corpus represents language as it is actually 
used (Biber, Conrad & Reppen, 1998; Kennedy, 1998; Leech, 1997). The corpus we use 
is the British National Corpus, World Edition (BNC2) (BNC2, 2001). 3 The BNC2 is 
widely used by research communities in corpus linguistics, computational linguistics, 
natural language processing, and stylistics. The BNC2 is designed to represent the range 
of British English speech (e.g., broadcast news, face-to-face spontaneous conversations,
and planned speech) and writing (e.g., academic-medicine, fiction-prose, and news print 
tabloid) (Lee, 2003b). The BNC2 includes more than 4,000 documents and nearly 100 
million words.  
An established technique of corpus linguistics is automatic part-of-speech (POS) 
tagging (Church, 1988). Automatic POS tagging annotates raw text with grammatical 
markers, such as parts of speech (e.g., nouns, verbs, and adjectives) and other markers, 
such as punctuation. This process makes it possible to count the frequency of these 
 
3 All references to BNC2 (unless otherwise specified) hereinafter refer to BNC2, 2001 (British National 
Corpus, World Edition, 2001.  http://www.natcorp.ox.ac.uk/)  (see References). 
8
markers in documents. BNC2 parses documents using CLAWS4 (Leech, n.d.; Leech, 
Garside, & Bryant, 1994; Garside & Smith, 1997). CLAWS4 employs probabilistic and 
non-probabilistic techniques to assign tags in a four-stage process. Simply put, the tagger 
identifies individual words and assigns tags to each word using a mixture of word lists 
and probabilistic procedures that take into account the context in which a word occurs 
(BNC2 POS-Tagging Manual, 2000). The BNC2 and CLAWS4 are discussed in greater 
detail in Chapter 5. 
In the next sections, we define genre (Section 2.1), discuss how characteristics of 
text can vary by genre (Section 2.2), and review published results relating to automatic 
genre classification (Section 2.3).    
2.1 Definition of Genre 
For this study, we adopt the definition of genre given by Lee (2001). According to 
Lee, genre is a document-level category  
assigned on the basis of external criteria such as intended audience, purpose, and 
activity type, that is, it refers to a conventional, culturally recognised grouping of 
texts based on properties other than lexical or grammatical (co-)occurrence 
features, which are, instead, the internal (linguistic) criteria forming the basis of 
text type categories. (Lee, 2001, p. 38) 
 
Thus, a news report is intended to inform, an editorial or opinion piece is intended to 
persuade, and a novel is intended to entertain.  
In other analytical settings, register is often used interchangeably with genre (see 
Biber et al., 1998). The definitions are compatible. According to Biber et al., registers are 
varieties of texts defined by external criteria based on situational characteristics, that is, 
on how they are intended to be used. This includes their “purpose, topic, setting, 
interactiveness, mode, etc.” (Biber et al., p. 135). These situational differences are based 
9
on the assumption that people will make different linguistic choices when writing a 
personal letter or when engaged in a casual conversation, for example, than when they are 
writing a formal report, or while giving a formal speech. Registers can be very general, 
such as fiction, press reportage, or academic writing, or quite restricted to reflect, for 
example, novels by a certain author, or abstracts from academic papers.
These external criteria are distinct from internal criteria of form or style, such as 
formal versus informal writing, imaginative versus informative writing, and Biber’s 
(1988) dimensions of speech and writing (e.g., Informational Interaction, Learned 
Exposition, and Involved Persuasion). Internal differences are based on the assumption 
that culture-specific conventions, which form the basis for assigning a text to a genre, are 
reflected in the style of the text. If that style can be represented quantitatively as 
exhibiting some linguistic features more often than others, then those features will 
discriminate one genre of text from another.  
2.2 Textual Variation by Genre 
In his seminal work, Variation Across Speech and Writing, Biber (1988) observed 
that genre variations among texts can be measured by counting the occurrence of textual 
features, such as the frequency of parts of speech. In Figure 2.1, Biber shows how a 
sample of academic writing and a sample of face-to-face conversation can vary by 
counting the occurrence of nouns (shaded) and personal pronouns (bold). We observe 
that the sample of academic writing has a high density of nouns (about 25%) and no 
personal pronouns. In contrast, the sample of face-to-face conversation contains virtually 
no nouns, but a relatively high density of personal pronouns (15%).    
 
10
Genre Sample 
Academic 
Writing 
 
Evidence has been presented for a supposed randomness in the 
movement of plankton animals. If valid, this implies that migrations 
involve kineses rather than taxes ….  
Face-to-Face 
Conversation 
Person A: and, mm I mean, when you get used to that beer, which at 
its best is simply, you know, superb, it really is. 
Person B: mm 
Person A: you know, I’ve really got it now, really, you know, got it to 
a T. 
Person B: yeah 
Nouns=shaded; Personal Pronouns=bold 
Figure 2.1: Representative Variation of Occurrence of Nouns and Personal Pronouns by Genre 
(Derived from Biber, 1988) 
 
Biber (1988) studied feature variation in 481 documents from 23 genres 
(discussed below). He generalized about the underlying dimensions of English discourse, 
demonstrating that genre differences are distinguished on a continuum, as opposed to 
discrete categories. Biber drew from scientific research in functional analysis of linguistic 
features (see Chafe, 1982). Functional analysis assigns distinct functions to linguistic 
features. For example, adjectives and adverbs generally “seem to expand and elaborate … 
information”; nominalizations “integrate information into fewer words” and convey 
“highly abstract … information (Biber, pp. 227, 237). Based on these insights, Biber (pp.  
223-245) selected 67 linguistic features (e.g., place adverbials, nominalizations, 
attributive and predicative adjectives, wh-clauses, possibility modals, be as main verb, 
existential there, type/token ratio, and word length) and studied their distributions across 
varieties of texts. 
 Biber (1988) hypothesized that frequently co-occurring features share common 
communicative functions and production circumstances. Production circumstances 
characterize the context within which the speaker/writer is situated (e.g., time-critical, 
11
interactive, etc.). Communicative functions, such as integration and involvement, reflect 
the primary purpose of the speaker/writer (see also Chafe, 1982).  
For example, Biber (1988) argued that integration is characteristic of text that 
condenses a large amount of information in relatively few words. Integrated text is 
typically characterized by precise word selection and complex organization. An example 
is academic prose which is carefully planned, integrates a large amount of information, 
and is not interactive. (Interaction refers to the listener’s unique opportunity to respond 
directly to speaking situations.) Academic prose is likely to contain few first- and second-
person pronouns (I and you) and contractions (e.g., it’s, I’m, and I’ve). On the other hand, 
academic prose is likely to contain many nominalizations (nouns with complex internal 
structure such as equation (equate + -ion => equation).  
In contrast, Biber (1988) argued that the communicative function of involvement 
is characteristic of text that is highly interactive between speaker and listener. It is 
typically non-informational. It is produced on the spot and is not carefully planned. Face-
to-face conversation is an example. This kind of text is likely to contain many first and 
second person pronouns and contractions, but few nominalizations (Biber, 1988; see also 
Biber et al., 1998).   
To test these assumptions, Biber (1988) selected the collection of 481 documents 
from the Lancaster-Olso-Bergen Corpus of British English (LOB) (15 written genres), the 
London-Lund Corpus of Spoken English (six spoken genres), and a supplementary (non-
published) corpus of written texts (professional letters and personal letters). In sum, the 
documents represent 17 written genres (e.g., press reportage, biographies, official 
documents, and academic prose), and six spoken genres (e.g., face-to-face conversation,
12
public conversations, broadcast, and planned speeches). Biber then selected 67 linguistic 
features. He based his selection criteria on his intuition that specific features will be 
representative of distinct and important communicative functions (as supported in 
functional analysis research). Factor analysis then reduced the 67 variables to seven 
dimensions of English discourse which Biber interpreted using functional analysis. For 
example, Biber labeled one of the dimensions Informational Production versus Involved 
Production.
According to Biber (1988), Informational Production is characteristic of text that 
selects precise, information-bearing words in circumstances where careful editing is 
possible. Typical texts belong to the genre of press reportage, and to varying degrees, the 
genres of academic prose, press reviews and editorials, biographies, and official 
documents. Texts of this type are characterized by high frequencies of nouns, 
prepositions, and carefully selected adjectives. 4 Biber (p. 105) interpreted this kind of 
text as having a "high informational focus,” and a production requirement that permits 
“ample opportunity for careful integration of information and precise lexical choice."  
In contrast, Involved Production is characteristic of text that is generally non-
informational, affective, and interactive (e.g., face-to-face conversation, or informal,
personal letter writing). Time and circumstances constrain the production requirements 
of this kind of text. There is little opportunity for contemplative selection of words, or the 
construction of complex sentences. Grammatically, this kind of text is characterized by 
frequent use of first and second person pronouns (e.g., I, we, and you) and present tense, 
private verbs (e.g., know, think, and believe). 
 
4 As a side note, we doubt whether automatic discriminant methods can ever easily distinguish adjectives 
that may be carefully selected.
13
The next section reviews previous genre classification research. While some 
studies include adjective and adverb variables in their models, our study is distinct from 
these in that we focus exclusively on the discriminating characteristics of distinct sets of 
adjectives and adverbs (e.g., speaker-oriented adverbs and trait adjectives). We advance 
genre classification research by exploring the role of adjectives and adverbs in 
distinguishing genre, based on insights from linguistics and psychology. Like many 
researchers, we represent documents as vectors of the frequency of occurrence of distinct 
words, but we focus exclusively on adjectives and adverbs. To advance earlier studies, 
we use a much larger corpus (N=4,053) compared to previous studies that generally use 
fewer than 1,000 documents. 
2.3 Automatic Genre Classification Research   
In this section, we show why it is difficult to compare results across automatic 
genre discrimination studies. In later chapters, we will introduce ways of overcoming this 
difficulty. Namely, (a) selecting features identified by other disciplines that can be 
generalized to other classification problems (Chapter 3), (b) using our model for 
systematic feature comparison (Chapter 5), and (c) using a new performance measure 
(accuracy gain) that can compare accuracy results across studies (Chapter 5).  
In order to classify documents automatically by genre, researchers usually select 
features ad hoc from large sets of diverse linguistic and textual categories (e.g., various 
parts of speech; punctuation; specific words; and count of words per document, or 
sentence; or characters per word) (Table 2.1). Our study is unique in that we select 
features a priori from only two grammatical classes (adjectives and adverbs), we base our 
selection on insights gained from prior research in the fields of linguistics and 
14
psychology, and argue that selecting features this way is extensible to other classification 
problems.  
Table 2.1: Feature Categories Used for Automatic Genre Classification  
 
Feature 
Category 
Examples 
Syntactic Parts of speech (e.g., adverbs, nouns, verbs, and prepositions) 
Lexical 
Terms of address (e.g., Mr., Mrs., Ms.) 
Content words (based on topic) 
Most frequent words in a corpus (e.g., the, of, and, a, and in)
Character-
Level 
Punctuation, character count, sentence count, word length in characters 
Derivative 
Ratio measures (e.g., average words per sentence, average characters per word, 
type/token ratio) 
Table 2.1 shows that researchers generally select features from four broad 
categories: syntactic, lexical, character-level, and derivative. Syntactic features are 
generally counts of parts of speech (e.g., adverbs, nouns, verbs, and prepositions). 
Lexical features are generally counts of distinct words (e.g., a, in, the, that, Mr., Mrs.,
and Ms.), or words based on topic. Character-level features typically include the count of 
(a) punctuation, (b) characters per word, and (c) characters per sentence. Derivative 
features generally involve mathematical calculations, such as average words per sentence, 
average characters per word, average characters per sentence, and type/token ratio (see 
Karlgren & Cutting, 1994; Kessler et al., 1997; Ng et al. 2000; Stamatatos et al., 2000; 
Wolters & Kirsten, 1999). Table 2.2 summarizes the classification research reviewed 
below. 
 
15
Table 2.2: Summary of Selected Genre Classification Research, 1994-2006 
 
Researchers # Features Feature 
Selection 
Criterion 
Corpus & 
 # Docs 
Classification 
Method 
Selected Results 
Accuracy Genres 
Karlgren & 
Cutting 
(1994) 
20 Subset of 
Biber (1988) 
based on 
ease of 
computation 
Brown 
Corpus 
N=500 
 
Discriminant 
Analysis 
96.0%  Informative 
vs. 
Imaginative 
Kessler et al. 
(1997) 
55 Ease of 
computation 
Brown 
Corpus 
N=500 
 
Logistic 
Regression 
94.0% Fiction vs. 
Not Fiction 
Wolters & 
Kirsten 
(1999) 
Indeterminate Statistical 
based on  
distributional 
differences 
LIMAS 
(German) 
Corpus 
N≈500 
k-Nearest 
Neighbor 
88.0% Humanities 
vs. Not 
Humanities 
Ng et al. 
(2000) 
4 Features 
independent 
of syntax 
and semantic 
TREC-4 
(N=144,121) 
Discriminant 
Analysis 
81.9% WSJ vs.  
Federal 
Register 
Stamatatos 
et al. (2000) 
58 Statistical: 
50 most 
frequent 
English 
words 
Wall Street 
Journal,
1989 
N≈280 
Discriminant 
Analysis 
>97.0% (Categories 
 not 
specified) 
Lee & 
Myaeng 
(2002) 
Range 86-
166 
Statistical: 
genre and 
subject terms 
Web derived 
(N=7,615)  
Similarity 
using tf and df 
ratios 
78.7% Reportage 
+ Editorial 
Lim et al. 
(2005) 
>300 Previous 
classification 
research + 
Web tags 
1,224 Web 
documents 
TiMBL (k-
Nearest 
Neighbor) 
95.0% Journalistic 
materials 
Finn & 
Kushmerick 
(2006) 
Indeterminate Bag-of 
Words, POS, 
Text 
statistics 
Two corpora 
Web-
derived 
(N=796; 
N=1,354) 
Decision tree 
C4.5 
94.1% Restaurant 
reviews 
Karlgren and Cutting (1994) used discriminant analysis to classify 500 documents 
from the Brown Corpus using a subset of 20 features derived from Biber (1988). They 
selected the 20 features ad hoc based on the pragmatic criterion of ease of computation 
using a standard part of speech tagger. For example, Karlgren and Cutting selected 
Biber’s feature of the count of first person pronouns (e.g., I, we, and me), and rejected the 
count of hedges (e.g., at about, something like, and more or less) because the latter 
16
pattern is difficult to reliably identify in text. Other examples of the 20 features include: 
(a) count of adverbs, prepositions, and nouns; (b) count of the words therefore, me, and I;
(c) characters per document; and (d) average words per sentence and type/token ratio.  
Karlgren and Cutting (1994) classified documents into two, four, ten, and fifteen 
genre categories. They achieved their best result for the binary problem informative and 
imaginative (96% accuracy). As expected, performance degraded for four-way problems 
(73%); ten-way problems (65%), and fifteen-way problems (52%). Clearly, the finer the 
distinction, the harder it is to detect genres this way. 
Kessler et al. (1997) classified 500 documents from the Brown Corpus using 
logistic regression, and an initial selection of 55 features (lexical, character-level, and 
derivative features). They selected features based on the ad hoc criterion of ease of 
computation without using a part of speech tagger. They used the statistical method of 
stepwise backward selection to select the final set of features for their models, resulting 
in different sets of variables for each binary discrimination problem (p. 35). Kessler et al. 
concluded that classification by genre is possible with reasonable accuracy using the ease 
of computation criterion as opposed to using more costly features that require part-of-
speech software. For instance, they report an accuracy of 94.0% for the two-way 
classification problem fiction vs. not-fiction.
Wolters and Kirsten (1999) used a k-nearest neighbor learning algorithm to 
classify 500 documents from the LIMAS German corpus. They selected five domain-
centered genres 5 (politics, law, economy, academic humanities, and academic sci-tech) 
and four “traditional” genres (press, high-quality press, fiction, and low-quality fiction). 
Wolters and Kirsten took a hybrid approach, combining the traditional IR "bag of words" 
 
5 The “domain-centered” definition of genre differs from our working definition as defined by Lee (2001). 
17
method with a traditional natural language processing (NLP) method they called, "bag of 
‘tagged’ words." The authors acknowledge the IR distinction between content words (that 
are related to topic) and function words (that are orthogonal to topic) (their function 
words include mostly stop words). They represented documents as vectors of the 
frequency of content word lemmas 6 and function words. Wolters and Kirsten 
hypothesized that adding part of speech (POS) information to the count of word vectors 
makes it possible to preserve some relational linguistic qualities, thus improving 
classification performance.  
Wolters and Kirsten (1999) did not specify which lemmas (or how many lemmas) 
they selected, but they found that the four most discriminating classes of content words 
are (a) finite forms of full verbs, 7 (b) nouns, (c) adverbial adjectives, 8 and (d) attributive 
adjectives. Wolters and Kirsten (p. 145) found a significant distributional difference for 
seven of the original set of 54 parts-of-speech (but they selected a total of 15 parts-of-
speech because these classes exhibited “characteristic distributions.” 9 Document vectors 
ranged from 50 to 200 features. With regard to the categories, Wolters and Kirsten found 
that binary test results are best for humanities and low-quality fiction. Average accuracy 
for humanities (as distinguished from not humanities) is 88% using content words, 
function words and punctuation, plus POS tags.   
 
6 Wolters and Kirsten (1999) do not define their use of the term lemma, which in this context, we take it to  
mean the base lexical form of a word rather than the stem of a word, as is used in informational retrieval. 
Thus, the lemma of went is go, whereas the stem of went is went. 
7 “Finite forms of full verbs:” This is a very technical distinction for a specific grammatical class. Briefly, it 
would include recites, as in She recites the poem clearly, but it would not include reciting as in She is 
reciting the poem clearly. In the first example, recites is both a full verb (not an auxiliary verb), and a finite 
verb (inflected for person or tense). However, in the second example, reciting is a full verb, but it is not a 
finite verb because of the presence of the auxiliary verb is.
8 Wolters and Kirsten (1999) do not define this class, but in German corpora, it may be that it is 
interchangeable with adjectival adverbs. 
9 Wolters and Kirsten (1999) do not define this criterion. 
18
Inspired by research in author attribution (see Burrows, 1992), Stamatatos et al. 
(2000) selected features by using statistical analysis to select the 50 most common words 
in the British National Corpus (BNC) (e.g., the, of, and, a, and in), as well as eight of the 
most frequent punctuation symbols (period, comma, colon, semicolon, quotes, 
parenthesis, question mark, and hyphen). They used these features, derived from one 
corpus (BNC), to “learn” from training data in another corpus, a corpus of Wall Street 
Journal articles from 1989 (total corpus ≈ 280 documents). Although Stamatatos et al. do 
not specify which genre categories are most accurately detected (editorial, letters, spot 
news, or reportage), they report an overall accuracy of greater than 97%. Their work is 
distinct in that they use one corpus (BNC) to statistically select their features, and another 
corpus (Wall Street Journal) to test their method.  
Finn and Kushmerick (2006) combine our working definition of “genre” (Lee, 
2001) with the traditional definition of “domain”, and with the qualitative concept of 
“subjectivity.” Their resulting categories are as diverse as football, finance, politics,
movie reviews and restaurant reviews. They used the C4.5 learning tree (Quinlan, 1993; 
cited in Finn & Kushmerick) to classify two sets of Web-generated corpora: 796 
documents for the categories: football, politics, and finance; and 1,354 documents for 
movie reviews and restaurant reviews. They selected features based on three statistical 
criteria: (a) bag-of-words (representing each document as a vector indicating the presence 
or absence of a word), (b) part-of-speech statistics (representing each document as a 
vector of 36 POS features); and (c) text statistics (e.g., average sentence length, average 
word length, and frequency of punctuation). The best results for classifying genre 
categories (which are consistent with our working definition of genre) are 76.8% for 
19
movie reviews using the bag-of-words approach, and 94.1% for restaurant reviews using 
text statistics. 
Lee and Myaeng (2002) hypothesized that genre classification can be improved 
using traditional subject-based text classification methods (term frequency statistics). Lee 
and Myaeng selected features statistically based on subject-classified and genre-classified 
training data using three criteria: (a) Find terms that occur in as many documents 
belonging to one genre as possible, and that are distributed as evenly as possible among 
all subject classes. (b) Eliminate terms that are too specific to a particular subject. (c) 
Downgrade terms that are common to many genres because they are not good 
discriminators between genres. Lee and Myaeng collected 7,615 documents from the 
Web using human subjects to assign a subject category, and one of seven genre 
categories: reportage, editorial, technical paper, critical review, personal homepage,
Q&A, and product specification. In a six-way classification experiment, Lee and Myaeng 
(2002) reported an accuracy of 78.7% for reportage and editorial documents using a 
combination of genre and subject terms. 
 Lim et al. (2005) selected features used in previous genre discrimination research 
(e.g., POS, punctuation, average words per phrase, and frequency of content words), plus 
Web-based technology features based on HTML tags and URL information. They 
selected 16 genre categories that occur in the Web, and divided them between textual 
genres and non-textual genres. Documents belonging to the non-textual genre category 
are technology-induced. Examples of these genres include personal homepages,
commercial homepages, image collections, and link collections. Examples of textual 
genres include journalistic materials, research reports, discussions, and product 
20
specifications. For their experiments, Lim et al. used automatic methods to collect 1,224 
Korean documents from the Web, and they used human subjects to assign the genre 
categories. They found the best accuracy for discriminating between textual versus non-
textual documents. Of the textual genres, research reports, journalistic materials, and 
discussions showed the best accuracy results (89%, 95%, and 85%, respectively).    
Using only four punctuation features (comma, period, colon, and semicolon), Ng 
et al. (2000) report an accuracy of 81.9% discriminating between Wall Street Journal 
(WSJ) and Federal Register (FR) documents in a large corpus of TREC-4 documents 
(N=144,121). They used linear discriminant analysis to investigate the effectiveness of 
features that are independent of the syntax and semantics of a feature. Thus, they pre-
selected six features: the relative frequency (with respect to word frequency) of commas, 
periods, colons, semicolons, and letters, and the relative frequency (with respect to 
paragraph frequency) of words. Of these features, the four punctuation features (cited 
above) proved to be most effective. 
It is clear that comparing research in automatic genre discrimination is difficult 
because researchers use a wide range of statistical methods, features, genre categories, 
and even different definitions of genre. In some cases, it is difficult to understand results 
because of the way results are reported in the literature. We also see that researchers 
usually select features ad hoc and test them on small corpora. Our study shows how some 
of these problems may be overcome by using features that can be generalized to other 
classification problems. In the next chapter, we discuss insights gained from other 
disciplines about the unique characteristics of adjectives and adverbs, and why these 
21
characteristics make adjectives and adverbs useful for classifying documents 
automatically by genre.   
 
22
Chapter 3: Adjectives and Adverbs as Discriminating Features 
of Genre  
3.1 Introduction 
As we discussed, researchers have tested the effectiveness of a wide range of 
discriminating features, and this work clearly deserved to be explored. But this work is 
largely heuristic. We advance this research by focusing, in-depth, on a narrow set of 
grammatical features (adjectives and adverbs), as inspired by insights gained from other 
disciplines (linguistics and psychology).  
In the following sections, we discuss how insights gained from other disciplines 
support the proposition that distinguishing characteristics of adjectives and adverbs are 
useful for genre discrimination. To illustrate this, Figure 3.1 shows two excerpts of 
written text from BNC2 documents. The sample of news contains more adjectives 
(shaded) than the sample of fiction (12% vs. 2%). The sample of fiction contains more 
adverbs (bold) than the sample of news (10% vs. 5%). 
Genre Sample 
News 
writing 
The plainclothed Stasis – mostly dull-faced youths in windcheaters – formed snatch 
squads and, with uniformed colleagues, kicked and beat demonstrators. On at least one 
occasion, demonstrators were pinned against the wall to make it easier to assault them. 
Busloads of Stasis were disgorged into the crowd. Many arrests were made. 
Fiction 
writing 
She went to sit on the sofa, and there she dreamed of Johnny, tenderly and with 
affection; she thought of the words that he had said, and they made a warm spot, 
infinitely sweet, deep inside her where it could not be touched; for just an hour or two, 
she began to believe that she truly loved him. 
Adjectives (shaded); Adverbs (bold)
Figure 3.1: Representative Variation of Occurrence of by Genre in BNC2 
 
23
3.2 Adjectives as a Grammatical Class 
As a grammatical category, adjectives modulate the meaning of nouns by 
emphasizing important or surprising properties of the noun being modified (e.g., a safe /
historical / unusual building). Adjectives are frequently indicators of judgment or 
opinion. The statement, She wrote a poem, is a statement of (presumed) fact. The 
statement, She wrote a beautiful / terrible poem, mixes a statement of fact with human 
judgment.  
The next section (Section 3.3) reviews how adjectives are associated by 
computational linguists with the concept of subjectivity in text. Section 3.4 reviews how 
adjectives are studies by psychologists to differentiate between personality types.  
3.3 Adjectives and Subjectivity in Documents 
Research indicates a correlation between human perceptions of subjectivity and 
the occurrence of adjectives in (a) sentences (Bruce & Wiebe, 1999; Wiebe, 2000a; 
Wiebe et al., 1999) and (b) documents (Rittman et al., 2004). This relationship seems to 
be expected because of the nature of adjectives themselves. Subjective expressions 
necessarily involve judgments and opinions about people and things, and we frequently 
use adjectives to evaluate and classify these kinds of distinctions.  
Wiebe et al. (1999) found that the mere presence of one or more adjectives in a 
sentence is one of several useful predictors of a trait called subjectivity. Bruce and Wiebe 
(1999) concluded that the occurrence of members of a class identified by Quirk, 
Greenbaum, Leech, and Svartvik (1985) as dynamic adjectives is more highly correlated 
with subjectivity than is the occurrence of adjectives in general or of the complement set 
24
of the class Quirk et al. identified as stative adjectives. Stative adjectives denote a 
property that is considered permanent (e.g., tall person, big boy, and blonde girl). On the 
other hand, dynamic adjectives denote attributes that may change (e.g., careful person, 
lazy boy, and serious girl). The person may be careful one time, but careless another 
time. Bruce and Wiebe conclude that the occurrence of adjectives in the dynamic class is 
more indicative of subjectivity than is the occurrence of members in the stative class.  
In later work, Wiebe (2000a) studied the lexical variables called polarity and 
gradability. Polarity denotes the evaluative nature of a word along a positive—negative 
continuum. Thus, good has a positive polarity; bad has a negative polarity. A word is 
gradable if it distinguishes a relative quality, as opposed to a binary quality. For instance, 
hot is a gradable adjective since something can also be more or less hot. In contrast, 
married or rectangular are binary qualities. Someone is either married, or not married.
Something is either rectangular, or it is not rectangular (see also Raskin & Nirenburg, 
1995; Staab & Hahn, 1997). Wiebe found that the mere presence of adjectives in 
sentences results in a baseline accuracy of 55.8%. Sentences that contain polar or 
gradable adjectives are more likely to be perceived as being subjective, resulting in an 
accuracy of 79.6% (see also Hatzivassiloglou & McKeown, 1997; Hatzivassiloglou & 
Wiebe, 2000). 
Rittman et al. (2004) studied the relationship between the occurrence of adjectives 
and human judgments of subjectivity at the document level in a corpus of 2,243 
documents. 10 They calculated the number of adjectives in each document, and recorded 
 
10 The corpus is comprised of document from (a) LA Times, Wall Street Journal, Financial Times of 
London, and the Associated Press provided in the TREC collection (Voorhees, 2001), (b) the Center for 
Nonproliferation Studies (Center for Nonproliferation Studies, 2007), (c) ARDA/AQUAINT (2003) 
25
human judgments for each document for the qualitative property objectivity. (They 
defined objectivity as the extent to which a document includes facts without distortion by 
personal or organizational biases.) Next, they calculated the occurrence of a set of 
automatically-derived subjective adjectives (e.g. absolute, artificial, grotesque, and 
lucky) defined by Wiebe (2000a, 2000b) in each document in the corpus (see Appendix 
3.1, Table 2 for a sample list). They found that the total count of unique adjectives in the 
corpus is 7,496, of which 955 are subjective adjectives, and 6,541 are members of the 
complement set of not-subjective adjectives. Rittman et al. found that the occurrence of 
members of the full set of all adjectives is negatively correlated with human judgments of 
objectivity (which is interpreted as a positive correlation with subjectivity). The 
correlation is strongest for the occurrence of the minority set of subjective adjectives. The 
correlation is weakest for the occurrence of members of the larger, complementary set of 
not subjective adjectives. This finding at the document level is consistent with Wiebe’s 
results at the sentence level (Bruce & Wiebe, 1999; Wiebe, 2000a; Wiebe et al., 1999).  
In sum, adjectives belong to one or more overlapping classes. Some adjectives 
help to identify subjectivity in texts more than do other adjectives, at both the document 
level and at the sentence level. Since some types of adjectives are more likely to occur in 
subjective documents, we seek to apply this insight to advance automatic genre 
classification. 
The next section (3.4) describes applied psychology research in the use of 
adjectives that describe human personality, known as trait adjectives. We include a brief 
history of trait adjective research, and explain how psychologists use these adjectives to 
 
documents from the Associated Press, New York Times, and Xinghua (English), and (d) Web documents 
collected using Google. 
26
describe human personality characteristics. Although trait adjective research was 
developed for personality assessment, and comes from a different theoretical perspective, 
we hope to exploit the insights gained by psychologists about adjectives to improve genre 
classification.  
3.4 Adjectives and Human Personality: Trait Adjectives 
The significance of adjectives in description and judgment has long been noted in 
psychology. Psychologists use adjectives, which they call trait adjectives, to describe 
human personality traits (e.g., nervous, energetic, accommodating, and careful). Trait 
adjectives are classified by the type of personality they indicate, based on theories of 
psychology. The development of these lists emerges from a long history of research 
dating to the 19th century (see Goldberg, 1993). Since trait adjectives frequently have 
evaluative connotations, at least in context, they are also possible indicators of 
subjectivity.  However, we seek to broaden this observation by considering adjectives as 
useful discriminators of genre. As we discuss in the next sections, we propose that certain 
classes of adjectives are more likely to occur in some genres than in others. 
Interest in trait adjectives originated with the lexical tradition in psychology, 
which is traced by John, Angleitner, and Ostendorf (1988) to Sir Francis Galton, the 
originator of modern personality research. 11 Galton’s contributions to psychology 
include the invention of word association tests, and the application of statistical methods 
to experimental data (Rushton, 1990). Galton (1884) recounted the beginning of his 
inquiry into the measurement of human character using adjectives.  
 
11 Galton is perhaps best known for his discredited ideas on eugenics. However, he was a polymath who 
made many important scientific contributions (Indiana University, 2004). 
27
“I tried to gain an idea of the number of the more conspicuous aspects of the 
character by counting in an appropriate dictionary the words used to express 
them. Roget's Thesaurus was selected for that purpose, and I examined many 
pages of its index here and there as samples of the whole, and estimated that it 
contained fully one thousand words expressive of character, each of which has a 
separate shade of meaning, while each shares a large part of its meaning with 
some of the rest.” (Galton, 1884, p. 181) 
 
Subsequent to Galton’s (1884) work, two early attempts to develop structured lists 
of trait adjectives include (a) Allport and Odbert (1936), who expanded Galton’s list of 
1,000 words using Webster’s Unabridged Dictionary, and (b) Norman (1967), who 
developed a list of 2,800 trait adjectives using Webster’s Third Edition (see Goldberg, 
1993). Using factor analysis on various lists of adjectives Goldberg (1992) proposed five 
dimensions of personality that are generally accepted as the “Big Five”: I. Extraversion, 
II. Agreeableness, III. Conscientiousness, IV. Emotional Stability, and V. Intellect. (Here, 
psychologists use the term dimension to indicate single scales whose endpoints are 
distinct.) 
The Big Five are operationalized in the revised version of the NEO Personality 
Inventory (NEO PI-R), a personality assessment instrument comprising 240 questions 
(Costa & McCrae, 1992). 12 The NEO PI-R is used in clinical practice, human resource 
development, industrial and organizational psychology, and vocational counseling. (For 
more on the history and development of trait adjectives, including the Big Five and 
related models, see Barbaranelli & Caprara, 2000; Benet-Martinez & Waller, 2002; 
Goldberg, 1993; Peabody & De Raad, 2002.) Although trait adjectives were used to 
develop the Big Five model, there is no universally agreed upon list of adjectives for each 
of the five factors. Various lists are published in the research literature, each supported by 
 
12 The NEO PI-R originated from a three-factor model (Neuroticism, Extraversion, and Openness) and 
assigns different labels to two of Goldberg’s (1993) factors. Goldberg: Emotional Stability => NEO: 
Neuroticism; Goldberg: Intellect => NEO: Openness.
28
empirical studies. An example that represents the Big Five using only 100 trait adjectives 
(Goldberg, 1992, pp. 34-35) is listed in Table 3.1. 13 
Table 3.1: Big Five Factors Derived from 100 Trait Adjectives (Goldberg, 1992) 
 
Another example is Peabody and De Raad’s (2002) meta-analysis that lists 795 
trait adjectives in seven major categories: Extraversion, Agreeableness, 
Conscientiousness, Emotional Stability, Intellect, Integrity-Values, and Conventionalness 
(see Appendix 3.1, Table 1 for sample list). Except for an unpublished study (Rittman, 
Wacholder, & Kantor, 2005) that found a significant correlation (r =-.185, p<.01) 
between a small set of these adjectives (Integrity-Values, n=47, see Appendix 3.1, Table 
4 for complete list) and subjectivity in documents, no one has tested whether trait 
 
13 Other five-factor lists are published in Goldberg (1990) and Goldberg and Somer (2000). McCrae and 
Costa, Jr. (1993) classified 300 adjectives based on work by Gough and Heilbrun (1983). Early lists that 
helped to develop the Big Five include Norman’s (1967) list of 2,800 trait adjectives, and Peabody’s (1967) 
analysis of 90 words. Other psychologists developed lists based on three to seven factors (see Benet-
Martinez & Waller, 2002; Peabody & De Raad, 2002; Saucier, 1997). 
Big Five Factors Trait Adjectives 
Factor I: 
Extraversion 
(Surgency 
active, assertive, bold, daring, energetic, extraverted, talkative, 
unrestrained, verbal, vigorous, -bashful, -inhibited, -introverted, -quiet,  
-reserved, -shy, -timid, -unadventurous, -untalkative, -withdrawn 
Factor II: 
Agreeableness 
agreeable, considerate, cooperative, generous, helpful, kind, pleasant, 
sympathetic, trustful, warm, -cold, -demanding, -distrustful, -harsh, -rude, 
-selfish, -uncharitable, -uncooperative, -unkind, -unsympathetic 
Factor III: 
Conscientiousness 
careful, conscientious, efficient, neat, organized, practical, prompt, 
steady, systematic, thorough, -careless, -disorganized, -haphazard,  
-impractical, -inconsistent, -inefficient, -negligent, -sloppy,  
-undependable, -unsystematic 
Factor IV: 
Emotional 
Stability 
imperturbable, relaxed, undemanding, unemotional, unenvious, 
unexcitable, -anxious, -emotional, -envious, -fearful, -fretful,  
-high-strung, -insecure, -irritable, -jealous, -moody, -nervous,  
-self-pitying, -temperamental, -touchy 
Factor V: Intellect 
artistic, bright, complex, creative, deep, imaginative, innovative, 
intellectual, introspective, philosophical, -imperceptive, -shallow,  
-simple, -uncreative, -unimaginative, -uninquisitive, -unintellectual,  
-unintelligent, -unreflective, -unsophisticated 
Negative symbol (“-”) indicates negative correlation with a factor 
29
adjectives are useful discriminators for classifying documents automatically by genre or 
other non-topical properties.14 
3.5 Summary of Adjectives Research 
Adjectives function as modifiers for specification, gradation and quantification of 
a sort that is consistent with opinion, especially when classified into subtypes such as 
dynamic and stative. Since adjectives frequently perform some degree of evaluation, we 
expect to find lots of them in subjective documents and fewer in more objective 
documents. This characteristic of adjectives makes it possible to distinguish fact and 
opinion. Based on these observations, computational linguists study the use of adjectives 
in real documents and particularly in relationship to types of documents that are 
characterized by subjectivity or the absence thereof. We propose that there is a 
relationship between a subjective/objective property in documents and genre.  
For instance, we expect to see fewer trait adjectives occur in the kinds of writing 
that constrain evaluation, but we expect to see more occur in documents that require 
evaluation, regardless of topic. This is supported in psychological research that records a 
long research tradition in personality assessment using the descriptive and evaluative 
characteristics of adjectives. However, with the exception of Heinström (2002), who 
classified searcher behavior using the Big Five personality dimensions, no one has 
systematically studied whether trait adjectives are useful discriminators for classification 
problems in information science. Thus, we will investigate the effectiveness of different 
classes of adjectives for the specific problem of automatic genre classification. 
 
14 The role of the Big Five personality traits in differentiating user search behavior has been studied by 
Heinström (2002). 
30
3.6 Variation of Adverbs in Text 
In his factor analysis of English speech and writing, Biber (1988, p. 110) observed 
that adverbs play an especially prominent role in a dimension of language he named 
Explicit versus Situation-Dependent Reference. (Here, Biber uses dimension to 
distinguish language types, not as psychologists use dimension to distinguish personality 
types.) Texts characterized as Explicit Reference, such as official documents, professional 
letters, and academic prose, tend to have few occurrences of time and place adverbials 
(e.g., early in the morning, on the table), and adverbs in general. This style of language 
requires highly explicit, text-internal references (e.g., see above, or discussed later). On 
the other hand, texts that are characterized as Situation-Dependent Reference, such as 
broadcasts and telephone conversations, tend to have many occurrences of time and 
place adverbials (e.g., just below us here, or last night). These are text-external references 
because they “permit extensive reference to the physical and temporal situation of 
discourse” (p. 144). These differences are one example of how adverbs can discriminate 
among documents by genre. 
Another difference is the role adverbs play as indicators of subjectivity. Rittman 
et al. (2004) reported that, as a class, the frequency of occurrence in documents of 
adverbs in general is negatively correlated with human judgments of objectivity (adverb 
tokens, normalized, r=.207, p<.01). (Tokens will be explained below in Chapter 5.) This 
negative correlation with objectivity is interpreted as a positive correlation with 
subjectivity in documents.  
 
31
3.7 Adverbs as a Grammatical Class  
Adverbs can modify verbs, adjectives, other adverbs, and noun phrases. 
According to Quirk et al. (1985), adverbs can be classified according to three major 
classes: two closed classes (simple and compound), and one open class (derivational). 
Many simple adverbs (such as back, down, near, out, and under) denote position and 
direction. Compound adverbs include somehow, somewhere, and therefore. Derivational 
adverbs are usually, but not invariably, derived from adjectives by adding the suffix –ly 
(odd => oddly, interesting => interestingly). Our research is most concerned with the 
broad class of derivational adverbs. 
Adverbs also serve four grammatical functions. They can occur as adjuncts, 
subjuncts, disjuncts, and conjuncts (Quirk et al., 1985) (Table 3.2). (Our research does 
not concern conjuncts.) Adjuncts and subjuncts are integrated within the structure of the 
clause. Disjuncts are peripheral to the sentence and “express an evaluation of what is 
being said either with respect to the form of the communication or to its meaning. We 
identify disjuncts with the speaker’s authority for, or comment on, the accompanying 
clause” (p. 440). Disjuncts like frankly (1.3) 15 (Table 3.2) focus on the speaker’s 
behavior, or the speaker’s choice of words in making an utterance. As Mittwoch (1977, p. 
183) explains, it is a way to “refer to one’s own words.” Disjuncts are also referred to as 
speaker-oriented adverbs (discussed below).  
Table 3.2: Three Grammatical Functions of Adverbs 
 
Reference
Code 
Example Grammatical 
Function 
(1.1) Slowly they walked back home Adjunct 
(1.2) Would you kindly wait for me? Subjunct 
(1.3) Frankly, I’m tired Disjunct 
15 In this chapter, examples are referenced sequentially using the convention: (1.1), (1.2), (1.3), and so on. 
32
In contrast to examples (1.1) and (1.2) in Table 3.2, in which slowly and kindly 
focus internally on the grammatical subject or verb phrase (i.e., they walked slowly, you 
wait kindly), frankly (1.3) focuses externally on the speaker (narrator) of the entire 
sentence, I am tired. Frankly is the speaker’s statement about his or her statement, I am
tired.
3.8 Lexical Semantics of Adverbs  
Linguistic research (Jackendoff, 1972; Ernst, 2002) indicates that (a) adverbs can 
refer to the speaker (narrator), the grammatical subject, or the manner in which an event 
occurs, (b) sentence position of adverbs affects meaning, (c) adverbs can occur in some 
positions and not in others, and that (d) adverb phrases can frequently be paraphrased 
using corresponding adjective phrases.  
3.8.1 Speaker-Oriented, Subject-Oriented, and Manner Adverbs 
Speaker-oriented adverbs refer to the speaker of the sentence, subject-oriented 
adverbs refer to the grammatical subject of the sentence, and manner adverbs refer to the 
main verb of the sentence. Speaker-oriented adverbs and subject-oriented adverbs modify 
sentences; manner adverbs modify verb phrases within sentences. Table 3.3 summarizes 
sentence position by adverb class. Our research focuses on speaker-oriented adverbs 
because they can be extracted easily from documents using a simple sentence-initial 
pattern (see Chapter 5). 
 
33
Table 3.3: Sentence Position by Adverb Class (Jackendoff, 1972) 
 
Adverb Class Sentence Position 
Initial Auxiliary Final 
Speaker-Oriented x x  
Subject-Oriented x x  
Manner, Degree, Time  x x 
We can recognize speaker-oriented adverbs by considering the adjectival 
paraphrase, and the position of the word in the phrase. Speaker-oriented adverbs, such as 
evidently (2.2), often have an adjectival paraphrase in which the adjective refers to the 
speaker (or narrator) of the sentence (2.1) (Jackendoff, 1972, p. 69). Speaker-oriented 
adverbs can take the sentence-initial position (2.2), or the auxiliary position (2.3 or 2.4), 
but not the final position (2.5). The final position is restricted to adverbs of manner 
(degree and time) (discussed below). A speaker-oriented adverb in the final position (2.5) 
will be ungrammatical unless the adverb is preceded by a pause and an accompanying 
pitch change.  
(2.1) It is evident (to me) that Frank is avoiding us. 
(2.2) Evidently, Frank is avoiding us. 
(2.3) Frank, evidently, is avoiding us.  
(2.4) Frank is evidently avoiding us. 
(2.5) * Frank is avoiding us evidently. 16 
In contrast, subject-oriented adverbs (3.2) have an adjectival paraphrase that 
refers to the grammatical subject (3.1) (Jackendoff, 1972, p. 70). Subject-oriented 
adverbs can occur in the sentence initial position (3.3), or the auxiliary position (3.2), but 
not in the final position (3.4). A subject-oriented-adverb in the final position is 
ungrammatical (3.4), and must take the manner reading in which the adverb modifies the 
 
16 The notation “*” indicates an ill-formed (ungrammatical) phrase. 
34
verb phrase. In this case, (3.4) must represent the manner reading in which John spilled 
the beans in a careful manner. 
(3.1) John was careful to spill the beans. 
(3.2) John carefully spilled the beans. 
(3.3) Carefully, John spilled the beans. 
(3.4) * John spilled the beans carefully.
Adverbs of manner (degree and time) (4.2) can be paraphrased as in (4.1) in 
which eloquent describes the manner that Dave speaks. Manner (degree and time) 
adverbs can take the final position (4.2), the auxiliary position (4.3), but not the initial 
position (4.4). 
(4.1) The manner in which Dave speaks is eloquent. 
(4.2) Dave speaks eloquently. 
(4.3) Dave eloquently speaks. 
(4.4) * Eloquently Dave speaks. 
3.8.2 Speaker- and Subject-Oriented Adverb Sub-Classes  
Ernst (2002) classified speaker-oriented adverbs into three semantic sub-classes: 
speech-act, evaluative, and epistemic (Table 3.4). According to Ernst, speaker-oriented 
adverbs are distinguished from subject-oriented adverbs by the fact that, in their clausal 
reading, speaker-oriented adverbs “do not make any reference to the subject” of the verb 
(Ernst, p. 69; see also Jackendoff, 1972). This difference is evident in the paraphrase of 
each speaker-oriented subclass in Table 3.5. In each case, the paraphrase does not 
reference the subject of the verb.  
 
35
Table 3.4: Subclasses of Adverbs 
 
Table 3.5: Paraphrase of Speaker-Oriented Adverbs 
 
Speech-act speaker-oriented adverbs (e.g., frankly, briefly, and simply) are similar 
to agent-oriented (subject-oriented) adverbs in that they are event-taking adverbs, except 
that they select also for events of expressing. Thus, they refer to the entire event (or 
proposition) (Ernst, 2002). The distinctiveness of speech-act adverbs is not absolute, as in 
other classes, since most of the members fit semantically into other classes (e.g., honestly 
can also function as an agent-oriented subject-oriented adverb) (Ernst, p. 70). However, 
Adverb 
Class 
Subclass Examples 
Speaker-
Oriented 
 
Speech-Act ** briefly*, candidly, confidently, frankly , generally*, honestly*, 
roughly*, seriously, simply, specifically  
Evaluative amazingly, curiously,  ideally, luckily, normally, oddly, 
predictably, preferably, unfortunately, strangely, surprisingly 
Epistemic: 
Modal 
certainly, definitely, maybe, necessarily, possibly, probably, surely 
Epistemic: 
Evidential 
clearly, obviously 
Subject-
Oriented 
 
Agent-Oriented cleverly, foolishly, intelligently, ostentatiously, rudely, secretly, 
stupidly,  tactfully, wisely  
Mental-Attitude absent-mindedly, anxiously, attentively, calmly, eagerly, 
frantically, gladly, intentionally, obstinately, reluctantly, sadly, 
vigilantly, willingly 
Pure 
Manner 
 loudly, tightly, woodenly 
* Not exclusive membership (May fit other classes) 
** See also Mittwoch (1977) 
Speaker-
Oriented 
Adverb  
Subclass 
Example Paraphrase 
Speech-
Act 
Honestly, who would do such a 
thing? 
Tell me honestly, who would do such a 
thing? 
Epistemic 
The markets will perhaps 
respond to lower interest rates. 
The proposition that the markets will 
respond to lower interest rates may be true. 
Evaluative 
Unbelievably, she decided to buy 
a camel. 
The fact that she decided to buy a camel is 
unbelievable. 
36
speech-act adverbs are unified in that class members have clausal readings of 
communication (the speaker’s act of communication).  
Epistemic speaker-oriented adverbs, in their clausal reading, refer to the truth of 
the proposition represented by the rest of the sentence. There are two groups (a) modals 
(5.1) and evidentials (5.3 and 5.4). Modals indicate the degree of the speaker’s certitude 
about the truth value of the proposition. Modals do not have manner readings because 
semantically, they can only select propositions. Evidentials express the manner of 
perceiving something. Evidentials can have manner readings because, semantically, one 
can perceive other things (5.4) besides the truth (5.3) of something. 
(5.1) Sam has probably made an appointment.  
(5.2) * Sam has made an appointment probably.  
(5.3) Clearly, they saw the sign. 
(5.4) They saw the sign clearly. 
Evaluative speaker-oriented adverbs represent the speaker’s evaluation of some 
state of affairs according to how good it is (luckily, unfortunately), its desirability 
(ideally, preferably), or how normal or abnormal it is (normally, strangely, curiously, 
surprisingly) (Ernst, 2002, p. 76). They often take prepositional phrase complements 
because they consider the effect on something.  
Additional examples of speaker-oriented adverbs (not cited by Ernst (2202) are 
listed in Chapter 6 (Table 6.5). We also include possible syntactic variations of speaker-
oriented adverbs other than the sentence-initial position (Table 6.6). 
Ernst (2002) classified subject-oriented adverbs into two subclasses: (a) agent-
oriented adverbs and (b) mental attitude adverbs (Table 3.4). Agent-oriented adverbs 
37
refer to the grammatical subject in light of the event. Thus, in the clausal reading in (6.1), 
foolishly is used according to its normative reading. As such, the senator is assessed as 
being foolish for having talked to reporters. The senator may have talked in a wise 
manner, and may be a wise person, but the senator is regarded as foolish for the event of 
talking to reporters. In contrast, (6.2) evaluates the senator as having talked to reporters in 
a foolish manner. 
(6.1) Foolishly, the senator has been talking to reporters. 
(6.2) The senator has been talking foolishly to reporters. 
Mental-attitude adverbs are subject-oriented adverbs that “describe, most 
fundamentally, a state of mind experienced by the referent of the subject of the verb” 
(Ernst, 2002, p. 63). Mental-attitude adverbs can be divided into two groups: (a) state
(7.1 and 7.2) and (b) intentional (8.1 and 8.2). Although these subclasses can take either 
reading, one is usually more salient than the other. The subclasses can also have a clausal 
reading (7.1, 8.1) and a manner reading (7.2, 8.2) (Ernst, 2002, p. 63).  
State mental-attitude adverbs carry the meaning that the subject experiences a 
certain mental state during an event (7.1). In the manner reading in (7.2), the comparison 
is restricted to the manner in which she left the room. Here her manner, not her mental 
state, is described. She could have been quite agitated, but left in a calm manner. In (8.1) 
the intentional mental-attitude adverb, willingly, has the meaning that the sailors’ 
intention was such that they entered into the event of singing with a favorable attitude. In 
the manner reading (8.2), the sailors sang in a willing manner. 
(7.1) She calmly had left the room. 
(7.2) She had left the room calmly. 
38
(8.1) Willingly, the sailors sang a few of the chanteys. 
(8.2) The sailors sang a few of the chanteys willingly (and a few others 
begrudgingly). 
3.9 Summary of Adverbs Research  
Drawing on the theoretical frameworks constructed by Jackendoff (1972) and 
Ernst (2002), our review shows that speaker-oriented adverbs indicate the narrator’s 
perspective in text, and subject-oriented adverbs indicate the perspective of the 
grammatical subject. Although both types can occur in the same sentence position, the 
difference depends on the narrator’s intent and situation. For this reason, we focus on 
speaker-oriented adverbs because of the possible parallel with genre. That is, speaker-
oriented adverbs indicate the narrator’s perspective on what is being said, and genre is 
indicative the author’s purpose, intended audience, and type of activity.  
3.10 Summary of Adjectives and Adverbs Research 
We exploit the observations of prior research about the characteristics of 
adjectives and adverbs to classify documents automatically by genre. Specifically, we 
focus on speaker-oriented adverbs (Ernst, 2002) and trait adjectives derived by Peabody 
and De Raad (2002). We also investigate the usefulness of subjective adjectives (Wiebe, 
2000b), ly-adverbs (derivational adverbs), adverbs derived from trait adjectives, and 
adjectives and adverbs in general.   
Next, we discuss our research problem, hypotheses, assumptions, and potential 
significance of our research in Chapter 4. 
 
39
Chapter 4: Research Problem  
We study three written genres (academic, news, and fiction) to explore the 
relationship between the variation of adjective and adverb frequencies in documents and 
non-topical properties of documents, specifically human-assigned genre distinctions. The 
proximate practical goal is of this research is to assist information seekers to access 
relevant information in a specific genre. The ultimate goal is to understand whether this 
principle of looking to other disciplines is valid and extensible. 
We address the following questions: 
• To what extent can adjectives and adverbs reliably distinguish one genre 
from another? 
• How does discrimination using adjectives and adverbs perform relative to 
discrimination based on other features? 
• How many features are necessary? 
• What is the best way to represent features (e.g., vector vs. count) 
4.1 Exploratory Hypotheses   
We propose two exploratory hypotheses 17:
• H1 Representing documents by counting the frequency of occurrence of 
adjectives and adverbs can reliably classify documents automatically by 
genre. 
 
17 Since this research developed over time as an exploration into role of adjectives and adverbs as 
discriminating features for automatic genre classification, the hypotheses are presented as exploratory, not 
confirmatory. 
40
• H2 Representing documents as vectors of the occurrence of a relatively 
few adjectives and adverbs is more effective than representing documents 
by counting the frequency of occurrence of many words by class.  
4.2 Evidence Leading to the Hypotheses   
We base our exploratory hypotheses on the evidence that (a) texts vary internally 
according to production circumstances and communicative functions, (b) these internal 
variations are detectable by measuring the occurrence of textual features (e.g., frequency 
of parts of speech, such as adjectives and adverbs), and (c) these internal variations are 
reflective of external differences that are useful in classifying documents by genre. 
4.3 Potential Significance 
One direction of current research in information science is concerned with 
developing automatic measures of non-topical document properties. In light of the 
growing scope and size of information resources, the general goal is to assist information 
seekers to sort through ever increasing volumes of relevant information by classifying 
documents according to non-topical requirements, such as genre.  
One specific application of this research will assist information analysts in 
different contexts. For example, an information analysts may want to access academic 
information (regarded as analytical), versus news reports (regarded as fact-gathering), 
versus editorials (regarded as opinionated) before concluding a report on a relevant topic.  
More generally, our research could improve information sources for college or 
high school students (even general users) who need relevant information in a particular 
genre, but do not have the opportunity to read entire documents. This is particularly 
41
important as we move into an information environment where only "snippets" of 
documents are placed on-line. It will be valuable to provide additional ways for people to 
decide whether they want to read a particular document when they cannot read more than 
a few sentences of it from the search engine. Our research contributes to this goal by 
investigating the effectiveness of adjectives and adjectives as discriminators of 
documents by genre. 
Our study also has broader applications in part due to the principle of selecting 
features from other disciplines that can be generalized to other classification problems (in 
contrast to selecting ad hoc features). Other applications include automatic (a) author 
identification, (b) detection of subjectivity versus objectivity in text, (c) classification of 
texts for question-answering (QA), (d) detection of customer review opinions within 
business environments, (e) detection of personality traits as exhibited within text, and (f) 
detection of people who might be susceptible to certain beliefs. These problems can also 
be applied to other corpora, such as Weblogs (blogs), email logs, and chat room logs. 
In the next chapters, we present our methodology, experimental design, and 
evaluation measures (Chapter 5), and data preparation (Chapter 6). 
 
42
Chapter 5: Methodology, Experimental Design, and 
Evaluation Measures  
In this chapter, we describe how we test different methods for classifying 
documents automatically by genre. We process documents in the corpus using predefined 
genre categories, a set of Perl scripts, and other automatic methods described below. Data 
is saved in a table and analyzed using SPSS 11.0. A separate classifier is learned for each 
test. 
We select features a priori based on knowledge about adjectives and adverbs as 
described in Chapter 3. We consult disciplines that have studied these words, specifically 
linguistics and psychology. Linguists understand the formal interplay between syntax and 
semantics, and how meaning can vary by word choice and sentence position. Personality 
researchers in psychology claim that a correlation exists between the evaluative and 
descriptive characteristics of trait adjectives and how people use these words to describe 
themselves and others. Our method of feature selection differs from previous genre 
classification research precisely in that we pre-select features from a narrow range of 
grammatical classes (adjectives and adverbs) based on insights from other disciplines 
(linguistics and psychology).  
In Section 5.1 we describe the corpus, and explain how it is parsed. Section 5.2 
describes the genre categories we select for this study. Section 5.3 describes our features, 
representations of features, and how we use this information about particular word 
classes as variables in our classification experiments (e.g., type vs. token, count vs. 
vector, normalized vs. non-normalized, and sentence position). Section 5.4 describes our 
43
experimental design and machine learning method, and Section 5.5 describes our 
evaluation measures. 
5.1 Corpus 
We use the British National Corpus, World Edition (BNC2). The BNC2 is widely 
used by research communities in corpus linguistics, computational linguistics, natural 
language processing, and stylistics. The BNC2 is designed to represent the range of 
British English speech and writing. It includes 4,053 18 documents, and nearly 100 
million words. BNC2 is encoded using ISO standard 8879 SGML (Standard Generalized 
Markup Language) to represent the structural properties of the documents (e.g., parts of 
speech, headings, paragraphs, and lists). 
BNC2 uses CLAWS4 to tag documents (BNC2 POS-Tagging Manual, 2000; 
Leech, n.d.; Leech et al., 1994; Garside & Smith, 1997) 19 CLAWS4 distinguishes 61 
linguistic features, including (a) parts of speech (POS), for example, adjectives (good,
old, and beautiful), adverbs (often, well, and longer), common nouns (aircraft, data, and 
committee), and coordinating conjunctions (and, or, and but), and (b) character-level 
features (e.g., punctuation and alphabetical symbols) (see Appendix 4.1). 
CLAWS4 is a hybrid tagger, employing a mixture of probabilistic and non-
probabilistic techniques. It assigns a tag (sometimes two tags) to each word (each word = 
one token) 20 as a result of four main processes: (a) tokenization, (b) initial assignment of 
tags, (c) tag selection (or disambiguation), and (d) idiomtagging.  
 
18 BNC2 includes 4,054 documents. One document has been excluded because it is a duplicate, as per Lee 
(2001). 
19 The error rate of CLAWS4 averages around 3% (BNC2 POS-Tagging Manual, 2000). 
20 We explain our use of the term token in Section 5.3. 
44
Tokenization is the first stage of CLAWS4 (BNC2 POS-Tagging Manual, 2000). 
Tokenization divides the text into individual word tokens and orthographic sentences. 
These are the segments usually demarcated by spaces and sentence boundaries. 
Initial assignment of tags is the second stage. This process assigns tags to each 
token. Many tokens are unambiguous, and so they will be marked with one tag (e.g., 
various = adjective). Other tokens are ambiguous, taking from two to seven potential 
tags. For example, the token paint can be tagged as a noun or as a verb.  
Tag selection (or disambiguation) is the third stage of CLAWS4 (BNC2 POS-
Tagging Manual, 2000). This process chooses the most probable tag from any set of tags 
associated with a token. This probabilistic procedure makes use of the context in which a 
word occurs. A single winning tag is selected for each word token. However, the winning 
tag is not necessarily the right answer. This is further refined by idiomtagging.
Idiomtagging is a matching procedure which operates on lists of rules that can be 
defined loosely as idioms. Two examples are: (a) a list of phrases (e.g., because of, so 
long as, and of course), and (b) a list of place name expressions (e.g., Mount X, where X
begins with a capital).  
5.2 Genre Categories  
BNC2 documents are classified into two high-level categories: written documents 
(n=3,192) and spoken documents (n=861) (Lee, 2003a; n.d.) (See Appendix 5.1). The 
distinction between written and spoken is based on the BNC2 concept of mode (Lee, 
2001; 2003b). Mode refers to aspects of communication delivery (or medium). We 
believe that mode can be included as an aspect of genre, that is, a document intended for 
a specific audience, purpose, and activity type (Lee, 2001). However, we believe the 
45
distinction between written and spoken documents (i.e., an academic lecture vs. an 
academic paper) is not as interesting as the distinction as to whether a document is 
academic prose versus news writing, for instance. Thus, we focus this study on written 
documents only.  
We use Lee’s (2003a, n.d.) classification of BNC2 documents into 70 genres (46 
written genres, and 24 spoken genres), and Davies’ (n.d.) conflation of the 70 genres into 
six superordinate genres (five written genres) academic, fiction, news, non-fiction, other,
and one spoken genre (see Appendix 5.1). For the 46 written genres of Lee, we adopt 
three of Davies’ five superordinate genres, namely, academic, news, and fiction. Based 
on the definition of genre by Lee (2001), we believe these three superordinate categories 
are necessarily exclusive (despite sub-genre distinctions), as opposed to other, which is 
diverse, and non-fiction, which may not be very distinct from academic or news.
In the next section, we describe the information we use about features, 
representations of features, and how we use this information about particular word 
classes as variables in our classification experiments (e.g., count vs. vector, normalized 
vs. non-normalized, and sentence position). 
5.3 Discriminating Features  
We represent features in numerous ways (see Chapter 6 for detailed discussion of 
feature selection). First, we represent features as (a) word types and (b) word tokens. 
Type represents the occurrence of a unique word in a document; token represents the 
frequency of occurrence of a unique word in a document. This distinction gives rise to 
two additional representations based on count and vector.
46
5.3.1 Count versus Vector 
Using the standard BNC2 documents tagged by CLAWS4 (BNC2 POS-Tagging 
Manual, 2000; Leech, n.d.; Leech et al., 1994; Garside & Smith, 1997), we represent our 
discriminating features as a count and as a vector. We further represent these features as 
normalized or non-normalized, and we also indicate sentence position. These 
representations are described below. 
5.3.1.1 Word Count (Type and Token) 
For each document, we count the frequency of occurrence of adjectives and 
adverbs from various classes. 21 For example, suppose that word_1, word_2 and word_3 
all belong to the target class under discussion (in this case, document_1).  And suppose 
that word_1 occurs three times, word_2 does not occur, and word_3 occurs 7 times. The 
count of types is 1+0+1=2. The count of tokens is 3+0+7=10 (Figure 5.1). 
 
Class Word ID Type Token 
Document_1 word_1 1 3 
word_2 0 0 
word_3 1 7 
Class Count  2 10 
Figure 5.1: Count of Types and Tokens for Words Belonging to a Given Class 
 
5.3.1.2 Word Vector (Type and Token) 
We also represent documents as vectors of the frequency of occurrence of distinct 
words. In some cases, we also mark the sentence position of the word, such as sentence-
initial position as distinguished from any sentence position. The vector approach also 
 
21 To compare the discriminating power of adjectives and adverbs with other features, we also count the 
frequency of occurrence of nouns, verbs, and punctuation. 
47
distinguishes types and tokens. For example, if word_1 occurs in document_1, then 
type=1; else type=0. If type=1, then token=frequency of the word in the document; else 
token=0). Thus, in the example of the occurrence of the three words in document_1 
(Figure 5.1), the vector for document_1 is (1, 3, 0, 0, 1, 7) (Figure 5.2).  
 
Document ID Word ID 
Document_1 Word_1 Word_2 Word_3 
Type Token Type Token Type Token 
Vector Document_1 1 3 0 0 1 7
Figure 5.2: Vector of Types and Tokens 
 
5.3.1.3 Count versus Vector in IR Parlance 
In contrast to IR’s typical bag-of-words approach, representing documents as 
vectors of the frequency of occurrence of smaller sets of pre-determined adjectives and 
adverbs can be thought of as the selected-bag-of-words (or mini-bag-of-words) approach. 
22 Geometrically, it is the projection of the full bag-of-words vector onto the axes 
belonging to adjectives and adverbs. A benefit of this approach, like bag-of-words, is the 
low cost of counting words 23 (as compared to counting tagged words, which requires 
part-of-speech software). A variation of the selected-bag-of-words approach preserves 
sentence position. We call this variation the position-aware-bag-of-words approach.  
The count approach can be thought of as “weighing the bag of words.” This 
method gives credit to each word which is a member of the pre-determined class. A 
limitation of this approach is that information about the contribution of individual words 
is not utilized. For example, if dishonest, faithful, hypocritical, and righteous are four 
members of a larger set of 100 adjectives that are more likely to occur in some documents 
 
22 Alternatively, the count methods are representations by “bags-of-bags-of-words”, with all the words of a 
given class lumped into a single bag. 
23 This assumes that the list will not expand in the future. 
48
than in others, the count approach merely counts the occurrence of the 100 words in each 
document. We would not know which if any of these four unique adjectives occurs in a 
given document. In contrast, the vector approach tells us which documents contain these 
four unique words. 
Finally, each feature, whether count or vector, is represented by normalized and 
non-normalized variables. The normalized version is calculated by dividing the feature 
frequency by the total count of words in the document.  
5.4 Experimental Design 
5.4.1 Methods, Problems and Models 
The table in Appendix 5.2 shows how we organize our classification experiments. 
In the far right column, we label our classification tasks as a problem. Since we are 
discriminating among three genres (academic, fiction, and news) in two-way 
experiments, we are studying a total of six problems (academic vs. fiction, academic vs. 
news, fiction vs. news, academic vs. not-academic, fiction vs. not-fiction, and news vs. 
not-news). The first three problems represent a one-against-one classification problem; 
the last three problems represent one-against-many classification problems. For each 
problem, we test the performance of various sets of features. In the first column, we label 
each unique set of features to be tested as a method. There are 54 unique methods. Each 
method includes a unique set of features, such as speaker-oriented adverbs, and different 
ways of representing the features, such as sentence-initial position, vector or count, or as 
normalized or non-normalized. The intersection of a method and a problem represents 
49
what we call a model. 24 Since there are 54 methods for six problems, we analyze a total 
of 324 models (representing 324 tests). This structure is illustrated in the table of our 
results in Appendix 7.1.  
For each experiment, 50% of cases are randomly selected for training data, and 
the remaining 50% of cases are used as testing data. Since we are studying six problems, 
we create six training/testing document sets (described in Chapter 6; see also Table 6.1).  
5.4.2 Learning Method: Discriminant Analysis   
We use the statistical machine learning method of discriminant analysis, 
specifically the statistical package provided by SPSS, Version 11.0. Discriminant 
analysis is a widely used classification method of multivariate statistics, and has been 
used in genre classification work by Karlgren and Cutting (1994), Ng et al. (2000), and 
Stamatatos et al. (2000).  
Discriminant analysis is a predictive multivariate statistical technique that 
analyzes differences between groups of populations (Eisenbeis & Avery, 1972). The 
theory of discriminant analysis is based on the assumptions that: 
• The groups being investigated are discrete and identifiable 
• Each observation in each group can be described by a set of 
measurements on m characteristics or variables 
• These m variables have a distinct multivariate normal distribution 
in each population. (Eisenbeis & Avery, p. 1) 25 
24 We call the intersection of a method and a problem a model. But the intersection can have two meanings: 
(a) It stands to "measuring the performance of a method against a problem" (what can be called a test), and 
(b) it stands for "the resulting set of variables selected by the discriminant analysis" (what we call a model). 
25 It is not customary in text classification to worry much about this last specification, except to hope that 
the distributions are distinguishable. 
50
Discriminant analysis calculates multivariate vectors of documents in observed 
groups (i.e., training data) and seeks to extract discriminant functions that distinguish the 
groups by finding that linear combination of the variables which makes the groups as 
statistically distinct as possible. The classifier can then make predictions about the 
membership of unseen cases (i.e., test data) (Eisenbeis & Avery, 1972; Stamatatos et al., 
2000). 
To equalize classifier performance (assuming that the sizes of the distinct groups 
are not equal), we calculate the proportionate size of each group as a parameter known as 
PRIORS. For instance, if the number of documents in group_1 is 75, and the number of 
documents in group_2 is 25; PRIORS for group_1=.75, and PRIORS for group_2=.25. 
This reality-based setting of PRIORS prevents the classifier from trying to force equal 
numbers of documents into each category (the default discriminant method setting) (see 
Section 5.5.1.) 
5.5 Evaluation Measure 
5.5.1 Classification Accuracy 
Classification accuracy is the standard measure for classification experiments 
(Witten & Frank, 2000). Classification accuracy measures the difference between the 
observed, true state, and the classifier’s prediction. The shaded cells in Table 5.3 show 
the two kinds of correct predictions: True Positive (TP) – the classifier correctly assigns a 
case to the positive class, and True Negative (TN) – the classifier correctly assigns a case 
to the negative class. The un-shaded cells show the two kinds of errors: False Positive 
(FP) – the classifier incorrectly assigns a negative case to the positive class, and False 
Negative (FN) – the classifier incorrectly assigns a positive case to the negative class. 
51
Accuracy is calculated by dividing the number of True Positive and True Negative cases 
by the total number of cases in the sample: (TP+TN)/(TP+TN+FP+FN) (Witten & Frank, 
2000). Simply put, accuracy is the fraction of correctly predicted cases. 
Table 5.3: Standard One-Model Performance Matrix 
 
Standard One-Model Performance Matrix 
Derived from Witten and Frank (2000) 
Predicted Group 
Positive Negative 
Positive True Positive (TP) False Negative (FN) 
Observed Group Negative False Positive (FP) True Negative (TN) 
Precision and Recall are related to Accuracy (Table 5.4). Precision is the fraction 
of returned cases that are correctly classified as positive: TP / (TP+FP). Recall is the 
fraction of all positive cases in the sample that are returned: TP / (TP+FN). For a binary 
distinction, these measures depend on which of the two classes is taken as the “one to be 
found”. 
Table 5.4: Standard Performance Measures 
 
Standard One-Model Performance Measures 
Measure Calculation 
Accuracy (TP+TN)/(TP+TN+FP+FN) 
Precision for Predicted Group 
“Positive” 
TP/(TP+FP) 
Recall for Predicted Group 
“Positive” 
TP/(TP+FN) 
5.5.2 Calculation of Rigorous Baseline and Accuracy Gain (AG) 
We seek to find the most rigorous performance measure. To do this, we first 
calculate the most rigorous baseline which is equal to the proportion of cases in the 
majority class. For example, if we know in advance that, of the 4,053 documents in the 
corpus, 505 are academic documents (12%) and 3,548 are not academic documents 
(88%), then we could build a hypothetical classifier which predicts that every document 
52
will belong to the majority class (not academic). As such, we would be correct 88% of 
the time. Thus, our rigorous baseline for the academic vs. not-academic problem is 88%. 
A less rigorous baseline assumes that random guessing results in a baseline of 50%. Our 
rigorous method recognizes a principle from the theory of Linear Programming, that is, 
the optimum for a linear problem with linear constraints will occur at an extreme point, 
not at some mixture.  
We use the most rigorous baseline (appropriate for each problem) to be the value 
of the PRIORS parameter for our discriminant analysis. This gives rise to a new 
performance measure which we call accuracy gain (AG). AG is derived by using the 
fraction of possible achievable improvement, over the “best guess” baseline as the single 
most informative and challenging measure of performance. AG also allows us to compare 
results across studies. 
We calculate AG by re-scaling (or normalizing) the rigorous baseline (Figure 
5.3). AG represents the improvement over the best guessing procedure (rigorous 
baseline) compared to the maximum possible improvement (100%). For instance, if a 
classification method results in accuracy of 90%, this is not very impressive compared to 
a baseline of 88%. However, if we re-scale the baseline so that 88% equals zero (the best 
possible performance achieved by always guessing the majority class), then a 
performance of 90% is equal to an accuracy gain (AG) of 17% (Figure 5.3). If classifier 
accuracy improves merely one point to 91%, AG would be 25% (compared with 17%). 
53
Figure 5.3: Calculation of Accuracy Gain (AG) 
The next chapter (Chapter 6) describes our data preparation, including the 
extraction of features from the corpus. 
 
O riginal Scale 
Best Guess <--- Accuracy  --------------------------> Perfect 
88%        90%      100% 
 
Calculation of Accuracy Gain (AG) 
(Accuracy-Best G uess)/(100-Best Guess) 
(90- 88)/(100-88) = 17%  Accuracy Gain (AG) 
Re-Scale 
Best Guess <--- AccuracyGain  --------------------> Perfect 
0%        17%      100% 
54
Chapter 6: Data Preparation 
For our experiments, we construct a table using SPSS 11.0. We begin by adopting 
three categorical variables from Lee’s BNC World Index (2003a) and Davies’ (n.d.) 
Variation in English Words and Phrases. These variables include: (a) document identifier 
(docid), (b) 70 genre categories (Lee), and (c) Davies’ six superordinate categories. In 
addition, we use the standard BNC2 count of document length (wordct) 26 reported by 
(Lee, 2003a). Of the original 4,054 documents in BNC2, we exclude one document 
(HDE) because it is marked as redundant by Lee (2003a). This results in a corpus of 
4,053 documents. See Appendix 6.2 for a complete list of variables. 
From the complete corpus (N=4,053), we randomly select 50% of the cases for 
training (n=2,029) and 50% of the cases for testing (n=2,024) using SPSS 11.0 (Table 
6.1). Based on this previous selection, we then break out six document sets according to 
the genres they belong to (academic vs. fiction, academic vs. news, fiction vs. news,
academic vs. not-academic, fiction vs. not-fiction, and news vs. not-news).  
 
26 Based on CLAWS4 (BNC2 POS-Tagging Manual, 2000) (See also Appendix 4.1).  
According to David Lee (personal communication, September 20, 2006), CLAWS4 is not a “simple-
minded” parser of word boundaries. For instance, "don't" is tagged as two words ("do" and "n't"), while "as 
well as" is tagged as one word. 
 
55
Table 6.1: Training—Testing Sets 
 
Corpus Problem Document Sets 
50% 
Training 
50% 
Testing Total PRIORS 
Full     2,029 2,024 4,053   
Acad vs. NotAcad Acad 276 229 505 0.12 
NotAcad 1,753 1,795 3,548 0.88 
Sub Total 2,029 2,024 4,053   
Fict vs. NotFict Fict 218 246 464 0.11 
NotFict 1811 1778 3589 0.89 
Sub Total 2029 2024 4053   
News vs. NotNews News 249 269 518 0.13 
NotNews 1780 1755 3,535 0.87 
Sub Total 2029 2024 4053   
Selected             
Acad vs. Fiction Acad 276 229 505 0.50 
Fict 218 246 464 0.50 
Sub Total 494 475 969   
Acad vs. News Acad 276 229 505 0.50 
News 249 269 518 0.50 
Sub Total 525 498 1023   
Fict vs. News Fict 218 246 464 0.50 
News 249 269 518 0.50 
Sub Total 467 515 982   
6.1 Extraction of Linguistic Features from Corpus 
We use the standard BNC2 documents tagged by CLAWS4 (Leech, n.d.; Leech et 
al., 1994; Garside & Smith, 1997) to count frequencies of parts of speech and other 
features, such as punctuation (see Appendix 6.1 for experimental scripts). Of the 61 
BNC2 tags, we conflate and use six major categories for our experiments: adjectives, 
adverbs+, adverbs-, nouns, punctuation, and verbs (see Appendix 4.1).  
 
56
6.1.1 Extraction of Speaker-Oriented Adverbs from Sentence-Initial Position  
We select the 30 speaker-oriented adverbs listed by Ernst (2002) (Table 6.2), and 
extract occurrences of these words in the sentence-initial position, capitalized, followed 
by a comma using the UNIX command in Figure 6.1. Examples of speaker-oriented 
adverbs in the sentence-initial position in actual BNC2 sentences are shown in Table 6.4. 
To extract this pattern, we first create a raw text version of BNC2 documents (see 
Appendix 6.1 for experimental scripts).  
 
Fig
 
The frequ
Results indicate t
sentence-initial p
position or any p
the sentence-initi
SOA per docume
unique SOAs if wgrep -r '[Word],' [directory] > output ure 6.1: UNIX Script Used to Extract Speaker-Oriented Adverbs 
Table 6.2: Thirty Speaker-Oriented Adverbs 
 
30 Speaker-Oriented Adverbs 
Derived from Ernst (2002) 
amazingly briefly candidly certainly clearly 
confidently curiously definitely frankly generally 
honestly ideally luckily maybe necessarily 
normally obviously oddly possibly predictably 
preferably probably roughly seriously simply 
specifically strangely surely surprisingly unfortunately 
ency of the 30 speaker-oriented adverbs (SOA) is reported in Table 6.3. 
hat there are relatively few cases where the 30 SOAs occur in the 
osition in BNC2, compared to occurrences in the not sentence-initial 
osition. For example, the average (mean) occurrence of the 30 SOAs in 
al position in a document is 1.07 types. In other words, about one unique 
nt occurs in the sentence-initial position in the BNC2, compared to 10 
e do not discriminate by sentence position.   
 
57
Table 6.3: Aggregated Frequency of Thirty Speaker-Oriented Adverbs in BNC2  
 30 Speaker-Oriented Adverbs 
Sentence Position 
Sentence-Initial Not Sentence-Initial Any Position 
Types Tokens Types Tokens Types Tokens 
Mean 1.07 1.60 9.05 40.24 10.12 41.84 
Median 0 0 9 28 10 29 
Sum 4,326 6,498 36,692 163,077 41,018 169,575 
Total 
Documents 4,053 4,053 4,053 4,053 4,053 4,053 
Table 6.4: Examples of Speaker-Oriented Adverbs in Sentence-Initial Position in BNC2 
 
Examples Superordinate 
Genre 
Genre 
Briefly, the mass media in Britain form a part of an 
international media scene with multinationals in 
charge of broadcasting and newspaper interests. 
Academic Academic: Politics, 
Law, Education 
Clearly, the banks have problems. News News: Report 
Candidly, I have my doubts.  Fiction Fiction: Prose 
Frankly, it won't break my heart if we don't see them 
at all. 
Fiction Fiction: Prose 
Generally, then, Marx's ideas seem to many people 
to have been “disproved” by twentieth-century 
developments. 
Academic Academic: Social 
Science 
Honestly, the house is fine as it is. Fiction Fiction: Prose 
Surprisingly, LA has only about half the police force 
of London for a city of ten million. 
News News: Other, 
Social 
We recognize that the list of 30 speaker-oriented adverbs (SOAs) derived from 
Ernst (2002) (Table 6.2) is not exclusive. However, we use it because we seek to exploit 
lists of words published by other researchers without adding to or altering these lists. In 
the future, we propose to advance our work by expanding on these lists (see Chapter 8). 
As such, we recognize that there are potentially many other SOAs, such as the 21 
examples listed in Table 6.5. 
 
58
Table 6.5: Twenty-One Speaker-Oriented Adverbs not Listed by Ernst (2002) 
 
Speaker-Oriented Adverbs 
actually apparently approximately 
basically bluntly broadly 
confidentially essentially evidently 
fortunately hopefully incidentally 
literally metaphorically paradoxically 
perhaps personally presumably 
truly truthfully undoubtedly 
We also recognize that SOAs can occur in more complex syntactic patterns, such as those 
listed in Table 6.6. 
Table 6.6: Possible Alternative Patterns of Speaker-Oriented Adverbs 
 
Sentence-Initial Position 
Certainly, I will help you.  
Possible Alternative Patterns 
I will certainly help you.  
I told you that, certainly, I will help you.  
But certainly, I will help you.  
That is certainly true (that I will help you).  
I am certain that I will help you. (Adjectival Paraphrase) 
In addition, we find that there is an indeterminate number of false positive cases 
using automatic extraction for the 30 SOA words in the sentence-initial position that 
necessarily results, as in the actual example in Figure 6.2. In this example, we see that our 
script “correctly” extracted seriously as a speaker-oriented adverb in the sentence-initial 
position because the rule specified that the word should be capitalized and followed by a 
comma. However, this instance is clearly an error because the word is used as a manner 
adverb and as the final word of a title.  
 
Figure 6.2: Error Analysis: Extraction of Speaker-Oriented Adverbs 
 
DocID=G2P 
 
From GARROW, J.S. (1981) Treat Obesity Seriously, Edinburgh: Churchill Livingstone. 
59
6.1.2 Extraction of Trait adjectives, Trait Adverbs, and Subjective Adjectives  
Using the 4,053 tagged documents in BNC2, we extract the following features 
using the chained process outlined below (see Appendix 1.2 for feature definitions) 
• 9,324 unique adverbs (Adverbs+)
• 9,222 adverbs (Adverbs-) if we exclude wh-adverbs (e.g., when, where, 
how, why) and adverb particles (e.g., about, along, back, down, in, on, up)
o 6,372 ly-Adverbs by selecting all adverbs ending in ly 
 A random selection of 30 ly-adverbs (Random ly-
Adverbs) (to compare performance to the 30 speaker-
oriented adverbs) 
• 147,038 unique Adjectives 
o 1,322 adjectives described by Wiebe (2000a; 2000b) as Subjective 
Adjectives 
o 732 Trait Adjectives (Peabody & De Raad, 2002) 27 
 A subset of 44 Trait Adjectives (described by Peabody & 
De Raad as indicative of Integrity-Values)
 539 Trait Adverbs derived by adding the suffix ly to the 
list of 732 trait adjectives (discarding ill-formed words such 
as balanced  => balancedly, cultured  => culturedly, 
cowardly => cowardlyly, and friendly  => friendlyly) 
 
27 To extract trait adjectives, we match the set of words listed by Peabody and DeRaad (2002) with 
adjectives tagged by CLAWS4 as adjectives to preclude selecting words that may belong to different 
grammatical class (e.g., lying as an adjective vs. lying a verb).  
60
 A subset of 36 well-formed Trait Adverbs by adding the 
suffix ly to the subset of 44 Trait Adjectives 
• 430,415 unique nouns 
• 60,083 unique verbs 
• 71 types of punctuation 
Table 6.7 reports the frequency of occurrence of these features (see Appendix 3.1 for 
samples of words in selected classes).  
 
Table 6.7: Word Classes Selected for Experiments 
 
Word Class # Words (Types) in 
Class 
# Words (Types) in 
BNC2 
Speaker-Oriented Adverbs 30 30 
Adverbs+ 9,324 9,324 
Adverbs- 9,222 9,222 
ly-Adverbs 6,372 6,372 
Random ly-Adverbs 30 30 
Adjectives 147,038 147,038 
Subjective Adjectives (Wiebe, 2000b) 1,484 1,322 
Trait Adjectives (Peabody & De Raad, 2002)   795 732 
Trait Adjectives Subset (Integrity-Values) 
(Peabody & De Raad, 2002) 
47 44 
Trait Adverbs   758 539 
Trait Adverbs Subset (Integrity-Values) 38 36 
Nouns 430,415 430,415 
Verbs 60,083 60,083 
Punctuation 71 71 
We use the 44 trait adjectives (Appendix 3.1) in our vector experiments because it 
is the smallest of Peabody and De Raad’s (2002) subclass (we are looking for the best 
performance using the smallest number of words), and because Rittman et al. (2005) 
found that these words are more highly correlated with user perceptions of subjectivity 
than other subsets of trait adjectives listed by Peabody and De Raad. We also use the 
subset of 36 trait adverbs in our vector experiments, deriving these words by adding the 
suffix ly to the list of trait adjectives (Appendix 3.1), and by discarding any ill-formed 
61
words as mentioned above. (Examples of these adjectives and adverbs in BNC2 
documents are listed in Appendix 3.1.) Combined with the 30 speaker-oriented adverbs 
(Ernst, 2002) (Table 6.2), this yields a total of 110 words used in our vector experiments 
(Table 6.8).  
Table 6.8: Word Classes Used in Vector Experiments 
 
Distinct Word Classes # Unique 
Words  
Speaker-Oriented Adverbs 30 
Trait Adjectives Subset 44 
Trait Adverbs Subset 36 
Total 110 
Combinations of Distinct Word Classes # Unique 
Words 
Trait Adjectives + Trait Adverbs Subsets 80 
Speaker-Oriented Adverbs + Trait 
Adjectives + Trait Adverbs Subsets 
110 
Next, Chapter 7 reports the results of our experiments. Chapter 8 concludes with a 
summary of results, conclusions, limitations of the study, and recommendations for future 
study. 
 
62
Chapter 7: Results 
7.1 Accuracy Gain is Better Performance Measure than Accuracy 
The scatter plot in Figure 7.1 shows the relationship between accuracy gain (AG) 
and accuracy. Simply put, it shows that there are many cases in which accuracy can be 
high, but AG is low, even negative. This is especially true for discrimination problems in 
which the split between positive and negative cases is large (as in academic vs. not 
academic, fiction vs. not fiction, and news vs. not news). Since AG adjusts for these 
differences using the most rigorous baseline, AG is a more accurate performance measure 
than simple accuracy, and it is extensible to other studies. 
 
Figure 7.1: Relation Between Accuracy and Accuracy Gain 
 
Relation Between Accuracy and Accuracy Gain
-10.0
10.0
30.0
50.0
70.0
90.0
0.0 20.0 40.0 60.0 80.0 100.0 120.0
Accuracy
A
cc
ur
ac
y
G
ai
n
63
7.2 Results Overview 
The table in Appendix 7.1 shows the results of our 54 methods for each of the six 
problems. There are 54 rows (one row for each method) and six columns at the far right 
(one column for each problem), resulting in 324 models (each cell represents one of the 
324 models.) 28 Model identifiers (ID) are determined by matching the method ID 
(prefix) in each row to the model ID (suffix) in the last six columns. Thus, for method 1 
we see that the model ID for academic vs. fiction is 1.1; the model ID for academic vs. 
news is 1.2, and so on (see also Appendix 5.2 for table of methods and models). The 
highest accuracy gain (AG) for each method is indicated by a shaded cell. These are the 
models for which each of the methods performed best. The highest AG for each problem 
(column) is indicated by bold print. These are the best models for each of the six 
problems.  
We distinguish adjective and adverb features (methods 1-48) from nouns, verbs, 
and punctuation (methods 49-54). We calculate an average AG for methods 1-48, 
methods 49-54, and an overall average (methods 1-54). We also group each of our six 
problems into one of two classification categories: one-against-one, and one-against-
many. One-against-one classification distinguishes one genre category from another 
genre category (academic vs. fiction, academic vs. news, and fiction vs. news). One-
against-many classification distinguishes one genre category from many genre categories 
(academic vs. not-academic, fiction vs. not-fiction, and news vs. not-news). The notation 
NR (no result) means that “no variables are qualified for the [discriminant] analysis” (see 
 
28 See Glossary. We use test and model for the intersection of a method and a problem. The intersection has 
two meanings: (a) It stands to "measuring the performance of a method against a problem" (what can be 
called a test), and (b) it stands for "the resulting set of variables selected by the discriminant analysis" (what 
can be called a model). 
64
SPSS 11.0). NR occurs 13 times among the 324 cases, and in all but one case, it occurs 
when only two count variables are entered into the analysis. The results table in 
Appendix 7.1 also shows the features, and feature representations (vector vs. count, and 
normalized vs. non-normalized) for each method.  
The table in Appendix 7.2 shows the number of unique words in our vector 
models. The table in Appendix 7.3 ranks our 54 methods by decreasing order of AG. 
Table 7.6 below ranks words in our vector models by a measure we call salience score.
As expected, the “softest” problems are one-against-one classification problems. 
The “hardest” problems are one-against-many classification problems. The best result for 
each method is always a one-against-one classification problem. The only negative AGs 
appear in one-against-many comparisons. The overall average AG for “soft” problems 
(academic vs. fiction=51.3%, academic vs. news=53.2%, and fiction vs. news=58.8%) is 
superior to the overall average AG for “hard” problems academic vs. not-academic=-
0.5%, fiction vs. not-fiction=5.7%, and news vs. not-news=0.5%).  
7.3 Adjective and Adverb Features versus Other Classes 
The table in Appendix 7.1 shows that, on average, models that contain only 
adjective and/or adverb features (methods 1-48) outperform models that contain only 
nouns, verbs, or punctuation features (methods 49-54) for distinguishing documents in 
four of the six problems: academic vs. news (average AG=54.2% vs. 45.4%), academic 
vs. not-academic (average AG=-0.2% vs. -2.8%), fiction vs. not-fiction (average 
AG=6.4% vs. 0.3%), and news vs. not-news (average AG=0.5% vs. 0.4%). But 
considering average results is not as interesting as looking at the best AG by method for 
each problem. Here, we see that at least one model containing only adjective and/or 
65
adverb features always outperforms any model containing only nouns, verbs, or 
punctuation for every problem.  
First, the highest AG for any problem is always achieved by the union of many 
kinds of adjective and adverb features (methods 47 and 48). For example, distinguishing 
fiction from academic writing, using the combination of many features in method 47, 
with the model condensed so that only the count of terms in each class is used in the 
analysis, but with the count normalized against the length of the document, achieves an 
astonishing 98.8% (model 47.1) of the possible gain in accuracy (AG). The same method 
applied to fiction vs. news (model 47.4) and academic vs. news (model 47.2) scores the 
second and third highest AG of 93.0% and 90.8%, respectively. For the “hard” problems, 
method 47, and the non-normalized counterpart (method 48) score highest for news vs. 
not-news (model 47.6, AG=13.5%), academic vs. not-academic (model 48.3, AG=15.9%) 
and an impressive AG of 63.9% for fiction vs. not-fiction (model 48.5).  
Second, the table in Appendix 7.1 shows that the next best results (second only to 
methods 47 and 48) are also achieved by models that contain only adjective and/or 
adverb features (as opposed to models that contain only nouns, verbs, or punctuation). 
Models containing only the vector of speaker-oriented adverbs (or also the vector of trait 
adjectives and adverbs) outperform the best models containing nouns, verbs or 
punctuation for four of the six problems: academic vs. fiction (model 3.1, AG=91.6% vs. 
model 50.1, AG=44.0%), academic vs. news (model 45.2, AG=79.6% vs. model 49.2, 
AG=68.6%), academic vs. not-academic (model 46.3, AG=15.0% vs. models 52.3, 53.3, 
54.3, AG=0.0%), and fiction vs. not-fiction (model 4.5, AG=44.3% vs. model 53.5, 
AG=28.7%). For the remaining two problems, models containing the count of only 
66
adverbs or adjectives outperform the best models containing only nouns, verbs, or 
punctuation. For fiction vs. news, the count of all adverbs is superior (model 27.4, 
AG=88.4% vs. 53.4, AG=83.4%). For news vs. not-news, the count of all adjectives is 
superior (model 17.6, AG=6.8% vs. model 49.6, AG=3.8%). 
These results lend support to Hypothesis 1 for three reasons. First, using accuracy 
gain (AG) as a performance measure, for every problem tested against a method 
containing adjectives and/or adverbs, some tests result in a positive AG (that is, a 
performance better than the best possible performance achieved by the most informed 
guessing). Second, in every problem, the combination of many kinds of adjective and 
adverb features achieves the highest AG of all models tested. Third, in every problem 
there is at least one model containing adjectives and or adverb features that performs 
second best to the combination of many kinds of adjective and adverb features, and is 
also superior to the performance of the best model containing only nouns, verbs, or 
punctuation. We report additional results in support of Hypothesis 1 in the next sections 
(7.3.1 and 7.3.2). 
7.3.1 Relative Performance of Methods by Model Sets 
The table in Appendix 7.3 shows the relative performance of the 54 methods by 
ranking them in decreasing order of AG for the set of six models for each method. The 
order is determined by calculating the highest AG from among the six models for each 
method, then ranking the 54 methods according to the highest AG for each set of models. 
In other words, method 47 = rank 1 because the highest AG for method 47 is model 47.1 
(98.8%), and it is also the highest AG of the 54 sets of models. Method 48 = rank 2 
because the highest AG for method 48 is model 48.1 (97.8%), and it is also the second 
67
highest AG of the 54 sets of models. The table also shows the feature representations and 
the number of unique words in the best vector model sets.  
Consistent with what we have already observed, the best performance (rank 1-2) 
is achieved by considering the union of many kinds of adjective and adverb features 
(methods 47-48). The next 10 best methods (rank 3-12) include vector models of speaker-
oriented adverbs and speaker-oriented adverbs combined with trait adjectives (TAdj2) 
and trait adverbs (TAdv2), and two models of the count of many adverbs. All but two of 
these top 12 method sets are effective for solving the academic vs. fiction classification 
problem, and eight of these method sets use vector models containing suitable sets of as 
few as 15 speaker-oriented adverbs.   
The best performance for methods using punctuation, nouns, and verbs rank at 
positions 13, 14, and 15, respectively, for solving the fiction vs. news classification 
problem. We also observe that the vector of trait adjectives alone, representing a set of 
only 17 unique words (model 24.1, AG=75.6%), and the union of trait adjectives and trait 
adverbs, representing a set of only 24 unique words (model 42.1, AG=75.6%), is 
comparable to the performance of the count of thousands of verbs (AG=76.0%).  
The relative performance of model sets that include other kinds of adjective and 
adverb features are ranked accordingly in Table 7.1. We find that the best model using 
the count of adjectives in general (rank 19) outperforms the best models using the count 
of (a) subjective adjectives (rank 25), (b) trait adjectives (rank 27), (c) trait adjectives 
subset (rank 37), (d) trait adverbs (rank 24), or (e) trait adverbs subset (rank 50). The best 
model (model 31.2) using only ly-adverbs (a reduction of all adverbs in method 17) (rank 
18) outperforms the best of all of the previous methods, and it is best for classifying 
68
documents by academic vs. news. Not surprisingly, the best model using the count of a 
random sample of 30 ly-adverbs scores lowest (rank 51).  
Table 7.1: Ranked Performance of Best Models of Various Adjective and Adverb Features 
 
Rank Method Model Problem Feature 
Feature 
Representation 
Accuracy 
Gain 
Sentence 
Position 
V
/
C
Nrm / 
NNrm  
18 31 31.2 Acad-News ly-Adv Any C Nrm 73.4 
19 17 17.1 Acad-Fict Adj Any C Nrm 72.2 
24 35 35.4 Fict-News TAdv Any C Nrm 68.2 
25 19 19.4 Fict-News SbjAdj Any C Nrm 67.0 
27 21 21.2 Acad-News TAdj Any C Nrm 66.2 
37 26 26.4 Fict-News TAdj2 Any C NNrm 52.6 
50 40 40.4 Fict-News TAdv2 Any C NNrm 37.4 
51 34 34.4 Fict-News 
R-ly-
Adv Any C NNrm 36.0 
Abbreviations 
Adj Adjectives ly-Adv ly-Adverbs 
R-ly-Adv Random ly-Adverbs SbjAdj Subjective Adjectives 
TAdj Trait Adjectives TAdj2 Trait Adjectives Subset 
TAdv Trait Adverbs TAdv2 Trait Adverbs Subset 
V Vector C Count 
Nrm Normalized NNrm Non-Normalized 
7.3.2 Universal Methods 
Another way to assess the performance of our methods is to see which choices of 
features produce an accuracy gain for all six problems. Table 7.2 reports the five methods 
that achieve a positive AG for each of the six problems. While the most effective overall 
method is the one with the largest collection of features, this result is expected. That is, 
with good machine classification methods, if added features are not improving the score, 
the program will give them negligible weights, and they will certainly not make the score 
worse. The ordering of the tests is almost constant across these methods.  However, what 
is more significant is that every method, which gives accuracy gains for all six problems, 
69
includes speaker oriented adverbs. This shows that, at least for this selection of tasks, the 
use of a linguistically defined construct guides us directly to the essential feature of the 
statistical models. Perhaps not surprisingly, it is easiest to distinguish academic writing 
from fiction, and most difficult to distinguish news from not-news. This might be due to 
the difficulty of distinguishing news from other types of non-fictional accounts, and not 
due to some blurring of the stylistic lines between news and fiction. 
 
Table 7.2: Methods Effective for All Problems 
Method Feature Representation Model Problem  Accuracy Gain 
Sentence 
Position V/C 
Nrm / 
NNrm  
4.1 Acad vs. Fict 87.0 
4.2 Acad vs. News 74.2 
4.4 
Fict vs. 
News 64.2 
4.5 Fict vs. Not-Fict 44.3 
4.3 
Acad vs. 
Not-Acad 8.0 
4 SOA NSI V NNrm 
4.6 News vs. Not-News 0.8 
7.1 Acad vs. Fict 89.4 
7.2 Acad vs. News 73.0 
7.4 Fict vs. News 72.0 
7.5 Fict vs. Not-Fict 13.9 
7.3 Acad vs. Not-Acad 1.8 
7 SOA Nrm 
7.6 News vs. Not-News 0.8 
8.1 Acad vs. Fict 87.0 
8.2 Acad vs. News 74.2 
8.4 
Fict vs. 
News 68.6 
8.5 Fict vs. Not-Fict 44.3 
8 SOA 
SI+NSI V 
NNrm 
8.3 
Acad vs. 
Not-Acad 14.2 
70
8.6 News vs. Not-News 1.5 
46.1 Acad vs. Fict 90.4 
46.2 Acad vs. News 79.6 
46.4 Fict vs. News 67.0 
46.5 Fict vs. Not-Fict 44.3 
46.3 Acad vs. Not-Acad 15.0 
46 SOA + Tadj2 + TAdv2 Any V NNrm 
46.6 News vs. Not-News 1.5 
47.1 Acad vs. Fict 98.8 
47.4 Fict vs. News 93.0 
47.2 Acad vs. News 90.8 
47.5 Fict vs. Not-Fict 52.5 
47.6 News vs. Not-News 13.5 
47 
SOA, Adj, 
SbjAdj, TAdj, 
TAdj2, Adv+, 
Adv-, ly-Adv, 
TAdv, TAdv2 
 
Nrm 
47.3 Acad vs. Not-Acad 10.6 
48.1 Acad vs. Fict 97.8 
48.4 Fict vs. News 85.6 
48.2 Acad vs. News 81.2 
48.5 Fict vs. Not-Fict 63.9 
48.3 Acad vs. Not-Acad 15.9 
48 
SOA, Adj, 
SbjAdj, TAdj, 
TAdj2, Adv+, 
Adv-, ly-Adv, 
TAdv, TAdv2 
Any C 
NNrm 
48.6 News vs. Not-News 1.5 
Abbreviations 
SOA Speaker-Oriented Adverbs TAdj2 Trait Adjectives Subset 
TAdv2 Trait Adverbs Subset TAdj Trait Adjectives 
TAdv Trait Adverbs Adv+ Adverbs 
Adv- Adverbs (less wh-adverbs + particles) ly-Adv ly-Adverbs 
Adj Adjectives SI Sentence-Initial Position 
NSI Not Sentence-Initial Position V Vector 
C Count Nrm Normalized 
NNrm Non-Normalized   
To put these results in perspective, we compare our results to published findings 
in Section 7.6. But first, we discuss results that support Hypothesis 2 in the next section. 
 
71
7.4 Vector versus count 
In this section, we consider performance by representing documents as a vector 
versus count in head-to-head comparisons of matched pairs of methods for each problem 
(see results table in Appendix 7.1). Here, we group otherwise identical methods 
according to whether we represent a feature as a count or as a vector. For example, 
method 1 and method 8 represent a matched pair because the only difference is whether 
the feature is represented as a vector (method 1) or as a count (method 8). We exclude 
any matched pair where any AG < 0.  
Thus, for speaker-oriented adverbs, we match methods 1-8 with the corresponding 
methods 9-16. For trait adjectives, we match methods 23-24 with the corresponding 
methods 25-26. For trait adverbs, we match methods 37-38 with the corresponding 
methods 39-40. For the union of trait adjectives and trait adverbs, we match methods 41-
42 with the corresponding methods 43-44. 
We find that representing documents as a vector always yields a higher AG for 
speaker-oriented adverbs. The same is true for trait adjectives and trait adverbs (if the 
frequency is not normalized). With the exception for news vs. not-news, the same is also 
true for the union of trait adjectives and trait adverbs (if the frequency is not normalized). 
7.4.1 Speaker-Oriented Adverbs in Vector Models 
Referring again to the tables in Appendixes 7.1 and 7.2, we observe that the 
discriminating power of models 3.1 and 5.1 (for academic vs. fiction), representing the 
vector of sets of fewer than 20 specific speaker-oriented adverbs (SOA) approximates the 
performance of our best-performing models (models 47.1 and 48.1), the combination of 
many kinds of adjective and adverb features. Specifically, the AG for models 47.1 and 
72
48.1 (98.8% and 97.8%, respectively), which represent the count of thousands of words, 
is comparable to the AG for models 3.1 and 5.1 (91.6%, respectively), relying on sets of 
only 19 unique words (Appendix 7.2).  
For the same problem (academic vs. fiction), we achieve comparable results using 
the vector model 4.1 (AG=87.0%, with 16 words), vector model 6.1 (AG=88.2%, with 15 
words), vector model 7.1 (AG=89.4%, with 17 words), and vector model 8.1 
(AG=87.0%, with 16 words). When we add trait adjectives and trait adverbs, vector 
model 45.1 (AG=88.6%, 36 words) and vector model 46.1 (AG=90.4%, 23 words) are 
also competitive. For all vector models cited, performance is superior to the count-of-
nouns (model 49.1, AG=81.4%), verbs (model, 51.1, AG=72.2%), and punctuation 
(model 53.1, AG=76.8%).    
For the problem academic vs. news, vector models 45.2 (AG=79.6%, 21 words) 
and 46.2 (AG=79.6%, 17 words) (the combination of speaker-oriented adverbs, trait 
adjectives, and trait adverbs) are comparable to the second best performing model (48.2, 
AG=81.2%), the combination of many kinds of adjective and adverb features. Model 45.2 
and model 46.2 easily outperform the count of nouns, verbs, and punctuation. 
Significantly, the statistics on just 12 speaker-oriented adverbs accounts for 77.6% of the 
possible gain in accuracy (AG) for academic vs. news (model 6.2), compared to only 
69.0% counting verbs, (model 51.2), 68.6% counting nouns (model 49.2), and 29.8% 
counting punctuation (model 54.2). For the problem fiction vs. news, the count-of-all-
adverbs (model 27.4, AG=88.4%) is second only to the performance of the best model 
(model 47.4, AG=93.0%). Model 27.4 also outperforms models based on nouns, verbs, 
and punctuation.  
73
For the hardest problem, academic vs. not-academic, speaker-oriented adverb 
vector model 8.3 (AG=14.2%, 18 words), and the model combining speaker-oriented 
adverbs, trait adjectives, and trait adverbs (vector model 46.3, AG=15.0%, 26 words), 
perform third and second highest, respectively (compared with the best model using the 
combination of many adjective and adverb features (model 48.3, AG=15.9%)). Models 
that include the count of nouns, verbs and punctuation perform poorly (in all case, AG ≤
0). 
For the next hardest problem, news vs. not-news, using the simple count of all 
adjectives (model 17.6, AG=6.8%) is second only to the combination of many adjective 
and adverb features (model 47.6, AG=13.5%). Noun, verb, and punctuation models are 
not comparable. Finally, AG for discriminating between fiction vs. not-fiction, using only 
19 speaker-oriented adverbs (model 4.5, AG=44.3%) is not better than the combination 
of many adjective and adverb features (model 48.5, AG=63.9%), but it is better than the 
best model using the count of nouns, verbs and punctuation (punctuation, model 53.5, 
AG= 28.7%).    
Thus, we see that a vector model using sets of a relatively few (<20) speaker-
oriented adverbs (methods 3, 4, 5, 8) performs comparably to the best performing models 
that use the combination of the count many adjective and adverb features (methods 47 
and 48) for these four problems: academic vs. fiction, fiction vs. not-fiction, academic vs. 
news, and academic vs. not-academic. Adding trait adjectives and trait adverbs to the 
model improves classifier performance somewhat for academic vs. news and academic 
vs. not-academic using suitable sets of fewer than 50 words. Discriminating between the 
74
remaining two problems (fiction vs. news and news vs. not-news) is effective using 
models containing only the count of all adverbs or all adjectives, respectively. 
7.4.2 Sentence Position of Speaker-Oriented Adverbs 
The performance of vector models of speaker-oriented adverbs (methods 1-2) in 
the sentence-initial position is less effective for all problems except news vs. not-news 
than is speaker-oriented adverbs in any other sentence position (not sentence-initial 
position, any position, sentence-initial position + not sentence-initial position) (methods 
3-8). The union (method 7 and 8) is superior to or ties other methods for four of six 
problems (although any position beats or ties three of six problems) (see table in 
Appendix 7.1). 
We believe that the relatively poor performance of this feature is due to the 
restrictive pattern we used to extract speaker-oriented adverbs in the sentence-initial 
position (SOA word, capitalized, followed by a comma). As we discussed in Section 
6.1.1, there are potentially many other linguistic patterns of SOAs, but these alternative 
patterns are much harder to extract reliably from text. By using this restrictive pattern, we 
believe we have under-counted the occurrence of SOAs in BNC2 documents. Support for 
this is evident in the fact that extracting SOAs without regard to syntactic position results 
in much higher frequencies for these words (see Table 6.3), and thus a more effective 
feature for genre discrimination. However, extracting SOAs without regard to sentence 
position necessarily includes occurrences of the manner reading. An example of this is 
clearly. Unless we can develop automatic extraction rules that distinguish between 
clearly as a speaker-oriented adverb (as in Clearly, she sang the song), we will confuse 
instances where the word is used as a manner adverb (as in She sang the song clearly).  
75
7.4.3 Model Performance of Speaker-Oriented Adverbs Combined With Trait 
Adjectives and Trait Adverbs 
Table 7.3 shows the best performance for vector models by matched pairs by 
method. In this section, matched pair indicates a grouping of methods in which the only 
difference is whether the frequency of the feature is normalized or non-normalized. For 
example, methods 1 and 2 are a matched pair, methods 3 and 4 are a matched pair, and 
methods 7 and 8 are a matched pair. 
Results show that vector models of speaker-oriented adverbs alone perform most 
effectively for academic vs. fiction, fiction vs. news, and fiction vs. not-fiction. When 
combined with trait adjectives and trait adverbs, model performance improves slightly for 
academic vs. news and academic vs. not-academic, and remains stable for fiction-not-
fiction. This means that adding trait adjectives and trait adverbs to the model does not 
improve performance for academic vs. fiction, fiction vs. news, and fiction-not-fiction.
However, there is one problem (news vs. not-news) for which the vector model of trait 
adjectives and trait adverbs alone is most effective. In this case, adding speaker-oriented 
adverbs to the model results is a stable AG. Next, we discuss the relative contribution of 
trait adjectives and trait adverbs to the vector models in more detail. 
 
76
Table 7.3: Best Accuracy Gain by Method for Matched Pairs for Speaker-Oriented Adverbs 
Compared to Models Including Trait Adjectives and Trait Adverbs 
 
Method Feature 
Sentence 
Position Problem 
Avg 
AG 
Acad 
vs. 
Fict 
Acad 
vs. 
News 
Fict 
vs. 
News 
Acad 
vs. 
Not-
Acad 
Fict 
vs. 
Not-
Fict 
News 
vs. 
Not-
News 
 
Model  
.1 .2 .4 .3 .5 .6  
Highest AG by Method  
1, 2 SOA SI 41.0 39.8 41.0 -5.3 10.7 0.8 21.3 
3, 4 SOA NSI 91.6 75.6 69.8 8.0 44.3 0.8 48.3 
5, 6 SOA Any 91.6 77.6 71.2 9.7 42.6 0.0 48.8 
7, 8 SOA SI+NSI 89.4 74.2 72.0 14.2 44.3 1.5 49.3 
23, 24 TAdj2 Any 75.6 48.6 55.8 4.4 11.5 1.5 32.9 
37, 38 TAdv2 Any 47.8 44.6 42.2 -2.7 4.9 0.8 22.9 
41, 42 
TAdj2 + 
TAdv2 Any 75.6 59.8 57.6 0.0 12.3 2.3 34.6 
45, 46 
SOA + 
TAdj2 + 
TAdv2 Any 
90.4 79.6 70.4 15.0 44.3 2.3 
50.3 
Average 75.4 62.5 60.0 5.4 26.8 1.2 38.6 
Shaded Cell = Best Accuracy Gain by Method; Bold Cell = Best Accuracy Gain by Problem 
Abbreviations 
SOA Speaker-Oriented Adverbs TAdj2 Trait Adjectives Subset 
TAdv2 Trait Adverbs Subset SI Sentence-Initial Position 
NSI Not Sentence-Initial Position  
7.4.4 Most Efficient Vector Models 
To evaluate the relative contribution of adding trait adjectives and trait adverbs to 
the vector models containing speaker-oriented adverbs, we first report the number of 
unique words in all vector models in Table 7.4. Again, we report results by matched pairs 
according to the distinction made in Section 7.4.3. In this table, we report the fewest 
number of unique words in the model for each matched pair (where at least one AG > 0 
for any matched pair). For example, we observe that the smallest number of unique words 
in the best model for method 1 and method 2 (speaker-oriented adverbs in the sentence-
77
initial position) for academic vs. fiction is 11. We also see that on average, relatively few 
words are needed in the models to discriminate between academic vs. news (average=9.4 
words), fiction vs. news (average=10.6 words), and news vs. not-news (average=11.6 
words). 
 
Table 7.4: Number of Unique Words in Vector Models 
 
Method Feature 
Sentence 
Position Problem 
Avg 
Words 
Acad 
vs. 
Fict 
Acad 
vs. 
News 
Fict vs. 
News 
Acad 
vs. 
Not-
Acad 
Fict 
vs. 
Not-
Fict 
News 
vs. 
Not-
News 
 
Model  
.1 .2 .4 .3 .5 .6  
Unique Words in Model  
1, 2 SOA SI 11 5 5 NR 12 4 7.4 
3, 4 SOA NSI 16 9 9 15 19 12 13.3 
5, 6 SOA Any 16 11 13 18 20 13 15.2 
7, 8 SOA SI+NSI 15 9 9 16 19 NR 13.6 
23, 24 TAdj2 Any 17 9 10 15 26 10 14.5 
37, 38 TAdv2 Any 7 4 6 NR 15 4 7.2 
41, 42 
TAdj2 + 
TAdv2 Any 24 11 13 NR 38 9 19.0 
45, 46 
SOA + 
TAdj2 + 
TAdv2 Any 23 17 20 26 46 29 26.8 
Average 16.1 9.4 10.6 18.0 24.4 11.6 15.0 
Shaded Cell = Fewest Words by Method; Bold Cell = Fewest Words by Problem 
Abbreviations 
SOA Speaker-Oriented Adverbs TAdj2 Trait Adjectives Subset 
TAdv2 Trait Adverbs Subset NR No Result 
SI Sentence-Initial Position NSI Not Sentence-Initial Position 
In Table 7.5 we calculate the relative efficiency of our vector models by dividing 
AG (Table 7.3) by the words used in each model (Table 7.4). The higher the value, the 
more efficient is the model. We observe that adding trait adjectives and trait adverbs to 
speaker-oriented adverb models does not improve efficiency. In other words, while AG 
may increase, the degree of improvement is not proportional to the number of words in 
78
the enlarged model. 29 Models that include only speaker-oriented adverbs or trait adverbs 
alone are most efficient, especially for academic vs. news.
Table 7.5: Relative Efficiency of Vector Models 
 
Method Feature 
Sentence 
Position Problem Avg 
Acad 
vs. 
Fict 
Acad 
vs. 
News 
Fict 
vs. 
News 
Acad 
vs. 
Not-
Acad 
Fict 
vs. 
Not-
Fict 
News 
vs. 
Not-
News  
Model  
.1 .2 .4 .3 .5 .6  
Efficiency of Model  
1, 2 SOA SI 3.7 8.0 8.2 NR 0.9 0.2 4.2 
3, 4 SOA NSI 5.7 8.4 7.8 0.5 2.3 0.1 4.1 
5, 6 SOA Any 5.6 6.7 5.5 0.8 2.2 0.1 3.5 
7, 8 SOA SI+NSI 6.1 8.6 7.9 0.6 2.2 NR 5.1 
23, 24 TAdj2 Any 4.4 5.4 5.6 0.3 0.4 0.2 2.7 
37, 38 TAdv2 Any 6.8 11.2 7.0 NR 0.3 0.2 5.1 
41, 42 Tadj2 + TAdv2 Any 3.2 5.4 4.4 NR 0.3 0.3 2.7 
45, 46 
SOA + TAdj2 + 
TAdv2 Any 3.9 4.7 3.5 0.6 1.0 0.1 2.3 
Avg 4.9 7.3 6.2 1.0 1.2 0.2 3.5 
Shaded Cell = Most Efficient by Method; Bold Cell = Most Efficient by Problem 
Abbreviations 
SOA Speaker-Oriented Adverbs TAdj2 Trait Adjectives Subset 
TAdv2 Trait Adverbs Subset NR  No Result 
SI Sentence-Initial Position NSI Not Sentence-Initial Position 
7.5 Salience of Words: Salience Score  
Table 7.6 ranks the 110 words that we entered into the various vector models. We 
rank the words by salience score (SS) where AG>0. SS is calculated by first ranking the 
words in each model in decreasing order by the Absolute Standardized Canonical 
 
29 It is not clear how to calculate the statistical significance of such differences, but we believe that the 
former difference is not of practical significance, while the latter, larger difference, would be practically 
significant. 
79
Discriminant Function Coefficient (ASCDFC) 30 and assigning a rank position (RP) 
accordingly. 31 The word with the highest ASCDFC is rank 1; the word with the second 
highest ASCDFC is rank 2, and so on for each model. Next, we assign a rank score (RS) 
to each word in the model depending on its relative rank position. The word in rank 1 is 
always given 100 points. The RS for each successive word in the model is reduced by 
subtracting, from the score of the next highest word, a value equal to 100 divided by the 
number of words in the model. For example, if there are five words in a model, the rank 
score of each successive word is reduced by 20 points (100/5=20). Thus, the rank score 
of word_1 is 100; word_2 is 80; word_3 is 60, and so on. Finally, we sort all words 
alphabetically, sum RS for each word, and divide by the number of models within which 
each word (types and tokens) could have occurred. 32 This final value is the salience 
score (SS), and it represents the normalized RS for each word. For instance, if word_1 
occurs in four of potentially 36 models, and it has rank scores of 100, 100, 80, and 20, the 
salience score (SS) is 8.33 (100+100+80+20) / 36). However, if word_1 occurs in the top 
rank position in only one of potentially 36 models, the salience score is 2.78 (100 / 36). 
Thus, SS combines in a single measure the goals of “highest rank positions” and for the 
“most potential models.” SS ranges from 0.0 to 100.0.  
Salience score does not distinguish words by problem. Rather, it normalizes the 
contribution of unique words to the vector models for solving the six classification 
problems. Table 7.6 ranks the 110 words in decreasing value by salience score. The table 
 
30 Absolute Standardized Canonical Discriminant Function Coefficient (ASCDFC) is the absolute value of 
the weight (negative or positive) calculated in the discriminant analysis by SPSS for each variable for each 
model (in this case, each word for each vector model). A high absolute value indicates a variable that is 
important (has high discriminating power) to the discriminant model.  
31 ASCDFC values might not be comparable across different models, but we use it only as a relative value 
to rank words within each model to derive salience score (SS). 
32 Speaker-oriented adverbs can potentially occur in 60 models. Trait adjectives and trait adverbs can 
potentially occur in 36 models. 
80
lists the words by class (speaker-oriented adverbs, trait adjectives, and trait adverbs), and 
sums the total words by class, and calculates the average SS for each class. The table also 
shows the number of models within which each word (type or token) actually occurs (far 
right column).  
 
Table 7.6: Words Ranked in Decreasing Order by Salience Score (Where AG > 0) 
 
Rank 
Speaker-
Oriented 
Adverb 
Trait 
Adjective 
Trait  
Adverb 
Salience 
Score 
#
Models 
1 bad   78.3 23 
2 maybe     71.8 45 
3 generally     60.4 39 
4 surely     59.9 37 
5 fairly 57.1 19 
6 necessarily     54.5 34 
7 clearly     53.6 34 
8 moral   47.9 24 
9 natural   46.4 20 
10 specifically     45.7 40 
11     badly 43.7 21 
12 strangely     35.7 28 
13 seriously     35.5 27 
14 curiously     31.4 24 
15 briefly     31.1 31 
16   characterless   29.8 18 
17     naturally 29.8 13 
18   honest   26.1 15 
19 simply     24.6 27 
20   fair   24.2 12 
21   noble   22.8 13 
22 probably     21.1 23 
23 oddly     19.2 17 
24   decent   18.9 11 
25 honestly     18.6 20 
26   upright   17.7 10 
27 certainly     17.4 22 
28     maliciously 17.3 14 
29 definitely     17.2 15 
30 unfortunately     17.1 25 
31 obviously     16.7 14 
32   ethical   16.7 12 
33 confidently     16.4 13 
34     morally 16.3 13 
81
35     decently 16.3 7 
36     truthfully 16.2 12 
37   lying   16.0 14 
38   principled   15.6 12 
39     nobly 14.1 6 
40 possibly     13.9 20 
41   loyal   13.7 11 
42   lustful   12.6 9 
43   biased   12.6 13 
44   unreliable   12.3 9 
45 roughly     12.2 17 
46   faithful   12.2 10 
47     sincerely 12.0 9 
48   frank   10.9 9 
49 ideally     10.9 21 
50     unreliably 10.8 10 
51 frankly     10.6 18 
52 amazingly     10.2 10 
53     virtuously 9.3 9 
54   disloyal   8.9 9 
55   unscrupulous   8.8 10 
56 predictably     8.5 12 
57 luckily     8.0 10 
58   insincere   7.5 7 
59   unfaithful   6.4 5 
60   just   6.2 5 
61 preferably     6.1 9 
62     dishonestly 5.6 4 
63 surprisingly     5.3 9 
64     loyally 5.3 5 
65     insincerely 4.8 4 
66   calculating   4.7 3 
67   mercenary   4.5 4 
68   sincere   4.2 6 
69     justly 3.7 5 
70     perfidiously 3.7 5 
71     ethically 3.4 5 
72   intriguing   3.4 6 
73   malicious   3.2 5 
74 candidly     3.0 5 
75   rapacious   3.0 6 
76 normally     2.9 10 
77   avaricious   2.9 3 
78   virtuous   2.6 3 
79   materialistic   2.5 6 
80   deceptive   2.3 3 
81     righteously 2.2 3 
82   dishonest   1.6 2 
82
83     lustfully 1.4 2 
84     intriguingly 1.2 2 
85     deceptively 1.0 2 
86   truthful   0.8 3 
87     hypocritically 0.8 1 
88     calculatingly 0.6 1 
89   righteous   0.4 2 
90   vulgar   0.4 1 
91   hypocritical   0.4 1 
92   trustworthy   0.2 1 
93     disloyally 0.2 1 
94     materialistically 0.1 1 
95   untruthful   0.1 1 
SOA 30   24.7 21.9 
TAdj  40  12.7 8.4 
TAdv   25 11.1 7.0 
The first thing we observe is that only 95 of the 110 words contribute to any 
vector model. These 95 words include the 30 speaker-oriented adverbs, 40 of the 44 trait 
adjectives, and only 25 of the 36 trait adverbs. On average, speaker-oriented adverbs have 
the highest SS (average=24.7) and occur most frequently in vector models (average=21.9 
models). Trait adjectives and trait adverbs have comparable average SS values (12.7 and 
11.1, respectively) and they contribute to about the same average number of models (8.4 
and 7.0, respectively).   
The second thing we observe is that half of the 30 speaker-oriented adverbs 
(n=15) rank in the top 25% (rank 1 to rank 27) of words ranked by SS, whereas only 
small numbers of the other classes occur above the same cutpoint (9 of 40 trait adjectives; 
3 of 25 trait adverbs). Clearly, speaker-oriented adverbs contribute most significantly to 
genre classification models. The connection may be related to the speaker’s intent, as 
indicated by the use of speaker-oriented adverbs in text, and the author’s intent as one of 
several distinguishing criterion of genre.     
 
83
7.6 Comparison of Results with Previous Classification Research 
Referring to our review of genre classification research in Section 2.3, there are 
four published studies with which we can compare our results (Karlgren & Cutting, 1994; 
Kessler et al., 1997; Lee & Myaeng, 2002; and Ng et al., 2000). (Table 7.7 reports 
comparisons with previous research.)  
 Head-to-head comparisons are not possible because studies use different corpora, 
features, automatic classification methods, genre categories, and definitions of genre. But 
we can find comparisons to some problems. In other words, we can compare the results 
of our adjective and adverb features in solving similar genre detection problems, although 
these comparisons are not always between parallel problems. For instance, Karlgren & 
Cutting discriminate between informative and imaginative texts, while we discriminate 
between news and fiction.
We cannot compare our results with certain studies for several reasons. 
Stamatatos et al. (2000) report results in graphs, not tables. Lim et al. (2005) discriminate 
among 12 genre categories, and we cannot calculate a baseline for a binary discrimination 
comparison. Our genre categories do not correspond to those of Finn and Kushmerick 
(2006). Their study mixes our working definition of “genre” with what is usually called 
“domain,” and the qualitative property of “subjectivity.”  
 
84
Table 7.7: Comparison with Published Results 
 
Published 
Study 
 
Our Study 
Problem Feature 
Model 
(Our 
Study) 
Extracted 
Baseline 
Reported 
Accuracy
Calculated 
Accuracy 
Gain 
Ng et al. 
(2000) 
 
WSJ-FR Punctuation 
 
71.9 81.9 35.5 
X Acad-News SOA, Any, V, Non-Nrm 
6.2 
50.0 88.8 77.6 
Kessler et 
al. (1997) 
 
Fict-NotFict 
Best of 55 
surface 
features 
 
81.4 94.0 67.7 
X Fict-NotFict Many Adj & Adv features 
48.5 
87.8 95.6 63.9 
X Fict-NotFict SOA, NSI, V, Nrm 
46.5 
87.8 93.2 44.3 
Kessler et 
al. (1997) 
 Reportage-
Not 
Reportage 
Best of 55 
surface 
features 
 
81.4 83.0 8.4 
X News-
NotNews 
Many Adj & 
Adv features 
47.6 
86.7 88.5 13.5 
X News-
NotNews Adjectives 
17.6 
86.7 87.6 6.8 
Karlgren 
& Cutting 
(1994) 
 Informative-
Imaginative 
Many kinds 
of features 
(N=20) 
 
74.8 95.6 82.5 
X Fict-NotFict Many Adj & Adv features 
48.5 
87.8 95.6 63.9 
X Fict-NotFict SOA, NSI, V, Nrm 
8.5 
87.8 93.2 44.3 
X Fict-News All adverbs 27.4 50.0 94.2 88.4 
X Fict-News Many Adj & Adv features 
47.4 
50.0 96.5 93.0 
X Acad-Fict SOA, Any, V, Nrm 
5.1 
50.0 95.8 91.6 
Lee & 
Myaeng 
(2002) 
 Newspaper-
Not 
Newspaper 
Genre/subject 
terms 
 
78.1 78.7 2.7 
X News-
NotNews 
Many Adj & 
Adv features 
47.6 
86.7 88.5 13.5 
X News-
NotNews Adjectives 
17.6 
86.7 87.6 6.8 
Abbreviations 
SOA Speaker-Oriented Adverbs Adj Adjectives 
Adv Adverbs Any Any Sentence Position 
NSI Not Sentence-Initial Position V Vector 
Nrm Normalized  
Using punctuation as a discriminating feature, Ng et al. (2000) report accuracy of 
81.9% in a two-way classification problem: Wall Street Journal (WSJ) documents versus 
85
Federal Register (FR) documents. (The Federal Register is a collection of formal 
documents of the federal government.) We calculate the baseline at 71.9%, based on the 
majority class of WSJ documents (50,880) versus the minority set of FR documents 
(19,860). This yields an AG of 35.5%. If we accept the (debatable) assumption that FR 
documents are comparable to academic documents, then a corresponding two-way 
problem in our study is academic versus news. Using our feature of speaker-oriented 
adverbs (a set of only 12 words), we achieve a superior AG of 77.6% (compared to 
35.5% for Ng et al.).  
Using an unspecified set of the most discriminating of 55 “surface features,” 
Kessler et al. (1997) report an accuracy of 94.0 in a two-way classification problem: 
fiction versus not fiction. We calculate their baseline at 81.4%, and an AG of 67.7%. 
Using our most discriminating adjective and adverb features (only 13 variables), our AG 
is comparable (63.9%), but not quite as good as Kessler et al. For the same problem, we 
achieve a substantially lower AG of 44.3% using a set of only 19 speaker-oriented 
adverbs.  
On the other hand, our method is superior when we compare our results for news 
versus not-news to Kessler et al. (1997) (reportage vs. not reportage). Using the same 
unspecified set of the most discriminating of 55 “surface features”, Kessler et al. report 
an accuracy of 83.0 in a two-way classification problem between reportage versus not
reportage documents. Using the same baseline of 81.4%, we calculate their AG at 8.4%, 
compared to our AG of 13.5%, using only nine adjective and adverb features. Using only 
the count of adjectives (two variables) for the same problem, our AG is 6.8%.  
86
In a two-way experiment, Karlgren and Cutting (1994) classified 500 texts into 
the broad categories of informative texts and imaginative texts using many kinds of 
features (N=20). We calculate their baseline at 74.8%. Converting their reported accuracy 
of 95.6%, we calculate their AG as equal to 82.5%.  
Imaginative texts correspond naturally to our fiction category. The informative 
category is problematic. However, if we substitute their informative category for either 
our news or academic categories, our results are superior. For fiction vs. news, our 
method (method 47, model 47.4), using only 10 adjective and adverb features, is superior 
(AG=93.0%). For academic vs. fiction, our method (method 47, model 47.1), using the 
same 10 adjective and adverb features) achieves an AG of 98.8%. If we use the more 
rigorous comparison of fiction versus not fiction, which we argue is not comparable to 
imaginative, versus informative, our performance is good (AG=63.9%, method 48, model 
48.5), but not better than their AG of 82.5% for informative versus imaginative texts.  
If we compare Karlgren and Cutting (1994) to our results for fiction versus news,
our results are better using the count of all adverbs (AG=88.4%). Our result is even better 
using the combination of many adjective and adverb features (AG=93.0%). If we 
compare Karlgren and Cutting to our results for academic versus fiction using a set of 
only 19 speaker-oriented adverbs, our AG is 91.6%. 
Lee and Myaeng (2002) discriminate between newspaper and not newspaper 
documents. We calculate their baseline at 78.1%. Using a combination of genre and 
subject terms, they report an accuracy of 78.7%, which we interpret as an AG of only 
2.7%. Comparing this to our results for news versus not news, we beat this performance 
87
using our combination of many kinds of adjective and adverb features (AG=13.5%), and 
the count of all adjectives (AG=6.8%). 
In fairness, we moderate our claims of success by stating that we examined many 
different methods for solving these problems. For each published result we looked for our 
“best” comparable result. In a practical setting one would generally not know which 
method to apply, and therefore the performance is not assured. However, the results 
reported here seem to show great potential and efficiency for the class of methods 
presented. However, the problem of knowing which model to apply to a particular 
problem is one of the areas for future research. These comparisons also illustrate the 
difficulty with comparing results across studies. Accuracy gain (AG) addresses the 
problem of comparing reported accuracy results across studies. The problem of 
comparing studies that use different genre categories remains unaddressed. 
 
88
Chapter 8: Conclusions 
8.1 Summary of Results 
In this study, we addressed the problem of machine classification for non-topical 
criteria, namely the automatic discrimination of genre. We tested the feasibility of 
automatically classifying documents by genre using adjectives and adverbs as 
discriminating features. We selected our features a priori based on an understanding of 
the role of adjectives and adverbs in language using insights from linguistics and 
psychology. We represented documents as (a) the count of many words from various 
adjective and adverb classes, and (b) as vectors of the frequency of relatively few specific 
words from selected adjective and adverb classes. We hypothesized that the former 
method will reliably distinguish documents automatically by genre (Hypothesis 1), and 
that the latter method is more effective than the former (Hypothesis 2). 
To test this, we used the 4,053 documents in the British National Corpus, 
classified by genre (Lee, 2003a; Davies, n.d.), and the statistical method of discriminant 
analysis using SPSS 11.0. We selected the three superordinate genre categories of 
academic writing, news writing, and fiction writing, resulting in six binary classification 
problems: one-against-one (academic vs. fiction, academic vs. news, and fiction vs. 
news), and one-against-many (academic vs. not-academic, fiction vs. not-fiction, and 
news vs. not-news). 
For our vector models, we selected 30 speaker-oriented adverbs, 44 trait 
adjectives and 36 trait adverbs. For our count models, we selected adjectives, adverbs, ly-
adverbs, 30 random ly-adverbs, subjective adjectives, trait adjectives, selected trait 
adjectives, and the adverbial correspondence for selected trait adjectives. We also tested 
89
the count of nouns, verbs and punctuation. We evaluated classification performance using 
accuracy gain (AG), the normalized accuracy achieved with respect to the most rigorous 
baseline. AG measures the improvement over the most informed guess. We also 
measured the importance of individual words in our vector experiments by using what we 
call the salience score (SS). SS represents the normalized, cumulative value of each word 
in relation to the power of its contribution to each model. In addition to count versus 
vector, we considered other representations of features, such as normalization by the 
length of the document, and sentence position. This resulted in 54 methods for the six 
problems, yielding 324 tests and models. 
As expected, the “softest” problems are one-against-one classification problems. 
The “hardest” problems are one-against-many classifications. The best result for any 
method is always a one-against-one classification model.   
At least one method containing only adjective and/or adverb features for any 
problem always outperforms any method containing either nouns, verbs, or punctuation 
features. We determine this by calculating that the highest accuracy gain (AG) for any 
problem is always achieved by methods that contain the union of the count of many kinds 
of adjective and adverb features. The second best AG for any problem is always achieved 
by methods that contain (a) the vector of speaker-oriented adverbs, (b) the vector of 
speaker-oriented adverbs, plus trait adjectives and trait adverbs, or (c) the count of only 
all adjectives or all adverbs. These results lend support to Hypothesis 1. 
Hypothesis 1 is also supported by our finding that, for the five methods for which 
there is an accuracy gain for every problem, every method includes speaker-oriented 
90
adverbs. This shows that at least for this selection of problems, the use of a linguistically 
defined construct guides us directly to the essential feature of the statistical models.   
Hypothesis 2 is supported by the finding that in head-to-head comparisons of 
matched pairs where at least one AG>0, representing documents as a vector of individual 
speaker-oriented adverbs and/or as trait adjectives and trait adverbs is always superior to 
representing documents by the corresponding variable calculated as a count. In addition, 
we find that representing documents as vectors of suitable sets of only a relatively few 
speaker-oriented adverbs (<20) is effective for most classification problems, as opposed 
to representing documents as the count of many words.  
When we calculate the normalized contribution of individual words to the various 
vector models in solving the six classification problems, we find that ranking individual 
words by salience score (SS) shows that speaker-oriented adverbs (as a class) rank 
highest compared to trait adjectives and trait adverbs. In other words, speaker-oriented 
adverbs on average contribute most to the various vector models for solving the six 
classification problems.  
This gives shape to our overall findings. We (a) demonstrated the efficacy of  
models based on features derived from external sources, (b) measured their 
discriminating power in a very rigorous way, (c) assessed the role of individual words in 
the most effective models, and (d) compared our results to comparable published studies 
using accuracy gain (AG).  
8.2 Conclusions 
Our findings demonstrate that starting with a narrow grammatical set of features 
(adjectives and adverbs) motivated by research in other disciplines, we can build more 
91
parsimonious classifiers whose power is equal to or greater than that of classifiers which 
use statistical or machine learning rules to select features ad hoc from a much larger and 
more diverse set of features.  
We demonstrate that, using only adjective and adverb features  
• It is possible to classify documents automatically by genre 
• Performance is generally superior to nouns, verbs, or punctuation features 
• Performance of a relatively few words in vector models is comparable to 
or second best to using the combination of the count of many adjectives 
and adverbs 
• Performance improves when we represent documents as a vector as 
opposed to a count  
• Performance of vector models using only speaker-oriented adverbs is 
generally superior to or comparable to vector models that combine trait 
adjectives and trait adverbs 
In general, our findings demonstrate the efficacy of 
• A new, more demanding, and transferable measure of performance which 
we call accuracy gain (AG).  
• Priming any machine-based effort with knowledge about potential features 
based on studies conducted in other disciplines (such as theoretical 
linguistics and applied psychology) 
• Systematically analyzing in-depth the performance of a great many 
features derived from a narrow selection of grammatical classes, resulting 
in specific lists of favored features 
92
We prescribe that researchers should apply the following three principles  
• Use our model for the systematic, in-depth, analysis of features for genre 
classification 
• Use insights from other disciplines to select generalizable features 
• Use the new metric of accuracy gain (AG) to compare classification 
results across studies to ensure a fair and accurate comparison 
In conclusion, we recommend that the principles and methods we studied can be 
applied to (a) solving problems in other applications, (b) using other corpora, and in (c) 
finding other features.  
Other applications include automatic  
• Author identification 
• Detection of subjectivity versus objectivity in text 
• Classification of texts for question-answering (QA) 
• Detection of customer review opinions in business environments 
• Detection of personality traits in text 
• Detection of people who might be susceptible to certain beliefs 
 Other corpora include 
• Weblogs (blogs)  
• Email logs 
• Chat room logs 
 Possible Sources for other features include the following domains 
• Stylistics 
• Communication 
93
• Journalism 
• Content analysis 
• Political discourse 
8.3 Limitations of the Study and Future Work 
There are several limitations to this study based on the restrictions we selected 
• Models are developed on full documents. Smaller “snippets” of full 
documents may be resistant to classification by these methods. The 
underlying linguistic qualities used here may well vary within a single 
document which, overall, belongs to a defined genre. 
• We performed only one discriminant analysis run for each of the 324 
models. Ten-fold cross-validation would require more than 3,000 results. 
• We used the 30 speaker-oriented adverbs specified by Ernst (2002). This 
is not the only possible such list.   
• These results are determined on a British corpus, and American English 
may have differences in style and spelling. 
Our results point the way to several additional research tasks. These relate to the 
fact that, in this project, we studied a very specific instance of a general idea. We 
examined the efficacy of using specific features and lists of words (derived from 
linguistics and psychology) to discriminate among (whole documents) selected from a 
specific corpus (BNC2) on the task of distinguishing among three specific categories, 
which are categories of genre. Any of the bulleted restrictions below can in principle be 
lifted, giving rise to an extension of this work.  Such extensions will be needed to 
replicate and validate our underlying claim. These restrictions include 
94
• Run the discriminant scores of our models on different corpora 
• Run the discriminant scores of our models on “snippets” of whole 
documents 
• Classify documents according to additional genre categories (including 
Web-based categories) using the discriminant scores from our models 
• Test performance of vector models consisting of larger sets of speaker-
oriented adverbs, trait adjectives and trait adverbs 
• Test for a significant correlation between Ernst’s (2002) sub-categories of 
speaker-oriented adverbs and genres 
• Test for a significant correlation between models consisting of classes of 
trait adjectives as defined by the Big Five personality dimensions and 
genres 
• Test for a significant correlation between specific adverbs and adjectives 
as unique discriminators of specific genres 
• Of the models we tested, determine which model to apply to a particular 
problem 
 
95
References 
Allport, G., & Odbert, H. (1936). Trait-names: A psycho-lexical study. 
Psychological Monographs, 47 (1, Whole Number 211). 
 
ARDA/AQUAINT. (2003). Advanced question and answering for intelligence. 
http://www.ic-arda.org/InfoExploit/aquaint/ 
 
Barbaranelli, C., & Caprara, G. (2000). Measuring the big five in self-report and 
other ratings: A multitrait-multimethod study. European Journal of Psychological 
Assessment, 16 (1), 31-43. 
 
Belew, R. (2000). Overview. In R. Belew. Finding out about: A cognitive 
perspective on search engine technology and the WWW (pp. 1-38). New York: 
Cambridge University Press. 
 
Benet-Martinez, V., & Waller, N. (2002). From adorable to worthless: Implicit 
and self-report structure of highly evaluative personality descriptors. European Journal 
of Personality, 16 (1), 1-41. 
 
Biber, D. (1988). Variation across speech and writing. New York: Cambridge 
University Press. 
 
Biber, D., Conrad, S., & Reppen, R. (1998). Corpus linguistics: Investigating 
language structure and use. Cambridge: Cambridge University Press. 
 
BNC2 (2001). British National Corpus, World Edition. 
http://www.natcorp.ox.ac.uk/ 
 
BNC2 POS-Tagging Manual. (2000). Retrieved March 17, 2006 from 
http://www.comp.lancs.ac.uk/ucrel/bnc2/bnc2autotag.htm 
 
Bruce, R., & Wiebe, J. (1999). Recognizing subjectivity: A case study in manual 
tagging. Natural Language Engineering 5 (2), 187-205. Retrieved January 27, 2003, from 
http://www.cs.pitt.edu/~wiebe/pubs/papers/nle99.ps 
 
Burrows, J. (1992). Not unless you ask nicely: The interpretative nexus between 
analysis and information. Literary and Linguistic Computing, 7(2), 91-109. 
 
Center for Nonproliferation Studies. (2007). http://cns.miis.edu/ 
 
Chafe, W. (1982). Integration and involvement in speaking, writing, and oral 
literature. In D. Tannen (ed.). Spoken and written language: Exploring orality and 
literacy (pp. 35-54). Norwood, NJ: Ablex. 
 
96
Church, K. (1988). A stochastic parts program and noun phrase parser for 
unrestricted text. Computational Linguistics, 19(1), 25-60. 
 
Costa, Jr., P., & McCrae, R. (1992). Revised NEO personality inventory (NEO PI-
R) and NEO Five-Factor inventory (NEO-FFI). Odessa, FL: Psychological Assessment 
Resources. 
 
Davies, M. (n.d.). VIEW: Variation in English words and phrases. Retrieved 
August 11, 2005 from http://view.byu.edu/ 
 
Eisenbeis, R., & Avery, R. (1972). Discriminant analysis and classification 
procedures: Theory and applications. Lexington, MA: D. C. Heath and Company.   
 
Ernst, T. (2002). The semantics of predicational adverbs, and The scopal basis of 
adverb licensing. In T. Ernst. The syntax of adjuncts (pp. 41-91, 92-148). New York: 
Cambridge University Press. 
 
Finn, A., & Kushmerick, N. (2006). Learning to classify documents according to 
genre. Journal of the American Society for Information Science and Technology, 57 (11), 
1506-1518. 
 
Galton, F. (1884). Measurement of character. Fortnightly Review, 36, 179-185. 
Retrieved November 10, 2003, from, 
http://www.er.uqam.ca/nobel/r30034/PSY4090/pages/galton01.html 
 
Garside R., & Smith N. (1997). A hybrid grammatical tagger: CLAWS4. In R. 
Garside, G. Leech, & A. McEnery (eds.). Corpus annotation: Linguistic information from 
computer text corpora (pp. 102-121). London: Addison Wesley Longman.  
 
Goldberg, L. (1990). An alternative description of personality: The Big Five 
factor structure. Journal of Personality and Social Psychology, 59, 1216-1229.  
 
Goldberg, L. (1992). The development of markers for the Big-Five factor 
structure. Psychological Assessment, 4, 26-42. 
 
Goldberg, L. (1993). The structure of phenotypic personality traits. American 
Psychologist, 48, (1), 26-34. 
 
Goldberg, L. & Somer, O. (2000). The hierarchical structure of common Turkish 
person-descriptive adjectives. European Journal of Personality, 14, 497-531. 
 
Gough, H., & Heilbrun, Jr., A. (1983). The adjective checklist manual. Palo Alto, 
CA: Consulting Psychologists Press, Inc. 
 
Hatzivassiloglou, V., & McKeown, K. (1997). Predicting the semantic orientation 
of adjectives. 35th Annual Meeting of the Association for Computational Linguistics and 
97
8th Conference of the European Chapter of the Association for Computational 
Linguistics, Madrid, 174–181. Retrieved November 4, 2003, from 
http://citeseer.nj.nec.com/hatzivassiloglou97predicting.html  
 
Hatzivassiloglou, V., & Wiebe, J. (2000). Effects of adjective orientation and 
gradability on sentence subjectivity. 18th International Conference on Computational 
Linguistics, COLING-2000. Retrieved November 4, 2003, from 
http://citeseer.nj.nec.com/432435.html 
 
Heinström, J. (2002). Fast surfers, broad scanners and deep divers: Personality 
and information-seeking behaviour. (Doctoral thesis in Information Studies, 2002). Abo 
Akademi University Press: Finland. 
 
Indiana University. (2004). Human intelligence: Francis Galton. Retrieved 
September 14, 2006 from http://www.indiana.edu/~intell/galton.shtml 
 
Jackendoff, R. (1972). Adverbs. In R. Jackendoff. Semantic interpretation in 
generative grammar (pp. 47-107). Cambridge, MA: MIT Press. 
 
John, O.P., Angleitner, A., & Ostendorf, F. (1988). The lexical approach to 
personality:  A historical review of trait taxonomic research. European Journal of 
Personality, 2, 171-203. 
 
Karlgren, J. & Cutting, D. (1994). Recognizing text genres with simple metrics 
using discriminant analysis. Proceedings of the 15th. International Conference on 
Computational Linguistics, Coling 94, Kyoto. 1071-1075. Retrieved January 30, 2003, 
from http://citeseer.nj.nec.com/ 
 
Kennedy, G. (1998). Introduction to corpus linguistics. NY: Addison Wesley 
Longman. 
 
Kessler, B., Nunberg, G., & Schutze, H. (1997). Automatic detection of text 
genre. Proceedings of the 35th Annual Meeting of the Association for Computational 
Linguistics, 32 - 38.
Lee, D. (2001). Genres, registers, text types, domains and styles: Clarifying the 
concepts and navigating a path through the BNC jungle. Language, Learning, and 
Technology, 5 (3), 37-72. Retrieved March 17, 2006 from 
http://opinion.nucba.ac.jp/~davidlee/devotedtocorpora/home/genre_register.zip 
 
Lee, D. (2003a). BNC World Index. Retrieved August 17, 2005 from 
http://personal.cityu.edu.hk/~davidlee/devotedtocorpora/home/BNC_WORLD_INDEX.Z
IP 
 
Lee, D. (2003b). Notes to accompany the BNC World Edition (bibliographical) 
index. Retrieved March 20, 2006 from 
98
http://opinion.nucba.ac.jp/~davidlee/devotedtocorpora/home/BNC_WORLD_INDEX.ZI
P
Lee, D. (n.d.). David Lee's Genre Classification Scheme. Retrieved August 17, 
2005 from http://homepage.mac.com/bncweb/manual/genres.html 
 
Lee Y. & Myaeng S. (2002). Text genre classification with genre-revealing and 
subject-revealing features. Proceedings of the 25th Annual International ACM SIGIR,
145-150. 
 
Leech, G. (1997). Introducing corpus annotation. In R. Garside, G. Leech, & A. 
McEnery (eds.). Corpus annotation: Linguistic information from computer text corpora 
(pp. 1-18). London: Addison Wesley Longman. 
 
Leech, G. (n.d.) A brief users' guide to the grammatical tagging of the British 
National Corpus. Retrieved March 9, 2006 from 
http://www.natcorp.ox.ac.uk/docs/gramtag.html 
 
Leech, G., Garside, R., and Bryant, M. (1994). CLAWS4: The tagging of the 
British National Corpus. In Proceedings of the 15th International Conference on 
Computational Linguistics (COLING 94). Kyoto, pp.622-628.  
 
Lim, C., Lee, K. & Kim, G. (2005). Multiple sets of features for automatic genre 
classification of web documents. Information Processing and Management, 41, 1263–
1276.     
 
Maglaughlin, K., & Sonnenwald, D. (2002). User perspectives on relevance 
criteria: A comparison among relevant, partially relevant, and not-relevant judgments. 
Journal of the American Society for Information Science, 53 (5), 327-342. 
 
McCrae, R. & Costa, Jr., P. (1993). Folk concepts, natural language, and 
psychological constructs: The California Psychological Inventory and the Five-Factor 
Model. Journal of Personality, 61, 1-26. 
 
Mittwoch, A. (1977). How to refer to one's own words: Speech act modifying 
adverbials and the performative analysis. Journal of Linguistics, 13, 177-189. 
 
Ng, K., Rieh, S., & Kantor, P. (2000). Signal detection methods and discriminant 
analysis applied to categorization of newspaper and government documents: A 
preliminary study. In Proceedings of Annual Conference of American Society for 
Information Science, pp. 227-236. 
 
Norman, W. (1967). 2800 Personality trait descriptors: Normative operating 
characteristics for a university population. Ann Arbor, MI: Department of Psychology, 
University of Michigan. 
 
99
Peabody, D. (1967). Trait inferences: Evaluative and descriptive aspects. Journal 
of Personality and Social Psychology, 7, 1–18.
Peabody, D., & De Raad, B. (2002). The substantive nature of psycholexical 
personality factors: A comparison across languages. Journal of Personality and Social 
Psychology, 83 (4), 983-997. 
 
Quinlan, J. (1993). C4.5: Programs for machine learning. San Francisco, Morgan 
Kaufmann. 
 
Quirk, R., Greenbaum, S., Leech, G., & Svartvik, J. (1985). A comprehensive 
grammar of the English language. London: Longman. 
 
Raskin, V., & Nirenburg, S. (1995). Lexical semantics of adjectives: A 
microtheory of adjectival meaning. Computing Research Laboratory, New Mexico State 
University, Memoranda in Computer and Cognitive Science MCCS-95-288. Retrieved 
November 21, 2003, from http://omni.cc.purdue.edu/~vraskin/adjective.pdf 
 
Rittman, R., Wacholder, N., Kantor, P. (2005). Trait adjectives as indicators of 
subjectivity in documents. Unpublished manuscript. 
 
Rittman, R., Wacholder, N., Kantor, P, Ng, K. B., Strzalkowski, T., Bai, B. et al. 
(2004). Adjectives as indicators of subjectivity in documents. Proceedings of the 67th 
Annual Meeting of the American Society for Information Science and Technology, 349-
359. 
 
Rushton, J. P. (1990). Sir Francis Galton, epigenetic rules, genetic similarity 
theory, and human life-history analysis. Journal of Personality, 58, 117-140. 
 
Saracevic, T. (1996). Relevance reconsidered. In Information science: Integration 
in perspectives. Proceedings of the Second Conference on Conceptions of Library and 
Information Science, Copenhagen, 201-218.  
 
Saracevic, T. (1999). Information science. Journal of the American Society for 
Information Science, 50 (9), 1051-1063.  
 
Salton, G. (1989). Conventional text-retrieval systems. In G. Salton. Automatic 
text processing: The transformation, analysis, and retrieval of information by computer 
(pp. 229-273). New York: Addison-Wesley. 
 
Saucier, G. (1997).  Effects of variable selection on the factor structure of person 
descriptors. Journal of Personality and Social Psychology, 73 (6), 1296-1312. 
 
Schamber, L., Eisenberg, M., & Nilan, M. (1990). A re-examination of relevance: 
Toward a dynamic, situational definition. Informational Processing and Management, 16 
(6), 755-776. 
100
Staab, S., & Hahn, U. (1997). Conceptualizing adjectives. Proceedings of the 21st 
Annual German Conference on Artificial Intelligence (KI-97), Freiburg, 267-278.  
Retrieved November 4, 2003, from 
http://citeseer.nj.nec.com/staab97conceptualizing.html 
 
Stamatatos, E., Fakotakis, N., & Kokkinakis, G. (2000). Text genre detection 
using common word frequencies. 18th International Conference on Computational 
Linguistics (COLING 2000), Luxembourg, Vol. 2, 808-814. Retrieved March 17, 2006, 
from http://www.wcl.ee.upatras.gr/ai/papers/stamatatos2.pdf 
 
Sun, Y. (2005). Automatic assessment of non-topical properties of text by 
machine learning methods. Unpublished doctoral dissertation, Rutgers, The State 
University of New Jersey.  
 
Tang, R., Ng, K.B., Strzalkowski, T., Kantor, P. (2003). Toward machine 
understanding of information quality. Proceedings of 2003 Annual Meeting of American 
Society for Information Science and Technology, 40, 213-220. 
 
van Rijsbergen, C. (1979). Introduction. In C. van Rijsbergen. Information 
retrieval (pp. 1-9). London: Butterworths. 
 
Voorhees, E. (2001). Overview of TREC 2001. In E. Voorhees (ed.) NIST Special 
Publication 500-250: The Tenth Text REtrieval Conference, pp. 1 – 15. Washington, 
D.C. 
 
Wiebe, J. (2000a). Learning subjective adjectives from corpora. Proceedings of 
the 17th National Conference on Artificial Intelligence (AAAI-2000), Austin. Retrieved 
December 4, 2003, from http://www.cs.pitt.edu/~wiebe/pubs/aaai00 
 
Wiebe, J. (2000b). [Learning subjective adjectives from corpora]. Unpublished 
list of subjective adjectives. Retrieved November 21, 2003, from 
http://www.cs.pitt.edu/~wiebe/pubs/aaai00/adjsMPQA 
 
Wiebe, J., Bruce, R., & O'Hara, T. (1999). Development and use of a gold 
standard data set for subjectivity classifications. Proceedings of the 37th Annual Meeting 
of the Association for Computational Linguistics, College Park: University of Maryland,
246-253. Retrieved December 4, 2003, from http://www.cs.pitt.edu/~wiebe/pubs/acl99 
 
Witten, I., & Frank, E. (2000). Data mining. San Diego: Academic Press. 
 
Wolters, M., & Kirsten, M. (1999). Exploring the use of linguistic features in 
domain and genre classification. Ninth Conference of the European Chapter of the 
Association for Computational Linguistics, Bergen, Norway, 142-149. Retrieved January 
30, 2003, from http://acl.ldc.upenn.edu/E/E99/E99-1019.pdf 
 
101
Appendix 1.1: Four Genres Returned by Google Relevant to 
Moby Dick 
(Search Criteria: "’Moby Dick’" filetype:txt”, March 7, 2006) 
 
Search 
Results
Genre Document URL 
Academic 
Writing 
Title: Art After Ahab 
Author: Jeffrey Insko, 
University 
of Massachusetts, Amherst 
http://www3.iath.virginia.edu/
pmc/text-only/issue.901/ 
12.1.r_insko.txt 
Fiction 
Writing 
Title: Moby Dick, Or The 
Whale  
[Text of novel] 
Author: Herman Melville 
http://www.textlibrary.com/ 
download/moby-dic.txt 
 Top 10 
 
Popular 
Writing 
 
Subject: Moby_Dick_ as 
Software 
Author: Kenneth M. Sheldon 
Excerpted from BYTE 
Magazine, 
July 1989. Contributed to 
HumourNet 
by Richard Perlotto 
http://www.humournet.com/ 
misc.humour/moby_dick.txt 
 
Specimen 1 
Title: Attention Grabbers:  
ISU-Based Journal Lists Top 
Novel  
Opening Lines. 
Author: Michele Steinbacher 
http://www.pantagraph.com/ 
articles/2006/02/04/news/ 
105662.txt 
 
Top 70 
 
News 
Writing 
 Specimen 2 
Title: Library Filters Could 
Block 
Out Ed Sites. 
Author: The Associated Press 
http://www.gazettetimes.com/ 
articles/2004/08/28/news/ 
oregon/satire07.txt 
 
102
Appendix 1.2: Glossary of Terms 
Absolute Standardized Canonical Discriminant Function Coefficient. The 
absolute value of the weight assigned to each variable in a discriminant function. We use 
this value to rank words in a model according to the weight assigned to each word by the 
discriminant function. (See also Rank Position, Rank Score, and Salience Score.) 
 
Accuracy. Classification accuracy measures the difference between the observed, 
true state, and the classifier’s prediction. There are two kinds of correct predictions: True 
Positive (TP) (the classifier correctly assigns a case to the positive class) and True 
Negative (TN) (the classifier correctly assigns a case to the negative class). There are two 
kinds of errors: False Positive (FP) (the classifier incorrectly assigns a negative case to 
the positive class) and False Negative (FN) (the classifier incorrectly assigns a positive 
case to the negative class). Accuracy is calculated by dividing the number of True 
Positive (TP) and True Negative (TN) cases by the total number of cases in the sample: 
(TP+TN) / (TP+TN+FP+FN). Basically, accuracy is the fraction of correctly predicted 
cases. (See also Accuracy Gain, Baseline, and Rigorous Baseline.)
Accuracy Gain (AG). Accuracy gain represents the improvement over the best 
guessing possibility compared to the maximum possible improvement. In other words, 
AG equals the fraction of possible achievable improvement, over the “best guess” 
baseline. AG is calculated accordingly: (Accuracy-Rigorous Baseline) / (100-Rigorous 
Baseline). (See also Accuracy, Baseline, and Rigorous Baseline.)
Adjectives (Adj). All words tagged in BNC2 as an adjective (includes 
comparative and superlative adjectives). 
 
Adverbs+ (Adv+). All words tagged in BNC2 as an adverb, including wh-
adverbs. (e.g., when, where, how, why) and adverb particles (e.g., about, along, back, 
down, in, on, up). 
 
Adverbs- (Adv-). All words tagged in BNC2 as an adverb, excluding wh-
adverbs. (e.g., when, where, how, why) and adverb particles (e.g., about, along, back, 
down, in, on, up). 
 
Baseline. A value used to evaluate accuracy. In binary classification problems 
this is often set as the “worst guess”, that is, 50% probability that a classifier will 
correctly assign a case to the true state, or observed class. (See also Rigorous Baseline.)
Count. Condensing a feature so that only the count of terms in each class is used 
in the analysis. The count approach can be thought of as Weighing-the-Bag-of-Words.
This method gives credit to each word as a member of a pre-determined class. Count 
means summing the occurrence of words in a class (e.g., adjectives, adverbs, nouns). (See 
also Vector.)
103
Document. A complete sample of a written or spoken text. (See also Text.)
Document Length. Total count of words in a document as calculated by 
CLAWS4. 
 
Domain. A classification of documents traditionally accepted as pertaining to 
subject (e.g., politics, history, and science), and not used in this study. 
 
Feature. In a broad sense, a feature is what we are looking for to discriminate 
among texts (or documents). A feature can be a specific part-of-speech, a unique word, 
the count of words in a sentence, or any derivative thereof. (See also Representation of 
Features)
Genre. A socially constructed classification of documents based on external 
criteria of use, such as the intended audience, the speaker’s purpose, and type of activity. 
 
ly-Adverbs (ly-Adv). Adverbs ending in -ly 
Method. An approach to classifying documents by problem (genre distinction) 
using features and representations of features. (See also Problem, Model / Test, and 
Feature.)
Model / Test. The intersection of a Method and a Problem is a Model and/or a 
Test. The intersection has two meanings: (a) It stands to "measuring the performance of a 
method against a problem" (what can be called a test), and (b) it stands for "the resulting 
set of variables selected by the discriminant analysis" (what can be called a model). 
 
Normalization. Against the length of the document. 
 
One-Against-Many. For binary discrimination, the classification of documents 
between one distinct category and all other categories in the corpus (e.g., academic vs. 
not academic, fiction vs. not fiction, or news vs. not news).   
 
One-Against-One. For binary discrimination, the classification of documents 
between one of two distinct categories (e.g., academic vs. fiction, academic vs. news, or
fiction vs. news).   
 
PRIORS. For binary discriminate analysis (using SPSS 11.0), pre-setting a value 
equal to the portion of documents in each class (as opposed to the default of 50% - 50% 
distribution).   
 
Problem. The task of automatically discriminating between genre categories 
(e.g., academic vs. fiction, fiction vs. news; fiction vs. not fiction). 
 
Random-ly-Adverbs (R-ly-Adv). Set of 30 randomly selected ly-adverbs. 
 
104
Rank Position (RP). Ordering of words in a model in decreasing value by 
Absolute Standardized Canonical Discriminant Function Coefficient (ASCDFC). The 
word with the highest ASCDFC is assigned rank position 1; the word with the second 
highest ASCDFC is assigned rank position 2; and so on. (See also Absolute Standardized 
Canonical Discriminant Function Coefficient, Rank Score and Salience Score.)
Rank Score (RS). The value assigned to each word in a model relative to its rank 
position. The word in rank position 1 is always given 100 points. The rank score for each 
successive word is reduced by subtracting from the score of the next highest word, a 
value equal to 100 divided by the number of words in the model. For example, if there 
are five words in a model, the rank score of each successive word is reduced by 20 points 
(100/5=20). Thus, the rank score of word_1 is 100; word_2 is 80; word_3 is 60, and so 
on. (See also Absolute Standardized Canonical Discriminant Function Coefficient,
Rank Position, and Salience Score.)
Representation of Features. How information about a feature is used, such as 
vector vs. count, normalized or non-normalized, and type vs. token.
Rigorous Baseline. A more demanding value used to evaluate accuracy. The 
rigorous baseline is calculated by predicting the “best guess” based on always assigning a 
case to the majority class. Thus, in a binary classification problem of 100 cases, if 90 
cases belong to one class, and 10 cases belong to the other class, then the rigorous 
baseline is 90%. (See also Baseline.)
Salience Score (SS). Salience score combines in a single measure the goals of 
“highest rank positions” and for the “most potential models.” For each word, sum the 
rank scores (RS) and divide by the number of models within which the word could have 
occurred. (See also Absolute Standardized Canonical Discriminant Function Coefficient,
and Rank Score.)
Sentence Position. Syntactic position of a word in a sentence, such as sentence-
initial position. 
 
Speaker-Oriented Adverbs (SOA). A set of 30 SOAs derived from Ernst 
(2002). SOAs may be condensed as a count of the complete set, or expanded as a vector 
of each word. SOAs may also be represented by sentence position, such as the sentence-
initial position.
Subjective Adjectives (SbjAdj). Set of 1,484 words listed by Wiebe (2000a, 
2000b) as subjective adjectives (N=1,322 in BNC2).  
 
Test (see Model). 
Text. A portion of written or spoken language; part of a document. (See 
Document.)
105
Token. The frequency of occurrence of a unique word in a text or document. (See 
also Type.)
Type. The occurrence of a unique word in a text or document. (See also Token.)
Trait Adjectives (TAdj). Set of 795 words listed by Peabody and De Raad 
(2002) as human trait adjectives (N=732 in BNC2).  
 
Trait Adjectives Subset (Integrity-Values) (TAdj2). Set of 44 trait adjectives 
listed by Peabody and De Raad (2002).  
 
Trait Adverbs (TAdv). Set of 539 adverbs derived by adding suffix ly to set of 
732 trait adjectives in BNC2. 
Trait Adverbs Subset (Integrity-Values) (TAdv2). Set of 36 adverbs derived by 
adding suffix ly to set of 44 trait adjectives (Integrity-Values) listed by Peabody and De 
Raad (2002). 
 
Variable. The numerical representation of features that are entered into a 
discriminate analysis test for distinguishing one document from another.  
(See also Feature and Representation of Features). 
Vector. Expanding a feature so that the frequency of each word is used in the 
analysis. The vector approach treats words as individual variables. (See also Count.)
106
Appendix 1.3: Abbreviations 
Abbreviation Name 
FEATURES 
SOA Speaker-Oriented Adverbs 
Adj Adjectives 
SbjAdj Subjective Adjectives 
TAdj Trait Adjectives 
TAdj2 Trait Adjectives Subset (Integrity-Values) 
Adv+ Adverbs (All) 
Adv- Adverbs (less wh-Adv + Adv Particles) 
TAdv Trait Adverbs 
TAdv2 Trait Adverbs Subset (Integrity-Values) 
ly-Adv ly-Adverbs 
R-ly-Adv Random ly-Adverbs 
Nouns Nouns 
Punc Punctuation 
Verbs Verbs 
GENRES 
Acad Academic 
Fict Fiction 
News News 
Acad vs. Fict Academic vs. Fiction 
Acad vs. News Academic vs. News 
Fict vs. News Fiction vs. News 
Not-Acad Not Academic 
Not-Fict Not Fiction 
Not-News Not News 
Acad vs. Not-Acad Academic vs. Not-Academic 
Fict vs. Not-Fict Fiction vs. Not-Fiction 
News vs. Not-News News vs. Not-News 
FEATURE REPRESENATIONS 
SI Sentence-initial Position 
NSI Not Sentence-initial Position 
SI+NSI Sentence-initial Position + Not Sentence-initial Position
107
Any Any Sentence Position 
V Vector 
C Count 
Nrm Normalized 
NNrm Non-Normalized 
108
Appendix 3.1: Sample Word Lists in BNC2 
Appendix 3.1, Table 1 
Random Sample of 60 Trait Adjectives in BNC2 
 
brilliant carefree caring changeable comfort-loving 
defiant dejected dense disdainful dominant 
dynamic even-tempered explosive extravagant flexible 
greedy gregarious haughty honest hostile 
hypersensitive impetuous inconstant intellectual intolerant 
intrusive invulnerable laughing lavish loving 
lying obedient obliging organized patient 
perfidious productive progressive provident sad 
scrupulous short-tempered sincere sloppy slow-moving 
sluggish stolid strong-minded strong temperate 
thoughtful tolerant trustworthy unconscientious uncultured 
unemphatic uninhibited vain vindictive warm 
Appendix 3.1, Table 2 
Random Sample of 60 Subjective Adjectives in BNC2 
 
absolute abstruse absurd acute apocalyptic 
artificial astute bonded brutal charitable 
chilean cleaner commonplace con contemporary 
cool cultural despondent diminutive discernible 
disdainful drastic ethical evocative fabulous 
fatuous frank fratricidal frilly gargantuan 
grotesque humanitarian ideal inoperable intolerant 
lucky luminary lush mischievous moral 
nearby noteworthy pragmatic privy questionable 
religious respiratory rudimentary significant smashing 
smooth thick totalitarian tuneful unsaleable 
unseen untouchable wise wretched young 
Appendix 3.1, Table 3 
Random Sample of 30 ly-Adverbs in BNC2 
 
annihilatingly assiduously besly 
comfortingly consistantly cordially 
equitably erratically even-handedly 
fatefully gilhooly glibly 
gregariously immanently interruptedly 
lankily liekly masterfully 
non-legally noncommittally over-zealously 
pessimistically polymorphously risibly 
ruefully sepulchrally single-mindedly 
verbally vexatiously warningly 
109
Appendix 3.1, Table 4 
44 Trait Adjectives Subset (Integrity / Values) in BNC2 
 
avaricious bad biased calculating characterless 
decent deceptive dishonest disloyal ethical 
fair faithful frank honest hypocritical 
insincere intriguing just loyal lustful 
lying malicious materialistic mercenary moral 
natural noble perfidious pharisaical principled 
rapacious righteous sincere trustworthy truthful 
underhanded unfaithful unreliable unscrupulous untruthful 
upright venal virtuous vulgar  
Appendix 3.1, Table 5 
36 Trait Adverbs Subset (Integrity / Values) in BNC2 
 
avariciously badly calculatingly decently deceptively 
dishonestly disloyally ethically fairly faithfully 
hypocritically insincerely intriguingly justly loyally 
lustfully maliciously materialistically morally naturally 
nobly perfidiously pharisaically rapaciously righteously 
sincerely truthfully underhandedly unfaithfully unreliably 
unscrupulously untruthfully uprightly valiantly virtuously 
vulgarly     
Appendix 3.1, Table 6 
Examples of Trait Adjectives and Trait Adverbs in BNC2 Documents 
Trait Adjectives Subset (Integrity/Values) 
BNC2 Text Superordinate Genre Genre 
David Blunkett, Labour's local government 
spokesman, said: “… 
If you believe it, you are naive, and if you 
don't you are dishonest in arguing for a non-
payment campaign.” 
News News: Broadsheet, 
National Report 
 
Why are you so hypocritical? Fiction Fiction: Prose 
Paris was full of rogues, card-sharps, 
brigands, footpads, dice-coggers, pimps, 
ponces, horse-stealers, bruisers, coin-
clippers -- the true children of wing-heeled 
Mercury, the lying patron of thieves and 
politicians. 
Fiction Fiction: Prose 
Nevertheless in Ezra we are dealing with a 
creative artist who never -- however 
impatient he was -- sold his birthright for a 
mess of pottage. 
Ezra can be mistaken -- more thoroughly 
mistaken than most people -- but he has 
Academic Academic: 
Humanities, Arts 
 
110
never been venal.
Trait Adverbs Subset (Integrity/Values) 
In the great tempting scene between Angelo 
and Isabella the deputy hypocritically 
exclaims on other men's sins – “Ha, fie, 
these filthy vices!” 
Academic Academic: 
Humanities, Arts 
 
A powerful 25-yard shot by Brown gave 
Bolton a justly deserved lead but Shearer's 
fine header robbed them of it almost 
immediately. 
News News: Broadsheet, 
National, Sports 
It is far better to like and admire something 
that is wrong, or to like and admire for the 
wrong reason, provided we do so sincerely,
than to follow slavishly the dictates or ideas 
of some other person. 
Academic Academic: 
Humanities, Arts 
 
When Meredith asked him to pop out for 
cigarettes, he replied vulgarly, “What did 
your last servant die of?” 
Fiction Fiction: Prose 
111
Appendix 4.1: CLAWS4 POS Tags in BNC2 
 BNC2 Tag BNC2 Feature 
BNC2 Tagged Features Used in Experiments 
AJ0 Adjective (general or positive) 
AJC Comparative adjective Adjectives 
AJS Superlative adjective 
General Adverbs AV0 – (AVP + AVQ) Adverb – (Adverb particle + Wh-adverb) 
All Adverbs AV0 + (AVP + AVQ) Adverb + (Adverb particle + Wh-adverb) 
NN0 Common noun, neutral for number 
NN1 Singular common noun 
NN2 Plural common noun Nouns 
NP0 Proper noun 
PUL Punc: left bracket 
PUN Punc: general separating mark 
PUQ Punc: quotation mark Punctuation 
PUR Punc: right bracket 
VBB The present tense forms of the verb BE 
VBD The past tense forms of the verb BE 
VBG The "-ing" form of the verb BE 
VBI The infinitive form of the verb BE 
VBN The past participle form of the verb BE: been 
VBZ The -s form of the verb BE: is, 's 
VDB The finite base form of the verb BE: do 
VDD The past tense form of the verb DO: did 
VDG The -ing form of the verb DO: doing 
VDI The infinitive form of the verb DO: do 
VDN The past participle form of the verb DO: done 
VDZ The -s form of the verb DO: does, 's 
VHB The finite base form of the verb HAVE: have, 've 
VHD The past tense form of the verb HAVE: had, 'd 
VHG The -ing form of the verb HAVE: having 
VHI The infinitive form of the verb HAVE: have 
VHN The past participle form of the verb HAVE: had 
VHZ The -s form of the verb HAVE: has, 's 
VM0 Modal auxiliary verb 
VVB The finite base form of lexical verbs 
VVD The past tense form of lexical verbs 
VVG The -ing form of lexical verbs 
VVI The infinitive form of lexical verbs 
VVN The past participle form of lexical verbs 
Verbs 
VVZ The -s form of lexical verbs 
BNC2 Tagged Features Not Used in Experiments 
AT0 Article 
CJC Coordinating conjunction 
CJS Subordinating conjunction 
CJT The subordinating conjunction "that" 
112
CRD Cardinal number 
DPS Possessive determiner 
DT0 General determiner 
DTQ Wh-determiner 
EX0 Existential there 
ITJ Interjection or other isolate 
ORD Ordinal numeral 
PNI Indefinite pronoun 
PNP Personal pronoun 
PNQ Wh-pronoun 
PNX Reflexive pronoun 
POS The possessive or genitive marker 's or ' 
PRF The preposition "of" 
PRP Preposition, except for "of" 
TO0 Infinitive marker "to" 
UNC Unclassified items 
XX0 The negative particle not or n't 
ZZ0 Alphabetical symbols 
113
Appendix 5.1: BNC2 Genre Categories 
Superordinate Genres Genres Documents 
BNC2 Genre Categories Used in This Study 
Written Genres 
Academic: Humanities 
 
87 
Academic: Medicine 24 
Academic: Natural Sciences 43 
Academic: Politics Law Education 186 
Academic: Social & Behavioural Sciences 142 
Academic: Technology Computing Engineering 23 
Academic 
 
Total Academic Documents 505 
Excerpts From Two Modern Drama Scripts 2 
Novels & Short Stories 432 
Single- and Multiple-Author Collections of 
Poems 30 
Fiction 
 
Total Fiction Documents 464 
TV Autocue Data 32 
Broadsheet National Newspapers: Arts/Cultural 
Material 51 
Broadsheet National Newspapers: Commerce & 
Finance 44 
Broadsheet National Newspapers: Personal & 
Institutional 12 
Editorials & Letters-To-The-Editor   
Broadsheet National Newspapers: Miscellaneous 
Material 95 
Broadsheet National Newspapers: Home & 
Foreign News Reportage 49 
Broadsheet National Newspapers: Science 
Material 29 
Broadsheet National Newspapers: Material on 
Lifestyle Leisure Belief & Thought 36 
Broadsheet National Newspapers: Sports Material 24 
Regional and Local Newspapers: Arts 15 
News 
 
Regional and Local Newspapers: Commerce & 
Finance 17 
114
Regional and Local Newspapers: Home & 
Foreign News Reportage 39 
Regional and Local Newspapers: Science 
Material 23 
Regional and Local Newspapers: Material on 
Lifestyle, Leisure, Belief & Thought 37 
Regional and Local Newspapers: Sports Material 9 
Tabloid Newspapers 6 
Total News Documents 518 
SubTotal Documents 1,487 
BNC2 Genre Categories Not Used in This Study 
News Popular Magazines 211 
Non-Academic/Non-Fiction: Humanities 111 
Non-Academic: Medical/Health Matters 17 
Non-Academic: Natural Sciences 62 
Non-Academic: Politics Law Education 94 
Non-Academic: Social & Behavioural Sciences 123 
Non-Academic: Technology Computing 
Engineering 123 
Religious Texts Excluding Philosophy 35 
Non-Fiction 
 
Total Non-Fiction Documents 565 
Administrative and Regulatory Texts In-House 
Use 12 
Print Advertisements 59 
Biographies/Autobiographies 100 
Commerce & Finance Economics 112 
E-Mail Sports Discussion List 7 
School Essays 7 
University Essays 3 
Hansard/Parliamentary Proceedings 4 
Official/Governmental Documents/Leaflets 
Company 43 
Annual Reports Etc.; Excludes Hansard   
Instructional Texts/DIY 15 
Personal Letters 6 
Professional/Business Letters 11 
Miscellaneous Texts 502 
Other 
 
Total Other Documents 881 
Total Written Documents 3,144 
Spoken Genres 
TV or Radio Discussions 53 
115
TV Documentaries 10 
TV or Radio News Broadcasts 12 
Non-Tertiary Classroom Discourse 58 
Mainly Medical & Legal Consultations 128 
Face-to-Face Spontaneous Conversations 153 
Legal Presentations or Debates 13 
'Live' Demonstrations 6 
Job Interviews & Other Tp 13 
Oral History Interviews/Narratives Some 
Broadcast 119 
Lectures on Economics Commerce & Finance 3 
Lectures on Humanities and Arts Subjects 4 
Lectures on The Natural Sciences 4 
Lectures on Politics Law or Education 7 
Lectures on The Social & Behavioural Sciences 13 
Business or Committee Meetings 132 
BNC-Transcribed Parliamentary Speeches 6 
Public Debates Discussions Meetings 16 
Religious Sermons 16 
Planned Speech, Whether Dialogue or Monologue 26 
More or Less Unprepared Speech, Whether 
Dialogue or Monologue 51 
'Live' Sports Commentaries and Discussions 4 
University-Level Tutorials 18 
Miscellaneous Spoken Genres 44 
Total Spoken Documents 909 
Total Corpus Documents 4,053 
116
Appendix 5.2: Methods and Models 
 
Method Feature Representation Model Problem 
Sentence 
Position V/C 
Nrm / 
NNrm  
1.1 Acad vs. Fict 
1.2 Acad vs. News 
1.3 Acad vs. NotAcad 
1.4 Fict vs. News 
1.5 Fict vs. NotFict 
1 Nrm 
1.6 News vs. NotNews 
2.1 Acad vs. Fict 
2.2 Acad vs. News 
2.3 Acad vs. NotAcad 
2.4 Fict vs. News 
2.5 Fict vs. NotFict 
2
SI 
NNrm 
2.6 News vs. NotNews 
3.1 Acad vs. Fict 
3.2 Acad vs. News 
3.3 Acad vs. NotAcad 
3.4 Fict vs. News 
3.5 Fict vs. NotFict 
3 Nrm 
3.6 News vs. NotNews 
4.1 Acad vs. Fict 
4.2 Acad vs. News 
4.3 Acad vs. NotAcad 
4.4 Fict vs. News 
4.5 Fict vs. NotFict 
4
NSI 
NNrm 
4.6 News vs. NotNews 
5.1 Acad vs. Fict 
5.2 Acad vs. News 
5.3 Acad vs. NotAcad 
5.4 Fict vs. News 
5.5 Fict vs. NotFict 
5 Nrm 
5.6 News vs. NotNews 
6.1 Acad vs. Fict 
6.2 Acad vs. News 
6.3 Acad vs. NotAcad 
6.4 Fict vs. News 
6.5 Fict vs. NotFict 
6
Speaker-Oriented Adverbs 
Any 
NNrm 
6.6 News vs. NotNews 
7.1 Acad vs. Fict 
7.2 Acad vs. News 
7.3 Acad vs. NotAcad 
7 Union {1, 3}  
(Speaker-Oriented Adverbs) 
SI+NSI 
V
Nrm 
7.4 Fict vs. News 
117
7.5 Fict vs. NotFict 
7.6 News vs. NotNews 
8.1 Acad vs. Fict 
8.2 Acad vs. News 
8.3 Acad vs. NotAcad 
8.4 Fict vs. News 
8.5 Fict vs. NotFict 
8 Union {2, 4}   (Speaker-Oriented Adverbs) NNrm 
8.6 News vs. NotNews 
9.1 Acad vs. Fict 
9.2 Acad vs. News 
9.3 Acad vs. NotAcad 
9.4 Fict vs. News 
9.5 Fict vs. NotFict 
9 Nrm 
9.6 News vs. NotNews 
10.1 Acad vs. Fict 
10.2 Acad vs. News 
10.3 Acad vs. NotAcad 
10.4 Fict vs. News 
10.5 Fict vs. NotFict 
10 
SI 
NNrm 
10.6 News vs. NotNews 
11.1 Acad vs. Fict 
11.2 Acad vs. News 
11.3 Acad vs. NotAcad 
11.4 Fict vs. News 
11.5 Fict vs. NotFict 
11 Nrm 
11.6 News vs. NotNews 
12.1 Acad vs. Fict 
12.2 Acad vs. News 
12.3 Acad vs. NotAcad 
12.4 Fict vs. News 
12.5 Fict vs. NotFict 
12 
NSI 
NNrm 
12.6 News vs. NotNews 
13.1 Acad vs. Fict 
13.2 Acad vs. News 
13.3 Acad vs. NotAcad 
13.4 Fict vs. News 
13.5 Fict vs. NotFict 
13 Nrm 
13.6 News vs. NotNews 
14.1 Acad vs. Fict 
14.2 Acad vs. News 
14.3 Acad vs. NotAcad 
14.4 Fict vs. News 
14.5 Fict vs. NotFict 
14 
Speaker-Oriented Adverbs 
Any 
NNrm 
14.6 News vs. NotNews 
15.1 Acad vs. Fict 
15.2 Acad vs. News 
15.3 Acad vs. NotAcad 
15 Union {9, 11}  
(Speaker-Oriented Adverbs 
SI+NSI 
C
Nrm 
15.4 Fict vs. News 
118
15.5 Fict vs. NotFict 
15.6 News vs. NotNews 
16.1 Acad vs. Fict 
16.2 Acad vs. News 
16.3 Acad vs. NotAcad 
16.4 Fict vs. News 
16.5 Fict vs. NotFict 
16 Union {10, 12}  (Speaker-Oriented Adverbs) NNrm 
16.6 News vs. NotNews 
17.1 Acad vs. Fict 
17.2 Acad vs. News 
17.3 Acad vs. NotAcad 
17.4 Fict vs. News 
17.5 Fict vs. NotFict 
17 Nrm 
17.6 News vs. NotNews 
18.1 Acad vs. Fict 
18.2 Acad vs. News 
18.3 Acad vs. NotAcad 
18.4 Fict vs. News 
18.5 Fict vs. NotFict 
18 
Adjectives Any C 
NNrm 
18.6 News vs. NotNews 
19.1 Acad vs. Fict 
19.2 Acad vs. News 
19.3 Acad vs. NotAcad 
19.4 Fict vs. News 
19.5 Fict vs. NotFict 
19 Nrm 
19.6 News vs. NotNews 
20.1 Acad vs. Fict 
20.2 Acad vs. News 
20.3 Acad vs. NotAcad 
20.4 Fict vs. News 
20.5 Fict vs. NotFict 
20 
Subjective Adjectives Any C 
NNrm 
20.6 News vs. NotNews 
21.1 Acad vs. Fict 
21.2 Acad vs. News 
21.3 Acad vs. NotAcad 
21.4 Fict vs. News 
21.5 Fict vs. NotFict 
21 Nrm 
21.6 News vs. NotNews 
22.1 Acad vs. Fict 
22.2 Acad vs. News 
22.3 Acad vs. NotAcad 
22.4 Fict vs. News 
22.5 Fict vs. NotFict 
22 
Trait Adjectives Any C 
NNrm 
22.6 News vs. NotNews 
23.1 Acad vs. Fict 
23.2 Acad vs. News 
23.3 Acad vs. NotAcad 
23 Trait Adjectives Subset Any V Nrm 
23.4 Fict vs. News 
119
23.5 Fict vs. NotFict 
23.6 News vs. NotNews 
24.1 Acad vs. Fict 
24.2 Acad vs. News 
24.3 Acad vs. NotAcad 
24.4 Fict vs. News 
24.5 Fict vs. NotFict 
24 NNrm 
24.6 News vs. NotNews 
25.1 Acad vs. Fict 
25.2 Acad vs. News 
25.3 Acad vs. NotAcad 
25.4 Fict vs. News 
25.5 Fict vs. NotFict 
25 Nrm 
25.6 News vs. NotNews 
26.1 Acad vs. Fict 
26.2 Acad vs. News 
26.3 Acad vs. NotAcad 
26.4 Fict vs. News 
26.5 Fict vs. NotFict 
26 
C
NNrm 
26.6 News vs. NotNews 
27.1 Acad vs. Fict 
27.2 Acad vs. News 
27.3 Acad vs. NotAcad 
27.4 Fict vs. News 
27.5 Fict vs. NotFict 
27 Nrm 
27.6 News vs. NotNews 
28.1 Acad vs. Fict 
28.2 Acad vs. News 
28.3 Acad vs. NotAcad 
28.4 Fict vs. News 
28.5 Fict vs. NotFict 
28 
Adverbs+ Any C 
NNrm 
28.6 News vs. NotNews 
29.1 Acad vs. Fict 
29.2 Acad vs. News 
29.3 Acad vs. NotAcad 
29.4 Fict vs. News 
29.5 Fict vs. NotFict 
29 Nrm 
29.6 News vs. NotNews 
30.1 Acad vs. Fict 
30.2 Acad vs. News 
30.3 Acad vs. NotAcad 
30.4 Fict vs. News 
30.5 Fict vs. NotFict 
30 
Adverbs- Any C 
NNrm 
30.6 News vs. NotNews 
31.1 Acad vs. Fict 
31.2 Acad vs. News 
31.3 Acad vs. NotAcad 
31 ly-Adverbs Any C Nrm 
31.4 Fict vs. News 
120
31.5 Fict vs. NotFict 
31.6 News vs. NotNews 
32.1 Acad vs. Fict 
32.2 Acad vs. News 
32.3 Acad vs. NotAcad 
32.4 Fict vs. News 
32.5 Fict vs. NotFict 
32 NNrm 
32.6 News vs. NotNews 
33.1 Acad vs. Fict 
33.2 Acad vs. News 
33.3 Acad vs. NotAcad 
33.4 Fict vs. News 
33.5 Fict vs. NotFict 
33 Nrm 
33.6 News vs. NotNews 
34.1 Acad vs. Fict 
34.2 Acad vs. News 
34.3 Acad vs. NotAcad 
34.4 Fict vs. News 
34.5 Fict vs. NotFict 
34 
Random-ly-Adverbs Any C 
NNrm 
34.6 News vs. NotNews 
35.1 Acad vs. Fict 
35.2 Acad vs. News 
35.3 Acad vs. NotAcad 
35.4 Fict vs. News 
35.5 Fict vs. NotFict 
35 Nrm 
35.6 News vs. NotNews 
36.1 Acad vs. Fict 
36.2 Acad vs. News 
36.3 Acad vs. NotAcad 
36.4 Fict vs. News 
36.5 Fict vs. NotFict 
36 
Trait Adverbs Any C 
NNrm 
36.6 News vs. NotNews 
37.1 Acad vs. Fict 
37.2 Acad vs. News 
37.3 Acad vs. NotAcad 
37.4 Fict vs. News 
37.5 Fict vs. NotFict 
37 Nrm 
37.6 News vs. NotNews 
38.1 Acad vs. Fict 
38.2 Acad vs. News 
38.3 Acad vs. NotAcad 
38.4 Fict vs. News 
38.5 Fict vs. NotFict 
38 
V
NNrm 
38.6 News vs. NotNews 
39.1 Acad vs. Fict 
39.2 Acad vs. News 
39.3 Acad vs. NotAcad 
39 
Trait Adverbs Subset Any 
C Nrm 
39.4 Fict vs. News 
121
39.5 Fict vs. NotFict 
39.6 News vs. NotNews 
40.1 Acad vs. Fict 
40.2 Acad vs. News 
40.3 Acad vs. NotAcad 
40.4 Fict vs. News 
40.5 Fict vs. NotFict 
40 NNrm 
40.6 News vs. NotNews 
41.1 Acad vs. Fict 
41.2 Acad vs. News 
41.3 Acad vs. NotAcad 
41.4 Fict vs. News 
41.5 Fict vs. NotFict 
41 
Union {23, 37}  
(Trait Adjectives + Trait 
Adverbs Subsets) 
Nrm 
41.6 News vs. NotNews 
42.1 Acad vs. Fict 
42.2 Acad vs. News 
42.3 Acad vs. NotAcad 
42.4 Fict vs. News 
42.5 Fict vs. NotFict 
42 
Union {24, 38}  
(Trait Adjectives + Trait 
Adverbs Subsets) 
V
NNrm 
42.6 News vs. NotNews 
43.1 Acad vs. Fict 
43.2 Acad vs. News 
43.3 Acad vs. NotAcad 
43.4 Fict vs. News 
43.5 Fict vs. NotFict 
43 
Union {25, 39}  
(Trait Adjectives + Trait 
Adverbs Subsets) 
Nrm 
43.6 News vs. NotNews 
44.1 Acad vs. Fict 
44.2 Acad vs. News 
44.3 Acad vs. NotAcad 
44.4 Fict vs. News 
44.5 Fict vs. NotFict 
44 
Union {26, 40}  
(Trait Adjectives + Trait 
Adverbs Subsets) 
Any 
C
NNrm 
44.6 News vs. NotNews 
45.1 Acad vs. Fict 
45.2 Acad vs. News 
45.3 Acad vs. NotAcad 
45.4 Fict vs. News 
45.5 Fict vs. NotFict 
45 
Union {5, 23, 37}  
(Speaker-Oriented Adverbs + 
Trait Adjectives + Trait 
Adverbs Subsets) 
Nrm 
45.6 News vs. NotNews 
46.1 Acad vs. Fict 
46.2 Acad vs. News 
46.3 Acad vs. NotAcad 
46.4 Fict vs. News 
46.5 Fict vs. NotFict 
46 
Union {6, 24, 38}  
(Speaker-Oriented Adverbs + 
Trait Adjectives + Trait 
Adverbs Subsets) 
Any V 
NNrm 
46.6 News vs. NotNews 
47.1 Acad vs. Fict 
47.2 Acad vs. News 
47.3 Acad vs. NotAcad 
47 Union {13, 17, 19, 21, 25, 27, 
29, 31, 35, 39}  
(Speaker-Oriented Adverbs, 
Adjectives, Subjective 
Any C Nrm 
47.4 Fict vs. News 
122
47.5 Fict vs. NotFict Adjectives, Trait Adjectives, 
Trait Adjectives Subset, 
Adverbs+, Adverbs-, ly-
Adverbs, Trait Adverbs, Trait 
Adverbs Subset 
47.6 News vs. NotNews 
48.1 Acad vs. Fict 
48.2 Acad vs. News 
48.3 Acad vs. NotAcad 
48.4 Fict vs. News 
48.5 Fict vs. NotFict 
48 
Union {14, 18, 20, 22, 26, 28, 
30, 32, 36, 40}  
(Speaker-Oriented Adverbs, 
Adjectives, Subjective 
Adjectives, Trait Adjectives, 
Trait Adjectives Subset, 
Adverbs+, Adverbs-, ly-
Adverbs, Trait Adverbs, Trait 
Adverbs Subset 
NNrm 
48.6 News vs. NotNews 
49.1 Acad vs. Fict 
49.2 Acad vs. News 
49.3 Acad vs. NotAcad 
49.4 Fict vs. News 
49.5 Fict vs. NotFict 
49 Nrm 
49.6 News vs. NotNews 
50.1 Acad vs. Fict 
50.2 Acad vs. News 
50.3 Acad vs. NotAcad 
50.4 Fict vs. News 
50.5 Fict vs. NotFict 
50 
Nouns Any C 
NNrm 
50.6 News vs. NotNews 
51.1 Acad vs. Fict 
51.2 Acad vs. News 
51.3 Acad vs. NotAcad 
51.4 Fict vs. News 
51.5 Fict vs. NotFict 
51 Nrm 
51.6 News vs. NotNews 
52.1 Acad vs. Fict 
52.2 Acad vs. News 
52.3 Acad vs. NotAcad 
52.4 Fict vs. News 
52.5 Fict vs. NotFict 
52 
Verbs Any C 
NNrm 
52.6 News vs. NotNews 
53.1 Acad vs. Fict 
53.2 Acad vs. News 
53.3 Acad vs. NotAcad 
53.4 Fict vs. News 
53.5 Fict vs. NotFict 
53 Nrm 
53.6 News vs. NotNews 
54.1 Acad vs. Fict 
54.2 Acad vs. News 
54.3 Acad vs. NotAcad 
54.4 Fict vs. News 
54.5 Fict vs. NotFict 
54 
Punctuation Any C 
NNrm 
54.6 News vs. NotNews 
123
Abbreviations 
SI Sentence-Initial Position NSI Not Sentence-Initial Position 
SI+NSI 
Sentence-Initial Position + Not 
Sentence-Initial Position Any Any Sentence Position 
V Vector C Count 
Nrm Normalized NNrm Non-Normalized 
124
Appendix 6.1: Experimental Scripts 
Appendix 6.1, Figure 1 
Perl Script to Extract Linguistic Features from BNC2 Tagged Texts 
(Created by Bing Bai, August 2005) 
 
 
Script name: extractPOS.perl 
#!/usr/bin/perl 
 
open FILE, $ARGV[0]; 
@lines=<FILE>; 
close FILE; 
foreach $line(@lines){ 
@fields=split(/</,$line); 
foreach $field(@fields){ 
if($field=~/$ARGV[1]/){ 
print "$field\n"; 
}
}
}
# Copy perl script to directory that contains the documents to be processed 
(Example: All files under directory “B”). 
 
# Change permissions: chmod +x extractPOS.perl 
 
# Change shell: bash 
 
# Run script (example: AV0 = adverbs) 
 
Line Command: 
 
for i in `ls */*` ; do  ./extractPOS.perl $i AV0 ; done > output_file Appendix 6.1, Figure 2 
Perl Script to Strip Tags and Meta-Data from BNC2 Tagged Texts to  
Create Raw Texts (Created by Bing Bai, August 2005) 2. Script name: strip.perl 
#!/usr/bin/perl 
 
while($line=<STDIN>){ 
 if($line=~/teiHder/){ 
 }else{ 
 $line=~s/<[^<>]*>//g; 
 print $line; 
 } 
}
125
Appendix 6.1, Figure 3 
UNIX Command to Extract Speaker-Oriented Adverbs in the  
Sentence-Initial Position, Capitalized, Followed by a Comma. 
 
 
grep -r 'Clearly,' [directory] > output Appendix 6.1, Figure 4 
SPSS Syntax to Create Training/Testing Document Sets 
 
Full Corpus: 50% Training / 50% Testing  
filter_$ (1) = Selected Cases = Training Cases. 
filter_$ (0) = Not Selected Cases = Testing Cases. 
 
USE ALL. 
COMPUTE filter_$=(uniform(1)<=.50). 
VARIABLE LABEL filter_$ 'Approximately 50 % of cases (SAMPLE)'. 
FORMAT filter_$ (f1.0). 
FILTER BY filter_$. 
EXECUTE . Appendix 6.1, Figure 5 
SPSS Class Codes by Genre Academic--Not Academic  
Academic  acad=1 
Not Academic  acad=0 
 
Fiction--Not Fiction  
Fiction   fict=1 
Not Fiction  fict=0 
 
News--Not News  
News   news=1 
Not News  news=0 
 
Academic--Fiction  
Academic  acadfict=1 
 Fiction  acadfict=2 
 
Academic--News 
Academic  acadnews=1 
 News   acadnews=2 
 
Fiction--News  
Fiction   fictnews=1 
News fictnews=2
126
Appendix 6.1, Figure 6 
SPSS Filters to Select Corpora 
 
Select Academic – Fiction 
 
Filter_0 
 
USE ALL. 
COMPUTE filter_0=(acadfict  > 0). 
VARIABLE LABEL filter_0 'acadfict  > 0 (FILTER)'. 
VALUE LABELS filter_0  0 'Not Selected' 1 'Selected'. 
FORMAT filter_0 (f1.0). 
FILTER BY filter_0. 
EXECUTE . 
 
Select Academic – News 
 
Filter_1 
 
USE ALL. 
COMPUTE filter_1=(acadnews  > 0). 
VARIABLE LABEL filter_1 ‘acadnews  > 0 (FILTER)'. 
VALUE LABELS filter_1  0 'Not Selected' 1 'Selected'. 
FORMAT filter_1 (f1.0). 
FILTER BY filter_1. 
EXECUTE . 
 
Select Fiction—News 
 
Filter_2  
 
USE ALL. 
COMPUTE filter_2=(fictnews > 0). 
VARIABLE LABEL filter_2 'fictnews > 0 (FILTER)'. 
VALUE LABELS filter_2  0 'Not Selected' 1 'Selected'. 
FORMAT filter_2 (f1.0). 
FILTER BY filter_2. 
EXECUTE . 
 
127
Appendix 6.1, Figure 7 
SPSS Sample Code to Run Discriminant Analysis 
 
News (news=1) vs. Not News (news=0) 
Non-normalized 30 “vectorized” speaker-oriented adverbs in sentence-initial position  
 
DISCRIMINANT 
 /GROUPS=news(0 1) 
 /VARIABLES=amaz_tp amaz_tk brie_tp brie_tk cand_tp cand_tk cert_tp 
 cert_tk clea_tp clea_tk conf_tp conf_tk curi_tp curi_tk defi_tp 
 defi_tk fran_tp fran_tk gene_tp gene_tk hone_tp hone_tk idea_tp 
 idea_tk luck_tp luck_tk mayb_tp mayb_tk nece_tp nece_tk norm_tp 
 norm_tk obvi_tp obvi_tk oddl_tp oddl_tk poss_tp poss_tk pred_tp 
 pred_tk pref_tp pref_tk prob_tp prob_tk roug_tp roug_tk seri_tp 
 seri_tk simp_tp simp_tk spec_tp spec_tk stra_tp stra_tk sure_tp 
 sure_tk surp_tp surp_tk unfo_tp unfo_tk 
 /SELECT=filter_$(1) 
 /ANALYSIS ALL 
 /SAVE=CLASS SCORES 
 /METHOD=WILKS 
 /FIN= 3.84 
 /FOUT= 2.71 
 /PRIORS  = .87, .13 
 /HISTORY 
 /STATISTICS=RAW TABLE 
 /CLASSIFY=NONMISSING POOLED . 
 
128
Appendix 6.2: Variables in SPSS Table 
 
Variable Label Variable Label 
docid Document ID wordct Document length 
genre Genre     
Acd 
Academic (acad=1), Not Academic 
(acad=0) fict Fiction (fict=1), Not Fiction (fict=0) 
news 
News (news=1), Not News 
(news=0) acadfict 
Academic (acadfict=1), Fiction 
(acadfict=2) 
acadnews 
Academic (acadnews=1), News 
(acadnews=2) fictnews 
Fiction (fictnews=1), News 
(fictnews=2) 
filter_$ 50% Training—50% Testing filter$a Select Testing Cases for ROC Curve 
filter_0 Select “Academic” vs. “Fiction” filter_1 Select “Academic” vs. “News” 
filter_2 Select “Fiction” vs. “News”     
spk_tp SOA Tp Hd spk_tpn SOA Tp Nrm Hd 
spk_tk SOA Tk Hd spk_tkn SOA Tk Nrm Hd 
spkrtp SOA Tp Any spkrtpn SOA Tp Nrm Any 
spkrtk SOA Tk Any spkrtkn SOA Tk Nrm Any 
spkr2tp SOA Tp NtHd spkr2tpn SOA Tp Nrm NtHd 
spkr2tk SOA Tk NtHd spkr2tkn SOA Tk Nrm NtHd 
adj_tp Adj Tp adj_tpn Adj Tp Nrm 
adj_tk Adj Tk adj_tkn Adj Tk Nrm 
all_rbtp All Adverb Tp allrbtpn All Adverb Tp Nrm 
all_rbtk All Adverb Tk allrbtkn All Adverb Tk Nrm 
avp_tp Adv Particle Tp avp_tpn Adv Particle Tp Nrm 
avp_tk Adv Particle Tk avp_tkn Adv Particle Tk Nrm 
avq_tp Wh-Adv Tp avq_tpn Wh-Adv Tp Nrm 
avq_tk Wh-Adv Tk avq_tkn Wh-Adv Tk Nrm 
adv_tp Adv Tp adv_tpn Adv Tp Nrm 
adv_tk Adv Tk adv_tkn Adv Tk Nrm 
alp_tp Alpha Symbol Tp alp_tpn Alpha Symbol Tp Nrm 
alp_tk Alpha Symbol Tk alp_tkn Alpha Symbol Tk Nrm 
art_tp Article Tp art_tpn Article Tp Nrm 
art_tk Article Tk art_tkn Article Tk Nrm 
con_tp Conjunction Tp con_tpn Conjunction Tp Nrm 
con_tk Conjunction Tk con_tkn Conjunction Tk Nrm 
det_tp Determiner Tp det_tpn Determiner Tp Nrm 
det_tk Determiner Tk det_tkn Determiner Tk Nrm 
int_tp Interjection Tp int_tpn Interjection Tp Nrm 
int_tk Interjection Tk int_tkn Interjection Tk Nrm 
neg_tp Negative Tp neg_tpn Negative Tp Nrm 
neg_tk Negative Tk neg_tkn Negative Tk Nrm 
nn_tp Noun Tp nn_tpn Noun Tp Nrm 
nn_tk Noun Tk nn_tkn Noun Tk Nrm 
num_tp Ordinal/Cardinal Tp num_tpn Ordinal/Cardinal Tp Nrm 
num_tk Ordinal/Cardinal Tk num_tkn Ordinal/Cardinal Tk Nrm 
pos_tp Possessive Tp pos_tpn Possessive Tp Nrm 
pos_tk Possessive Tk pos_tkn Possessive Tk Nrm 
129
prf_tp Prep "of" Tp prf_tpn Prep "of" Tp Nrm 
prf_tk Prep "of" Tk prf_tkn Prep "of" Tk Nrm 
prp_tp Preposition Tp prp_tpn Preposition Tp Nrm 
prp_tk Preposition Tk prp_tkn Preposition Tk Nrm 
pro_tp Pronoun Tp pro_tpn Pronoun Tp Nrm 
pro_tk Pronoun Tk pro_tkn Pronoun Tk Nrm 
pun_tp Punc Tp pun_tpn Punc Tp Nrm 
pun_tk Punc Tk pun_tkn Punc Tk Nrm 
the_tp Existential "there" Tp the_tpn Existential "there" Tp Nrm 
the_tk Existential "there" Tk the_tkn Existential "there" Tk Nrm 
to_tp Infinitive "to" Tp to_tpn Infinitive "to" Tp Nrm 
to_tk Infinitive "to" Tk to_tkn Infinitive "to" Tk Nrm 
unc_tp Unclassified Tp unc_tpn Unclassified Tp Nrm 
unc_tk Unclassified Tk unc_tkn Unclassified Tk Nrm 
vb_tp Verb Tp vb_tpn Verb Tp Nrm 
vb_tk Verb Tk vb_tkn Verb Tk Nrm 
ly_tp ly-Adverb Tp ly_tpn ly-Adverb Tp Nrm 
ly_tk ly-Adverb Tk ly_tkn ly-Adverb Tk Nrm 
randtp1 30 Random ly-Adverb Tp randtpn1 30 Random ly-Adverb Tp Nrm 
randtk1 30 Random ly-Adverb Tk randtkn1 30 Random ly-Adverb Tk Nrm 
amaz_tp amazingly Tp Hd amaz_tpn amazingly Tp Nrm Hd 
amaz_tk amazingly Tk Hd amaz_tkn amazingly Tk Nrm Hd 
brie_tp briefly Tp Hd brie_tpn briefly Tp Nrm Hd 
brie_tk briefly Tk Hd brie_tkn briefly Tk Nrm Hd 
cand_tp candidly Tp Hd cand_tpn candidly Tp Nrm Hd 
cand_tk candidly Tk Hd cand_tkn candidly Tk Nrm Hd 
cert_tp certainly Tp Hd cert_tpn certainly Tp Nrm Hd 
cert_tk certainly Tk Hd cert_tkn certainly Tk Nrm Hd 
clea_tp clearly Tp Hd clea_tpn clearly Tp Nrm Hd 
clea_tk clearly Tk Hd clea_tkn clearly Tk Nrm Hd 
conf_tp confidently Tp Hd conf_tpn confidently Tp Nrm Hd 
conf_tk confidently Tk Hd conf_tkn confidently Tk Nrm Hd 
curi_tp curiously Tp Hd curi_tpn curiously Tp Nrm Hd 
curi_tk curiously Tk Hd curi_tkn curiously Tk Nrm Hd 
defi_tp definitely Tp Hd defi_tpn definitely Tp Nrm Hd 
defi_tk definitely Tk Hd defi_tkn definitely Tk Nrm Hd 
fran_tp frankly Tp Hd fran_tpn frankly Tp Nrm Hd 
fran_tk frankly Tk Hd fran_tkn frankly Tk Nrm Hd 
gene_tp generally Tp Hd gene_tpn generally Tp Nrm Hd 
gene_tk generally Tk Hd gene_tkn generally Tk Nrm Hd 
hone_tp honestly Tp Hd hone_tpn honestly Tp Nrm Hd 
hone_tk honestly Tk Hd hone_tkn honestly Tk Nrm Hd 
idea_tp ideally Tp Hd idea_tpn ideally Tp Nrm Hd 
idea_tk ideally Tk Hd idea_tkn ideally Tk Nrm Hd 
luck_tp luckily Tp Hd luck_tpn luckily Tp Nrm Hd 
luck_tk luckily Tk Hd luck_tkn luckily Tk Nrm Hd 
mayb_tp maybe Tp Hd mayb_tpn maybe Tp Nrm Hd 
mayb_tk maybe Tk Hd mayb_tkn maybe Tk Nrm Hd 
130
nece_tp necessarily Tp Hd nece_tpn necessarily Tp Nrm Hd 
nece_tk necessarily Tk Hd nece_tkn necessarily Tk Nrm Hd 
norm_tp normally Tp Hd norm_tpn normally Tp Nrm Hd 
norm_tk normally Tk Hd norm_tkn normally Tk Nrm Hd 
obvi_tp obviously Tp Hd obvi_tpn obviously Tp Nrm Hd 
obvi_tk obviously Tk Hd obvi_tkn obviously Tk Nrm Hd 
oddl_tp oddly Tp Hd oddl_tpn oddly Tp Nrm Hd 
oddl_tk oddly Tk Hd oddl_tkn oddly Tk Nrm Hd 
poss_tp possibly Tp Hd poss_tpn possibly Tp Nrm Hd 
poss_tk possibly Tk Hd poss_tkn possibly Tk Nrm Hd 
pred_tp predictably Tp Hd pred_tpn predictably Tp Nrm Hd 
pred_tk predictably Tk Hd pred_tkn predictably Tk Nrm Hd 
pref_tp preferably Tp Hd pref_tpn preferably Tp Nrm Hd 
pref_tk preferably Tk Hd pref_tkn preferably Tk Nrm Hd 
prob_tp probably Tp Hd prob_tpn probably Tp Nrm Hd 
prob_tk probably Tk Hd prob_tkn probably Tk Nrm Hd 
roug_tp roughly Tp Hd roug_tpn roughly Tp Nrm Hd 
roug_tk roughly Tk Hd roug_tkn roughly Tk Nrm Hd 
seri_tp seriously Tp Hd seri_tpn seriously Tp Nrm Hd 
seri_tk seriously Tk Hd seri_tkn seriously Tk Nrm Hd 
simp_tp simply Tp Hd simp_tpn simply Tp Nrm Hd 
simp_tk simply Tk Hd simp_tkn simply Tk Nrm Hd 
spec_tp specifically Tp Hd spec_tpn specifically Tp Nrm Hd 
spec_tk specifically Tk Hd spec_tkn specifically Tk Nrm Hd 
stra_tp strangely Tp Hd stra_tpn strangely Tp Nrm Hd 
stra_tk strangely Tk Hd stra_tkn strangely Tk Nrm Hd 
sure_tp surely Tp Hd sure_tpn surely Tp Nrm Hd 
sure_tk surely Tk Hd sure_tkn surely Tk Nrm Hd 
surp_tp surprisingly Tp Hd surp_tpn surprisingly Tp Nrm Hd 
surp_tk surprisingly Tk Hd surp_tkn surprisingly Tk Nrm Hd 
unfo_tp unfortunately Tp Hd unfo_tpn unfortunately Tp Nrm Hd 
unfo_tk unfortunately Tk Hd unfo_tkn unfortunately Tk Nrm Hd 
trait_tp TAdj Tp traittpn TAdj Tp Nrm 
trait_tk TAdj Tk traittkn TAdj Tk Nrm 
tradvtp TAdv Tp tradvtpn TAdv Tp Nrm 
tradvtk TAdv Tk tradvtkn TAdv Tk Nrm 
subjjtp SbjAdj Tp subjjtpn SbjAdj Tp Nrm 
subjjtk SbjAdj Tk subjjtkn SbjAdj Tk Nrm 
ivtrjjp TAdj2 Tp ivtrjjpn TAdj2 Tp Nrm 
ivtrjjk TAdj2 Tk ivtrjjkn TAdj2 Tk Nrm 
ivtrrbp TAdv2 Tp ivtrrbpn TAdv2 Tp Nrm 
ivtrrbk TAdv2 Tk ivtrrbkn TAdv2 Tk Nrm 
amaz1tp amazingly Tp Any amaz1tpn amazingly Tp Any 
amaz1tk amazingly Tk Any amaz1tkn amazingly Tk Nrm Any 
brie1tp briefly Tp Any brie1tpn briefly Tp Nrm Any 
brie1tk briefly Tk Any brie1tkn briefly Tk Nrm Any 
cand1tp candidly Tp Any cand1tpn candidly Tp Nrm Any 
cand1tk candidly Tk Any cand1tkn candidly Tk Nrm Any 
cert1tp certainly Tp Any cert1tpn certainly Tp Nrm Any 
131
cert1tk certainly Tk Any cert1tkn certainly Tk Nrm Any 
clea1tp clearly Tp Any clea1tpn clearly Tp Nrm Any 
clea1tk clearly Tk Any clea1tkn clearly Tk Nrm Any 
conf1tp confidently Tp Any conf1tpn confidently Tp Nrm Any 
conf1tk confidently Tk Any conf1tkn confidently Tk Nrm Any 
curi1tp curiously Tp Any curi1tpn curiously Tp Nrm Any 
curi1tk curiously Tk Any curi1tkn curiously Tk Nrm Any 
defi1tp definitely Tp Any defi1tpn definitely Tp Nrm Any 
defi1tk definitely Tk Any defi1tkn definitely Tk Nrm Any 
fran1tp frankly Tp Any fran1tpn frankly Tp Nrm Any 
fran1tk frankly Tk Any fran1tkn frankly Tk Nrm Any 
gene1tp generally Tp Any gene1tpn generally Tp Nrm Any 
gene1tk generally Tk Any gene1tkn generally Tk Nrm Any 
hone1tp honestly Tp Any hone1tpn honestly Tp Nrm Any 
hone1tk honestly Tk Any hone1tkn honestly Tk Nrm Any 
idea1tp ideally Tp Any idea1tpn ideally Tp Nrm Any 
idea1tk ideally Tk Any idea1tkn ideally Tk Nrm Any 
luck1tp luckily Tp Any luck1tpn luckily Tp Nrm Any 
luck1tk luckily Tk Any luck1tkn luckily Tk Nrm Any 
mayb1tp maybe Tp Any mayb1tpn maybe Tp Nrm Any 
mayb1tk maybe Tk Any mayb1tkn maybe Tk Nrm Any 
nece1tp necessarily Tp Any nece1tpn necessarily Tp Nrm Any 
nece1tk necessarily Tk Any nece1tkn necessarily Tk Nrm Any 
norm1tp normally Tp Any norm1tpn normally Tp Nrm Any 
norm1tk normally Tk Any norm1tkn normally Tk Nrm Any 
obvi1tp obviously Tp Any obvi1tpn obviously Tp Nrm Any 
obvi1tk obviously Tk Any obvi1tkn obviously Tk Nrm Any 
oddl1tp oddly Tp Any oddl1tpn oddly Tp Nrm Any 
oddl1tk oddly Tk Any oddl1tkn oddly Tk Nrm Any 
poss1tp possibly Tp Any poss1tpn possibly Tp Nrm Any 
poss1tk possibly Tk Any poss1tkn possibly Tk Nrm Any 
pred1tp predictably Tp Any pred1tpn predictably Tp Nrm Any 
pred1tk predictably Tk Any pred1tkn predictably Tk Nrm Any 
pref1tp preferably Tp Any pref1tpn preferably Tp Nrm Any 
pref1tk preferably Tk Any pref1tkn preferably Tk Nrm Any 
prob1tp probably Tp Any prob1tpn probably Tp Nrm Any 
prob1tk probably Tk Any prob1tkn probably Tk Nrm Any 
roug1tp roughly Tp Any roug1tpn roughly Tp Nrm Any 
roug1tk roughly Tk Any roug1tkn roughly Tk Nrm Any 
seri1tp seriously Tp Any seri1tpn seriously Tp Nrm Any 
seri1tk seriously Tk Any seri1tkn seriously Tk Nrm Any 
simp1tp simply Tp Any simp1tpn simply Tp Nrm Any 
simp1tk simply Tk Any simp1tkn simply Tk Nrm Any 
spec1tp specifically Tp Any spec1tpn specifically Tp Nrm Any 
spec1tk specifically Tk Any spec1tkn specifically Tk Nrm Any 
stra1tp strangely Tp Any stra1tpn strangely Tp Nrm Any 
stra1tk strangely Tk Any stra1tkn strangely Tk Nrm Any 
sure1tp surely Tp Any sure1tpn surely Tp Nrm Any 
132
sure1tk surely Tk Any sure1tkn surely Tk Nrm Any 
surp1tp surprisingly Tp Any surp1tpn surprisingly Tp Nrm Any 
surp1tk surprisingly Tk Any surp1tkn surprisingly Tk Nrm Any 
unfo1tp unfortunately Tp Any unfo1tpn unfortunately Tp Nrm Any 
unfo1tk unfortunately Tk Any unfo1tkn unfortunately Tk Nrm Any 
avaritp avariciously Tp avaritpn avariciously Tp Nrm 
avaritk avariciously Tk avaritkn avariciously Tk Nrm 
badltp badly Tp badltpn badly Tp Nrm 
badltk badly Tk badltkn badly Tk Nrm 
calcutp calculatingly Tp calcutpn calculatingly Tp Nrm 
calcutk calculatingly Tk calcutkn calculatingly Tk Nrm 
decentp decently Tp decentpn decently Tp Nrm 
decentk decently Tk decentkn decently Tk Nrm 
deceptp deceptively Tp deceptpn deceptively Tp Nrm 
deceptk deceptively Tk deceptkn deceptively Tk Nrm 
dishotp dishonestly Tp dishotpn dishonestly Tp Nrm 
dishotk dishonestly Tk dishotkn dishonestly Tk Nrm 
dislotp disloyally Tp dislotpn disloyally Tp Nrm 
dislotk disloyally Tk dislotkn disloyally Tk Nrm 
ethictp ethically Tp ethictpn ethically Tp Nrm 
ethictk ethically Tk ethictkn ethically Tk Nrm 
fairltp fairly Tp fairltpn fairly Tp Nrm 
fairltk fairly Tk fairltkn fairly Tk Nrm 
faithtp faithfully Tp faithtpn faithfully Tp Nrm 
faithtk faithfully Tk faithtkn faithfully Tk Nrm 
hypoctp hypocritically Tp hypoctpn hypocritically Tp Nrm 
hypoctk hypocritically Tk hypoctkn hypocritically Tk Nrm 
insintp insincerely Tp insintpn insincerely Tp Nrm 
insintk insincerely Tk insintkn insincerely Tk Nrm 
intritp intriguingly Tp intritpn intriguingly Tp Nrm 
intritk intriguingly Tk intritkn intriguingly Tk Nrm 
justltp justly Tp justltpn justly Tp Nrm 
justltk justly Tk justltkn justly Tk Nrm 
loyaltp loyally Tp loyaltpn loyally Tp Nrm 
loyaltk loyally Tk loyaltkn loyally Tk Nrm 
lustftp lustfully Tp lustftpn lustfully Tp Nrm 
lustftk lustfully Tk lustftkn lustfully Tk Nrm 
malictp maliciously Tp malictpn maliciously Tp Nrm 
malictk maliciously Tk malictkn maliciously Tk Nrm 
matertp materialistically Tp matertpn materialistically Tp Nrm 
matertk materialistically Tk matertkn materialistically Tk Nrm 
moraltp morally Tp moraltpn morally Tp Nrm 
moraltk morally Tk moraltkn morally Tk Nrm 
naturtp naturally Tp naturtpn naturally Tp Nrm 
naturtk naturally Tk naturtkn naturally Tk Nrm 
nobltp nobly Tp nobltpn nobly Tp Nrm 
nobltk nobly Tk nobltkn nobly Tk Nrm 
perfitp perfidiously Tp perfitpn perfidiously Tp Nrm 
133
perfitk perfidiously Tk perfitkn perfidiously Tk Nrm 
pharitp pharisaically Tp pharitpn pharisaically Tp Nrm 
pharitk pharisaically Tk pharitkn pharisaically Tk Nrm 
rapactp rapaciously Tp rapactpn rapaciously Tp Nrm 
rapactk rapaciously Tk rapactkn rapaciously Tk Nrm 
righttp righteously Tp righttpn righteously Tp Nrm 
righttk righteously Tk righttkn righteously Tk Nrm 
sincetp sincerely Tp sincetpn sincerely Tp Nrm 
sincetk sincerely Tk sincetkn sincerely Tk Nrm 
truthtp truthfully Tp truthtpn truthfully Tp Nrm 
truthtk truthfully Tk truthtkn truthfully Tk Nrm 
undertp underhandedly Tp undertpn underhandedly Tp Nrm 
undertk underhandedly Tk undertkn underhandedly Tk Nrm 
unfaitp unfaithfully Tp unfaitpn unfaithfully Tp Nrm 
unfaitk unfaithfully Tk unfaitkn unfaithfully Tk Nrm 
unreltp unreliably Tp unreltpn unreliably Tp Nrm 
unreltk unreliably Tk unreltkn unreliably Tk Nrm 
unscrtp unscrupulously Tp unscrtpn unscrupulously Tp Nrm 
unscrtk unscrupulously Tk unscrtkn unscrupulously Tk Nrm 
untrutp untruthfully Tp untrutpn untruthfully Tp Nrm 
untrutk untruthfully Tk untrutkn untruthfully Tk Nrm 
uprigtp uprightly Tp uprigtpn uprightly Tp Nrm 
uprigtk uprightly Tk uprigtkn uprightly Tk Nrm 
valiatp valiantly Tp valiatpn valiantly Tp Nrm 
valiatk valiantly Tk valiatkn valiantly Tk Nrm 
virtutp virtuously Tp virtutpn virtuously Tp Nrm 
virtutk virtuously Tk virtutkn virtuously Tk Nrm 
vulgatp vulgarly Tp vulgatpn vulgarly Tp Nrm 
vulgatk vulgarly Tk vulgatkn vulgarly Tk Nrm 
avartp avaricious Tp avartpn avaricious Tp Nrm 
avartk avaricious Tk avartkn avaricious Tk Nrm 
bad2tp bad Tp bad2tpn bad Tp Nrm 
bad2tk bad Tk bad2tkn bad Tk Nrm 
biastp biased Tp biastpn biased Tp Nrm 
biastk biased Tk biastkn biased Tk Nrm 
calctp calculating Tp calctpn calculating Tp Nrm 
calctk calculating Tk calctkn calculating Tk Nrm 
chartp characterless Tp chartpn characterless Tp Nrm 
chartk characterless Tk chartkn characterless Tk Nrm 
decetp decent Tp decetpn decent Tp Nrm 
decetk decent Tk decetkn decent Tk Nrm 
dece2tp deceptive Tp dece2tpn deceptive Tp Nrm 
dece2tk deceptive Tk dece2tkn deceptive Tk Nrm 
dishtp dishonest Tp dishtpn dishonest Tp Nrm 
dishtk dishonest Tk dishtkn dishonest Tk Nrm 
disltp disloyal Tp disltpn disloyal Tp Nrm 
disltk disloyal Tk disltkn disloyal Tk Nrm 
ethitp ethical Tp ethitpn ethical Tp Nrm 
134
ethitk ethical Tk ethitkn ethical Tk Nrm 
fairtp fair Tp fairtpn fair Tp Nrm 
fairtk fair Tk fairtkn fair Tk Nrm 
faittp faithful Tp faittpn faithful Tp Nrm 
faittk faithful Tk faittkn faithful Tk Nrm 
frantp frank Tp frantpn frank Tp Nrm 
frantk frank Tk frantkn frank Tk Nrm 
honetp honest Tp honetpn honest Tp Nrm 
honetk honest Tk honetkn honest Tk Nrm 
hyptp hypocritical Tp hyptpn hypocritical Tp Nrm 
hyptk hypocritical Tk hyptkn hypocritical Tk Nrm 
insitp insincere Tp insitpn insincere Tp Nrm 
insitk insincere Tk insitkn insincere Tk Nrm 
intrtp intriguing Tp intrtpn intriguing Tp Nrm 
intrtk intriguing Tk intrtkn intriguing Tk Nrm 
justp just Tp justpn just Tp Nrm 
justk just Tk justkn just Tk Nrm 
loyatp loyal Tp loyatpn loyal Tp Nrm 
loyatk loyal Tk loyatkn loyal Tk Nrm 
lusttp lustful Tp lusttpn lustful Tp Nrm 
lusttk lustful Tk lusttkn lustful Tk Nrm 
lyintp lying Tp lyintpn lying Tp Nrm 
lyintk lying Tk lyintkn lying Tk Nrm 
malitp malicious Tp malitpn malicious Tp Nrm 
malitk malicious Tk malitkn malicious Tk Nrm 
matetp materialistic Tp matetpn materialistic Tp Nrm 
matetk materialistic Tk matetkn materialistic Tk Nrm 
merctp mercenary Tp merctpn mercenary Tp Nrm 
merctk mercenary Tk merctkn mercenary Tk Nrm 
moratp moral Tp moratpn moral Tp Nrm 
moratk moral Tk moratkn moral Tk Nrm 
natutp natural Tp natutpn natural Tp Nrm 
natutk natural Tk natutkn natural Tk Nrm 
nobtp noble Tp nobtpn noble Tp Nrm 
nobtk noble Tk nobtkn noble Tk Nrm 
perftp perfidious Tp perftpn perfidious Tp Nrm 
perftk perfidious Tk perftkn perfidious Tk Nrm 
phartp pharisaical Tp phartpn pharisaical Tp Nrm 
phartk pharisaical Tk phartkn pharisaical Tk Nrm 
printp principled Tp printpn principled Tp Nrm 
printk principled Tk printkn principled Tk Nrm 
rapatp rapacious Tp rapatpn rapacious Tp Nrm 
rapatk rapacious Tk rapatkn rapacious Tk Nrm 
rightp righteous Tp rightpn righteous Tp Nrm 
rightk righteous Tk rightkn righteous Tk Nrm 
sinctp sincere Tp sinctpn sincere Tp Nrm 
sinctk sincere Tk sinctkn sincere Tk Nrm 
trustp trustworthy Tp trustpn trustworthy Tp Nrm 
135
trustk trustworthy Tk trustkn trustworthy Tk Nrm 
truttp truthful Tp truttpn truthful Tp Nrm 
truttk truthful Tp truttkn truthful Tp Nrm 
undetp underhanded Tp undetpn underhanded Tp Nrm 
undetk underhanded Tk undetkn underhanded Tk Nrm 
unfatp unfaithful Tp unfatpn unfaithful Tp Nrm 
unfatk unfaithful Tk unfatkn unfaithful Tk Nrm 
unretp unreliable Tp unretpn unreliable Tp Nrm 
unretk unreliable Tk unretkn unreliable Tk Nrm 
unsctp unscrupulous Tp unsctpn unscrupulous Tp Nrm 
unsctk unscrupulous Tk unsctkn unscrupulous Tk Nrm 
untrtp untruthful Tp untrtpn untruthful Tp Nrm 
untrtk untruthful Tk untrtkn untruthful Tk Nrm 
upritp upright Tp upritpn upright Tp Nrm 
upritk upright Tk upritkn upright Tk Nrm 
venatp venal Tp venatpn venal Tp Nrm 
venatk venal Tk venatkn venal Tk Nrm 
virttp virtuous Tp virttpn virtuous Tp Nrm 
virttk virtuous Tk virttkn virtuous Tk Nrm 
vulgtp vulgar Tp vulgtpn vulgar Tp Nrm 
vulgtk vulgar Tk vulgtkn vulgar Tk Nrm 
136
Appendix 7.1: Results Table (324 Tests / Models) 
 
Method Feature Representation Problem 
Sent 
Pos 
V /
C
Nrm / 
NNrm One-Against-One One-Against-Many 
Acad 
vs. 
Fict 
Acad 
vs. 
News 
Fict 
vs. 
News 
Acad 
vs. 
Not-
Acad 
Fict 
vs. 
Not-
Fict 
News 
vs. 
Not- 
News 
MODEL 
.1 .2 .4 .3 .5 .6 
ACCURACY GAIN 
1 SOA SI V Nrm 40.2 39.4 39.4 -8.8 2.5 0.8 
2 SOA SI V NNrm 41.0 39.8 41.0 -5.3 10.7 0.8 
3 SOA NSI V Nrm 91.6 75.6 69.8 -1.8 16.4 -1.5 
4 SOA NSI V NNrm 87.0 74.2 64.2 8.0 44.3 0.8 
5 SOA Any V Nrm 91.6 73.8 71.2 -3.5 14.8 -0.8 
6 SOA Any V NNrm 88.2 77.6 65.8 9.7 42.6 0.0 
7 {1,3} (SOA) SI+NSI V Nrm 89.4 73.0 72.0 1.8 13.9 0.8 
8 {2,4} (SOA) SI+NSI V NNrm 87.0 74.2 68.6 14.2 44.3 1.5 
9 SOA SI C Nrm 17.0 38.2 NR -3.5 NR NR 
10 SOA SI C NNrm 17.4 33.4 32.4 -1.8 0.0 0.0 
11 SOA NSI C Nrm 12.8 61.8 58.4 -1.8 0.0 -1.5 
12 SOA NSI C NNrm 15.8 49.4 52.2 -3.5 0.0 0.0 
13 SOA Any C Nrm 15.0 62.6 58.4 -2.7 0.0 -1.5 
14 SOA Any C NNrm 14.6 51.8 50.0 -2.7 0.0 0.0 
15 {9,11} (SOA) SI+NSI C Nrm 12.8 61.8 58.4 -4.4 0.0 -2.3 
16 {10,12} (SOA) 
SI+
NSI C NNrm 21.6 49.4 48.8 -3.5 0.0 0.0 
17 Adj Any C Nrm 72.2 65.0 61.6 9.7 0.0 6.8 
18 Adj Any C NNrm 68.4 61.0 48.8 -6.2 -6.6 0.0 
19 SbjAdj Any C Nrm 26.8 61.4 67.0 0.0 0.0 0.8 
20 SbjAdj Any C NNrm 57.4 59.8 53.0 -6.2 -5.7 0.0 
21 TAdj Any C Nrm 43.6 66.2 62.0 1.8 NR -2.3 
22 TAdj Any C NNrm 61.6 57.4 51.0 -6.2 -8.2 0.0 
23 TAdj2 Any V Nrm 68.4 48.6 46.0 -3.5 -5.7 1.5 
24 TAdj2 Any V NNrm 75.6 48.2 55.8 4.4 11.5 -0.8 
25 TAdj2 Any C Nrm 17.8 37.8 49.6 0.0 NR -1.5 
26 TAdj2 Any C NNrm 40.2 32.2 52.6 0.0 -1.6 0.0 
27 Adv+ Any C Nrm 77.6 41.8 88.4 0.0 -4.9 -0.8 
28 Adv+ Any C NNrm 52.4 39.8 60.4 0.0 -5.7 0.0 
29 Adv- Any C Nrm 48.6 42.2 84.0 0.0 0.0 -0.8 
30 Adv- Any C NNrm 33.0 44.6 60.4 0.0 -2.5 0.0 
31 ly-Adv Any C Nrm 17.0 73.4 72.4 -6.2 0.0 3.0 
137
32 ly-Adv Any C NNrm 38.6 63.0 60.4 -4.4 4.9 0.0 
33 R-ly-Adv Any C Nrm 23.0 NR 20.8 NR 4.1 NR 
34 R-ly-Adv Any C NNrm 23.0 15.2 36.0 NR 8.2 0.0 
35 TAdv Any C Nrm 52.8 65.4 68.2 -3.5 13.1 1.5 
36 TAdv Any C NNrm 55.4 55.8 66.6 -3.5 27.9 0.0 
37 TAdv2 Any V Nrm 47.8 21.6 38.2 -2.7 -1.6 -2.3 
38 TAdv2 Any V NNrm 43.6 44.6 42.2 -6.2 4.9 0.8 
39 TAdv2 Any C Nrm 8.6 25.4 26.6 0.0 NR NR 
40 TAdv2 Any C NNrm 17.4 27.8 37.4 0.0 -131.1 0.0 
41 
{23,37} 
(TAdj2 + 
TAdv2) 
Any V Nrm 71.8 51.8 48.8 -4.4 -0.8 2.3 
42 
{24,38} 
(TAdj2 + 
TAdv2) 
Any V NNrm 75.6 59.8 57.6 0.0 12.3 1.5 
43 
{25,39} 
(TAdj2 + 
TAdv2) 
Any C Nrm 17.8 41.0 49.6 0.0 NR -1.5 
44 
{26,40} 
(TAdj2 + 
TAdv2) 
Any C NNrm 40.2 27.8 47.6 0.0 0.0 0.0 
45 
{5,23,37} 
(SOA + 
(TAdj2 + 
TAdv2) 
Any V Nrm 88.6 79.6 70.4 -2.7 14.8 2.3 
46 
{6,24,38} 
(SOA + TAdj2 
+ TAdv2) 
Any V NNrm 90.4 79.6 67.0 15.0 44.3 1.5 
47 
{13, 17, 19, 
21, 25, 27, 29, 
31, 35, 39} 
(SOA, Adj, 
SbjAdj, TAdj, 
TAdj2,  Adv+, 
Adv-, ly-Adv, 
TAdv, TAdv2) 
Any C Nrm 98.8 90.8 93.0 10.6 52.5 13.5 
48 
{14, 18, 20, 
22, 26, 28,  
30, 32, 36, 40} 
(SOA, Adj, 
SbjAdj, TAdj, 
TAdj2,  Adv+, 
Adv-, ly-Adv, 
TAdv, TAdv2) 
Any C NNrm 97.8 81.2 85.6 15.9 63.9 1.5 
Avg  AG 49.9 54.2 57.0 -0.2 6.4 0.5 
49 Nouns Any C Nrm 81.4 68.6 83.0 -2.7 0.0 3.8 
50 Nouns Any C NNrm 44.0 39.8 NR -3.5 0.0 0.0 
51 Verbs Any C Nrm 72.2 69.0 76.0 -10.6 -8.2 0.0 
52 Verbs Any C NNrm 45.6 44.6 63.4 0.0 -9.0 0.0 
53 Punctuation Any C Nrm 76.8 20.4 83.4 0.0 28.7 -1.5 
54 Punctuation Any C NNrm 53.6 29.8 72.0 0.0 -9.8 0.0 
138
Avg AG 62.3 45.4 75.6 -2.8 0.3 0.4 
Final Avg 
AG 51.3 53.2 58.8 -0.5 5.7 0.5 
Abbreviations 
SOA Speaker-Oriented Adverbs Adj Adjectives 
SbjAdj Subjective Adjectives TAdj Trait Adjectives 
TAdj2 Trait Adjectives Subset Adv+ Adverbs (All) 
Adv- Adverbs (less wh- + Adv Particles) TAdv Trait Adverbs 
TAdv2 Trait Adverbs Subset ly-Adv ly-Adverbs 
R-ly-Adv Random ly-Adverbs Punc Punctuation 
SI Sentence-Initial Position NSI Not Sentence-Initial Position 
SI+NSI 
Sentence-Initial Position + Not 
Sentence-Initial Position Any Any Sentence Position 
V Vector C Count 
Nrm Normalized NNrm Non-Normalized 
139
Appendix 7.2: Number of Unique Words in Vector Models 
Method Model Problem Feature Representation 
Acc 
 Gain 
#
Unique 
Words 
in
Model 
Sent 
Pos 
V /
C
Nrm / 
NNrm   
1 1.1 
Acad vs. 
Fict SOA SI V Nrm 40.2 13 
1 1.2 
Acad vs. 
News SOA SI V Nrm 39.4 5 
1 1.3 
Acad vs. 
Not-Acad SOA SI V Nrm 8.8 8 
1 1.4 
Fict vs. 
News SOA SI V Nrm 39.4 5 
1 1.5 
Fict vs. 
Not-Fict SOA SI V Nrm 2.5 9 
1 1.6 
News vs. 
Not-News SOA SI V Nrm 0.8 5 
2 2.1 
Acad vs. 
Fict SOA SI V NNrm 41.0 11 
2 2.2 
Acad vs. 
News SOA SI V NNrm 39.8 8 
2 2.3 
Acad vs. 
Not-Acad SOA SI V NNrm 5.3 9 
2 2.4 
Fict vs. 
News SOA SI V NNrm 41.0 6 
2 2.5 
Fict vs. 
Not-Fict SOA SI V NNrm 10.7 12 
2 2.6 
News vs. 
Not-News SOA SI V NNrm 0.8 4 
3 3.1 
Acad vs. 
Fict SOA NSI V Nrm 91.6 19 
3 3.2 
Acad vs. 
News SOA NSI V Nrm 75.6 9 
3 3.3 
Acad vs. 
Not-Acad SOA NSI V Nrm 1.8 10 
3 3.4 
Fict vs. 
News SOA NSI V Nrm 69.8 9 
3 3.5 
Fict vs. 
Not-Fict SOA NSI V Nrm 16.4 14 
3 3.6 
News vs. 
Not-News SOA NSI V Nrm 1.5 13 
4 4.1 
Acad vs. 
Fict SOA NSI V NNrm 87.0 16 
4 4.2 
Acad vs. 
News SOA NSI V NNrm 74.2 11 
4 4.3 
Acad vs. 
Not-Acad SOA NSI V NNrm 8.0 15 
4 4.4 
Fict vs. 
News SOA NSI V NNrm 64.2 11 
140
4 4.5 
Fict vs. 
Not-Fict SOA NSI V NNrm 44.3 19 
4 4.6 
News vs. 
Not-News SOA NSI V NNrm 0.8 12 
5 5.1 
Acad vs. 
Fict SOA Any V Nrm 91.6 19 
5 5.2 
Acad vs. 
News SOA Any V Nrm 73.8 9 
5 5.3 
Acad vs. 
Not-Acad SOA Any V Nrm 3.5 11 
5 5.4 
Fict vs. 
News SOA Any V Nrm 71.2 9 
5 5.5 
Fict vs. 
Not-Fict SOA Any V Nrm 14.8 14 
5 5.6 
News vs. 
Not-News SOA Any V Nrm 0.8 16 
6 6.1 
Acad vs. 
Fict SOA Any V NNrm 88.2 15 
6 6.2 
Acad vs. 
News SOA Any V NNrm 77.6 12 
6 6.3 
Acad vs. 
Not-Acad SOA Any V NNrm 9.7 16 
6 6.4 
Fict vs. 
News SOA Any V NNrm 65.8 12 
6 6.5 
Fict vs. 
Not-Fict SOA Any V NNrm 42.6 19 
6 6.6 
News vs. 
Not-News SOA Any V NNrm 0.0 10 
7 7.1 
Acad vs. 
Fict SOA 
SI+N
SI V Nrm 89.4 17 
7 7.2 
Acad vs. 
News SOA 
SI+N
SI V Nrm 73.0 11 
7 7.3 
Acad vs. 
Not-Acad SOA 
SI+N
SI V Nrm 1.8 13 
7 7.4 
Fict vs. 
News SOA 
SI+N
SI V Nrm 72.0 13 
7 7.5 
Fict vs. 
Not-Fict SOA 
SI+N
SI V Nrm 13.9 19 
7 7.6 
News vs. 
Not-News SOA 
SI+N
SI V Nrm 0.8 16 
8 8.1 
Acad vs. 
Fict SOA 
SI+N
SI V NNrm 87.0 16 
8 8.2 
Acad vs. 
News SOA 
SI+N
SI V NNrm 74.2 17 
8 8.3 
Acad vs. 
Not-Acad SOA 
SI+N
SI V NNrm 14.2 18 
8 8.4 
Fict vs. 
News SOA 
SI+N
SI V NNrm 68.6 13 
8 8.5 
Fict vs. 
Not-Fict SOA 
SI+N
SI V NNrm 44.3 20 
8 8.6 
News vs. 
Not-News SOA 
SI+N
SI V NNrm 1.5 13 
23 23.1 
Acad vs. 
Fict TAdj2 Any V Nrm 68.4 17 
141
23 23.2 
Acad vs. 
News TAdj2 Any V Nrm 48.6 11 
23 23.3 
Acad vs. 
Not-Acad TAdj2 Any V Nrm 3.5 9 
23 23.4 
Fict vs. 
News TAdj2 Any V Nrm 46.0 10 
23 23.5 
Fict vs. 
Not-Fict TAdj2 Any V Nrm 5.7 12 
23 23.6 
News vs. 
Not-News TAdj2 Any V Nrm 1.5 10 
24 24.1 
Acad vs. 
Fict TAdj2 Any V NNrm 75.6 17 
24 24.2 
Acad vs. 
News TAdj2 Any V NNrm 48.2 9 
24 24.3 
Acad vs. 
Not-Acad TAdj2 Any V NNrm 4.4 15 
24 24.4 
Fict vs. 
News TAdj2 Any V NNrm 55.8 13 
24 24.5 
Fict vs. 
Not-Fict TAdj2 Any V NNrm 11.5 26 
24 24.6 
News vs. 
Not-News TAdj2 Any V NNrm 0.8 8 
37 37.1 
Acad vs. 
Fict TAdv2 Any V Nrm 47.8 10 
37 37.2 
Acad vs. 
News TAdv2 Any V Nrm 21.6 5 
37 37.3 
Acad vs. 
Not-Acad TAdv2 Any V Nrm 2.7 6 
37 37.4 
Fict vs. 
News TAdv2 Any V Nrm 38.2 6 
37 37.5 
Fict vs. 
Not-Fict TAdv2 Any V Nrm 1.6 10 
37 37.6 
News vs. 
Not-News TAdv2 Any V Nrm 2.3 7 
38 38.1 
Acad vs. 
Fict TAdv2 Any V NNrm 43.6 7 
38 38.2 
Acad vs. 
News TAdv2 Any V NNrm 44.6 4 
38 38.3 
Acad vs. 
Not-Acad TAdv2 Any V NNrm 6.2 8 
38 38.4 
Fict vs. 
News TAdv2 Any V NNrm 42.2 7 
38 38.5 
Fict vs. 
Not-Fict TAdv2 Any V NNrm 4.9 15 
38 38.6 
News vs. 
Not-News TAdv2 Any V NNrm 0.8 4 
41 41.1 
Acad vs. 
Fict TAdj2 + TAdv2 Any V Nrm 71.8 23 
41 41.2 
Acad vs. 
News TAdj2 + TAdv2 Any V Nrm 51.8 15 
41 41.3 
Acad vs. 
Not-Acad TAdj2 + TAdv2 Any V Nrm 4.4 13 
41 41.4 
Fict vs. 
News TAdj2 + TAdv2 Any V Nrm 48.8 13 
142
41 41.5 
Fict vs. 
Not-Fict TAdj2 + TAdv2 Any V Nrm 0.8 19 
41 41.6 
News vs. 
Not-News TAdj2 + TAdv2 Any V Nrm 2.3 17 
42 42.1 
Acad vs. 
Fict TAdj2 + TAdv2 Any V NNrm 75.6 24 
42 42.2 
Acad vs. 
News TAdj2 + TAdv2 Any V NNrm 59.8 11 
42 42.3 
Acad vs. 
Not-Acad TAdj2 + TAdv2 Any V NNrm 0.0 18 
42 42.4 
Fict vs. 
News TAdj2 + TAdv2 Any V NNrm 57.6 19 
42 42.5 
Fict vs. 
Not-Fict TAdj2 + TAdv2 Any V NNrm 12.3 38 
42 42.6 
News vs. 
Not-News TAdj2 + TAdv2 Any V NNrm 1.5 9 
45 45.1 
Acad vs. 
Fict 
SOA + TAdj2 + 
TAdv2 Any V Nrm 88.6 36 
45 45.2 
Acad vs. 
News 
SOA + TAdj2 + 
TAdv2 Any V Nrm 79.6 21 
45 45.3 
Acad vs. 
Not-Acad 
SOA + TAdj2 + 
TAdv2 Any V Nrm 2.7 18 
45 45.4 
Fict vs. 
News 
SOA + TAdj2 + 
TAdv2 Any V Nrm 70.4 20 
45 45.5 
Fict vs. 
Not-Fict 
SOA + TAdj2 + 
TAdv2 Any V Nrm 14.8 30 
45 45.6 
News vs. 
Not-News 
SOA + TAdj2 + 
TAdv2 Any V Nrm 2.3 29 
46 46.1 
Acad vs. 
Fict 
SOA + TAdj2 + 
TAdv2 Any V NNrm 90.4 23 
46 46.2 
Acad vs. 
News 
SOA + TAdj2 + 
TAdv2 Any V NNrm 79.6 17 
46 46.3 
Acad vs. 
Not-Acad 
SOA + TAdj2 + 
TAdv2 Any V NNrm 15.0 26 
46 46.4 
Fict vs. 
News 
SOA + TAdj2 + 
TAdv2 Any V NNrm 67.0 20 
46 46.5 
Fict vs. 
Not-Fict 
SOA + TAdj2 + 
TAdv2 Any V NNrm 44.3 46 
46 46.6 
News vs. 
Not-News 
SOA + TAdj2 + 
TAdv2 Any V NNrm 1.5 23 
Abbreviations 
SOA Speaker-Oriented Adverbs TAdj2 Trait Adjectives Subset 
TAdv2 Trait Adverbs Subset SI Sentence-Initial Position 
NSI Not Sentence-Initial Position SI+NSI 
Sentence-Initial Position + Not 
Sentence-Initial Position 
Any Any Sentence Position V Vector 
C Count Nrm Normalized 
NNrm Non-Normalized   
143
Appendix 7.3: Relative Performance of Methods Ranked in 
Decreasing Order of Accuracy Gain by Model Sets 
 
Rank Method 
Best  
Model Problem 
Feature 
 
Representation 
 
AG 
# Uniq 
Wrds in 
Best 
Model 
Sent 
 Pos 
V
/
C
Nrm / 
NNrm  
1 47 47.1 
Acad vs. 
Fict 
SOA, 
Adj, 
SbjAdj, 
TAdj, 
TAdj2, 
Adv+, 
Adv-, ly-
Adv, 
TAdv, 
TAdv2 Any C Nrm 98.8   
2 48 48.1 
Acad vs. 
Fict 
SOA, 
Adj, 
SbjAdj, 
TAdj, 
TAdj2, 
Adv+, 
Adv-, ly-
Adv, 
TAdv, 
TAdv2 Any C NNrm 97.8   
3 3 3.1 
Acad vs. 
Fict SOA NSI V Nrm 91.6 19 
4 5 5.1 
Acad vs. 
Fict SOA Any V Nrm 91.6 19 
5 46 46.1 
Acad vs. 
Fict 
SOA + 
TAdj2 + 
TAdv2 Any V NNrm 90.4 23 
6 7 7.1 
Acad vs. 
Fict SOA SI+NSI V Nrm 89.4 17 
7 45 45.1 
Acad vs. 
Fict 
SOA + 
TAdj2 + 
TAdv2 Any V Nrm 88.6 36 
8 27 27.4 
Fict vs. 
News 
All 
Adverbs Any C Nrm 88.4   
9 6 6.1 
Acad vs. 
Fict SOA Any V NNrm 88.2 15 
10 4 4.1 
Acad vs. 
Fict SOA NSI V NNrm 87.0 16 
11 8 8.1 
Acad vs. 
Fict SOA SI+NSI V NNrm 87.0 16 
12 29 29.4 
Fict vs. 
News Adv- Any C Nrm 84.0   
144
13 53 53.4 
Fict vs. 
News Punc Any C Nrm 83.4   
14 49 49.4 
Fict vs. 
News Nouns Any C Nrm 83.0   
15 51 51.4 
Fict vs. 
News Verbs Any C Nrm 76.0   
16 24 24.1 
Acad vs. 
Fict TAdj2 Any V NNrm 75.6 17 
17 42 42.1 
Acad vs. 
Fict 
TAdj2 + 
TAdv2 Any V NNrm 75.6 24 
18 31 31.2 
Acad vs. 
News ly-Adv Any C Nrm 73.4   
19 17 17.1 
Acad vs. 
Fict Adj Any C Nrm 72.2   
20 54 54.4 
Fict vs. 
News Punc Any C NNrm 72.0   
21 41 41.1 
Acad vs. 
Fict 
TAdj2 + 
TAdv2 Any V Nrm 71.8 23 
22 18 18.1 
Acad vs. 
Fict Adj Any C NNrm 68.4   
23 23 23.1 
Acad vs. 
Fict TAdj2 Any V Nrm 68.4 17 
24 35 35.4 
Fict vs. 
News TAdv Any C Nrm 68.2   
25 19 19.4 
Fict vs. 
News SbjAdj Any C Nrm 67.0   
26 36 36.4 
Fict vs. 
News TAdv Any C NNrm 66.6   
27 21 21.2 
Acad vs. 
News TAdj Any C Nrm 66.2   
28 52 52.4 
Fict vs. 
News Verbs Any C NNrm 63.4   
29 32 32.2 
Acad vs. 
News ly-Adv Any C NNrm 63.0   
30 13 13.2 
Acad vs. 
News SOA Any C Nrm 62.6   
31 11 11.2 
Acad vs. 
News SOA NSI C Nrm 61.8   
32 15 15.2 
Acad vs. 
News SOA SI+NSI C Nrm 61.8   
33 22 22.1 
Acad vs. 
Fict TAdj Any C NNrm 61.6   
34 28 28.4 
Fict vs. 
News Adv+ Any C NNrm 60.4   
35 30 30.4 
Fict vs. 
News Adv- Any C NNrm 60.4   
36 20 20.2 
Acad vs. 
News SbjAdj Any C NNrm 59.8   
37 26 26.4 
Fict vs. 
News TAdj2 Any C NNrm 52.6   
38 12 12.4 
Fict vs. 
News SOA NSI C NNrm 52.2   
39 14 14.2 
Acad vs. 
News SOA Any C NNrm 51.8   
145
40 25 25.4 
Fict vs. 
News 
TAdj2 + 
TAdv2 Any C Nrm 49.6   
41 43 43.4 
Fict vs. 
News 
TAdj2 + 
TAdv2 Any C Nrm 49.6   
42 16 16.2 
Acad vs. 
News SOA SI+NSI C NNrm 49.4   
43 37 37.1 
Acad vs. 
Fict TAdv2 Any V Nrm 47.8 10 
44 44 44.4 
Fict vs. 
News 
TAdj2 + 
TAdv2 Any C NNrm 47.6   
45 38 38.2 
Acad vs. 
News TAdv2 Any V NNrm 44.6 4 
46 50 50.1 
Acad vs. 
Fict Nouns Any C NNrm 44.0   
47 2 2.1 
Acad vs. 
Fict SOA SI V NNrm 41.0 11 
48 1 1.1 
Acad vs. 
Fict SOA SI V Nrm 40.2 13 
49 9 9.2 
Acad vs. 
News SOA SI C Nrm 38.2   
50 40 40.4 
Fict vs. 
News TAdv2 Any C NNrm 37.4   
51 34 34.4 
Fict vs. 
News R-ly-Adv Any C NNrm 36.0   
52 10 10.2 
Acad vs. 
News SOA SI C NNrm 33.4   
53 39 39.4 
Fict vs. 
News TAdv2 Any C Nrm 26.6   
54 33 33.1 
Acad vs. 
Fict R-ly-Adv Any C Nrm 23.0   
Abbreviations 
SOA Speaker-Oriented Adverbs Adj Adjectives 
SbjAdj Subjective Adjectives TAdj Trait Adjectives 
TAdj2 Trait Adjectives Subset Adv+ Adverbs (All) 
Adv- 
Adverbs (less wh-Adv + Adv 
Particles) TAdv Trait Adverbs 
TAdv2 Trait Adverbs Subset ly-Adv ly-Adverbs 
R-ly-
Adv Random ly-Adverbs Punc Punctuation 
SI Sentence-Initial Position NSI Not Sentence-Initial Position 
SI+NSI 
Sentence-Initial Position + Not 
Sentence-Initial Position Any Any Sentence Position 
V Vector C Count 
Nrm Normalized NNrm Non-Normalized 
146
Curriculum Vita 
 
Robert John Rittman 
 
EDUCATION 
 
Ph.D. (Information Science)  
May 2007 
School of Communication, Information & Library Studies  
Rutgers, The State University of New Jersey, New Brunswick, NJ 
 
Dissertation Title: Automatic Discrimination of Genres: The Role of Adjectives and 
Adverbs as Suggested by Linguistics and Psychology 
 
Master of Library Science, May 2001 
School of Communication, Information & Library Studies 
Rutgers, The State University of New Jersey, New Brunswick, NJ 
 
Certificate (Modern Archives Administration), January 1979 
Institute: Introduction to Modern Archives Administration  
National Archives and Records Administration, Washington, D. C.  
 
Master of Arts (U.S. History), May 1979 
Seton Hall University, South Orange, NJ 
 
Bachelor of Arts (History and American Studies), May 1975 
Seton Hall University, South Orange, NJ 
 
RESEARCH EXPERIENCE 
 
High-Quality Interactive Question-Answering (HITIQA) 
Graduate Assistant, February 2002 – May 2005 
 
Metrics for Question Answering Systems (ARDA Challenge Workshop) Summer, 
2004 
Summer Internship at Pacific Northwest National Laboratory, Richland, WA. Advisor: 
Emile Morse, NIST 
 
Prototype for Evaluating a Complex Collaborative Information Finding System for 
the World-Wide Web: Evaluation of the Antworld System 
Graduate Assistant, October 2001 - May 2002 
 
PUBLICATIONS 
 
147
Ng, K. B., Kantor, P., Strzalkowski, T., Wacholder, N., Tang, R., Bai, B., 
Rittman, R., Song, P., Sun, Y. (2006). Automated judgment of document 
qualities. Journal of the American Society for Information Science and 
Technology 57(9), 1155-1164.  
 
Wacholder, N., Kelly, D., Rittman, R., Sun, Y., Kantor, P., Small, S., & 
Strzalkowski, T. (to appear). A model for realistic evaluation of an end-to-end 
question answering system. Journal of the American Society for Information 
Science and Technology. 
Kelly, D., Wacholder, N., Rittman, R., Sun, Y., Kantor, P., Small, S., & 
Strzalkowski, T. (to appear). Using interview data to identify evaluation criteria 
for interactive, analytical question answering systems. Journal of the American 
Society for Information Science and Technology.  
 
WORK EXPERIENCE 
Reference Librarian, October 2001 – June 2002 
Somerville Public Library, Somerville, NJ 
 
Librarian, June - August 2001 
Hunterdon County Library, Flemington, NJ 
 
Director, 1981-2001 
Morris County Media Services Center, Whippany, NJ 
 
