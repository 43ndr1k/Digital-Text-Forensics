An Evaluation of Writeprint Matching Method to 
Identify the Authors of Thai Online Messages 
 
Rangsipan Marukatat 1, Siravich Khongrod 
Department of Computer Engineering, Faculty of Engineering 
Mahidol University 
Nakhon Pathom, Thailand 
1 rangsipan.mar@mahidol.ac.th 
 
 
Abstract— This research studies the author identification of 
Thai online messages, based on 54 writing attributes. The method 
in focus is writeprint matching, which employs frequent pattern 
mining to create the writeprint of each suspect and computes a 
similarity score between this writeprint and the pattern found in 
an anonymous message. It achieved an average accuracy of 82%, 
while other well-known methods, support vector machine (SVM) 
and C4.5 decision tree, achieved average accuracies of 89% and 
81%, respectively. As for the identification of individual author, 
all three methods were as good as each other in most cases. The 
writeprint matching method had potential in reducing Type I 
error or the chance of dismissing real offenders. However, its 
performance was still limited when the suspects had too similar 
writing styles.  
Keywords— author identification; writeprint; online messages 
I.  INTRODUCTION 
The Internet users in Thailand have reached 18 millions or 
29% of the total population recently. Social networks such as 
Facebook, Twitter, and webboards are among the most popular 
online activities for more than 10 million users [1]. With such 
popularity, concerns have been raised about the posting and 
spreading of inappropriate and unlawful messages. In many 
cases, the offenders hide their identities or pretend to be other 
persons; however, their habits and preferences in writing can 
still be traced. The concept of author identification is therefore 
as follows. Firstly, writing attributes (such as the use of certain 
words or symbols) are extracted from sample messages written 
by each suspect and from the anonymous offending message. 
Identifying the most likely suspect can be done by supervised 
methods (i.e. classification) or unsupervised methods such as 
pattern matching. 
In classification, each class represents each suspect. The 
samples are used to train a classifier, whose task is to predict 
class for the anonymous message. A popular classifier for such 
purpose is support vector machine (SVM). But despite high 
accuracy [2]-[5], the SVM process is heavily mathematical. An 
explanation for a certain prediction, which is possibly needed 
when implicating a suspect, can be difficult to convey. This 
limitation was also pointed out by Igbal et al. [6]. On the other 
hand, decision tree classifier produces a more comprehensible  
decision model, a tree. But its accuracy can be inferior to that 
of SVM in some cases, e.g. as reported in [5]. 
In pattern matching, a suspect’s writeprint is created from 
his or her samples and compared with the anonymous message 
[6]-[7]. Then, similarity scores of all the suspects are reported. 
This is flexible in allowing the authorities to pick more than 
one probable suspect for further investigation. Therefore, our 
research follows this approach. It adapts the concepts proposed 
in [6] to create the writeprints from frequent itemsets, the by-
product of Apriori association analysis [8]. The Apriori itself is 
not heavily mathematical and hence can be more user-friendly 
than SVM.  
The rest of the paper is organized as follows. Section II 
reviews some related work in author identification. Section III 
describes main characteristics of Thai online messages and the 
writing attributes selected in this research. Section IV describes 
the writeprint matching method and implementation. Section V 
reports experimental results. Finally, Section VI concludes the 
paper and proposes further studies. 
II. RELATED WORK IN AUTHOR IDENTIFICATION 
Previous studies on author identification of online messages 
were mostly based on English. For example, de Vel et al. [2] 
classified 3 authors of English emails by using SVM and 191 
writing attributes. They reported that character-based attributes 
contributed most to the classification. Content-related attributes 
were redundant because the email topics were diverse. Similar 
findings about content-related attributes were reported in [5].  
In many studies, the classifiers could handle a few authors 
well, but their performance dropped greatly as the number of 
authors increased. A survey and analysis about this scalability 
issue can be found in [9]. Interestingly, Sun et al. [3] managed 
to classify 20-50 authors of Amazon’s online reviews, by using 
SVM, and achieved accuracies as high as 72% for 50 authors 
and 92% for 20 authors. Their achievement was down to a very 
large number of attributes, i.e. more than 10,000 character n-
grams. Abbasi and Chen [7] extracted n-grams in many levels,  
including character, word, digit, and part-of-speech (POS) tag, 
in order to create author writeprints.  
Instead of finding individual who wrote a message, some 
research focused on author profiles. Cheng et al. [10] predicted 
the author’s gender, while Estival et al. [4] predicted age range, 
gender, education, and personality traits.  
This research is sponsored by the National Research Council of Thailand 
(NRCT). 
978-1-4799-8676-7/15/$31.00 copyright 2015 IEEE
SNPD 2015, June 1-3 2015, Takamatsu, Japan
Recent studies have also targeted major languages such as 
Arabic [4], [11] and Chinese [5]. In [4], Arabic morphological 
attributes were proposed for building the author profiles from 
Arabic messages. These attributes were helpful especially in 
gender identification, as gender is morphologically marked in 
Arabic. More challenges in Arabic language were discussed in 
[11], among others. In [5], the accuracy of author identification 
of Chinese messages was inferior to that of English messages. 
They explained that ambiguous word boundary in Chinese text 
led to inaccurate attribute values being extracted. Therefore,  
some characteristics of a language needed special handling and 
could have impact on the author identification.  
III. WRITING ATTRIBUTES IN THAI ONLINE MESSAGES 
Due to the diversity of languages and online cultures, the 
writing attributes used in different studies may vary. In case of 
Thai language and online culture, their differences from the 
English counterparts are highlighted as follows. 
Firstly, Thai words are written consecutively until the end 
of the sentence. Sentences are punctuated by space. There are 
first-person pronouns that can be gender-specific or gender-
neutral. Male users tend to use the former. Female users tend to 
use the latter. Gender-ending words can be added at the end of 
spoken sentences to address politeness. Using English letters to 
spell Thai words can dodge censorship in some webboards. It 
is also because, with a much bigger set of Thai alphabet, typing 
Thai letters especially with mobile devices is more laborious 
than typing English ones. Emoticons are commonly used, both 
in Western (tilted) and Asian (vertical) styles. But the latter is 
more popular among young users. Finally, some symbols are 
unique to Thai online culture. For example, “555” is read “ha 
ha ha” (laughing). 
Based on the above, 54 attributes that indicate a person’s 
writing style are chosen, as shown in Table I. The category of 
analysis is adapted from [10]. Attribute values extracted from 
each message are numeric. They make up a record in Weka’s 
attribute-relation file format (ARFF). 
TABLE I.  SELECTED WRITING ATTRIBUTES 
Attribute Description 
 
C1 
C2 
C3 
C4 
C5 
C6 
C7 
C8 
C9 
Analysis of individual characters 
Number of characters 
Number of Thai letters 
Number of English letters 
Number of English capital letters 
Number of digits 
Number of whitespaces 
Number of tabs 
Number of line breaks 
Numer of special characters 
 
W1 
W2 
W3 
W4 
W5 
W6 
W7 
W8 
W9 
W10 
Analysis of individual and variation of words 
Number of words 
Average number of letters per word 
Number of unique words 
Number of long words (more than 8 letters) 
Number of short words (less than 3 letters) 
Yule’s characteristics K [10] 
Simpson’s diversity index [10] 
Sichel’s S measure [10] 
Honore’s vocabulary richness [10] 
Entropy measure 
Attribute Description 
 
P1 
P2 
P3 
P4 
P5 
P6 
P7 
P8 
P9 
P10 
Analysis of punctuation 
Number of periods 
Number of commas 
Number of colons 
Number of semi-colons 
Number of hyphens and dashes 
Number of question marks 
Number of exclamation marks 
Number of brackets  
Number of repeating marks (Thai) 
Proportion of repeating marks and spaces (Thai) 
 
E1 
E2 
E3 
E4 
E5 
E6 
E7 
E8 
E9 
E10 
Analysis of emotion indicators 
Number of at least 3 consecutive periods 
Number of at least 3 consecutive question marks 
Number of at least 3 consecutive exclamation marks 
Number of at least 3 consecutive repeating marks (Thai) 
Number of at least 3 consecutive 5’s (Thai) 
Number of words with repeat-ending letters 
Number of tilted emoticons with noses, e.g. :-) 
Number of tilted emoticons without noses, e.g. :) 
Number of vertical emoticons with cheeks, e.g. (^_^) 
Number of vertical emoticons without cheeks, e.g. ^_^ 
 
S1 
S2 
S3 
S4 
S5 
Analysis of message structure 
Number of lines 
Number of empty lines 
Number of sentences 
Average length (in characters) of non-empty lines  
Average length (in characters) of sentences 
 
T1 
T2 
T3 
T4 
T5 
T6 
T7 
T8 
T9 
T10 
Analysis of message content (Thai) 
Number of redundant words 
Number of polite-ending words for female 
Number of polite-ending words for male 
Number of shorten or distort spelling words 
Number of profanities 
Number of exclamations 
Number of first-person pronouns for female 
Number of first-person pronouns for males 
Number of gender-neutral first-person pronouns 
Number of specific keywords 
 
IV. WRITEPRINT MATCHING METHOD 
A. Literature Background 
Suppose that Vi is a vector of writing attributes extracted 
from message i and all attributes are binary. So, an attribute 
value being 1 indicates that the certain feature exists in Vi or 
the message. A(s) denotes a set of attribute vectors belonging to 
suspect s. This research adapts the concepts proposed by Igbal 
et al. [6] to create a writeprint of s or WP(s), and measure its 
similarity to the pattern found in the anonymous message. The 
writeprint creation works in two major steps: 
1. Find all frequent patterns of s, denoted FP(s). They are 
the by-product of Apriori algorithm [8]. Each frequent 
pattern – also called frequent itemset – is a set of items 
or features that exist frequently in A(s), and therefore 
has a support greater than Apriori’s minimum support 
or minsup threshold. 
2. Compare FP(s) to every other FP(t) where s ≠ t, and 
remove patterns that co-exist. The remaining patterns 
in FP(s) are the ones uniquely existing in the messages 
written by s. They are called the writeprint of s. 
Then, given Vx, corresponding to an anonymous message x, 
its similarity score to WP(s) is the total supports of all matched 
patterns normalized by the size of WP(s). The similarity scores 
of all suspects are reported and the one with the highest score is 
identified as the most likely author of the anonymous message. 
B. Implementation and Rationale for Parameter Setup 
We implemented the aforementioned methods using Weka 
API [12]. Frequent patterns are discovered by its Apriori class. 
A known limitation of Apriori is that it does not scale well with 
high dimensional data. In our study, 54 numeric attributes are 
[0,1]-normalized, discretized, and binarized. The number of 
binary attributes grows with the number of discretization bins. 
Too low minsup deems too many patterns frequent and may 
cause Weka to run out of memory. 
Nonetheless, the above factors do not affect the accuracy of 
identifying authors. In our preliminary experiments [13], when 
using different minsup values (0.7, 0.8, and 0.9), the difference 
in average accuracies was insignificant. The same was found 
when using different numbers of equal-width bins (4, 5, and 
10). We observed that although the similarity score of a suspect 
varied with each setting, the order between suspects barely 
changed. These findings agreed with the original work in [6] 
regarding the effect of discretization and its interaction with 
minsup. But the finding about the effect of minsup alone was 
different, as they reported that the accuracy decreased when 
minsup increased. (Note that they used different set of writing 
attributes and ran the threshold from 0.1 to 0.9.) 
These preliminary results are the basis for parameter setup 
in further experiments, as we used 4 discretization bins and 0.7 
minsup threshold. 
V. EXPERIMENTAL RESULTS 
A. Experimental Setup 
Summarized in Table II, 25 messages were collected from 
each of 6 persons who posted messages on pantip.com, one of 
Thailand’s most influential webboards, and their fanpages. The 
webboard had censorship while the fanpages had not. Authors 
1, 2, and 3 were females; Authors 4, 5, and 6 were males. The 
message length was 312 words on average.  
Fig. 1 displays the experimental process. The identification 
of author X was repeated 10 times. In each time, 20 messages 
written by each author were randomed and used as the training 
data; the other 5 messages of X were used as the anonymous 
ones. In real practice, the authorities would also look for words 
that stood out in the messages. Therefore, up to 10 keywords 
were selected from X’s messages and used to extract attribute 
T10 in each experimental run.  
The writeprint matching method was compared with SVM 
and C4.5 decision tree. Their parameters were set as follows. 
1. Writeprint matching (WP): 
discretization bin = 4; 
minsup = 0.7. 
2. Support vector machine (SVM), using Weka’s SMO: 
kernel = polynomial of degree 2;  
filter type = normalized training data; 
the other parameters were as default. 
3. C4.5 decision tree (C4.5), using Weka’s J48: 
binary split = true;  
use Laplace = true;  
pruning strategy = subtree raising; 
reduced error pruning = false; 
the other parameters were as default. 
TABLE II.  MESSAGES USED IN THE EXPERIMENTS 
Author Topics 
Message Length (average 
over 25 messages/author) 
Characters Words 
1 Actors and beauty  (pantip.com) 1214 369 
2 Actors (pantip.com) 1247 293 
3 Actors and family  (pantip.com) 1211 291 
4 Politics (pantip.com) 1246 299 
5 Political and social satires (fanpage 1) 1226 319 
6 Political and social satires (fanpage 2) 1204 303 
Average 1225 312 
 
 
B. Results 
In each experimental run, the accuracy rates of WP, SVM, 
and C4.5 were computed, along with the F-measures yielded 
by these methods in identifying each author. The F-measure of 
author X is the harmonic mean of X’s recall and precision. The 
former is the proportion of X’s anonymous messages that were 
retrieved or identified as his/hers; the latter is the proportion of 
retrieved messages that were actually written by X.  
From Table III, SVM performed significantly better than 
WP, and from Table IV, much difference in their performance 
was due to the identification of Author 4. For the other authors, 
WP achieved slightly lower F-measures but the difference was 
not significant (α = 0.05). Compared with C4.5, WP performed 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Fig. 1. Experimental process 
Extract 53 writing attributes, excluding T10, from 150 
messages (25 messages/author) 
For each experimental run = 1 to 10 
{ 
 Sample 120 training messages (20 messages/author) 
 For each author X = 1 to 6 
 { 
  Use the other 5 messages of Author X as anonymous  
  messages 
  Select 10 keywords from X’s messages 
  Use X’s keywords to extract T10 from training and 
  anonymous messages   
  Identify X’s anonymous messages by WP, SVM, C4.5 
 } 
} 
insignificantly better, and much difference between them was 
due to the identification of Author 3. 
TABLE III.  OVERALL ACCURACY (OVER 10 RUNS) 
Measures WP SVM C4.5 
Min. accuracy 
Max. accuracy 
Mean accuracy 
SD 
73% 
93% 
82% 
6.10 
77% 
97% 
89% ** 
7.57 
73% 
87% 
81% 
4.22 
** Significantly differ from WP’s mean accuracy (by paired samples t-test, α = 0.05) 
TABLE IV.  THE F-MEASURE OF EACH AUTHOR (OVER 10 RUNS) 
Author 
Average F-measure 
WP SVM C4.5 
1 
2 
3 
4 
5 
6 
0.90 
0.76 
0.81 
0.77 
0.83 
0.83 
0.97 
0.86 
0.78 
0.89 ** 
0.90 
0.82 
0.95 
0.82 
0.67 ** 
0.77 
0.84 
0.79 
** Significantly differ from WP’s mean F-measure (by paired samples t-test, α = 0.05) 
 
Moreover, in each experimental run with WP, the following 
measures were recorded. 
1. The rank of the real author, as the scores were sorted in 
descending order. 
2. The gap from the real author’s score to the wrong 
author’s (in case of wrong prediction), normalized by 
the score range. Suppose that all (author, score) tuples 
were: (Author 1, 90), (Author 2, 85), (Author 3, 35), 
(Author 4, 25), (Author 5, 20), and (Author 6, 10). If 
Author 3 was the real author, the prediction would be 
wrong and the gap would be (90-35)/(90-10) = 0.688.  
3. The gap from the real author’s score to the runner-up’s 
(in case of correct prediction), normalized by the score 
range. From the above example, if Author 1 was the 
real author, the prediction would be correct and the gap 
would be (90-85)/(90-10) = 0.063.  
As shown in Table V, the scores of the real authors were 
among the top two on average. Authors 1, 5, and 6 were clearly 
distinguished from the others as their gaps to runner-ups were 
quite big. Authors 1, 3, and 6 marginally lost out when wrong 
authors were identified. In practice, the authorities could also 
investigate them, thus reducing the chance of dismissing the 
real offenders (i.e. Type I error). 
On the other hand, WP struggled to identify authors with 
less unique styles such as Authors 2 and 4. Their gaps to wrong 
authors were as big as gaps to runner-ups. If such gaps were 
deemed too big, they (as the real offenders) could be dismissed 
when wrong prediction was made. But if the gaps were deemed 
small, many runner-ups could be retained, thus increasing the 
burden of re-investigation. 
VI. CONCLUSION 
This paper studies the author identification of Thai online 
messages by writeprint matching method (WP) adapted from 
the concepts proposed in [6]. It uses Apriori to mine frequent 
itemsets and create suspects’ writeprints. Then similarity scores 
between the writeprints and the pattern found in an anonymous 
message are calculated. Our analysis is based on 54 writing 
attributes, some of which are unique to Thai online culture. The 
method is user friendly and robust to parameter setup.  
WP was compared with support vector machine (SVM) and 
C4.5 decision tree. From the experiments, SVM had higher 
accuracy than WP, and WP had higher accuracy than C4.5. 
They were as good as each other at identifying most authors, 
and much difference between them was due to only few cases. 
WP had an advantage in enabling the authorities to consider 
more than one probable suspect. However, this did not help if 
many suspects had too similar writing styles. 
Extensions will be made to improve WP. To improve the 
accuracy of author identification, more writing attributes will 
be added to the analysis. As for the method’s scalability, other 
frequent itemset mining algorithms, such as FP-growth [14], 
will be examined. Moreover, we have been studying the use of 
maximal and closed itemsets, instead of frequent itemsets, to 
create writeprints. Maximal and closed itemsets are frequent 
itemsets with some constraints. The former have no frequent 
superset; the latter have no superset with the same frequency 
[15]-[16]. The alternative concepts yield much fewer patterns 
than the current one. But those patterns may better capture each  
author’s unique style, and hence lead to more accurate author 
identification. 
TABLE V.  THE RANK AND GAP OF EACH AUTHOR (OVER 10 RUNS), 
WHEN HE/SHE WAS THE REAL AUTHOR 
Author Average Rank 
Average Gap 
To Wrong Author  
(wrong prediction) 
To Runner-Up  
(correct prediction) 
1 
2 
3 
4 
5 
6 
1.20 
1.54 
1.38 
1.20 
1.30 
1.02 
0.070 
0.218 
0.087 
0.135 
0.126 
0.008 
0.796 
0.244 
0.390 
0.257 
0.464 
0.453 
 
REFERENCES 
[1] National Statistical Office (Thailand), The 2013 Information and 
Communication Technology Survey in Household. ISSN 1686-4212. 
[2] O. de Vel, A. Anderson, M. Corney, and G. Mohey, “Mining e-mail 
content for author identification forensics,” SIGMOD Record, 30(4): 55-
64, 2001. 
[3] J. Sun, Z. Yang, and S. Liu, “Applying stylometric analysis techniques 
to counter anonymity in cyberspace,” Journal of Networks, 7(2): 259-
266, 2012. 
[4] D. Estival, T. Gaustad, S. B. Pham, W. Radford, and B. Hutchinson, 
“TAT: an author profiling tool with application to Arabic emails,” 
Proceedings of the Australasian Language Technology Workshop, 
Melbourne, Australia,  2007, pp. 21-30. 
[5] R. Zheng, J. Li, H. Chen, and Z. Huang, “A framework for authorship 
identification of online messages: writing-style features and classi-
fication techniques,” Journal of the American Society for Information 
Science and Technology, 57(3): 378-393, 2006. 
[6] F. Iqbal, R. Hadjidj, B. C. M. Fung, and M. Debbabi, “A novel approach 
of mining write-prints for authorship attribution in e-mail forensics,” 
Digital Investigation, 5(1): 42-51, 2008. 
[7] A. Abbasi and H. Chen, “Writeprints: a stylometric approach to identity-
level identification and similarity detection in cyberspace,” ACM 
Transactions on Information Systems, 26(2), Article 7, 2008. 
[8] R. Agrawal and R. Srikant, “Fast algorithms for mining association rules 
in large databases,” In C. A. Bocca et al. (Eds): VLDB’94, Proceedings 
of 20th International Conference on Very Large Data Bases, pp. 487-
499, Morgan Kaufmann, 1994. 
[9] K. Luyckx, Scalability Issues in Authorship Attribution. PhD. Thesis, 
Universiteit Antwerpen, Belgium, 2010. 
[10] N. Cheng N, R. Chandramouli, and K. P. Subbalakshmi, “Author gender 
identification from text,” Digital Investigation, 8(1): 78-88, 2011. 
[11] A. S. Altheneyan and M. E. B. Menai, “Naïve Bayes classifiers for 
authorship attribution of Arabic texts,” Journal of King Saud University 
– Computer and Information Science, Special Issue on Arabic NLP, 
26(4): 473-484, 2014. 
[12] Weka 3.7 API. http://weka.sourceforge.net/doc.dev/ 
[13] R. Marukatat, Finding the author of online messages using authorship 
attribution (in Thai), Research Report, National Research Council of 
Thailand, 2014.  
[14] J. Han, J. Pei, and Y. Yin, “Mining frequent pattern without candidate 
generation,” Proceedings of the 2000 ACM-SIGMOD International 
Conference on Management of Data, Dallas, TX, 2000, pp. 1-12. 
[15] R. J. Bayardo Jr., “Efficiently mining long patterns from databases,” 
Proceedings of the 1998 ACM-SIGMOD International Conference on 
Management of Data, Seattle, WA, 1998, pp. 85-93. 
[16] N. Pasquier, Y. Bastide, R. Taouil, and L. Lakhal, “Discovering frequent 
closed itemsets for association rules,” Proceedings of the 7th 
International Conference on Database Theory, Jerusalem, Israel, 1999, 
pp. 398-416. 
 
 
