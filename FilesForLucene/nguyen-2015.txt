ar
X
iv
:1
50
8.
07
54
4v
1 
 [
cs
.C
L
] 
 3
0 
A
ug
 2
01
5
Computational Sociolinguistics: A Survey
Dong Nguyen
University of Twente
A. Seza Doğruöz
Tilburg University
Carolyn P. Rosé
Carnegie Mellon University
Franciska de Jong
University of Twente/
Erasmus University Rotterdam
Language is a social phenomenon and inherent to its social nature is that it is constantly
changing. Recently, a surge of interest can be observed within the computational linguistics
(CL) community in the social dimension of language. In this article we present a survey of the
emerging field of ‘Computational Sociolinguistics’ that reflects this increased interest. We aim
to provide a comprehensive overview of CL research on sociolinguistic themes, featuring topics
such as the relation between language and social identity, language use in social interaction and
multilingual communication. Moreover, we demonstrate the potential for synergy between the
research communities involved, by showing how the large-scale data-driven methods that are
widely used in CL can complement existing sociolinguistic studies, and how sociolinguistics can
inform and challenge the methods and assumptions employed in CL studies. We hope to convey
the possible benefits of a closer collaboration between the two communities and conclude with a
discussion of open challenges.
1. Introduction
Science has experienced a paradigm shift along with the increasing availability of
large amounts of digital research data [113]. In addition to the traditional focus on
the description of natural phenomena, theory development and computational science,
data-driven exploration and discovery have become a dominant ingredient of many
methodological frameworks. In line with these developments, the field of computa-
tional linguistics (CL) has also evolved.
Human communication conveys both verbal and nonverbal information. Research
on computational linguistics has primarily focused on capturing the information from
verbal content layers and the structure of verbal information transfer, with a particular
focus on literal aspects of language use. In the words of Krishnan and Eisenstein [147],
computational linguistics has made great progress in modeling language’s informa-
tional dimension, but with a few notable exceptions, computation has had little to
contribute to our understanding of language’s social dimension. The recent increase
in interest of computational linguists to study language in social contexts is partly
driven by the ever increasing availability of social media data. Language is one of the
instruments by which people construct their online identity and manage their social
network. Data from social media platforms provide a strong incentive for innovation
of the CL research agenda, but it raises challenges as well. For example, social media
language is more colloquial and contains more linguistic variation, such as the use of
slang and dialects, than in datasets that have been commonly used in CL research (e.g.,
newspapers) [65]. However, an even greater challenge is that the relation between social
Nguyen et al. Computational Sociolinguistics: A Survey
variables and language is typically fluid and tenuous, while the CL field commonly
focuses on the level of literal meaning and language structure.
The tenuous connection between social variables and language choices arises be-
cause of the symbolic nature of the connection, which affords speakers1 the ability to
make choices in how to use their linguistic repertoire in order to achieve social goals, a
capacity often referred to as the agency of speakers. The symbolic association renders
these choices as akin to a currency that can be invested. Speakers may thus make use of
specific words or stylistic elements to represent themselves in a certain way. However,
because of this agency, social variables cease to have an essential connection with lan-
guage choices. It may be the case, for example, that female speakers display certain char-
acteristics in their language choices, more so than their male counterparts. Nevertheless,
in specific circumstances, females may choose to de-emphasize their identity as females
by modulating their language usage to sound more male. Thus, while this exception
serves to highlight rather than challenge the commonly accepted symbolic association
between gender and language choices, it nevertheless means that it is less feasible to
predict how a female will sound in a randomly selected context. Speaker agency also
enables creative violations of conventional language choice patterns to communicate
subtle meanings as well as for the association between language choices and social
variables to evolve over time. Thus, agency is a key factor explaining the variation in
and dynamic nature of language practices (both within individual speakers and across
speakers). This variation is manifest at various levels of expression – the choice of lexical
elements, phonological variants, semantic alternatives and grammatical patterns – and
plays a central role in the phenomenon of linguistic change. The audience, demographic
variables (e.g., gender, age), and speaker goals are among the factors that influence how
variation is exhibited in specific contexts. Agency thus increases the intricate complexity
of language that must be captured in order to achieve a social interpretation of language.
Sociolinguistics investigates the reciprocal influence of society and language on
each other. Sociolinguists traditionally work with spoken data using qualitative and
quantitative approaches. Surveys and ethnographic research have been the main meth-
ods of data collection [61, 173, 174, 248, 254, 267]. The datasets used are often selected
and/or constructed to facilitate controlled statistical analyses and insightful observa-
tions. However, the resulting datasets are often small in size compared to the standards
adopted by the CL community. The massive volumes of data that have become available
from sources such as social media platforms have provided the opportunity to investi-
gate language variation more broadly. The challenge for the field of sociolinguistics is to
address questions using this massive data that would be difficult to answer in reduced
data sets after sampling the data down, which would be the typical practice in order to
bring the scale within the bounds of what is practical with established methods.
As more and more researchers in the field of CL seek to interpret language from a
social perspective, an increased awareness of insights from the field of sociolinguistics
could inspire modeling refinements and potentially lead to performance gains. But
the rich repertoire of theory and practice developed by sociolinguists could impact
the field of CL also in more fundamental ways. The social nature of language has
traditionally not received much attention within CL, and as a consequence in a large
body of literature the impact of agency has not been sufficiently taken into account.
Various recent studies [121, 244, 259] have demonstrated that existing NLP tools can
1 We use the term ‘speaker’ for an individual who has produced a message, either as spoken word or in
textual format. When discussing particular social media sites, we may refer to ’users’ as well.
2
Nguyen et al. Computational Sociolinguistics: A Survey
be improved by accounting for linguistic variation due to social variables. Hovy and
Søgaard [122] have drawn attention to the fact that biases in frequently used corpora,
such as the Wall Street Journal, causes NLP tools to perform better on texts written by
certain user groups. An understanding of linguistic agency explains why that is the
case and enables predicting where that might be more or less of a problem. This issue is
discussed in depth in some recent computational work related to gender, specifically
Bamman et al. [17] and Nguyen et al. [187] who provide a critical reflection on the
operationalization of gender in CL studies.
The increasing interest in analyzing and modeling the social dimension of language
within CL encourages collaboration between sociolinguistics and CL in various ways.
However, the potential for synergy between the two fields has not been explored
systematically so far [65] and to date there is no overview of the common and com-
plementary aspects of the two fields. This paper aims to present an integrated overview
of research published in the two communities and to describe the state-of-the-art in
the emerging multidisciplinary field that could be labeled as ‘Computational Sociolin-
guistics’. The envisaged audiences are CL researchers interested in sociolinguistics and
sociolinguists interested in computational approaches to study language use. We hope
to argue that there is enough substance that warrants the recognition of ’Computational
Sociolinguistics’ as an autonomous yet multidisciplinary research area, which deserves
the development of a research agenda that is recognized in a wider scholarly commu-
nity maintaining links with both sociolinguistics and computational linguistics.
In the remaining part of this section we discuss the rationale and scope of our
survey in more detail. Also the potential impact of integrating the social dimensions
of language use in the development of practical NLP applications will be discussed. In
Section 2 we discuss Methods for Computational Sociolinguistics, in which we reflect on
methods used in sociolinguistics and computational linguistics. In Section 3, Language
and Social Identity Construction, we discuss how speakers use language to shape percep-
tion of their identity and focus on computational approaches to model language varia-
tion based on gender, age and geographical location. In Section 4 on Language and Social
Interaction, we move from individual speakers to pairs, groups and communities and
discuss how language shapes personal relationships, style-shifting, and the adoption of
norms and language change in communities. In Section 5 we discuss Multilingualism
and Social Interaction, in which we present an overview of tools for processing multi-
lingual communication, such as parsers and language identification systems. We will
also discuss approaches for analyzing patterns in multilingual communication from a
computational perspective. We conclude with a summary of major challenges within
this emerging field.
1.1 Rationale for a Survey of Computational Sociolinguistics
The increased interest in studying a social phenomenon such as language use from a
data-driven or computational perspective exemplifies a more general trend in scholarly
agendas. The study of social phenomena through computational methods is commonly
referred to as ’Computational Social Science’ [152]. The increasing interest of social
scientists in computational methods can be regarded as illustrating the general increase
of attention for cross-disciplinary research perspectives. ‘Multidisciplinary’, ‘interdis-
ciplinary’, ‘cross-disciplinary’ and ‘transdisciplinary’ are among the labels used to
mark the shift from monodisciplinary research formats to models of collaboration that
embrace diversity in the selection of data and methodological frameworks. However,
in spite of various attempts to harmonize terminology, the adoption of such labels is
3
Nguyen et al. Computational Sociolinguistics: A Survey
often poorly supported by definitions and they tend to be used interchangeably. The
objectives of research rooted in multiple disciplines often include the ambition to resolve
real world or complex problems, to provide different perspectives on a problem, or to
create cross-cutting research questions, to name a few [39].
The emergence of research agendas for (aspects of) computational sociolinguistics
fits in this trend. We will use the term Computational Sociolinguistics for the emerging
research field that integrates aspects of sociolinguistics and computer science in study-
ing the relation between language and society from a computational perspective. This
survey article aims to show the potential of leveraging massive amounts of data to study
social dynamics in language use by combining advances in computational linguistics
and machine learning with foundational concepts and insights from sociolinguistics.
Our goals for establishing Computational Sociolinguistics include the development of
tools to support sociolinguists, the establishment of new statistical methods for the mod-
eling and analysis of data that contains linguistic content as well as information on the
social context, to the development or refinement of NLP tools based on sociolinguistic
insights.
1.2 Scope of Discussion
Given the breadth of this field, we will limit the scope of this survey as follows. First
of all, the coverage of sociolinguistics topics will be selective and primarily determined
by the work within computational linguistics that touches on sociolinguistic topics. For
readers with a wish for a more complete overview of sociolinguistics, we recommend
the following introductory readings [20, 119, 169]. The availability of social media and
other online data is one of the primary driving factors for the emergence of computa-
tional sociolinguistics. A relevant research area, but beyond the scope of this survey, is
therefore the study of Computer Mediated Communication (CMC) [109]. Considering
the strong focus on speech data within sociolinguistics, there is much potential for
computational approaches to be applied on speech data as well. Moreover, the increased
availability of recordings of spoken word data and transcribed speech has raised a
revival in the study of the social dimensions of spoken language [130], as well as in
the analysis of the relation between the verbal and the nonverbal layers in spoken
dialogues [256]. In the study of communication in pairs and groups, the individual
contributions are often analyzed in context. Therefore, much of the work on language
use in settings with multiple speakers draws from foundations in Discourse Analysis
[78, 129, 166, 229], Pragmatics (such as Speech Act Theory [10, 235]), Rhetorical Structure
Theory [164, 247] and Social Psychology [87, 206, 223]. For studies that fall within the
scope of computational sociolinguistics that build upon these fields the link with the
foundational frameworks will be indicated. Another relevant field is computational
stylometry [47, 117, 243] which focuses on computational models of writing style for
various tasks such as plagiarism detection, author profiling and authorship attribution.
We limit our discussion to publications within this field that focus on topics like the link
between style and social variables.
1.3 NLP Applications
Besides yielding new insights into language use in social contexts, research in com-
putational sociolinguistics could potentially also impact the development of applica-
tions for the processing of textual social media and other content. For example, user
profiling tools might benefit from research on automatically detecting the gender [33],
4
Nguyen et al. Computational Sociolinguistics: A Survey
age [186], geographical location [68] or other affiliations of users [204] based on their
texts. The challenge is that the cases where it is most helpful to use variables such as
age and gender to properly treat the interpretation of the language are the same cases
where it is most difficult to automatically detect those variables. Nevertheless, there are
some published proofs of concept that suggest potential value in advancing past the
typical ‘static’ view of language embodied in current NLP tools. For example, incor-
porating how language use varies across social groups has improved word prediction
systems [244], algorithms for cyberbullying detection [46] and sentiment-analysis tools
[121, 259]. Hovy and Søgaard [122] shows that POS taggers trained on well-known
corpora such as the English Penn Treebank perform better on texts written by older
authors. They draw attention to the fact that texts in various frequently used corpora
are from a biased sample of authors in terms of demographic factors. Many NLP tools
currently assume that the input consists of monolingual text, but this assumption does
not hold in all domains. For example, social media users may employ multiple language
varieties, even within a single message. To be able to automatically process these texts,
NLP tools that are able to deal with multilingual texts are needed [241].
2. Methods for Computational Sociolinguistics
As discussed, one important goal of this article is to stimulate collaboration between
the fields of sociolinguistics in particular and social sciences research related to com-
munication more broadly on the one hand, and computational linguistics on the other
hand. By addressing the relationship with methods from both sociolinguistics and the
social sciences in general we are able to underline two expectations. First of all, we are
convinced that sociolinguistics and related fields can help the field of computational
linguistics to build richer models that are more effective for the tasks they are or
could be used for. Second, the time seems right for the CL community to contribute
to sociolinguistics and the social sciences, not only by developing and adjusting tools
for sociolinguists, but also by refining the theoretical models within sociolinguistics
using computational approaches and contributing to the understanding of the social
dynamics in natural language.
At the outset of multidisciplinary collaboration, it is necessary to understand differ-
ences in goals and values between communities, as these differences strongly influence
what counts as a contribution within each field, which in turn influences what it would
mean for the fields to contribute to one another. Towards that end, we first discuss
the related but distinct notions of reliability and validity, as well as the differing roles
these notions have played in each field (Section 2.1). This will help lay a foundation
for exploring differences in values and perspective between fields. Here it will be most
convenient to begin with quantitative approaches in the social sciences as a frame of
reference. In Section 2.2 we discuss contrasting notions of theory and empiricism as well
as the relationship between the two, as that will play an important and complementary
role in addressing the concern over differing values. In Section 2.3 we broaden the scope
to the spectrum of research approaches within the social sciences, including strongly
quantitative and strongly qualitative, and the relationship between CL and the social
disciplines involved. This will help to further specify the concrete challenges that must
be overcome in order for a meaningful exchange between communities to take place.
In Section 2.4 we illustrate how these issues come together in the role of data, as the
collection, sampling, and preparation of data are of central importance to work in both
fields.
5
Nguyen et al. Computational Sociolinguistics: A Survey
2.1 Validation of Modeling Approaches
The core of much research in the field of computational linguistics, in the past decade
especially, is the development of new methods for computational modeling, such as
probabilistic graphical models, and deep learning within a neural network approach.
These novel methods are valued both for the creativity that guided the specification
of novel model structures and the corresponding requirement for new methods of
inference as well as the achievement of predictive accuracy on tasks for which there is
some notion of a correct answer.
Development of new modeling frameworks is part of the research production cycle
both within sociolinguistics (and the social sciences in general) and the CL community,
and there is a lot of overlap with respect to the types of methods used. For example,
logistic regression is widely employed by variationist sociolinguists using a program
called VARBRUL [248]. Similarly, logistic regression is widely used in the CL commu-
nity, especially in combination with regularization methods when dealing with thou-
sands of variables (e.g., for age prediction [186]). As another example, latent variable
modeling approaches [143] have grown in prominence within the CL community for
dimensionality reduction, managing heterogeneity in terms of multiple domains or
multiple tasks [278], and approximation of semantics [25, 100]. Similarly, it has grown
in prominence within the quantitative branches of the social sciences for modeling
causality [90], managing heterogeneity in terms of group effects and subpopulations
[43], and time series modeling [216, 217].
The differences in reasons for the application of similar techniques are indicative of
differences in values. While in CL there is a value placed on creativity and predictive
accuracy, within the social sciences, the related notions of validity and reliability under-
line the values placed on conceptual contributions to the field. Validity is primarily a
measure of the extent to which a research design isolates a particular issue from con-
founds so that questions can receive clear answers. This typically requires creativity, and
frequently research designs for isolating issues effectively are acknowledged for this
creativity in much the same way a novel graphical model would be acknowledged for
the elegance of its mathematical formulation. Reliability, on the other hand, is primarily
a measure of the reproducibility of a result. While this might seem to be a distinct notion
from predictive accuracy, the connection is apparent when one considers that a common
notion of reliability is the extent to which two human coders would arrive at the same
judgment about a set of data points, whereas predictive accuracy is the extent to which
a model would arrive at the same judgment on a set of data points as an agreed upon
set of judgments decided ahead of time by one or more humans. While at some deep
level there is much in common between the goals and values of the two communities,
the distinctions between validity and creativity as treated in the field of CL as well as
between reliability and predictive accuracy as treated the social sciences nevertheless
pose challenges for mutual exchange.
Validity is a multi-faceted notion, and it is important to properly distinguish it from
the related notion of reliability. If one considers shooting arrows at a target, one can
consider reliability to be a measure of how much convergence is achieved in location
of impact of multiple arrows. On the other hand, validity is the extent to which the
point of convergence centers on the target. Reproducibility of results is highly valued in
both fields, which requires reliability wherever human judgment is involved, such as in
the production of a gold standard [35, 55]. However, before techniques from CL will be
adopted by social science researchers, standards of validation from the social sciences
6
Nguyen et al. Computational Sociolinguistics: A Survey
will likely need to be addressed [146]. And we will see that this notion requires more
than the related notion of creativity as appreciated within the field of CL.
One aspect that is germane to the notion of validity that goes beyond pure creativity
is the extent to which the essence that some construct actually captures corresponds to
the intended quantity. This aspect of validity is referred to as face validity. For example,
the face validity of a sentiment analysis tool could be tested as follows. First, an au-
tomatic measure of sentiment would be applied on a text corpus. Then, texts would
be sorted by the resulting sentiment score and the data points from the end points
and middle compared with one another. Are there consistent and clear distinctions
in sentiment between beginning, middle, and end? Is sentiment the main thing that
is captured in the contrast, or is something different really going on? While the CL
community have frequently upheld high standards of reliability, it is rare to find work
that deeply questions whether the models are measuring the right thing. Nevertheless,
this deep questioning is core to high quality work in the social sciences, and without it,
the work may appear weak.
Another important notion is construct validity, or the extent to which the experi-
mental design manages extraneous variance effectively. If the design fails to do so,
it affects the interpretability of the result. This notion applies when we interpret the
learned weights of features in our models to make statements about language use. When
not controlling for confounded variables, the feature weights are misleading and valid
interpretation is not possible. For example, many studies on gender prediction (see
Section 3) ignore extraneous variables such as age, while gender and age are known
to interact highly. Where confounds may not have been properly eliminated in an
investigation, again the results may appear weak regardless of the numbers associated
with the measure of predictive accuracy.
Triangulation is an important way of strengthening research findings in the social
sciences. This process can again be illustrated with sentiment analysis as an example.
Consider a blog corpus where the age of each blogger is noted and assume that a
particular predictive model for predicting age placed high weights on some sentiment
carrying related words. The experimenters may conclude that this is evidence that the
model is consistent with previous findings that older people express more positive
sentiment. What might confirm this interpretation would be to use a sentiment analysis
technique to measure sentiment explicitly for each blog. If this measured sentiment
correlates with the age of bloggers across the corpus, this would provide confirming
evidence for the interpretation. This type of confirming evidence is referred to as a
measure of convergent validity, since an expected similar relationship is confirmed.
Another form of triangulation is where distinctions known to exist are confirmed.
For this example, assume that a particular predictive model for predicting political
affiliation placed high weights on some sentiment related words in a corpus related
to issues for which those affiliated with one political perspective would take a different
stance than those affiliated with a different perspective, and this affiliation is known
for all data points. The experimenters may conclude that this evidence is consistent
with previous findings suggesting that voters express more positive sentiment towards
political stances they are in favor of. If this is true, then if the model is applied to a corpus
where both parties agree on a stance, the measure of sentiment should become irrele-
vant. Assuming the difference in the role of sentiment between the corpora is consistent
with what is expected, the interpretation is strengthened. This is referred to as divergent
validity since an expected difference in relationship is confirmed. Seeking convergent
and divergent validity is a mark of high quality work in the social sciences, but it is rare
in evaluations in the field of CL, and without it, again, the results may appear weak
7
Nguyen et al. Computational Sociolinguistics: A Survey
from a social science perspective. In order for methods from CL to be acceptable for use
within the social sciences, these perceived weaknesses must be addressed.
2.2 Theory versus Empiricism
Above we discussed the importance placed on validity within the social sciences that
stems from the goal of isolating an issue in order to answer questions. In order to
clarify why that’s important, it is necessary to discuss the value placed on theory versus
empiricism.
Within the CL community a paradigm shift took place after the middle of the
1990s. Initially, approaches that combined symbolic and statistical methods were of
interest [141]. But with the focus on very large corpora and new frameworks for large-
scale statistical modeling, symbolic and knowledge driven methods have been largely
left aside. Along with older symbolic methods that required carefully crafted gram-
mars and lexicons, the concept of knowledge source became strongly associated with
the notion of theory, which is consistent with the philosophical notion of linguistic
theory advocated by Chomskyan linguistics and other theories of formal linguistics
[11, 97, 232, 272]. As knowledge-based methods were replaced with statistical models, a
grounding in linguistic theory grew more and more devalued. A desire to replace theory
with empiricism became the Zeitgeist that drove progress within the field. The term
theory is associated with old, outdated, symbolic approaches, and thus it has a negative
connotation. Often in computational linguistics, theory is opposed to empiricism, which
is associated with contemporary modeling approaches believed to have a greater ability
to offer insights about language.
In contrast, in the social sciences the value of a contribution is measured in terms
of the extent to which it contributes towards theory. Theories may begin with human
originated ideas. But these notions are only treated as valuable if they are confirmed
through empirical methods. As these methods are applied, theoretical models gain
empirical support. Findings are ratified and then accumulated. And theories thus be-
come storehouses for knowledge obtained through empirical methods. Atheoretical
empiricism is not attractive within the social sciences where the primary value is on
building theory and engaging theory in interpretation of models.
As CL seeks to contribute to sociolinguistics and the social sciences, this culture
clash must be addressed in order to avoid the fields talking at cross purposes. To stimu-
late a collaboration between fields, it is important to not only focus on task performance,
but also to integrate existing theories into the computational models and use these
models to refine or develop new theories.
2.3 Quantitative versus Qualitative Approaches
The social sciences have both strong qualitative and quantitative branches. Similarly,
sociolinguistics has branches in qualitative research (e.g., interactional sociolinguis-
tics) and quantitative research (variationist sociolinguistics). From a methodological
perspective, most computational sociolinguistics work has a strong resemblance with
variationist sociolinguistics, the subfield that has a strong focus on statistical analysis to
uncover the distribution of sociolinguistic variables [248]. Thus, so far we have mostly
reflected on methods used in CL and their commonality with the methods used in the
quantitative branches in sociolinguistics and the social sciences. However, these fields
contain equally strong branches that utilize qualitative methods.
8
Nguyen et al. Computational Sociolinguistics: A Survey
The tension between qualitative and quantitative branches can be illustrated with
the extent to which the agency of speakers is taken into account. As explained in the
introduction, linguistic agency refers to the freedom of speakers to make choices about
how they present themselves in interaction. A contrasting notion is the extent to which
social structures influence the linguistic choices speakers make. Regardless of research
tradition, it is acknowledged that speakers both have agency and are simultaneously
influenced by social structures. The question is which is privileged in the research
approach. Quantitative researchers believe that the most important variance is captured
by representation of the social structure. They recognize that this is a simplification, but
the value placed on quantification for the purpose of identifying causal connections
between variables makes the sacrifice of accuracy worth it. In the field of CL, this
valuing is analogous to the well-known saying that all models are wrong, but some
are nevertheless useful. On the other side are researchers committed to the idea that the
most important and interesting aspects of language use are the ones that violate norms
in order to achieve a goal. These researchers may also doubt that the bulk of choices
made by speakers can be accounted for by social structures. We see this balance and
tension within sociolinguistics in that many studies within variationist sociolinguistics
focus on the influence of social structures [20], but more recently the agency of speakers
is playing a more central role [63].
While in CL qualitative research is sometimes dismissed as being quantitative work
that lacks rigor, one could argue that high quality qualitative research has a separate
notion of rigor and depth that is all its own [177]. An important role for qualitative
research is to challenge the operationalizations constructed by quantitative researchers.
To achieve the adoption of CL methods and models by social science researchers, the
challenges from the qualitative branches of the social sciences will become something
to consider carefully.
As computational linguistics shares more values with variationist sociolinguistics,
many studies within computational sociolinguistics also focus on the influence of social
structures. For example, work on prediction of social variables such as gender (Sec-
tion 3) is based on the assumption that gender determines the language use of speakers.
However, such research ignores the agency of speakers: Speakers use language to
construct their identity and thus not everyone might write in a way that reflects their
biological sex. Moving forward it would make sense for researchers in Computational
Sociolinguistics to reflect on the dominant role of social structures over agency. Some
work in CL has already begun to acknowledge the agency of speakers when interpreting
findings [17, 187].
One way of conceptualizing the contrast between the usage of computational mod-
els in the two fields is to reconsider the trade-off between maximizing interpretability
— typical of the social sciences and sociolinguistics —, versus maximizing predictive
accuracy, typical of CL. Both fields place a premium on rigor in evaluation and general-
ization of results across datasets. To maintain a certain standard of rigor, the CL commu-
nity has produced practices for standardization of metrics, sampling, and avoidance of
over-fitting or over-estimation of performance through careful separation of training
and testing data at all stages of model development. Within the social sciences, the
striving for rigor also has produced statistical machinery for analysis, but most of all
it has resulted in an elaborate process for validation of such modeling approaches and
practices for careful application and interpretation of the results.
One consequence of the focus on interpretability within the social sciences is that
models tend to be kept small and simple in terms of the number of parameters, fre-
quently no more than 10, or at least no more than 100. Because the models are kept
9
Nguyen et al. Computational Sociolinguistics: A Survey
simple, they can be estimated on smaller datasets, as long as sampling is done carefully
and extraneous variance is controlled. In the CL community, it is more typical for
models to include tens of thousands of parameters or more. For such large models,
massive corpora are needed to prevent over-fitting. As a result, research in the CL
community is frequently driven by the availability of large corpora, which explains the
large number of recent papers on data from the web, such as Twitter and Wikipedia.
Because of this difference in scale, a major focus on parallelization and approximate
inference has been an important focus of work in CL [112], whereas interest in such
methods have only recently grown within the social sciences.
2.4 Spotlight on Corpora and Other Data
Data collection is a fundamental step in the research cycle for researchers in both
sociolinguistics and computational linguistics, but there are important differences in
the practices and traditions within both fields. Traditionally sociolinguists have been
interested in datasets that capture informal speech (also referred to as the ’vernacular’),
i.e., the kind of language used when speakers are not paying attention [248]. A variety of
methods have been used to collect data, including observation, surveys and interviews
[163, 248]. The sociolinguistic datasets are carefully prepared to enable in-depth analy-
ses of how a speech community operates, carefully observing standards of reliability
and validity as discussed above. Inevitably, the data collection methods are labor-
intensive and time-consuming. The resulting datasets are often small in comparison to
the ones used within computational linguistics. The small sizes of these datasets made
the work in sociolinguistics of limited interest to the field of CL. The tide began to turn
with the advent of Web 2.0, which brought with it the possibility to obtain much larger
linguistic data sets from social contexts.
Content generated by users of online platforms (often referred to as User-Generated
Content (UGC), such as social media [70, 144], web forums [82, 184] and online reviews
[52, 123]), is a rich and easy to access source of large amounts of informal speech coming
together with information about the context (e.g., the users, social network structure,
the time and geolocation at which it was generated) that can be used for the study of
language in social contexts on a large scale. As an example, based on data from Twitter
(a popular microblogging site) dialectical variation has been mapped using a fraction of
the time, costs and effort that was needed in traditional studies [60].
The availability of this data is not only of interest within CL, but also within
sociolinguistics. A complicating issue in work in sociolinguistics is that participants
might adjust their language use towards the expectations of the data collector. This
phenomenon is known as the ‘observer’s paradox’ and was first coined by Labov [149]:
’the aim of linguistic research in the community must be to find out how people talk
when they are not being systematically observed; yet we can only obtain these data by
systematic observation’. In social media, the observer’s paradox could potentially be
argued to have lost much of its strength, making it a promising resource to complement
traditional data collection methods. While a convenient source of data, the use of social
media data does introduce new challenges which must be addressed regardless of field,
and this offers a convenient beginning to a potential exchange between fields.
First, social media users are usually not representative of the general population
[175, 186]. A better understanding of the demographics could aid the interpretation of
findings, but often little is known about the users. Collecting demographic information
requires significant effort, or might not even be possible in some cases due to ethical con-
cerns. Furthermore, in many cases the complete data is not fully accessible through an
10
Nguyen et al. Computational Sociolinguistics: A Survey
API, requiring researchers to apply a sampling strategy (e.g., randomly, by topic, time,
individuals/groups, phenomenon [4, 110]), which could introduce additional biases or
remove contextual information. This is complicated by the fact that datasets might be
reused for secondary analysis by other researchers and that during the data collection it
is not known yet which variables will be relevant for future research questions.
Social media data also introduce new units of analysis (such as messages and
threads) that do not correspond entirely with traditional analysis units (such as sen-
tences and turns) [4]. This raises the question about valid application of findings from
prior work. Furthermore, while most sociolinguistic studies are based on oral data and
often focus on phonological variation, the use of social media data in computational
linguistics has led to a higher focus on lexical variation. However, there are concerns that
a focus on lexical variation without regard to other aspects may threaten the validity of
conclusions. Phonology does impact social media orthography at both the word level
and structural level [66], suggesting that studies on phonological variation could inform
studies on texts in social media and vice versa.
There are practical concerns as well. First, while both access and content have
often been conceptualized as either public or private, in reality this distinction is not
as absolute (a user might discuss a private topic on a public social media site). In
view of the related privacy issues, Bolander and Locher [26] argue for more awareness
regarding the ethical implications of research using social media data.
Automatically processing social media data is more difficult compared to various
other types of data that have been used within computational linguistics. Many de-
veloped tools (e.g., parsers, named entity recognizers) do not work well due to the
informal nature of many social media texts. While the dominant response has been to
focus on text normalization and domain adaptation, Eisenstein [65] argues that doing
so is throwing away meaningful variation. For example, building on work on text
normalization, Gouws et al. [94] showed how various transformations (e.g., dropping
the last character of a word) vary across different user groups on Twitter. How the
different online data sources compare to each other in terms of language use has only
received limited attention so far [14, 124].
Apart from raw interaction data provided by Web 2.0, another promising resource
for studying language in use from a social perspective is crowdsourcing. So far, crowd-
sourcing is mostly used to obtain large numbers of annotations (e.g., [238]), but another
purpose of involving ’crowds’ are large-scale perception studies, i.e., studying how
non-linguists interpret messages and identify social characteristics of speakers [41].
Within sociolinguistics, surveys have been one of the instruments to collect data for
perception studies, and the use of crowdsourcing can be seen as building upon that
methodology. Crowdsourcing has already been used to collect data for studying how
English utterances are perceived differently across language communities [162] and
how Twitter users are perceived based on their tweets [187]. However, the additional
effort for quality control and the fact that little is known is about the workers, such as
the communities they are part of, pose new challenges.
11
Nguyen et al. Computational Sociolinguistics: A Survey
3. Language and Social Identity
Speakers use language to construct their social identity [31]. Being involved in commu-
nicative exchange can be functional for the transfer of information, but at the same it
functions as a staged performance in which users select specific codes (e.g., language,
dialect, style [264]) that shape their communication. Consciously or unconsciously
speakers adjust their performance to the specific social context and to the impression
they intend to make on their audience. Each speaker has a personal linguistic repertoire
to draw linguistic elements or codes from. Selecting from the repertoire is partially
subject to ’identity work’, referring to the range of activities that individuals engage in to
create, present, and sustain personal identities that are congruent with and supportive
of the self-concept [237].
Language is one of the instruments that speakers use in shaping their identities, but
there are limitations (e.g. physical or genetic constraints) to the variation that can be
achieved. For example, somebody with a smoker’s voice will not be able to speak with
clear voice but many individual characteristics still leave room for variation. Although
traditionally attributed an absolute status, personal features (e.g., age and gender) are
increasingly considered social rather than biological variables. Within sociolinguistics, a
major thrust of research is to uncover the relation between social variables (e.g., gender,
age, ethnicity, status) and language use [62, 64, 120, 262].
By recognizing that language use can reveal social patterns, many studies in com-
putational linguistics have focused on automatically inferring social variables from
text. This task can be seen as a form of automatic metadata detection that can provide
information on author features. The growing interest in trend analysis tools is one of
the drivers for the interest in the development and refinement of algorithms for this
type of metadata detection. However, tasks such as gender and age prediction do not
only appeal to researchers and developers of trend mining tools. Various public demos
have been able to attract the attention of the general public (e.g., TweetGenie2 [188] and
Gender Guesser3), which can be attributed to widespread interest in the entertaining
dimension of the linguistic dimension of identity work. The automatic prediction of
individual features such as age and gender based on only text are nontrivial tasks.
Studies that have compared the performance of humans with that of automatic systems
for gender and age prediction based on text alone found that automatic systems perform
better than humans [33, 186]. A system based on aggregating guesses from a large
number of people still predicted gender incorrectly for 16% of the Twitter users [187].
While most studies use a supervised learning approach, a recent study explored a
lightly supervised approach using soft constraints such as derived from Census data
(e.g., the proportion of people below or above 25 years in a county) [5].
This section will first focus on the datasets that have been used to investigate
social identity and language variation in computational linguistics (Subsection 3.1).
In the subsections 3.2-3.4 an overview will be presented about computational studies
of language variation based on gender, age and geographical location. These three
variables have received the most attention in computational linguistics, compared to
other variables such as ethnicity [5, 201, 221] and social class. While many studies
have focused on individual social variables, these variables are not independent. For
example, there are indications that linguistic features that are used more by males
2 http://www.tweetgenie.nl
3 http://www.hackerfactor.com/GenderGuesser.php
12
Nguyen et al. Computational Sociolinguistics: A Survey
increase in frequency with age as well [7]. As another example, some studies have
suggested that language variation across gender tends to be stronger among younger
people and tends to fade away with older ages [18]. Eckert notes that the appropriate
age for cultural events often differs for males and females [62], which could influence
language variation patterns. The interaction between these variables is complicated by
the fact that in many uncontrolled settings the gender distribution may not be equal for
different age ranges (as observed in blogs [32] and Twitter [186]). Therefore, failing to
control for gender while studying age (and vice versa) can lead to misinterpretation
of the findings. We conclude with a discussion of how various NLP tasks, such as
sentiment detection, can be improved by accounting for linguistic variation related to
the social identity of speakers (Subsection 3.5).
3.1 Data Sources
Early computational studies on social variables and language use were based on formal
texts, such as the British National Corpus [6, 145], or datasets collected from controlled
settings, such as recorded conversations [236] and telephone conversations [27, 81, 258]
where protocols were used to coordinate the conversations (such as topic selection).
With the advent of social media, a shift is observed towards more informal texts
collected from uncontrolled settings. Much of the initial work in this domain focused
on blogs. The Blog Authorship Corpus [231], collected in 2004 from blogger.com data,
has been used in various studies on gender and age [7, 84, 93, 185, 227]. Others have
created their own blog corpus from various sources including LiveJournal and Xanga
[32, 179, 192, 225, 228, 274].
More recent studies are focusing on Twitter data, which contains richer interactions
in comparison to blogs. Burger et al. [33] created a large corpus by following links
to blogs that contained author information provided by the authors themselves. The
dataset has been used in various subsequent studies [22, 258, 259]. Others created their
own Twitter dataset [69, 142, 158, 220, 276]. While early studies focused on English,
recent studies have studied non-English Twitter data as well (Dutch [186], Spanish
and Russian [259], and Japanese, Indonesian, Turkish, and French [40]). Besides blogs
and Twitter, other web sources have been explored, including LinkedIn [142], IMDB
[194], YouTube [77], e-mails [44], a Belgian social network site [198] and Facebook
[221, 227, 233].
Two aspects can be distinguished that are often involved in the process of creating
datasets to study the relation between social variables and language use.
Labeling. Datasets derived from uncontrolled settings such as social media often lack
explicit information regarding the identity of users, such as their gender, age or location.
Researchers have used different strategies to acquire adequate labels:
• User-provided information. Many researchers deploy information provided
by the social media users themselves, for example based on explicit fields
in user profiles [33, 231, 274], or by searching for specific patterns (e.g.,
birthday announcements [276]). While this information is probably highly
accurate, such information is often only available for a small set of users
(e.g., for age, 0.75% on Twitter [158] and 55% in blogs [32]). Locations of
users have been derived based on geotagged messages [68] or locations in
user profiles [178].
13
Nguyen et al. Computational Sociolinguistics: A Survey
• Manual annotation. Another option is manual annotation based on personal
information revealed in the text, profile information, and public
information on other social media sites [40, 186]. In the manual annotation
scenario, a random set of authors is annotated; however, the required
effort is much higher resulting in smaller datasets and biases of the
annotators themselves might influence the annotation process.
Furthermore, for some users not enough information may be available to
even manually assign labels.
• Exploiting names. Some labels can be automatically extracted based on the
name of a user. For example, gender information for names can be derived
from census information from the US Social Security Administration
[17, 212], or from Facebook data [79]. In some languages, such as Russian,
the morphology of the names can also be used to collect gender labels
[259]. However, users who do not provide their names, or have
uncommon names, will remain unlabeled. In addition, acquiring labels
this way has not been well studied yet for other languages and cultures
and for other type of labels (such as geographical location or age).
Sample selection. In many cases, it is necessary to limit the study to a sample of persons.
Sometimes the selected sample is directly related to how labels are obtained, for exam-
ple by only including people that explicitly list their gender or age in their social media
profile [33], who have a gender specific first name [17], or who have geotagged tweets
[68]. Others have used random sampling, or as random as possible due to restrictions of
targeting a specific language [186]. However, in these cases the labels may not be readily
available which increases the annotation effort (e.g., annotators need to manually go
through the texts or inspect external websites to derive the gender or age). Another
approach is focused sampling, for example by starting with social media accounts re-
lated to gender-specific behavior (e.g., male/female hygiene products, sororities) [220].
However, such an approach has the danger of creating biased datasets, which could
influence the prediction performance numbers [42].
3.2 Gender
The study of gender and language variation has received much attention in sociolin-
guistics [64, 120]. Various studies have highlighted gender differences. According to
Tannen [250], women engage more in ‘rapport’ talk, focusing on establishing connec-
tions, while men engage more in ’report’ talk, focusing on exchanging information.
Similarly, according to Holmes [118], in women’s communication the social function
of language is more salient, while in men’s communication the referential function
(conveying information) tends to be dominant. Argamon et al. [6] make a distinction be-
tween involvedness (more associated with women) and informational (more associated
with men). However, with the increasing view that speakers use language to construct
their identity, such generalizations have also been met with great criticism. Many of
these studies rely on small sample sizes, ignore other variables (such as ethnicity, social
class) and the many similarities between genders. Such generalizations contribute to
stereotypes and the view of gender as an inherent property.
3.2.1 Modeling Gender. Within computational linguistics, researchers have focused
primarily on automatic gender classification based on text. Gender is then treated as
14
Nguyen et al. Computational Sociolinguistics: A Survey
a binary variable based on biological characteristics, resulting in a binary classification
task. A variety of machine learning methods have been explored, including SVMs
[27, 40, 44, 79, 84, 179, 192, 198, 220, 276], Logistic Regression [22, 194], Naive Bayes
[93, 179, 274] and the Winnow algorithm [33, 231]. However, treating gender as a binary
variable based on biological characteristics assumes that gender is fixed and is some-
thing people have, instead of something people do [34], i.e., such a setup neglects the
agency of speakers. Many sociolinguists, together with scholars from a variety of other
disciplines such as the social sciences, view gender as a social construct, emphasizing
that gendered behavior is a result of social conventions rather than inherent biological
characteristics.
3.2.2 Features and Patterns. Rather than focusing on the underlying machine learn-
ing models, most studies have focused on developing predictive features. Token-level
and character-level unigrams and n-grams have been explored in various studies
[17, 33, 79, 228, 274]. Sarawgi et al. [228] found character-level language models to be
more robust than token-level language models. Grouping words by meaningful classes
could improve interpretation and possibly performance of the models. LIWC [202],
a word counting program based on manually defined dictionaries, has been used in
experiments on Twitter data [79] and blogs [192, 231]. However, models based on LIWC
alone tend to perform worse than unigram/ngram models [79, 192]. By analyzing the
developed features, these studies have shown that males tend to use more numbers [17],
technology words [17] and URLs [231, 186], while females use more terms referring to
family and relation issues [27].
Various features based on grammatical structure have been explored, includ-
ing features capturing individual POS frequencies [6, 194] as well as POS patterns
[6, 8, 17, 231]. Males tend to use more prepositions [7, 8, 194, 231] and more articles
[7, 192, 194, 231, 233], however Bamman et al. [17] did not find these to be significant
in their Twitter study. Females tend to use more pronouns [6, 7, 8, 17, 194, 231, 233] (in
particular first person singular [186, 194, 233]). A measure introduced by Heylighen and
Dewaele [114] to measure formality based on the frequencies of different word classes
has been used in experiments on blogs [179, 193]. Sarawgi et al. [228] experimented
with probabilistic context-free grammars (PCFGs). In some languages such as French,
the gender of nouns (including the speaker) is often marked in the syntax. For example,
a male would say ’je suis allé’, while a female would say ’je suis allée’ (‘I went’). By
detecting such ’je suis’ constructions, Ciot et al. [40] improved performance of gender
classification in French.
Stylistic features have been widely explored as well. Studies have reported that
males tend to use longer words, sentences and texts [93, 194, 236] and more swear words
[27, 233]. Females use more emotion words [17, 192, 233], emoticons [17, 84, 220, 259],
and CMC words like omg and lol [17, 231].
Groups can be characterized by their attributes, for example females tend to have
maiden names. Bergsma and Van Durme [22] used such distinguishing attributes, ex-
tracted from common nouns for males and females (e.g., granny, waitress), to improve
classification performance. Features based on first names have also been explored.
Although not revealing much about language use itself, they can improve prediction
performance [22, 33, 221].
Genre. So far, not many studies have analyzed the influence of genre and domain [153]
on language use, but a better understanding will aid the interpretation of observed
language variation patterns. Using data from the British National Corpus, Argamon
15
Nguyen et al. Computational Sociolinguistics: A Survey
et al. [6] found a strong correlation between characteristics of male and non-fiction
writing and likewise, between female and fiction writing. Based on this observation,
they trained separate prediction models for fiction and non-fiction [145]. Building on
these findings, Herring et al. [111] investigated whether gender differences would still
be observed when controlling for genre in blogs. They did not find a significant relation
between gender and previously identified gender features, however the study was
based on a relatively small sample.
Studies focusing on gender prediction have tested the generalizability of gender
prediction models by training and testing on different datasets. Although models tend
to perform worse when tested on a different dataset than the one used for training,
studies have shown that prediction performance is still higher than random, suggesting
that there are indeed gender-specific patterns of language variation that go beyond
genre and domain [227, 228]. Gianfortoni et al. [84] proposed the use of ’stretchy
patterns’, flexible sequences of categories, to model stylistic variation and to improve
generalizability across domains.
Social Interaction. Most computational studies on gender-specific patterns in language
use have studied speakers in isolation. As the conversational partner4 or social network
of speakers influence the language use of speakers, several studies have extended their
focus by also considering contextual factors. For example, this led to the finding that
speakers use more gender-specific language in same-gender conversations [27]. On the
Fisher and Switchboard corpus (telephone conversations), classifiers dependent on the
gender of the conversation partner improve performance [81]. However, exploiting the
social network of speakers on Twitter has been less effective so far. Features derived
from the friends of Twitter users did not improve gender classification (but it was effec-
tive for age) [276]. Likewise, Bamman et al. [17] found that social network information
of Twitter users did not improve gender classification when enough text was available.
Not all computational studies on gender in interaction contexts have focused on
gender classification itself. Some have used gender as a variable when studying other
phenomena. In a study on language and power, Prabhakaran et al. [212] showed how
the gender composition of a group influenced how power is manifested in the Enron
corpus. In a study on language change in online communities, Hemphill and Otter-
bacher [108] found that females write more like men over time in the IMDb community
(a movie review site), which they explain by men receiving more prestige in the com-
munity. Jurafsky et al. [136] automatically classified speakers according to interactional
style (awkward, friendly, or flirtatious) using various types of features, including lexical
features based on LIWC [202], prosodic, and discourse features. Differences, as well as
commonalities, were observed between genders, and incorporating features from both
speakers improved classification performance.
3.2.3 Interpretation of Findings. As mentioned before, most computational approaches
adopt a simplistic view of gender as an inherent property based on biological char-
acteristics. Only recently, the computational linguistics community has noticed the
limitations of this simplistic view by acknowledging the agency of speakers. Two of
these studies based their argumentation on an analysis of the social networks of the
users. Automatic gender predictions on YouTube data correlated more strongly with
4 An individual who participates in a conversation, sometimes also referred to as interlocutor and
addressee
16
Nguyen et al. Computational Sociolinguistics: A Survey
the dominant gender in a user’s network than the user-reported gender [77]. Likewise,
in experiments by Bamman et al. [17] incorrectly labeled Twitter users also had have
fewer same-gender connections. In addition, clusters were identified of users who used
linguistic markers that conflicted with general population-level findings. Another study
was based on data collected from an online game [187]. Thousands of players guessed
the age and gender of Twitter users based on their tweets, and the results revealed that
many Twitter users do not tweet in a gender stereotypical way.
Thus, language is inherently social and while certain language features are on
average used more by males or females, individual speakers may diverge from the
stereotypical images that tend to be highlighted by many studies. In addition, gender is
shaped differently depending on the culture and language, and thus presenting gender
as a universal social variable can be misleading. Furthermore, linguistic variation within
speakers of the same gender holds true as well.
3.3 Age
Aging is a universal phenomenon and understanding the relation between language
and age can provide interesting insights in many ways. An individual at a specific time
represents both a place in history as well as a life stage [62], and thus observed patterns
can generate new insights into language change as well as how individuals change their
language use as they move through life. Within computational linguistics, fewer studies
have focused on language variation according to age compared to studies focusing on
gender, possibly because obtaining age labels requires more effort than gender labels
(e.g., the gender of people can often be derived from their names; cf. Section 3.1). Most
of these studies have focused on absolute chronological age, although age can also be
seen as a social variable like gender.
Sociolinguistic studies have found that adolescents use the most non-standard
forms, because at young age the group pressure to not conform to established societal
conventions is the largest [62, 119]. In contrast, adults are found to use the most standard
language, because for them social advancement matters and they use standard language
to be taken seriously [20, 62]. These insights can explain why predicting the ages of older
people is harder (e.g., distinguishing between a 15 and a 20 year old person based on
their language use is easier than distinguishing between a 40 and 45 years old person)
[186, 187].
3.3.1 Modeling Age. A fundamental question is how to model age, and so far researchers
have not reached a consensus yet. Eckert [62] distinguishes between chronological age
(number of years since birth), biological age (physical maturity) and social age (based
on life events). Speakers are often grouped according their age, because the amount
of data is in many cases not sufficient to make more fine-grained distinctions [62]. Most
studies consider chronological age and group speakers based on age spans [18, 148, 254].
However, chronological age can be misleading since persons with the same chronolog-
ical age may have had very different life experiences. Another approach is to group
speakers according to ‘shared experiences of time’ (e.g., high school students) [62].
Within computational linguistics the most common approach is to model age-
specific language use based on the chronological age of speakers (except Nguyen et
al. [186] who have explored the option of classification into life stages). However, even
when focusing on chronological age, the task can be framed in different ways as well.
Chronological age prediction has mostly been approached as a classification problem,
modeling the chronological age as a categorical variable. Based on this task formulation,
17
Nguyen et al. Computational Sociolinguistics: A Survey
various classical machine learning models have been used, such as SVMs [198, 220],
logistic regression [186, 225] and Naive Bayes [249].
The boundaries used for discretizing age have varied depending on the dataset and
experimental setup. Experiments on the blog authorship corpus [231] used categories
based on the following age spans: 13-17, 23-27, and 33-47, removing the age ranges in
between to simplify the task. Rangel et al. [218] adopted this approach in the Author
Profiling task at PAN 2013. The following year, the difficulty of the task at PAN 2014
was increased by considering the more fine-grained categories of 18-24, 25-34, 35-49, 50-
64 and 65+ [219]. Zamal et al. [276] classified Twitter users into 18-23 and 25-30. Other
studies explored boundaries at 30 [220], at 20 and 40 [186], at 40 [81] and at 18 [32].
In several studies experiments have been done by varying the classification bound-
aries. Peersman et al. [198] experimented with binary classification and boundaries at
16, 18 and 25. Tam and Martell [249] experimented with classifying teens versus 20s, 30s,
40s, 50s and adults. Not surprisingly, in both studies a higher performance was obtained
when using larger age gaps (e.g., teens versus 40s/50s) than when using smaller age
gaps (e.g., teens versus 20s/30s) [198, 249]. Rosenthal and McKeown [225] explored a
range of splits to study differences in performance when predicting the birth year of
blog authors. They related their findings to pre- and post social media generations.
For many applications, modeling age as a categorical variable might be sufficient.
However, it does have several limitations. First, selecting age boundaries has proven to
be difficult. It is not always clear which categories are meaningful. Secondly, researchers
have used different categories depending on the age distribution of their dataset which
makes it difficult to make comparisons across datasets.
Motivated by such limitations, recent studies have modeled age as a continuous vari-
able removing the need to define age categories. Framing age prediction as a regression
task, a frequently used method has been linear regression [185, 186, 227, 233]. Liao et
al. [158] experimented with a latent variable model that jointly models age and topics.
In their model, age-specific topics obtain low standard deviations of age, while more
general topics obtain high standard deviations.
3.3.2 Features and Patterns. The majority of studies on age prediction have focused on
identifying predictive features. While some features tend to be effective across domains,
others are domain specific [185]. Features that characterize male speech have been found
to also increase with age [7], thus simply said, males tend to sound older than they are.
Unigrams alone already perform well [185, 186, 198]. Features based on part of
speech are effective as well. For example, younger people tend to use more first and
second person singular pronouns (e.g., I, you), while older people more often use first
person plural pronouns (e.g., we) [18, 186, 225]. Older people also use more prepositions
[8, 186], determiners [8, 185] and articles [233]. Most of these studies focused on English
and therefore some of these findings might not hold in other languages (for example,
some languages do not have pronouns).
Various studies have found that younger people use less standard language. They
use more alphabetical lengthening (e.g., niiiice) [186, 220], more contractions without
apostrophes (e.g., dont) [8], more Internet acronyms (e.g., lol) [225], more slang [18, 225],
more swear words [18, 185], and more capitalized words (e.g., HAHA) [186, 225]. Spe-
cific words such as like are also highly associated with younger ages [18, 185]. Younger
people also use more features that indicate stance and emotional involvement [18], such
as intensifiers [18, 186] and emoticons [225]. Younger people also use shorter words and
sentences and write shorter tweets [32, 186, 225].
18
Nguyen et al. Computational Sociolinguistics: A Survey
3.3.3 Interpretation of Findings. Age prediction experiments are usually done on
datasets collected at a specific point in time. Based on such datasets, language use is
modeled and compared between users with different ages. Features that are found to
be predictive or that correlate highly with age are used to highlight how differently
’younger’ and ’older’ people talk or write. However, the observed differences in lan-
guage use based on such datasets could be explained in multiple ways. Linguistic
variation can occur as an individual moves through life (age grading), and in that case
the same trend is observed for individuals at different time periods. Linguistic variation
can also be a result of changes in the community itself as it moves through time
(generational change) [20, 226]. For example, suppose we observe that younger Twitter
users include more smilies in their tweets. This could indicate that smiley usage is
higher at younger ages, but that when Twitter users grow older they decrease their
usage of smileys. However, this could also indicate a difference in smiley usage in
generations (i.e. the generation of the current younger Twitter users use more smileys
compared to the generation of the older Twitter users). This also points to the relation
between synchronic variation and diachronic change. Synchronic variation is variation
across different speakers or speech communities at a particular point in time, while
diachronic change is accumulation of synchronic variation in time and frequency. In
order for a change to be effective, synchronic variations should increase their frequency
over time. To have a better understanding of change, we need to understand the spread
of variation across time and amount. As is the case for gender, age can be considered
a social variable and thus when only modeling chronological age, we are ignoring the
agency of speakers and that speakers follow different trajectories in their lives.
3.4 Location
Regional variation has been extensively studied in sociolinguistics and related areas
such as dialectology [38] and dialectometry [270]. The use of certain words, grammatical
constructions, or the pronunciation of a word, can often reveal where a speaker is from.
For example, ‘yinz’ (a form of the second-person pronoun) is mostly used around Pitts-
burgh, which can be observed on Twitter as well [67]. Dialectology traditionally focuses
on the geographical distribution of individual or small sets of linguistic variables [38].
A typical approach involves identifying and plotting isoglosses, lines that divide maps
into regions where specific values of the variable predominate. The next step involves
identifying bundles of isoglosses, often followed by the identification of dialect regions.
While these steps have usually been done manually, recently Grieve et al. [99] illustrated
how statistical methods can be used for automating such an analysis.
The study of regional variation is perhaps one of the main areas within sociolin-
guistics that has been heavily influenced by new statistical approaches, such as from
computational linguistics, machine learning and spatial analysis. A separate branch,
dialectometry [270], has emerged. In contrast to dialectology which involves individual
linguistic variables, dialectometry involves aggregating linguistic variables to examine
linguistic differences between regions. This aggregation step led to the introduction of
various statistical methods, including clustering, dimensionality reduction techniques
and regression approaches [106, 182, 269]. Recently, researchers within dialectometry
have explored the automatic identification of characteristic features of dialect regions
[269], a task which aligns more closely with the approaches taken by dialectologists.
While the datasets typically used in dialectology and dialectometry studies are
still small compared to datasets used in computational linguistics, similar statistical
19
Nguyen et al. Computational Sociolinguistics: A Survey
methods have been explored. This has created a promising starting point for closer
collaboration with computational linguistics.
3.4.1 Modeling Regional Dialects. Within CL, we find two lines of work on computa-
tionally modeling dialects.
Supervised. The first approach starts with documents labeled according to their dialect,
which can be seen as a supervised learning approach. Most studies taking this approach
focus on automatic dialect identification, which is a variation of automatic language
identification, a well-studied research topic within the field of computational linguistics
[13, 128], that started in the 1960s with work by Gold [91]. While some have considered
automatic language identification a solved problem [168], still many outstanding issues
exist [128], including the identification of dialects and closely related languages. In stud-
ies of automatic dialect identification, various dialects have been explored including
Arabic [54, 74, 275], Turkish [59], English [160], Swiss German [230] and Dutch [253]
dialects.
Unsupervised. An alternative approach is to start with location-tagged data to automati-
cally identify dialect regions. While the models are given labels indicating the locations
of speakers, the dialect labels themselves are not observed. In the context of modeling
dialects, we consider it an unsupervised approach (although it can be considered a
supervised approach when the task is framed as a location prediction task). The majority
of the work in this area has used Twitter data, because it contains fine-grained location
information in the form of GPS data for tweets, or user-provided locations in user
profiles.
Much of the research that starts with location-tagged data is done with the aim
of automatically predicting the location of speakers. The setup is thus similar to the
setup for the other tasks that we have surveyed in this section (e.g., gender and age
prediction). Eisenstein et al. [68] developed a topic model to identify geographically
coherent linguistic regions and words that are highly associated with these regions. The
model was tested by predicting the locations of Twitter users based on their tweets.
While the topic of text-based location prediction has received increasing attention [104,
271], using these models for the discovery of new sociolinguistic patterns is an option
that has not been fully explored yet since most studies primarily focus on prediction
performance.
Various approaches have been explored to model the location of speakers, an aspect
that is essential in many of the studies that start with location-tagged data. In Wing
and Baldridge [271], locations are modeled using geodesic grids, but these grids do not
always correspond to administrative or language boundaries. Users can also be grouped
based on cities [104], but such an approach is not suitable for users in rural areas or when
the focus is on more-fine grained geographical variation (e.g., within a city). Eisenstein
et al. [68] model regions using Gaussian distributions, but only focused on the United
States and thus more research is needed to investigate the suitability of this approach
when considering other countries or larger regions.
3.4.2 Features and Patterns. Word and character n-gram models have been frequently
used in dialect identification [140, 160, 253, 275]. Similarly, many text-based location
prediction systems make use of unigram word features [68, 104, 271].
Features inspired by sociolinguistics could potentially improve performance. Dar-
wish et al. [54] showed that for identifying Arabic dialects a better classification perfor-
20
Nguyen et al. Computational Sociolinguistics: A Survey
mance could be obtained by incorporating known lexical, morphological and phono-
logical differences in their model. Scherrer and Rambow [230] also found that using
linguistic knowledge improves over an n-gram approach. Their method is based on a
linguistic atlas for the extraction of lexical, morphological and phonetic rules and the
likelihood of these forms across German-speaking Switzerland. Doğruöz and Nakov
[59] explored the use of light verb constructions to distinguish between two Turkish
dialects.
To support the discovery of new sociolinguistic patterns and to improve prediction
performance, several studies have focused on automatically identifying characteristic
features of dialects. Han et al. [104] explored various feature selection methods to
improve location prediction and the selected features may reflect dialectal variation but
this was not the focus of the study. The method by Prokić et al. [214] was based on
in-group and out-group comparisons based on data in which linguistic varieties were
already grouped (e.g., based on clustering). Peirsman et al. [199] compared frequency-
based measures, such as chi-square and log-likelihood tests with distributional meth-
ods. Automatic methods may identify many features that vary geographically such
as topic words and named entities, and an open challenge is to separate this type
of variation from the more sociolinguistically interesting variations. For example, the
observation that the word ‘beach’ is used more often near coastal areas or that ‘Times
Square’ is used more often in New York is not interesting from the perspective of a
sociolinguist.
Making use of location-tagged data, several studies have focused on analyzing
patterns of regional variation. Doyle [60] analyzed the geographical distribution of
dialectal variants (e.g., the use of double modals like ‘might could’) based on Twitter data,
and compared it with traditional sociolinguistic data collection methods. Starting with
a query-based approach, he uses baseline queries (e.g., ‘I’) for estimating a conditional
distribution of data given metadata. His approach achieved high correlations with data
from sociolinguistic studies. Jørgensen et al. [132] studied the use of three phonological
features of African American Vernacular English using manually selected word pairs.
The occurrence of the features were correlated with location data (longitude and lati-
tude) as well as demographic information obtained from the US census bureau. While
these approaches start with attested dialect variants, automatic discovery of unknown
variation patterns could potentially lead to even more interesting results. To study how
a word’s meaning varies geographically, Bamman et al. [16] extended the skip gram
model by Mikolov et al. [171] by adding contextual variables that represent states from
the US. The model then learns a global embedding matrix and additional matrices for
each context (i.e. state) to capture the variation of a word’s meaning.
The increasing availability of longitudinal data has made it possible to study the
spreading of linguistic innovations geographically and over time on a large scale. A
study by Eisenstein et al. [70] based on tweets in the United States indicates that
linguistic innovations spread through demographically similar areas, in particular with
regard to race.
3.4.3 Interpretation of Findings. Labeling texts by dialect presumes that there are
clear boundaries between dialects. However, it is not easy to make absolute distinc-
tions between language varieties (e.g., languages, dialects). Chambers and Trudgill [38]
illustrates this with the example of traveling from village to village. Speakers from
villages at larger distances have more difficulty understanding each other compared to
villages that are closer to each other, but there is no clear-cut distance at which speakers
are no longer mutually intelligible. Besides linguistic differences, boundaries between
21
Nguyen et al. Computational Sociolinguistics: A Survey
language varieties are often influenced by other factors including political boundaries
[38]. Therefore, deciding on the appropriate labels to describe linguistic communication
across different groups of speakers (in terms of language, dialect, minority language,
regional variety, etc.) is an on-going issue of debate. The arbitrariness of the distinction
between a language and dialect is captured with the popular expression "A language
is a dialect with an army and navy" [1]. Methods that do not presume clear dialect
boundaries are therefore a promising alternative. However, such methods then rely on
location-tagged data, which is usually only available for a portion of the data.
3.5 Text Classification Informed by Identity Information
So far we have focused on automatically predicting the variables themselves (e.g.,
gender, age, location) but linguistic variation related to the identity of speakers can also
be used to improve various other NLP tasks. Dadvar et al. [46] trained gender-specific
classifiers to detect instances of cyberbullying, noticing that language used by harassers
varies by gender. To improve the prediction performance of detecting the power direc-
tion between participants in emails, Prabhakaran et al. [212] incorporated the gender of
participants in e-mail conversations and the overall ’gender environment’ as features
in their model. Volkova et al. [259] studied gender differences in the use of subjective
language on Twitter. Representing gender as a binary feature was not effective, but
the use of features based on gender-dependent sentiment terms improved subjectivity
and polarity classification. Hovy [121] found that training gender or age-specific word
embeddings improved tasks such as sentiment analysis and topic classification.
4. Language and Social Interaction
In the previous section we explored computational approaches to study how language
is shaping our identity. We discussed variables such as gender, age and geographical
location, thereby focusing on variation between speakers. However, speakers do not act
in isolation, but they are part of pairs, groups and communities. Therefore, there is also
much variation within individual speakers, as language also varies according to the
context, including the addressee or audience, topic, and social goals of the speakers.
The variation that is related to the context of interaction will be the focus of this section.
We start this section with a discussion of data sources for large-scale analyses of
language use in pairs, groups and communities (Section 4.1). Next, we discuss com-
putational approaches to studying how language is reflecting and shaping our social
relationships (Section 4.2). Much of this work has revolved around the role of language
in power dynamics by studying how speakers use language to maintain and change
power relations [75]. We will continue with a discussion on style-shifting (i.e., the use
of different styles by a single speaker) in Section 4.3. We will discuss two prominent
frameworks within sociolinguistics (Audience Design [19] and Communication Accom-
modation Theory [89]), and discuss how these frameworks have been studied within
the computational linguistics community. Finally, we move our focus to the community
level and discuss computational studies on how members adapt their language to their
community and how this leads to language change over time (Section 4.4).
4.1 Data Sources
Many of the types of data that are relevant for the investigation of concepts of social
identity, are also relevant for work on communication dynamics in pairs, groups and
22
Nguyen et al. Computational Sociolinguistics: A Survey
communities. The availability of detailed interaction recordings in online data has
driven and enabled much of the work on this topic within computational linguistics.
A variety of online discussion forums have been analyzed, including online cancer
support communities [184, 263] and even a street gang forum [204]. Review sites, such as
TripAdvisor [170], IMDb [108] and beer review communities [52], have also been used in
studies on language in online communities. The Enron email corpus features in analyses
of power relationships [56, 85, 208, 212], since Enron’s organizational structure is known
and can be integrated in studies on hierarchical power. For studies that involve more
dynamic notions of power (e.g., identifying individuals who are pursuing power), other
resources have also been explored, including Wikipedia Talk Pages [21, 28, 50, 246],
transcripts of political debates [210, 211] and transcripts of Supreme Court arguments
[50].
4.2 Shaping Social Relationships
Language is not only a means to exchange information but also contributes to the
performance of action within interaction. Language serves simultaneously as a reflec-
tion of the relative positioning of speakers to their conversation partners as well as
actions that accompany those positions [222]. At a conceptual level, this work draws
heavily from a foundation in linguistic pragmatics [98, 155] as well as sociological
theories of discourse [83, 251]. Concepts related to expectations or norms that provide
the foundation for claiming such positions may be described from a philosophical
perspective or a sociological one [206]. In viewing interaction as providing a context
in which information and action may flow towards the accomplishment of social goals,
speakers position themselves and others as sources or recipients of such information
and action [165]. When performatives break norms related to social positions, they have
implications for relational constructs such as politeness [30], which codifies rhetorical
strategies for acknowledging and managing relational expectations while seeking to
accomplish extra-relational goals. In the remaining part of this section, we focus on
computational studies within this theme. We first discuss the general topic of automatic
extraction of social relationships from text, and then focus on power and politeness.
Automatic Extraction of Social Relationships. Recognizing that language use may reveal
cues about social relationships, studies within CL have explored the automatic extrac-
tion of different types of social relationships based on text. One distinction that has been
made is between weak ties (e.g., acquaintances) and strong ties (e.g., family and close
friends) [96]. Gilbert and Karahalios [86] explored how different types of information
(including messages posted) can be used to predict tie strength on Facebook. In this
study, the predictions were done for ties within a selected sample. Bak et al. [12] studied
differences in self-disclosure on Twitter between strong and weak ties using automati-
cally identified topics. Twitter users disclose more personal information to strong ties,
but they show more positive sentiment towards weak ties, which may explained by
social norms regarding first-time acquaintances on Twitter.
Other studies have automatically extracted social relationships from more extensive
datasets, enabling analyses of the extracted network structures. These studies have
focused on extracting signed social networks, i.e., networks with positive and negative
edges, for example based on positive and negative affinity between individuals or
formal and informal relationships. Work within this area has drawn from Structural
Balance Theory [107], which captures intuitions such as that when two individuals have
a mutual friend, they are likely to be friends as well, and from Status Theory [154],
23
Nguyen et al. Computational Sociolinguistics: A Survey
which involves edges that are directed and reflect status differences. Hassan et al. [105]
developed a machine learning classifier to extract signed social networks and found that
the extracted network structure mostly agreed with Structural Balance Theory. Krishnan
and Eisenstein [147] proposed an unsupervised model for extraction of signed social
networks which they used to extract formal and informal relations in a movie-script
corpus. Furthermore, their model also induced the social function of address terms
(e.g., dude). West et al. [268] focused on predicting the sentiment between individuals.
The task was formulated as an optimization problem in which they combined the costs
calculated based on an sentiment analysis model and based on Status and Structural
Balance Theory.
Power. Work on power relations draws from social psychological concepts of relative
power in social situations [101], in particular aspects of relative power that operate at the
level of individuals in relation to specific others within groups or communities. Relative
power may be thought of as operating in terms of horizontal positioning or vertical
positioning: Horizontal positioning relates to closeness and related constructs such as
positive regard, trust and commitment, while vertical positioning relates to authority
and related constructs such as approval and respect among individuals within commu-
nities. Within the areas of linguistics and computational linguistics, investigations have
focused on how speakers use language to maintain and change power relations [75].
Within computational linguistics, much of this work has focused on automatically
identifying power relationships from text. Usually this task is framed as a classification
task. Earlier studies have focused on hierarchical power relations based on the organiza-
tional structure, thereby frequently making use of the Enron corpus. Bramsen et al. [29]
extracted messages between pairs of participants and developed a machine learning
classifier to automatically determine whether the messages of an author were UpSpeak
(directed towards a person of higher status) or DownSpeak (directed towards a person
of lower status). With a slightly different formulation of the task, Gilbert [85] used
logistic regression to classify power relationships in the Enron corpus and identified the
most predictive phrases. Besides formulating the task as a classification task, ranking
approaches have been explored as well [56, 190, 210]. For example, Prabhakaran et al.
[210] predicted the ranking of participants in political debates according to their relative
poll standings.
Studies based on external power structures, such as the organizational structure of
a company, treat power relations as static. Recent studies have adopted more dynamic
notions of power. For example, Prabhakaran et al. [208] discusses a setting with an
employee in a Human Resources department who interacts with an office manager. The
HR employee has power over the office manager when the situation is about enforcing
a HR policy, but the power relation will be reversed when the topic is allocation of new
office space. In their study using the Enron corpus, they compared manual annotations
of situational power with the organization hierarchy and found that these were not
well aligned. Other studies have focused on identifying persons who are pursuing
power [28, 246], detecting influencers [24, 126, 190, 215], and studying how language
use changes when users change their status in online communities [50].
Depending on the type of power and the used dataset, labels for the relations or
roles of individuals have been collected in different ways, such as based on the organi-
zational structure of Enron [29, 85], the number of followers in Twitter [49], standings
in state and national polls to study power in political debates [210], admins and non-
admins in Wikipedia [21, 50] and manual annotation [24, 190, 207].
24
Nguyen et al. Computational Sociolinguistics: A Survey
Many computational approaches within this sphere build on a foundation from
pragmatics related to speech act theory [10, 235], which has most commonly been rep-
resented in what are typically referred to as conversation, dialog or social acts [21, 76].
Such categories can also be combined into sequences [28]. Other specialized representa-
tions are also used, such as features related to turn taking style [210, 246], topic control
[190, 211, 245], and ’overt displays of power’, which Prabhakaran et al. [209] define as
utterances that constrain the addressee’s actions beyond what the underlying dialog act
imposes.
Politeness. Polite behavior contributes to maintaining social harmony and avoiding
social conflict [119]. Automatic classifiers to detect politeness have been developed to
study politeness strategies on a large scale. According to politeness theory by Brown
and Levinson [30], three social factors influence linguistically polite behavior: social dis-
tance, relative power, and ranking of the imposition (i.e., cost of the request). Drawing
from this theory, Peterson et al. [203] performed a study on the Enron corpus by training
classifiers to automatically detect formality and requests. Emails that contained requests
or that were sent to people of higher ranks indeed tended to be more formal. According
to politeness theory, speakers with greater power than their addressees are expected to
be less polite [30]. Danescu-Niculescu-Mizil et al. [51] developed a politeness classifier
and found that in Wikipedia polite editors were indeed more likely to achieve higher
status, but once promoted, they became less polite. In StackExchange, a site with an
explicit reputation system, users with higher reputation were less polite than users with
low reputation. Their study also revealed new interactions between politeness markings
(e.g., please) and morphosyntactic context.
4.3 Style Shifting
According to Labov, there are no single-style speakers [149] since speakers may switch
across styles (style-shifting) depending on their communication partners (e.g., ad-
dressee’s age, gender and social background). Besides the addressee, other factors such
as the topic (e.g., politics vs. religion) or the context (e.g., a courtroom vs. family dinner)
can contribute to style shifting. In early studies Labov stated that ’styles can be arranged
along a single dimension, measured by the amount of attention paid to speech’ [149],
which thus views style shifting as mainly something responsive. The work by Labov
on style has been highly influential, but not everyone agreed with his explanation for
different speech styles. We will discuss two theories (Communication Accommodation
Theory and Audience Design) that have received much attention in both sociolinguistics
and computational linguistics and that focus on the role of audiences and addressees
on style. Even more recent theories are emphasizing the agency of speakers as they
employ different styles to represent themselves in a certain way or initiate a change in
the situation.
Communication Accommodation Theory. Communication Accommodation Theory (CAT)
[88, 89, 239] seeks to explain why speakers accommodate5 to each other during con-
versations. Speakers can shift their behavior to become more similar (convergence) or
more different (divergence) to their conversation partners. Convergence reduces the
5 The phenomenon of adapting to the conversation partner has also been known as ‘alignment’,
‘coordination’ and ‘entrainment’
25
Nguyen et al. Computational Sociolinguistics: A Survey
social distance between speakers and converging speakers are often viewed as more
favorably and cooperative. CAT has been developed in the 1970s and has its roots in the
field of social psychology. While CAT has been studied extensively in controlled settings
(e.g., [92]), only recently studies have been performed in uncontrolled settings such
as Twitter conversations [49], online forums [131], Wikipedia Talk pages and Supreme
Court arguments [50], and even movie scripts [48].
Speakers accommodate to each other on a variety of dimensions, ranging from
pitch, and gestures, to the words that are used. Within computational linguistics, re-
searchers have focused on measuring linguistic accommodation. Linguistic Inquiry and
Word Count (LIWC) has frequently been employed in these studies to capture stylistic
accommodation, for example reflected in the use of pronouns [48, 49, 131, 191]. Speakers
do not necessarily converge on all dimensions [89], which has also been observed on
Twitter [49]. Although earlier studies used correlations of specific features between
participants, on turn-level or overall conversation-level [156, 191, 234], these correla-
tions fail to capture the temporal aspect of accommodation. The measure developed by
Danescu-Niculescu-Mizil et al. [49] is based on the increase in probability of a response
containing a certain stylistic dimension given that the original message contains that
specific stylistic dimension. Jones et al. [131] proposed a measure that takes into account
that speakers differ in their tendency to accommodate to others, Jain et al. [130] used a
Dynamic Bayesian Model to study speech accommodation, and Wang et al. [263] used a
measure based on repetition of words (or syntactic structures) between target and prime
posts.
Social roles and orientations taken up by speakers influence how conversations play
out over time and computational approaches to measure accommodation have been
used to study power dynamics [49, 50, 131]. In a study on power dynamics in Wikipedia
Talk pages and Supreme court debates, Danescu-Niculescu-Mizil et al. [50] found that
people with a lower status accommodated more than people with a higher status. In
addition, users accommodated less once they became an admin in Wikipedia.
Audience Design. In a classical study set in New Zealand, Alan Bell found that news-
readers used different styles depending on which radio station they were talking for,
even when they were reporting the same news on the same day. Bell’s audience design
framework [19] explains style shifting as a response to audiences and shares similarities
with CAT. One of the differences with CAT is that different types of audiences are
defined from the perspective of the speaker (ranging from addressee to eavesdropper)
and thus can also be applied to settings in which there is only a one-way interaction
(such as broadcasting). Social media provides an interesting setting to study how au-
diences influence style. In many social media platforms, such as Twitter or Facebook,
multiple audiences (e.g., friends, colleagues) are collapsed into a single context. Users
of such platforms often imagine an audience when writing messages and they may
target messages to different audiences [167].
Twitter has been the focus of several recent large-scale studies on audience design.
In a study on how audiences influence the use of minority languages on Twitter, Nguyen
et al. [189] showed how characteristics of the audience influence language choice on
Twitter by analyzing tweets from multilingual users in the Netherlands using automatic
language identification. Tweets directed to larger audiences were more often written in
Dutch, while within conversations users often switched to the minority language. In
another study on audience on Twitter, Bamman and Smith [15] showed that incorpo-
rating features of the audience improved sarcasm detection. Furthermore, their results
suggested that users tend to use the hashtag #sarcasm when they are less familiar with
26
Nguyen et al. Computational Sociolinguistics: A Survey
their audience. Pavalanathan and Eisenstein [197] studied two types of non-standard
lexical variables: those strongly associated with specific geographical regions of the
United States and variables that were frequently used in Twitter but considered non-
standard in other media. The use of non-standard lexical variables was inversely related
to the size of the intended audiences. Furthermore, non-standard lexical variables were
more often used in tweets addressed to individuals from the same metropolitan area.
Using a different data source, Michael and Otterbacher [170] showed that reviewers on
the TripAdvisor site adjust their style to the style of preceding reviews. Moreover, the
extent to which reviewers are influenced correlates with attributes such as experience
of the reviewer, or their sentiment towards the reviewed attraction.
4.4 Community Dynamics
As we just discussed, people adapt their language use towards their conversation
partner. Within communities norms emerge over time through interaction between
members, such as slang words and domain-specific jargon [52, 184], or conventions for
indicating retweets in Twitter [144]. Members of communities employ such markers
to signal their affiliation. In an online gangs forum for example, graffiti style features
were used to signal group affiliation [204]. To become a core member of a community,
members adopt such community norms. As a result, often a change in behavior can
be observed when someone joins a community. Multiple studies have reported that
members of online communities decrease their use of first person singular pronouns
(e.g., ‘I’) over time and increase their use of first person plural pronouns (e.g., ‘we’)
[36, 52, 184], suggesting a stronger focus on the community. Thus, local accommodation
effects are driving long-term language changes [150, 151]. Fine-grained, large-scale
analyses of language change are difficult in offline settings, but the emergence of online
communities has enabled computational approaches for analyzing language change
within communities.
Early investigations of this topic were based on data from non-public communi-
ties, such as email exchanges between students during a course [206] and data from
the Junior Summit ’98, an online community with children from across the world to
discuss global issues [36, 127]. In these communities, members joined at the same time.
Furthermore, the studies were based on data spanning only several months.
More recent studies have used data from public, online communities, such as online
forums and review sites. Data from these communities typically span longer time
periods (e.g., multiple years). Members join these communities intermittently and thus,
when new users join, community norms have already been established. Nguyen and
Rosé [184] analyzed an online breast cancer community, in which long-time members
used forum-specific jargon, highly informal style, and showed familiarity and emo-
tional involvement with other members. Time periods were represented by the distribu-
tion of high frequency words and measures such as Kullback-Leibler divergence were
used to study how language changed over time. Members who joined the community
showed increasing conformity to community norms during the first year of their partic-
ipation. Based on these observations, a model was developed to determine membership
duration. Hemphill and Otterbacher [108] also studied how members adopt community
norms over time but focused specifically on gender differences. They studied changes
in the use of various characteristics, such as hedging, word/sentence complexity and
vocabulary richness, on IMDb (the Internet Movie Database), a community in which
males tend to receive higher prestige than females.
27
Nguyen et al. Computational Sociolinguistics: A Survey
Not only members change their behavior over time as they participate in a com-
munity, communities themselves are also constantly evolving as members join and
leave. Danescu-Niculescu-Mizil et al. [52] analyzed language change on both member-
level and community-level in two beer review communities. Language models were
created based on monthly snapshots to capture the linguistic state of a community over
time. Cross-entropy was then used to measure how much a certain post deviated from
a language model. Members in these communities turned out to follow a two-stage
lifecycle: They first align with the language of the community (innovative learning
phase), however at some point they stop adapting their language (conservative phase).
The point at which members enter the conservative phase turned out to be dependent
on how long a user would end up staying in the community.
These studies illustrate the potential of using large amounts of online data to study
language change in communities in a quantitative manner, which was not possible
in the past. Further research could focus on more in-depth analyses, for example by
focusing on subcommunities within online communities or by incorporating more
characteristics of the individual members into the analyses.
5. Multilingualism and Social Interaction
Languages evolve due to interaction of speakers within and outside their speech com-
munities. Within sociolinguistics, multilingual speakers and their speech communities
have been studied widely with respect to the contexts and conditions of language mix-
ing and switching. This section is dedicated to computational approaches for analyzing
multilingual communication within the social and linguistic contexts. We first start
with a brief introduction to multilingual communication from a sociolinguistic point of
view. Later, we expand the discussion to analyzing multilingual communication using
computational approaches.
Human mobility has always been the main reason for interaction among speakers
of different languages. In 1953, Weinreich [266] was one of the first to explain why
and how languages come into contact and evolve under each other’s influence in a
systematic manner. According to Thomason [252], social factors [23, 80] and the dura-
tion of contact could influence language contact and change more than the linguistic
factors alone. At the linguistic level, language mixing (sometimes used interchangeably
with code-switching) is the initial outcome of contact between languages and it usually
starts with the borrowing of vocabulary. As the contact increases, underlying levels of
grammar (e.g., morphology and syntax) are influenced and eventually change as well.
Sociolinguists [9, 102, 181, 205] have studied various aspects of language contact and
mixing across different contact settings.
Both language mixing and code-switching are used interchangeably and there is no
consensus on the terminology. Each contact situation is unique and overpasses universal
linguistic categories. The borders between language and dialect are fuzzy, because they
are often determined by social and political factors rather than linguistic factors. There-
fore, we use the term ’multilingual speaker’ covering both language and/or dialect
mixing.
Language mixing refers to the mixing of languages within the same text or conver-
sation [102]. The switching between languages can take place at various levels. Wei [265]
refers to language alternations at or above the clause level (and calls it code-mixing).
Romaine [224] differentiates between inter-sentential (i.e., across sentences) and intra-
sentential (i.e., within the same sentence) switches. Poplack et al. [205] acknowledge
complete languages shifts of individual users as well (and call it code-switching). We
28
Nguyen et al. Computational Sociolinguistics: A Survey
use the term language mixing for a continuum that starts with occasional switches
(e.g., words or fixed multi-word expressions) and extends to grammatical changes
at the underlying levels as the duration and intensity of contact between languages
increase. When the frequency of switches increases (for individual speakers or for the
community), these switches are regarded as borrowed/loan words (e.g., hip hop related
Anglicisms in a German hip hop forum [82]). As the contact increases, underlying gram-
matical levels (e.g., morphology, syntax) are affected as well. Attesting the language
change at the underlying levels is harder since these changes go beyond borrowed
words [57, 58].
Earlier studies of language mixing were mostly based on multilingual spoken data
expressed in controlled or naturalistic settings [9, 180]. Nowadays, the wide-spread use
of internet in multilingual populations provides ample opportunities for large-scale
and in-depth analyses of mixed language use in online media [53, 115, 116, 195, 257].
Still most of these studies have focused on qualitative analyses of multilingual online
communication with limited data in terms of size and duration.
Example (1) illustrates switching between Turkish (TR) and Dutch (NL) in the same
utterance in an online forum [196]. A Turkish-Dutch bilingual user starts the post in
Dutch and switches to Turkish for the rest of the post.
Example (1)
<NL>vakantie boeken</NL> <TR> yaptim annecigimee </TR>
Tra: "<NL>booking a holiday </NL> <TR> for my mother</TR>"
The rest of this section presents a discussion of data sources for studying multilin-
gual communication on a large scale (Section 5.1). Consequently, we discuss research on
adapting various NLP tools to process mixed-language texts (Section 5.2). We finalize
this section with a discussion of studies that analyze, or even try to predict, the use of
multiple languages in multilingual communication (Section 5.3).
5.1 Data Sources
In sociolinguistics, conversational data is usually collected by the researchers them-
selves, either among small groups of speakers at different times [58] or from the same
group of speakers longitudinally [172, 255]. The manual transcription and annotation of
data is time-intensive and costly. Multilingual data from online environments is usually
extracted in small volumes and for short periods; automatic analysis of these data has
been difficult for most languages, especially when resources or technical support are
lacking.
Within computational linguistics, there is a growing interest in the automatic
processing of mixed-language texts. Lui et al. [161] and Yamaguchi and Tanaka-Ishii
[273] studied automatic language identification in mixed-language documents from
Wikipedia by artificially concatenating texts from monolingual sources into multilin-
gual documents. However, such approaches lead to artificial language boundaries. Most
recently, social media (such as Facebook [261], Twitter [137, 200, 242] and online forums
[183]) provide large volumes of data for analyzing multilingual communication in social
interaction. Transcriptions of conversations have been explored by Solorio and Liu
[241], however their data was limited to three speakers. Language pairs that have been
studied for multilingual communication include English-Hindi [261], Spanish-English
[200, 240, 241], Turkish-Dutch [183], Mandarin-English [2, 200], and French-English
[137]. Besides being a valuable resource for studies on multilingual social interaction,
29
Nguyen et al. Computational Sociolinguistics: A Survey
multilingual texts in social media have also been used to improve general purpose
machine translation systems [125, 159].
Processing and analyzing mixed-language data often requires identification of lan-
guages at the word level. Several datasets are publicly available to stimulate research
on language identification in mixed-language texts, including data from the shared
task on Language Identification in Code-Switched Data [242] covering four different
language pairs on Twitter, romanized Algerian Arabic and French comments on an
online Algerian newspaper [45], Turkish-Dutch forum posts [183] and web documents
in different languages [139].
Annotation on a fine-grained level such as individual words has introduced new
challenges in the construction of datasets. More fine-grained annotations require more
effort and sometimes the segments are so short that they can no longer be clearly
attributed to a particular language. For example, annotating the language of named
entities remain a challenge in mixed-language texts. Named entities have been labeled
according to the context [139], ignored in the evaluation [73, 183] or treated as a separate
category [72, 242]. Annotation at sentence-level is also challenging. For example, Zaidan
and Callison-Burch [275] annotated a large corpus for Arabic dialect identification using
crowdsourcing. Their analysis indicated that many annotators over-identify their native
dialect (i.e., they were biased towards labeling texts as written in their own dialect).
Elfardy and Diab [72] presented guidelines to annotate texts written in dialectal variants
of Arabic and Modern Standard Arabic on a word level.
5.2 NLP Tools for Multilingual Data
Most of the current NLP tools, such as parsers, are developed for texts written in a
single language. Therefore, such tools are not optimized for processing texts containing
multiple languages. In this section, we discuss the development of NLP tools that
specifically aim to support the processing of multilingual texts. We start with research
on automatic language identification, which is an important step in the preprocessing
pipeline of many language-specific analysis tasks. Mixed language documents have
introduced new challenges to this task. We then continue with a discussion of work
on various other NLP tools (e.g., parsers, topic modeling).
Automatic Language Identification. Automatic language identification is often the first
step for systems that process mixed-language texts [261]. Furthermore, it supports
large-scale analyses of patterns in multilingual communication [137, 138, 196]. Most
of the earlier research on automatic language identification focused on document-
level identification of a single language [13]. To handle mixed-language texts, more
fine-grained approaches have been explored, ranging from language identification at
the sentence [74, 275, 277] and word level [73, 139, 183, 242, 260], approaches for
text segmentation [273], and estimating the proportion of the various languages used
within documents [161, 213]. Depending on the application, different approaches may
be suitable, but studies that analyze patterns in multilingual communication have
mostly focused on word-level identification [183, 242]. Off-the-shelf tools developed for
language identification at the document-level (e.g., the TextCat program [37]) are not ef-
fective for word-level identification [3, 183]. Language models [73, 183] and dictionaries
[3, 73, 183], which are also commonly used in automatic language identification at the
document level, have been explored. Furthermore, the context around the words have
been exploited using Conditional Random Fields to improve performance on language
identification at the word level [139, 183].
30
Nguyen et al. Computational Sociolinguistics: A Survey
Parsing. Early studies on language mixing within computational linguistics focused
on developing grammars to model language mixing [95, 133]. However, the models
developed in these early studies were not tested on empirical data. The more recently
developed systems have been validated on large, real-world data. Solorio and Liu [241]
explored various strategies to combine monolingual taggers to parse mixed-language
texts. The best performance was obtained by including the output of the monolingual
parsers as features in a machine learning algorithm. Vyas et al. [261] studied the impact
of different preprocessing steps on POS tagging of English-Hindi mixed language text
collected from Facebook. Language identification and transliteration were the major
challenges that impacted POS performance.
Language and Topic Models. Language models have been developed to improve speech
recognition for mixed language speech, by adding POS and language information to the
language models [2] or by incorporating syntactic inversion constraints [157]. Peng et al.
[200] developed a topic model that infers language-specific topic distributions based
on mixed language text. The main challenge for their model was aligning the inferred
topics across languages.
5.3 Analysis and Prediction of Multilingual Communication
According to Thomason [252], Gardner-Chloros and Edwards [80], and Bhatt and
Bolonyai [23], social factors (e.g., attitudes and motives of the speakers, social and
political context) can play an important role in addition to the linguistic factors in multi-
lingual settings. Large-scale analysis of social factors in multilingual communication has
only recently been possible since the availability of automatic language identification
tools.
Twitter is frequently used as a resource for such studies. Focusing on language
choice at the user level, researchers have extracted network structures, based on follow-
ers and followees [71, 138], or mentions and retweets [103], and analyzed the relation
between the composition of such networks and the language choices of users. Users
tweeting in multiple languages are often found to function as a bridge between com-
munities tweeting in one language. Besides analyzing language choice at the user level,
there is also an interest in the language choices for individual tweets. Jurgens et al. [137]
studied tweets written in one language but containing hashtags in another language.
Automatic language identification was used to identify the languages of the tweets.
However, as they note, some tweets were written in another language because they
were automatically generated by applications rather than being a conscious choice of
the user. Nguyen et al. [189] studied users in the Netherlands who tweeted in a minority
language (Limburgish or Frisian) as well as in Dutch. Most tweets were written in
Dutch, but during conversations users often switched to the minority language. Mocanu
et al. [176] analyzed the geographic distribution of languages in multilingual regions
and cities (such as New York and Montreal) using Twitter.
In addition to the analysis of patterns in multilingual communication, several
studies have explored the automatic prediction of language switches. The task may
seem similar to automatic language identification yet there are differences between the
two tasks. Rather than determining the language of an utterance, it involves predicting
whether the language of the next utterance is the same without having access to the next
utterance itself. Solorio and Liu [240] were the first to predict whether a speaker will
switch to another language in English-Spanish bilingual spoken conversations based
on lexical and syntactic features. The approach was evaluated using standard ma-
31
Nguyen et al. Computational Sociolinguistics: A Survey
chine learning metrics as well as human evaluators who rated the naturalness/human-
likeness of the sentences the system generated. Papalexakis et al. [196] predicted when
multilingual users switch between languages in a Turkish-Dutch online forum using
various features, including features based on multi-word units and emoticons.
6. Research Agenda
Computational sociolinguistics is an emerging multidisciplinary field. Closer collab-
oration between sociolinguists and computational linguists could be beneficial to re-
searchers from both fields. In this paper we have outlined some challenges related to
differences in data and methods that must be addressed in order for the exchange to be
effective. In this section, we summarize the main challenges for advancing the field of
computational sociolinguistics. These fall under three main headings, namely, expand-
ing the scope of inquiry of our field, adapting our methods to increase compatibility,
and offering tools.
6.1 Expanding the Scope of Inquiry
The field of computational linguistics has begun to investigate issues that overlap with
those of the field of sociolinguistics. The emerging availability of data that is of interest
to both communities is an important factor, but in order for real synergy to come out
of this, additional angles in the research agendas and tuning of the methodological
frameworks in the respective communities would be needed.
Going beyond lexical and stylistic variation. Many studies within CL focus on lexical varia-
tion (e.g., Section 3 on social identity), possibly driven by the focus on prediction perfor-
mance. Although stylistic variation has received attention, most of these studies (e.g.,
on linguistic style accommodation) employ selected categories from LIWC to capture
broad grammatical categories [202], such as determiners, prepositions and pronouns.
Advances in the area of stylometry [243] could inspire the exploration of new features
to capture style. Besides lexical and stylistic variation, linguistic variation also occurs
on many other layers (e.g., phonology, syntax, morphology). These different layers
of variation have been well studied within sociolinguistics, but within computational
sociolinguistics they have received limited attention so far (e.g., phonological [66, 132]
and syntactic [60, 84] variation).
Extending focus to other social variables. A large body of work exists on the modeling
of gender, age and regional variation (Cf. Section 3). Other variables (e.g., social class
[148]) have barely received any attention so far within computational sociolinguistics.
Although it is more difficult to obtain labels for some social variables, they are essential
for a richer understanding of language variation and more robust analyses.
Going beyond English and monolingual data. The world is multilingual and multicultural,
but English has received much more attention within computational sociolinguistics
than other languages. There is a need for research to validate the generalizability of
findings based on English data for other languages [53]. Furthermore, most studies
within computational linguistics generally assume that texts are written in one lan-
guage. However, these assumptions may not hold true especially in social media. A
single user may use multiple languages, sometimes even within a syntactic unit, while
most NLP tools are not optimized to process such texts. Tools that are able to process
32
Nguyen et al. Computational Sociolinguistics: A Survey
mixed-language texts will improve analyses of such data and shed more light on the
social and linguistic factors involved in multilingual communication.
6.2 Adapting Methodological Frameworks to Increase Compatibility
To make use of the rich repertoire of theory and practice from sociolinguistics and to
contribute to it, we have to appreciate the methodologies that underlie sociolinguistic
research, e.g., the rules of engagement for joining into the ongoing scientific discourse.
However, as we have begun to do so, immediately a culture clash can be observed.
While the CL community has experienced a history in which theory and empiricism
are treated as the extreme ends of a spectrum, in the social sciences there is no such di-
chotomy, and empiricism contributes substantially to theory. Moving forward, research
within computational sociolinguistics should build on and seek to partner in extending
existing sociolinguistic theories and insights. However, what this requires is placing a
stronger focus on interpretability of the developed models.
Controlling for multiple variables. Sociolinguistic studies typically control for multiple
social variables (e.g., gender, age, social class, ethnicity). However, many studies in
computational sociolinguistics focus on individual variables (e.g., only gender, or only
age), which can be explained by the focus on social media data. The uncontrolled nature
of social media makes it challenging to obtain data about the social backgrounds of the
speakers and to understand the various biases that such datasets might have. The result
is that models are frequently confounded, which results in low interpretability as well
as limited justification for generalization to other domains.
On the other hand, much work in the CL community has focused on structured
modeling approaches that take a step towards addressing these issues [134, 135]. These
modeling approaches are very similar to hierarchical modeling approaches used in
sociolinguistic research to control for multiple sources of variation and thus avoid
misattributing weight to extraneous variables. A stronger partnership within the field
of CL between researchers interested in computational sociolinguistics and researchers
interested in multi-domain learning would be valuable for addressing some of these
limitations. In this regard, inferring demographic variables automatically (see Section 3)
may also help, since these predicted demographic variables could then be used in
structuring the models. Another approach is the use of census data when location data
is already available. For example, Eisenstein et al. [70] studied lexical change in social
media by using census data to obtain demographic information of the geographical
locations. They justified their approach by assuming that lexical change is influenced
by the demographics of the population in these locations, and not necessarily by the
demographics of the particular Twitter users in these locations. However, such an
assumption may not hold for other research questions.
Developing models that generalize across domains. On a related note, recently several studies
within the area of computational sociolinguistics have performed experiments across
domains [227, 228] and explored the effectiveness of domain adaptation approaches
[84, 185, 204]. Such approaches could enable distinguishing the variables that work
differently in different contexts, but this would require identifying the relevant domains
in the studied data. Other directions involve reconsidering the features used in an
attempt to include more features with a deep connection with the predicted variable of
interest. For example, Gianfortoni et al. [84] show that features such as n-grams, usually
reported to be predictive for gender classification, did not perform well after control-
33
Nguyen et al. Computational Sociolinguistics: A Survey
ling for occupation in a blog corpus, but pattern-based features inspired by findings
related to gender based language practices did. However, Herring et al. [111] found no
significant relation between gender and previously identified gender features in their
blog corpus after controlling for genre. Furthermore, considering the implications of
linguistic agency, when the focus is on interpretability of the models, we must consider
that the resulting average prediction performance of interpretable models may be lower
[204].
Using sociolinguistics and the social sciences as a source of inspiration for methodological
reflection. Going forward, we need to appreciate where our work stands along an impor-
tant continuum that represents a fundamental tension in the social sciences: qualitative
approaches that seek to preserve the complexity of the phenomena of interest, versus
quantitative approaches that discretize (but thereby also simplify) the phenomena to
achieve more generalizability. For computational linguistics, a primarily quantitative
field, work from research areas with a less strong or less exclusive focus on quantitative
measures, such as sociolinguistics and the social sciences, could serve as a source of
inspiration for methodological reflection. In this survey, we have questioned the op-
erationalization of the concept of gender (Section 3.2), age (Section 3.3) and language
(Section 3.4) as discrete, static, categories, based on insights from sociolinguistics. Fur-
ther critical reflection on such operationalizations could lead to deeper insight into the
limitations of the developed models and the incorrect predictions that they sometimes
make. Furthermore, it could also be helpful in communicating the methods and findings
to other research communities.
6.3 Tuning NLP Tools to Requirements of Sociolinguistics Research
As a final important direction, we should consider what would be required for NLP
tools to be supportive for sociolinguistic work.
Developing models that can guide users of data analysis systems in taking next steps. Sociolin-
guists are primarily interested in new insights about language use. In contrast, much of
the work within CL is centered around highly specific analysis tasks that are isolated
from scenarios of use, and the focus on the obtained performance figures for such tasks
is fairly dominant. Only for few analysis methods, validation of the outcomes has been
pursued (Have we measured the right thing?) in view of the potential for integration of
the models outside lab-like environments. Furthermore many of the models developed
within CL make use of thousands of features. As a result, it is often hard to deploy the
predictions in a real-life setting and their value for practical data exploration tasks is
therefore often limited. Sparse models (e.g., [69]) that identify small sets of predictive
features would be more suited for exploratory analysis.
Developing pre-processing tools to support the analysis of language variation. The perfor-
mance of many developed NLP tools is lower on informal text. For example, POS
taggers perform less on texts written by certain user groups (e.g., younger people [122])
or on texts in certain language varieties (e.g., AAVE [132]). One of the approaches to
improve the performance of tools has been to normalize the texts, but as Eisenstein [65]
argues doing so is removing the variation that is central to the study of sociolinguistics.
To support deeper sociolinguistic analyses and to go beyond shallow features, we thus
need pre-processing tools, such as POS taggers, that are able to handle the variation
found in informal texts and that are not biased towards certain social groups.
34
Nguyen et al. Computational Sociolinguistics: A Survey
7. Conclusion
While the computational linguistics field has historically emphasized interpretation and
manipulation of the propositional content of language, another valid perspective on
language is that it is a dynamic, social entity. While some aspects of language viewed
from a social perspective are predictable, and thus behave much like other aspects
more commonly the target of inquiry in the field, we must acknowledge that linguistic
agency is a big part of how language is used to construct social identities, to build and
maintain social relationships, and even to define the bounds of communities. With the
increasing research on social media data and in parallel with the emergence of compu-
tational social science, we have observed a surge of interest within the computational
linguistics community on sociolinguistic topics. In this article, we have defined and set
out a research agenda for the emerging field of ‘Computational Sociolinguistics’. We have
aimed to provide a comprehensive overview of studies published within the field of
CL that touch upon sociolinguistic themes in order to provide an overview of what has
been accomplished so far and where there is room for growth. In particular, we have
endeavored to illustrate how the large-scale data driven methods of our community can
complement existing sociolinguistic studies, but also how sociolinguistics can inform
and challenge our methods and assumptions.
35
Nguyen et al. Computational Sociolinguistics: A Survey
References
[1] Notes. Language in Society, 26:469–470, 9 1997.
[2] Heike Adel, Thang Ngoc Vu, and Tanja Schultz. Combination of recurrent neural
networks and factored language models for code-switching language modeling.
In Proceedings of the 51st Annual Meeting of the Association for Computational Linguis-
tics (Volume 2: Short Papers), pages 206–211, Sofia, Bulgaria, 2013.
[3] Beatrice Alex. An unsupervised system for identifying English inclusions in
German text. In Proceedings of the ACL Student Research Workshop, pages 133–138,
Ann Arbor, Michigan, 2005.
[4] Jannis Androutsopoulos. Online data collection. In Christine Mallinson, Becky
Childs, and Gerard Van Herk, editors, Data Collection in Sociolinguistics: Methods
and Applications. Routledge, 2013.
[5] Ehsan Mohammady Ardehaly and Aron Culotta. Inferring latent attributes of
Twitter users with label regularization. In Proceedings of the 2015 Conference of
the North American Chapter of the Association for Computational Linguistics: Human
Language Technologies, pages 185–195, 2015.
[6] Shlomo Argamon, Moshe Koppel, Jonathan Fine, and Anat Rachel Shimoni. Gen-
der, genre, and writing style in formal written texts. Text, 23(3):321–346, 2003.
[7] Shlomo Argamon, Moshe Koppel, James W. Pennebaker, and Jonathan Schler.
Mining the blogosphere: Age, gender and the varieties of self-expression. First
Monday, 12(9), 2007.
[8] Shlomo Argamon, Moshe Koppel, James W. Pennebaker, and Jonathan Schler.
Automatically profiling the author of an anonymous text. Communications of the
ACM, 52(2):119–123, 2009.
[9] Peter Auer. A conversation analytic approach to code-switching and transfer.
In M. Heller, editor, Codeswitching: Anthropological and sociolinguistic perspectives,
pages 187–213. Berlin: Mouton de Gruyter, 1988.
[10] John Langshaw Austin. How to do things with words. Oxford University Press,
1975.
[11] Rolf Backofen and Gert Smolka. A complete and recursive feature theory. In
Proceedings of the 31st annual meeting on Association for Computational Linguistics,
pages 193–200, Columbus, Ohio, 1993.
[12] JinYeong Bak, Suin Kim, and Alice Oh. Self-disclosure and relationship strength in
Twitter conversations. In Proceedings of the 50th Annual Meeting of the Association for
Computational Linguistics (Volume 2: Short Papers), pages 60–64, Jeju Island, Korea,
2012. Association for Computational Linguistics.
[13] Timothy Baldwin and Marco Lui. Language identification: The long and the short
of the matter. In Human Language Technologies: The 2010 Annual Conference of the
North American Chapter of the Association for Computational Linguistics, pages 229–
237, Los Angeles, California, 2010.
[14] Timothy Baldwin, Paul Cook, Marco Lui, Andrew MacKinlay, and Li Wang. How
noisy social media text, how diffrnt social media sources? In Proceedings of the
Sixth International Joint Conference on Natural Language Processing, pages 356–364,
Nagoya, Japan, 2013.
[15] David Bamman and Noah A. Smith. Contextualized sarcasm detection on Twitter.
In Proceedings of the Ninth International AAAI Conference on Web and Social Media,
pages 574–577, 2015.
[16] David Bamman, Chris Dyer, and A. Noah Smith. Distributed representations of
geographically situated language. In Proceedings of the 52nd Annual Meeting of the
36
Nguyen et al. Computational Sociolinguistics: A Survey
Association for Computational Linguistics (Volume 2: Short Papers), pages 828–834,
Baltimore, Maryland, 2014.
[17] David Bamman, Jacob Eisenstein, and Tyler Schnoebelen. Gender identity and
lexical variation in social media. Journal of Sociolinguistics, 18(2):135–160, 2014.
[18] Federica Barbieri. Patterns of age-based linguistic variation in American English.
Journal of Sociolinguistics, 12(1):58–88, 2008.
[19] Allan Bell. Language style as audience design. Language in Society, 13:145–204,
1984.
[20] Allan Bell. The guidebook to sociolinguistics. John Wiley & Sons, 2013.
[21] M. Emily Bender, T. Jonathan Morgan, Meghan Oxley, Mark Zachry, Brian
Hutchinson, Alex Marin, Bin Zhang, and Mari Ostendorf. Annotating social acts:
Authority claims and alignment moves in Wikipedia talk pages. In Proceedings of
the Workshop on Language in Social Media (LSM 2011), pages 48–57, 2011.
[22] Shane Bergsma and Benjamin Van Durme. Using conceptual class attributes to
characterize social media users. In Proceedings of the 51st Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers), pages 710–720,
Sofia, Bulgaria, 2013.
[23] Rakesh M. Bhatt and Agnes Bolonyai. Code-switching and the optimal grammar
of bilingual language use. Bilingualism: Language and Cognition, 14(04):522–546,
2011.
[24] Or Biran, Sara Rosenthal, Jacob Andreas, Kathleen McKeown, and Owen Ram-
bow. Detecting influencers in written online conversations. In Proceedings of the
2012 Workshop on Language in Social Media (LSM 2012), pages 37–45, 2012.
[25] David M. Blei, Andrew Y. Ng, and Michael I. Jordan. Latent Dirichlet allocation.
Journal of Machine Learning Research, 3:993–1022, 2003.
[26] Brook Bolander and Miriam A. Locher. Doing sociolinguistic research on
computer-mediated data: A review of four methodological issues. Discourse,
Context & Media, 3(0):14 – 26, 2014.
[27] Constantinos Boulis and Mari Ostendorf. A quantitative analysis of lexical dif-
ferences between genders in telephone conversations. In Proceedings of the 43rd
Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 435–
442, 2005.
[28] David B. Bracewell, Marc Tomlinson, and Hui Wang. A motif approach for
identifying pursuits of power in social discourse. In 2012 IEEE Sixth International
Conference on Semantic Computing, pages 1–8, 2012.
[29] Philip Bramsen, Martha Escobar-Molano, Ami Patel, and Rafael Alonso. Extract-
ing social power relationships from natural language. In Proceedings of the 49th
Annual Meeting of the Association for Computational Linguistics: Human Language
Technologies, pages 773–782, 2011.
[30] Penelope Brown and Stephen C. Levinson. Politeness: Some universals in language
usage, volume 4 of Studies in Interactional Sociolinguistics. Cambridge University
Press, 1987.
[31] Mary Bucholtz and Kira Hall. Identity and interaction: A sociocultural linguistic
approach. Discourse studies, 7(4-5):585–614, 2005.
[32] John D. Burger and John C. Henderson. An exploration of observable features
related to blogger age. In AAAI Spring Symposium: Computational Approaches to
Analyzing Weblogs, pages 15–20, 2006.
[33] John D. Burger, John Henderson, George Kim, and Guido Zarrella. Discriminating
gender on Twitter. In Proceedings of the 2011 Conference on Empirical Methods in
Natural Language Processing, pages 1301–1309, 2011.
37
Nguyen et al. Computational Sociolinguistics: A Survey
[34] Judith Butler. Gender Trouble: Feminism and the Subversion of Identity. Routledge,
1990.
[35] Jean Carletta. Assessing agreement on classification tasks: The Kappa statistic.
Computational Linguistics, 22(2):249–254, June 1996.
[36] Justine Cassell and Dona Tversky. The language of online intercultural commu-
nity formation. Journal of Computer-Mediated Communication, 10(2), 2005.
[37] William B. Cavnar and John M. Trenkle. N-gram-based text categorization. In Pro-
ceedings of SDAIR-94, 3rd Annual Symposium on Document Analysis and Information
Retrieval, pages 161–175, Las Vegas, US, 1994.
[38] Jack K. Chambers and Peter Trudgill. Dialectology. Cambridge University Press,
1998.
[39] Bernard C. K. Choi and Anita W. P. Pak. Multidisciplinarity, interdisciplinarity
and transdisciplinarity in health research, services, education and policy: 1. Def-
initions, objectives, and evidence of effectiveness. Clin Invest Med, 29(6):351–364,
2006.
[40] Morgane Ciot, Morgan Sonderegger, and Derek Ruths. Gender inference of Twit-
ter users in non-English contexts. In Proceedings of the 2013 Conference on Empirical
Methods in Natural Language Processing, pages 1136–1145, 2013.
[41] Cynthia G. Clopper. Experiments. In Data Collection in Sociolinguistics: Methods
and Applications. Routledge, 2013.
[42] Raviv Cohen and Derek Ruths. Classifying political orientation on Twitter: It’s
not easy! In Proceedings of the Seventh International AAAI Conference on Weblogs and
Social Media, pages 91–99, 2013.
[43] Linda M. Collins and Stephanie T. Lanza. Latent class and latent transition analysis:
With applications in the social, behavioral, and health sciences. John Wiley & Sons,
2010.
[44] Malcolm Corney, Olivier de Vel, Alison Anderson, and George Mohay. Gender-
preferential text mining of e-mail discourse. In Proceedings of the 18th Annual
Computer Security Applications Conference (ACSAC ’02), pages 282–289, 2002.
[45] Ryan Cotterell, Adithya Renduchintala, Naomi Saphra, and Chris Callison-Burch.
An Algerian Arabic-French code-switched corpus. In LREC-2014 Workshop on
Free/Open-Source Arabic Corpora and Corpora Processing Tools, 2014.
[46] Maral Dadvar, Franciska M. G. de Jong, Roeland Ordelman, and Dolf Trieschnigg.
Improved cyberbullying detection using gender information. In Proceedings of
the Twelfth Dutch-Belgian Information Retrieval Workshop (DIR 2012), pages 23–25,
Ghent, Belgium, 2012.
[47] Walter Daelemans. Explanation in computational stylometry. In Proceedings
of the 14th international conference on Computational Linguistics and Intelligent Text
Processing (CICLing’13) - Volume 2, pages 451–462, 2013.
[48] Cristian Danescu-Niculescu-Mizil and Lillian Lee. Chameleons in imagined con-
versations: A new approach to understanding coordination of linguistic style in
dialogs. In Proceedings of the 2nd Workshop on Cognitive Modeling and Computational
Linguistics, pages 76–87, 2011.
[49] Cristian Danescu-Niculescu-Mizil, Michael Gamon, and Susan Dumais. Mark my
words!: Linguistic style accommodation in social media. In Proceedings of the 20th
International Conference on World Wide Web, pages 745–754, Hyderabad, India, 2011.
[50] Cristian Danescu-Niculescu-Mizil, Lillian Lee, Bo Pang, and Jon Kleinberg.
Echoes of power: Language effects and power differences in social interaction.
In Proceedings of the 21st international conference on World Wide Web, pages 699–708,
Lyon, France, 2012.
38
Nguyen et al. Computational Sociolinguistics: A Survey
[51] Cristian Danescu-Niculescu-Mizil, Moritz Sudhof, Dan Jurafsky, Jure Leskovec,
and Christopher Potts. A computational approach to politeness with application
to social factors. In Proceedings of the 51st Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers), pages 250–259, 2013.
[52] Cristian Danescu-Niculescu-Mizil, Robert West, Dan Jurafsky, Jure Leskovec, and
Christopher Potts. No country for old members: User lifecycle and linguistic
change in online communities. In Proceedings of the 22nd international conference
on World Wide Web (WWW ’13), pages 307–318, 2013.
[53] Brenda Danet and Susan C. Herring, editors. The multilingual Internet: Language,
culture, and communication online. Oxford University Press, 2007.
[54] Kareem Darwish, Hassan Sajjad, and Hamdy Mubarak. Verifiably effective Arabic
dialect identification. In Proceedings of the 2014 Conference on Empirical Methods in
Natural Language Processing (EMNLP), pages 1465–1468, Doha, Qatar, 2014.
[55] Barbara Di Eugenio and Michael Glass. The kappa statistic: A second look.
Computational Linguistics, 30(1):95–101, 2004.
[56] Christopher P. Diehl, Galileo Namata, and Lise Getoor. Relationship identification
for social network discovery. In Proceedings of the Twenty-Second AAAI Conference
on Artificial Intelligence, pages 546–552, 2007.
[57] A. Seza Doğruöz and Ad Backus. Postverbal elements in immigrant Turkish:
Evidence of change? International Journal of Bilingualism, 11(2):185–220, 2007.
[58] A. Seza Doğruöz and Ad Backus. Innovative constructions in Dutch Turkish:
An assessment of ongoing contact-induced change. Bilingualism: Language and
Cognition, 12:41–63, 1 2009.
[59] A. Seza Doğruöz and Preslav Nakov. Predicting dialect variation in immigrant
contexts using light verb constructions. In Proceedings of the 2014 Conference
on Empirical Methods in Natural Language Processing (EMNLP), pages 1391–1395,
Doha, Qatar, 2014.
[60] Gabriel Doyle. Mapping dialectal variation by querying social media. In Proceed-
ings of the 14th Conference of the European Chapter of the Association for Computational
Linguistics, pages 98–106, 2014.
[61] Penelope Eckert. Jocks and burnouts: Social categories and identity in the high school.
Teachers College Press, 1989.
[62] Penelope Eckert. Age as a sociolinguistic variable. In Florian Coulmas, editor, The
handbook of sociolinguistics. Blackwell Publishers, 1997.
[63] Penelope Eckert. Three waves of variation study: the emergence of meaning in
the study of sociolinguistic variation. Annual Review of Anthropology, 41:87–100,
2012.
[64] Penelope Eckert and Sally McConnell-Ginet. Language and gender. Cambridge
University Press, 2013.
[65] Jacob Eisenstein. What to do about bad language on the internet. In Proceedings of
the 2013 Conference of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages 359–369, 2013.
[66] Jacob Eisenstein. Phonological factors in social media writing. In Proceedings of the
Workshop on Language Analysis in Social Media (LASM 2013), pages 11–19, Atlanta,
Georgia, 2013.
[67] Jacob Eisenstein. Written dialect variation in online social media. In Charles
Boberg, John Nerbonne, and Dom Watt, editors, Handbook of Dialectology. Wiley,
2015.
[68] Jacob Eisenstein, Brendan O’Connor, Noah A. Smith, and Eric P. Xing. A la-
tent variable model for geographic lexical variation. In Proceedings of the 2010
39
Nguyen et al. Computational Sociolinguistics: A Survey
Conference on Empirical Methods in Natural Language Processing, pages 1277–1287,
Cambridge, MA, 2010.
[69] Jacob Eisenstein, Noah A. Smith, and Eric P. Xing. Discovering sociolinguistic
associations with structured sparsity. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Human Language Technologies-Volume
1, pages 1365–1374, Portland, Oregon, USA, 2011.
[70] Jacob Eisenstein, Brendan O’Connor, Noah A. Smith, and Eric P. Xing. Diffusion
of lexical change in social media. PLoS ONE, 9(11):e113114, 11 2014.
[71] Irene Eleta and Jennifer Golbeck. Multilingual use of Twitter: Social networks at
the language frontier. Computers in Human Behavior, 41:424 – 432, 2014.
[72] Heba Elfardy and Mona Diab. Simplified guidelines for the creation of large scale
dialectal Arabic annotations. In Proceedings of the Eighth International Conference on
Language Resources and Evaluation (LREC-2012), 2012.
[73] Heba Elfardy and Mona Diab. Token level identification of linguistic code switch-
ing. In Proceedings of COLING 2012: Posters, pages 287–296, 2012.
[74] Heba Elfardy and Mona Diab. Sentence level dialect identification in Arabic. In
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics
(Volume 2: Short Papers), pages 456–461, 2013.
[75] Norman Fairclough. Language and power. London: Longman, 1989.
[76] Oliver Ferschke, Iryna Gurevych, and Yevgen Chebotar. Behind the article: Rec-
ognizing dialog acts in Wikipedia talk pages. In Proceedings of the 13th Conference of
the European Chapter of the Association for Computational Linguistics, pages 777–786,
2012.
[77] Katja Filippova. User demographics and language in an implicit social network.
In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language
Processing and Computational Natural Language Learning, pages 1478–1488, Jeju
Island, Korea, 2012.
[78] Anna De Fina, Deborah Schiffrin, and Michael Bamberg, editors. Discourse and
Identity. Cambridge University Press, 2006.
[79] Clay Fink, Jonathon Kopecky, and Maksym Morawski. Inferring gender from the
content of tweets: A region specific example. In Proceedings of the Sixth International
AAAI Conference on Weblogs and Social Media, pages 459–462, 2012.
[80] Penelope Gardner-Chloros and Malcolm Edwards. Assumptions behind gram-
matical approaches to code-switching: When the blueprint is a red herring. Trans-
actions of the Philological Society, 102(1):103–129, 2004.
[81] Nikesh Garera and David Yarowsky. Modeling latent biographic attributes in
conversational genres. In Proceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint Conference on Natural Language
Processing of the AFNLP, pages 710–718, Suntec, Singapore, 2009.
[82] Matt Garley and Julia Hockenmaier. Beefmoves: Dissemination, diversity, and
dynamics of English borrowings in a German hip hop forum. In Proceedings of the
50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short
Papers), pages 135–139, 2012.
[83] James Paul Gee. An Introduction to Discourse Analysis: Theory and Method. New
York: Routledge, third edition, 2011.
[84] Philip Gianfortoni, David Adamson, and P. Carolyn Rosé. Modeling of stylistic
variation in social media with stretchy patterns. In Proceedings of the First Workshop
on Algorithms and Resources for Modelling of Dialects and Language Varieties, pages
49–59, Edinburgh, Scotland, 2011.
40
Nguyen et al. Computational Sociolinguistics: A Survey
[85] Eric Gilbert. Phrases that signal workplace hierarchy. In Proceedings of the ACM
2012 Conference on Computer Supported Cooperative Work, CSCW ’12, pages 1037–
1046, 2012.
[86] Eric Gilbert and Karrie Karahalios. Predicting tie strength with social media. In
Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pages
211–220, 2009.
[87] Howard Giles and Nikolas Coupland. Language: Contexts and consequences. Map-
ping Social Psychology Series. Brooks/Cole Publishing Company, 1991.
[88] Howard Giles, Donald M. Taylor, and Richard Bourhis. Towards a theory of
interpersonal accommodation through language: some Canadian data. Language
in Society, 2(2):177–192, 1973.
[89] Howard Giles, Nikolas Coupland, and Justine Coupland. Accommodation the-
ory: Communication, context, and consequence. In Howard Giles, Justine Cou-
pland, and Nikolas Coupland, editors, Contexts of Accommodation, pages 1–68.
Cambridge University Press, 1991.
[90] Clark Glymour, Richard Scheines, Peter Spirtes, and Kevin Kelly. Discovering
causal structure: Artificial intelligence, philosophy of science, and statistical modeling.
Academic Press, 1987.
[91] E. Mark Gold. Language identification in the limit. Information and control, 10(5):
447–474, 1967.
[92] Amy L. Gonzales, Jeffrey T. Hancock, and James W. Pennebaker. Language
style matching as a predictor of social dynamics in small groups. Communication
Research, 37(1):3–19, 2010.
[93] Sumit Goswami, Sudeshna Sarkar, and Mayur Rustagi. Stylometric analysis of
bloggers’ age and gender. In Proceedings of the Third International ICWSM Confer-
ence, pages 214–217, 2009.
[94] Stephan Gouws, Donald Metzler, Congxing Cai, and Eduard Hovy. Contextual
bearing on linguistic variation in social media. In Proceedings of the Workshop on
Language in Social Media (LSM 2011), pages 20–29, 2011.
[95] P Goyal, Manav R Mital, A Mukerjee, Achla M Raina, D Sharma, P Shukla, and
K Vikram. A bilingual parser for Hindi, English and code-switching structures.
In EACL 2003, 2003.
[96] Mark S. Granovetter. The strength of weak ties. American Journal of Sociology, 78
(6):1360–1380, 1973.
[97] Neil Green. Meaning-text theory: Linguistics, lexicography, and implications.
Machine Translation, 7(3):195–198, 1992.
[98] H. Paul Grice. Logic and Conversation, Syntax and Semantics, volume 3. Academic
Press, 1975.
[99] Jack Grieve, Dirk Speelman, and Dirk Geeraerts. A statistical method for the
identification and aggregation of regional linguistic variation. Language Variation
and Change, 23:193–221, 7 2011.
[100] Thomas L. Griffiths and Mark Steyvers. Finding scientific topics. Proceedings of
the National Academy of Sciences, 101(suppl 1):5228–5235, 2004.
[101] Ana Guinote and Theresa K. Vescio, editors. The Social Psychology of Power. The
Guilford Press, 2010.
[102] John J. Gumperz. Discourse strategies. Cambridge University Press, 1982.
[103] Scott A. Hale. Global connectivity and multilinguals in the Twitter network.
In Proceedings of the 32nd annual ACM conference on Human factors in computing
systems, pages 833–842, Toronto, Canada, 2014.
41
Nguyen et al. Computational Sociolinguistics: A Survey
[104] Bo Han, Paul Cook, and Timothy Baldwin. Geolocation prediction in social media
data by finding location indicative words. In Proceedings of COLING 2012, pages
1045–1062, 2012.
[105] Ahmed Hassan, Amjad Abu-Jbara, and Dragomir Radev. Detecting subgroups
in online discussions by modeling positive and negative relations among partic-
ipants. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural Language Learning, pages 59–70,
2012.
[106] Wilbert Heeringa and John Nerbonne. Language and Space. An International Hand-
book of Linguistic Variation, Volume III: Dutch, chapter Dialectometry. De Gruyter
Mouton, 2013.
[107] Fritz Heider. Attitudes and cognitive organization. Journal of Psychology, 21:107–
112, 1946.
[108] Libby Hemphill and Jahna Otterbacher. Learning the lingo?: Gender, prestige
and linguistic adaptation in review communities. In Proceedings of the ACM
2012 conference on Computer Supported Cooperative Work, pages 305–314, Seattle,
Washington, USA, 2012.
[109] Susan C. Herring, editor. Computer-mediated communication: Linguistic, social, and
cross-cultural perspectives, volume 39 of Pragmatics & Beyond New Series. John
Benjamins Publishing, 1996.
[110] Susan C. Herring. Computer-mediated discourse analysis: An approach to re-
searching online behavior. In Sasha Barab, Rob Kling, and James H. Gray, editors,
Designing for Virtual Communities in the Service of Learning, pages 338 – 376. New
York: Cambridge University Press, 2004.
[111] Susan C. Herring and John C. Paolillo. Gender and genre variation in weblogs.
Journal of Sociolinguistics, 10(4):439–459, 2006.
[112] Tom Heskes, Kees Albers, and Bert Kappen. Approximate inference and con-
strained optimization. In Proceedings of the Nineteenth conference on Uncertainty in
Artificial Intelligence, pages 313–320. Morgan Kaufmann Publishers Inc., 2002.
[113] Tony Hey, Stewart Tansley, and Kristin Tolle, editors. The fourth paradigm: data-
intensive scientific discovery. Microsoft Research, 2009.
[114] Francis Heylighen and Jean-Marc Dewaele. Variation in the contextuality of
language: An empirical measure. Foundations of Science, 7(3):293–340, 2002.
[115] Volker Hinnenkamp. Deutsch, Doyc or Doitsch? Chatters as languagers–The case
of a German–Turkish chat room. International Journal of Multilingualism, 5(3):253–
275, 2008.
[116] Lars Hinrichs. Codeswitching on the Web: English and Jamaican Creole in E-mail
Communication. John Benjamins Publishing Company, 2006.
[117] David I. Holmes. The evolution of stylometry in humanities scholarship. Literary
and Linguistic Computing, 13(3):111–117, 1998.
[118] Janet Holmes. Women, men and politeness. Routledge, 1995.
[119] Janet Holmes. An introduction to sociolinguistics. Routledge, 2013.
[120] Janet Holmes and Miriam Meyerhoff, editors. The handbook of language and gender.
Wiley-Blackwell, 2003.
[121] Dirk Hovy. Demographic factors improve classification performance. In Proceed-
ings of the 53rd Annual Meeting of the Association for Computational Linguistics and
the 7th International Joint Conference on Natural Language Processing (Volume 1: Long
Papers), pages 752–762, 2015.
[122] Dirk Hovy and Anders Søgaard. Tagging performance correlates with author
age. In Proceedings of the 53rd Annual Meeting of the Association for Computational
42
Nguyen et al. Computational Sociolinguistics: A Survey
Linguistics and the 7th International Joint Conference on Natural Language Processing
(Volume 2: Short Papers), pages 483–488, 2015.
[123] Dirk Hovy, Anders Johannsen, and Anders Søgaard. User review sites as a re-
source for large-scale sociolinguistic studies. In Proceedings of the 24th International
Conference on World Wide Web (WWW ’15), pages 452–461, 2015.
[124] Yuheng Hu, Kartik Talamadupula, and Subbarao Kambhampati. Dude, srsly?:
The surprisingly formal nature of Twitter’s language. Proceedings of the Seventh
International AAAI Conference on Weblogs and Social Media, pages 244–253, 2013.
[125] Fei Huang and Alexander Yates. Improving word alignment using linguistic code
switching data. In Proceedings of the 14th Conference of the European Chapter of the
Association for Computational Linguistics, pages 1–9, 2014.
[126] David Huffaker. Dimensions of leadership and social influence in online commu-
nities. Human Communication Research, 36(4):593–617, 2010.
[127] David Huffaker, Joseph Jorgensen, Francisco Iacobelli, Paul Tepper, and Justine
Cassell. Computational measures for language similarity across time in online
communities. In Proceedings of the HLT-NAACL 2006 Workshop on Analyzing Con-
versations in Text and Speech, pages 15–22, New York City, New York, 2006.
[128] B. Hughes, T. Baldwin, S. Bird, J. Nicholson, and A. Mackinlay. Reconsidering
language identification for written language resources. In Proceedings of the Fifth
International Conference on Language Resources and Evaluation (LREC’06), 2006.
[129] Ken Hyland. Disciplinary Discourses: Social Interactions in Academic Writing. The
University of Michigan Press, 2004.
[130] Mahaveer Jain, John McDonough, Gahgene Gweon, Bhiksha Raj, and Carolyn
Penstein Rosé. An unsupervised dynamic bayesian network approach to mea-
suring speech style accommodation. In Proceedings of the 13th Conference of the
European Chapter of the Association for Computational Linguistics, pages 787–797,
Avignon, France, 2012.
[131] Simon Jones, Rachel Cotterill, Nigel Dewdney, Kate Muir, and Adam Joinson.
Finding zelig in text: A measure for normalising linguistic accommodation. In
Proceedings of COLING 2014, the 25th International Conference on Computational
Linguistics: Technical Papers, pages 455–465, 2014.
[132] Anna Jørgensen, Dirk Hovy, and Anders Søgaard. Challenges of studying and
processing dialects in social media. In Proceedings of the Workshop on Noisy User-
generated Text, pages 9–18, 2015.
[133] Aravind K. Joshi. Processing of sentences with intra-sentential code-switching.
In Coling 1982: Proceedings of the Ninth International Conference on Computational
Linguistics, pages 145–150, 1982.
[134] Mahesh Joshi, William W. Cohen, Mark Dredze, and Carolyn P. Rosé. Multi-
domain learning: when do domains matter? In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pages 1302–1312, 2012.
[135] Mahesh Joshi, Mark Dredze, William W. Cohen, and Carolyn P. Rosé. What’s
in a domain? Multi-domain learning for multi-attribute data. In Proceedings of
NAACL-HLT 2013, pages 685–690, 2013.
[136] Dan Jurafsky, Rajesh Ranganath, and Dan McFarland. Extracting social meaning:
Identifying interactional style in spoken conversation. In Proceedings of Human
Language Technologies: The 2009 Annual Conference of the North American Chapter
of the Association for Computational Linguistics, pages 638–646, Boulder, Colorado,
2009. Association for Computational Linguistics.
43
Nguyen et al. Computational Sociolinguistics: A Survey
[137] David Jurgens, Stefan Dimitrov, and Derek Ruths. Twitter users# codeswitch
hashtags!# moltoimportante# wow. In Proceedings of the First Workshop on Com-
putational Approaches to Code Switching, pages 51–61, Doha, Qatar, 2014.
[138] Suin Kim, Ingmar Weber, Li Wei, and Alice Oh. Sociolinguistic analysis of Twitter
in multilingual societies. In Proceedings of the 25th ACM conference on Hypertext and
social media, pages 243–248, Santiago, Chile, 2014.
[139] Ben King and Steven Abney. Labeling the languages of words in mixed-language
documents using weakly supervised methods. In Proceedings of the 2013 Conference
of the North American Chapter of the Association for Computational Linguistics: Human
Language Technologies, pages 1110–1119, 2013.
[140] Ben King, Ann Arbor, Dragomir Radev, and Steven Abney. Experiments in sen-
tence language identification with groups of similar languages. In Proceedings of
the First Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects,
pages 146–154, Dublin, Ireland, 2014.
[141] Judith L. Klavans and Philip Resnik, editors. The balancing act: Combining symbolic
and statistical approaches to language. MIT press, 1996.
[142] Athanasios Kokkos and Theodoros Tzouramanis. A robust gender inference
model for online social networks and its application to LinkedIn and Twitter. First
Monday, 19(9), 2014.
[143] Daphne Koller and Nir Friedman. Probabilistic graphical models: principles and
techniques. MIT press, 2009.
[144] Farshad Kooti, Haeryun Yang, Meeyoung Cha, Krishna Gummadi, and Winter
Mason. The emergence of conventions in online social networks. In Proceedings of
the Sixth International AAAI Conference on Weblogs and Social Media, pages 194–201,
2012.
[145] Moshe Koppel, Shlomo Argamon, and Anat Rachel Shimoni. Automatically
categorizing written texts by author gender. Literary and Linguistic Computing,
17(4):401–412, 2002.
[146] Klaus Krippendorff. Content Analysis: An Introduction to Its Methodology, chapter
Validity. SAGE Publications, 2013.
[147] Vinodh Krishnan and Jacob Eisenstein. “You’re Mr. Lebowski, I’m the dude”:
Inducing address term formality in signed social networks. In Proceedings of the
2015 Conference of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages 1616–1626, 2015.
[148] William Labov. The social stratification of English in New York City. Washington,
D.C.: Center for Applied Linguistics, 1966.
[149] William Labov. Sociolinguistic Patterns. Philadelphia: University of Pennsylvania
Press, 1972.
[150] William Labov. Principles of Linguistic Change, Volume I, Internal Factors. Wiley-
Blackwell, 1994.
[151] William Labov. Principles of Linguistic Change, Volume II, Social Factors. Wiley-
Blackwell, 2001.
[152] David Lazer, Alex Sandy Pentland, Lada Adamic, Sinan Aral, Albert Laszlo
Barabasi, Devon Brewer, Nicholas Christakis, Noshir Contractor, James Fowler,
Myron Gutmann, Tony Jebara, Gary King, Michael Macy, Deb Roy, and Marshall
Van Alstyne. Life in the network: the coming age of computational social science.
Science (New York, NY), 323(5915):721–723, 2009.
[153] David Y.W. Lee. Genres, registers, text types, domains and styles: clarifying the
concepts and navigating a path through the BNC jungle. Language Learning &
Technology, 5(3):37–72, 2001.
44
Nguyen et al. Computational Sociolinguistics: A Survey
[154] Jure Leskovec, Daniel Huttenlocher, and Jon Kleinberg. Signed networks in social
media. In Proceedings of the SIGCHI Conference on Human Factors in Computing
Systems, pages 1361–1370, 2010.
[155] Stephen C. Levinson. Pragmatics. Cambridge University Press, 1983.
[156] Rivka Levitan, Agustin Gravano, and Julia Hirschberg. Entrainment in speech
preceding backchannels. In Proceedings of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language Technologies, pages 113–117, 2011.
[157] Ying Li and Pascale Fung. Code-switch language model with inversion con-
straints for mixed language speech recognition. In Proceedings of COLING 2012,
pages 1671–1680, 2012.
[158] Lizi Liao, Jing Jiang, Ying Ding, Heyan Huang, and Ee-Peng Lim. Lifetime lexical
variation in social media. In Proceedings of the Twenty-Eighth AAAI Conference on
Artificial Intelligence, pages 1643–1649, 2014.
[159] Wang Ling, Guang Xiang, Chris Dyer, Alan Black, and Isabel Trancoso. Mi-
croblogs as parallel corpora. In Proceedings of the 51st Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers), pages 176–186,
2013.
[160] Marco Lui and Paul Cook. Classifying English documents by national dialect. In
Proceedings of the Australasian Language Technology Association Workshop 2013 (ALTA
2013), pages 5–15, 2013.
[161] Marco Lui, Jey Han Lau, and Timothy Baldwin. Automatic detection and lan-
guage identification of multilingual documents. Transactions of the Association for
Computational Linguistics, 2(1):27–40, 2014.
[162] Maxim Makatchev and Reid Simmons. Perception of personality and naturalness
through dialogues by native speakers of American English and Arabic. In Pro-
ceedings of the SIGDIAL 2011 Conference, pages 286–293, Portland, Oregon, 2011.
[163] Christine Mallinson, Becky Childs, and Gerard Van Herk, editors. Data Collection
in Sociolinguistics: Methods and Applications. Routledge, 2013.
[164] William C. Mann and Sandra A. Thompson. Rhetorical structure theory: Toward
a functional theory of text organization. Text, 8(3):243–281, 1988.
[165] James R. Martin and David Rose. Working with Discourse: Meaning Beyond the
Clause. Continuum, 2003.
[166] James R. Martin and Peter R. R. White. The Language of Evaluation: Appraisal in
English. Palgrave Macmillan, 2005.
[167] Alice E. Marwick and danah boyd. I tweet honestly, I tweet passionately: Twitter
users, context collapse, and the imagined audience. New Media & Society, 13(1):
114–133, 2011.
[168] Paul McNamee. Language identification: a solved problem suitable for under-
graduate instruction. Journal of Computing Sciences in Colleges, 20(3):94–101, 2005.
[169] Miriam Meyerhoff. Introducing Sociolinguistics. Routledge, 2011.
[170] Loizos Michael and Jahna Otterbacher. Write like I write: Herding in the language
of online reviews. In Proceedings of the Eighth International AAAI Conference on
Weblogs and Social Media, pages 356–365, 2014.
[171] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation
of word representations in vector space. In Proceedings of ICLR Workshop, 2013.
[172] James Milroy and Lesley Milroy. Belfast: change and variation in an urban
vernacular, 1978.
[173] James Milroy and Lesley Milroy. Linguistic change, social network and speaker
innovation. Journal of linguistics, 21(2):339–384, 1985.
45
Nguyen et al. Computational Sociolinguistics: A Survey
[174] Lesley Milroy and Matthew Gordon. Sociolinguistics: Method and Interpretation.
Wiley-Blackwell, 2003.
[175] Alan Mislove, Sune Lehmann, Yong-Yeol Ahn, Jukka-Pekka Onnela, and J. Niels
Rosenquist. Understanding the demographics of Twitter users. In Proceedings of
the Fifth International AAAI Conference on Weblogs and Social Media, pages 554–557,
2011.
[176] Delia Mocanu, Andrea Baronchelli, Nicola Perra, Bruno Gonçalves, Qian Zhang,
and Alessandro Vespignani. The Twitter of Babel: Mapping world languages
through microblogging platforms. PLoS ONE, 8(4):e61981, 2013. doi: 10.1371/
journal.pone.0061981.
[177] Raymond A. Morrow and David D. Brown. Contemporary Social Theory: Criti-
cal Theory and Methodology, chapter Deconstructing the Conventional Discourse
of Methodology: Quantitative versus Qualitative Methods. SAGE Publications,
1994.
[178] Hamdy Mubarak and Kareem Darwish. Using Twitter to collect a multi-dialectal
corpus of Arabic. In Proceedings of the EMNLP 2014 Workshop on Arabic Natural
Language Processing (ANLP), pages 1–7, Doha, Qatar, 2014.
[179] Arjun Mukherjee and Bing Liu. Improving gender classification of blog authors.
In Proceedings of the 2010 conference on Empirical Methods in Natural Language Pro-
cessing, pages 207–217, Cambridge, MA, 2010.
[180] Carol Myers-Scotton. Social motivations for codeswitching: Evidence from Africa.
Oxford University Press, 1995.
[181] Carol Myers-Scotton. Contact linguistics: Bilingual encounters and grammatical out-
comes. Oxford: Oxford University Press, 2002.
[182] John Nerbonne and Martijn Wieling. Statistics for aggregate variationist analy-
ses. In Charles Boberg, John Nerbonne, and Dominic Watt, editors, Handbook of
Dialectology. Boston: Wiley, 2015.
[183] Dong Nguyen and A. Seza Doğruöz. Word level language identification in online
multilingual communication. In Proceedings of the 2013 Conference on Empirical
Methods in Natural Language Processing, pages 857–862, Seattle, Washington, USA,
2013.
[184] Dong Nguyen and Carolyn P. Rosé. Language use as a reflection of socialization
in online communities. In Proceedings of the Workshop on Language in Social Media
(LSM 2011), pages 76–85, 2011.
[185] Dong Nguyen, Noah A. Smith, and Carolyn P. Rosé. Author age prediction
from text using linear regression. In Proceedings of the 5th ACL-HLT Workshop
on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages
115–123, Portland, OR, USA, 2011.
[186] Dong Nguyen, Rilana Gravel, Dolf Trieschnigg, and Theo Meder. “How old do
you think I am? A study of language and age in Twitter. In Proceedings of the
Seventh International AAAI Conference on Weblogs and Social Media, pages 439–448,
2013.
[187] Dong Nguyen, Dolf Trieschnigg, A. Seza Doğruöz, Rilana Gravel, Mariët Theune,
Theo Meder, and Franciska de Jong. Why gender and age prediction from tweets
is hard: Lessons from a crowdsourcing experiment. In Proceedings of COLING
2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1950–1961, Dublin, Ireland, 2014.
[188] Dong Nguyen, Dolf Trieschnigg, and Theo Meder. Tweetgenie: Development,
evaluation, and lessons learned. In Proceedings of COLING 2014, the 25th Interna-
tional Conference on Computational Linguistics: System Demonstrations, pages 62–66,
46
Nguyen et al. Computational Sociolinguistics: A Survey
Dublin, Ireland, 2014.
[189] Dong Nguyen, Dolf Trieschnigg, and Leonie Cornips. Audience and the use
of minority languages on Twitter. In Proceedings of the Ninth International AAAI
Conference on Web and Social Media, pages 666–669, 2015.
[190] Viet-An Nguyen, Jordan Boyd-Graber, Philip Resnik, Deborah A. Cai, Jennifer E.
Midberry, and Yuanxin Wang. Modeling topic control to detect influence in
conversations using nonparametric topic models. Machine Learning, 95(3):381–
421, 2014.
[191] Kate G. Niederhoffer and James W. Pennebaker. Linguistic style matching in
social interaction. Journal of Language and Social Psychology, 21(4):337–360, 2002.
[192] Scott Nowson and Jon Oberlander. The identity of bloggers: Openness and gender
in personal weblogs. In AAAI Spring Symposium: Computational Approaches to
Analyzing Weblogs, pages 163–167, 2006.
[193] Scott Nowson, Jon Oberlander, and Alastair J. Gill. Weblogs, genres and individ-
ual differences. In Proceedings of the 27th Annual Conference of the Cognitive Science
Society, pages 1666–1671, 2005.
[194] Jahna Otterbacher. Inferring gender of movie reviewers: exploiting writing style,
content and metadata. In Proceedings of the 19th ACM international conference on
Information and knowledge management, pages 369–378, 2010.
[195] John C. Paolillo. Language variation on Internet Relay Chat: A social network
approach. Journal of sociolinguistics, 5(2):180–213, 2001.
[196] Evangelos E. Papalexakis, Dong Nguyen, and A. Seza Doğruöz. Predicting code-
switching in multilingual communication for immigrant communities. In Pro-
ceedings of the First Workshop on Computational Approaches to Code Switching, pages
42–50, Doha, Qatar, 2014.
[197] Umashanthi Pavalanathan and Jacob Eisenstein. Audience-modulated variation
in online social media. American Speech, 90(2):187–213, 2015.
[198] Claudia Peersman, Walter Daelemans, and Leona Van Vaerenbergh. Predicting
age and gender in online social networks. In Proceedings of the 3rd international
workshop on search and mining user-generated contents (SMUC ’11), pages 37–44,
2011.
[199] Yves Peirsman, Dirk Geeraerts, and Dirk Speelman. The automatic identification
of lexical variation between language varieties. Natural Language Engineering, 16
(04):469–491, 2010.
[200] Nanyun Peng, Yiming Wang, and Mark Dredze. Learning polylingual topic
models from code-switched social media documents. In Proceedings of the 52nd
Annual Meeting of the Association for Computational Linguistics (Volume 2: Short
Papers), pages 674–679, 2014.
[201] Marco Pennacchiotti and Ana-Maria Popescu. A machine learning approach to
Twitter user classification. In Proceedings of the Fifth International AAAI Conference
on Weblogs and Social Media, pages 281–288, 2011.
[202] James W. Pennebaker, Martha E. Francis, and Roger J. Booth. Linguistic Inquiry
and Word Count: LIWC 2001. Mahwah, NJ: Lawrence Erlbaum, 2001.
[203] Kelly Peterson, Matt Hohensee, and Fei Xia. Email formality in the workplace:
A case study on the Enron corpus. In Proceedings of the Workshop on Language in
Social Media (LSM 2011), pages 86–95, 2011.
[204] Mario Piergallini, Seza A. Doğruöz, Phani Gadde, David Adamson, and Car-
olyn P. Rose. Modeling the use of graffiti style features to signal social relations
within a multi-domain learning paradigm. In Proceedings of the 14th Conference of
the European Chapter of the Association for Computational Linguistics, pages 107–115,
47
Nguyen et al. Computational Sociolinguistics: A Survey
2014.
[205] Shana Poplack, David Sankoff, and Christopher Miller. The social correlates and
linguistic processes of lexical borrowing and assimilation. Linguistics, 26(1):47–
104, 1988.
[206] Tom Postmes, Russell Spears, and Martin Lea. The formation of group norms in
computer-mediated communication. Human Communication Research, 26(3):341–
371, 2000.
[207] Vinodkumar Prabhakaran and Owen Rambow. Written dialog and social power:
Manifestations of different types of power in dialog behavior. In Proceedings of the
Sixth International Joint Conference on Natural Language Processing, pages 216–224,
Nagoya, Japan, 2013.
[208] Vinodkumar Prabhakaran, Owen Rambow, and Mona Diab. Who’s (really) the
boss? Perception of situational power in written interactions. In Proceedings of
COLING 2012, pages 2259–2274, 2012.
[209] Vinodkumar Prabhakaran, Owen Rambow, and Mona Diab. Predicting overt dis-
play of power in written dialogs. In Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computational Linguistics: Human Language
Technologies, pages 518–522, 2012.
[210] Vinodkumar Prabhakaran, Ajita John, and D. Dorée Seligmann. Who had the
upper hand? Ranking participants of interactions based on their relative power. In
Proceedings of the Sixth International Joint Conference on Natural Language Processing,
pages 365–373, 2013.
[211] Vinodkumar Prabhakaran, Ashima Arora, and Owen Rambow. Staying on topic:
An indicator of power in political debates. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing (EMNLP), pages 1481–1486, 2014.
[212] Vinodkumar Prabhakaran, E. Emily Reid, and Owen Rambow. Gender and
power: How gender and gender environment affect manifestations of power. In
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Process-
ing (EMNLP), pages 1965–1976, 2014.
[213] John M. Prager. Linguini: Language identification for multilingual documents. In
Proceedings of the 32nd Annual Hawaii International Conference on Systems Sciences,
pages 1–11, 1999.
[214] Jelena Prokić, Çağrı Çöltekin, and John Nerbonne. Detecting shibboleths. In
Proceedings of the EACL 2012 Joint Workshop of LINGVIS & UNCLH, pages 72–80,
2012.
[215] Daniele Quercia, Jonathan Ellis, Licia Capra, and Jon Crowcroft. In the mood for
being influential on Twitter. In Proceedings of the 3rd IEEE International Conference
on Social Computing, 2011.
[216] Sophia Rabe-Hesketh and Anders Skrondal. Multilevel and Longitudinal Modeling
Using Stata. Stata Press, 2012.
[217] Sophia Rabe-Hesketh, Anders Skrondal, and Andrew Pickles. GLLAMM Manual.
U.C. Berkeley Division of Biostatistics Working Paper Series, Paper 160, 2004.
[218] Francisco Rangel, Paolo Rosso, Moshe Koppel, Efstathios Stamatatos, and Gia-
como Inches. Overview of the author profiling task at PAN 2013. Notebook Papers
of CLEF, 2013.
[219] Francisco Rangel, Paolo Rosso, Irina Chugur, Martin Potthast, Martin Trenkmann,
Benno Stein, Ben Verhoeven, and Walter Daelemans. Overview of the 2nd author
profiling task at PAN 2014. In CLEF 2014 Evaluation Labs and Workshop – Working
Notes Papers, 2014.
48
Nguyen et al. Computational Sociolinguistics: A Survey
[220] Delip Rao, David Yarowsky, Abhishek Shreevats, and Manaswi Gupta. Classify-
ing latent user attributes in Twitter. In Proceedings of the 2nd international workshop
on search and mining user-generated contents (SMUC ’10), pages 37–44, 2010.
[221] Delip Rao, Michael J. Paul, Clayton Fink, David Yarowsky, Timothy Oates, and
Glen Coppersmith. Hierarchical bayesian models for latent attribute detection in
social media. In Proceedings of the Fifth International AAAI Conference on Weblogs
and Social Media, pages 598–601, 2011.
[222] Branca Telles Ribeiro. Footing, positioning, voice. are we talking about the same
things? In Anna De Fina, Deborah Schiffrin, and Michael Bamberg, editors,
Discourse and Identity, pages 48–82. Cambridge University Press, 2006.
[223] Keith Richards. Language and Professional Identity: Aspects of Collaborative Interac-
tion. Palgrave Macmillan, 2006.
[224] Suzanne Romaine. Bilingualism (2nd edition). Malden, MA: Blackwell Publishers,
1995.
[225] Sara Rosenthal and Kathleen McKeown. Age prediction in blogs: a study of style,
content, and online behavior in pre- and post-social media generations. In Pro-
ceedings of the 49th Annual Meeting of the Association for Computational Linguistics:
Human Language Technologies, pages 763–772, Portland, Oregon, USA, 2011.
[226] Gillian Sankoff. Age: Apparent time and real time. Encyclopedia of Language and
Linguistics. Oxford, UK: Elsevier, 2006.
[227] Maarten Sap, Gregory Park, Johannes Eichstaedt, Margaret Kern, David Stillwell,
Michal Kosinski, Lyle Ungar, and Andrew Hansen Schwartz. Developing age and
gender predictive lexica over social media. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing (EMNLP), pages 1146–1151, 2014.
[228] Ruchita Sarawgi, Kailash Gajulapalli, and Yejin Choi. Gender attribution: Tracing
stylometric evidence beyond topic and genre. In Proceedings of the Fifteenth Confer-
ence on Computational Natural Language Learning, pages 78–86, Portland, Oregon,
USA, 2011.
[229] Emanuel A. Schegloff. Sequence Organization in Interaction: A Primer in Conversation
Analysis, volume 1. Cambridge University Press, 2007.
[230] Yves Scherrer and Owen Rambow. Word-based dialect identification with georef-
erenced rules. In Proceedings of the 2010 Conference on Empirical Methods in Natural
Language Processing, pages 1151–1161, 2010.
[231] Jonathan Schler, Moshe Koppel, Shlomo Argamon, and James W. Pennebaker.
Effects of age and gender on blogging. In Proceedings of AAAI Spring Symposium
on Computational Approaches to Analyzing Weblogs, 2006.
[232] Gerold Schneider, James Dowdall, and Fabio Rinaldi. A robust and hybrid deep-
linguistic theory applied to large-scale parsing. In Proceedings of the 3rd workshop
on RObust Methods in Analysis of Natural Language Data (ROMAND 2004), pages
14–23, 2004.
[233] H. Andrew Schwartz, Johannes C. Eichstaedt, Margaret L. Kern, Lukasz Dzi-
urzynski, Stephanie M. Ramones, Megha Agrawal, Achal Shah, Michal Kosinski,
David Stillwell, Martin E. P. Seligman, and Lyle H. Ungar. Personality, gender,
and age in the language of social media: The open-vocabulary approach. PLoS
ONE, 8(9):e73791, 2013.
[234] Lauren E. Scissors, Alastair J. Gill, Kathleen Geraghty, and Darren Gergle. In CMC
we trust: The role of similarity. In Proceedings of the SIGCHI Conference on Human
Factors in Computing Systems (CHI ’09), pages 527–536, 2009.
[235] John R. Searle. Speech acts: An essay in the philosophy of language. Cambridge:
Cambridge University Press, 1969.
49
Nguyen et al. Computational Sociolinguistics: A Survey
[236] Sameer Singh. A pilot study on gender differences in conversational speech on
lexical richness measures. Literary and Linguistic Computing, 16(3):251–264, 2001.
[237] David A. Snow and Leon Anderson. Identity work among the homeless: The ver-
bal construction and avowal of personal identities. American Journal of Sociology,
92(6):1336–1371, 1987.
[238] Rion Snow, Brendan O’Connor, Daniel Jurafsky, and Andrew Y. Ng. Cheap and
fast—but is it good?: Evaluating non-expert annotations for natural language
tasks. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language
Processing, pages 254–263, Honolulu, Hawaii, 2008.
[239] Jordan Soliz and Howard Giles. Relational and identity processes in communi-
cation: A contextual and meta-analytical review of Communication Accommo-
dation Theory. In Elisia L. Cohen, editor, Communication Yearbook 38. Routledge,
2014.
[240] Thamar Solorio and Yang Liu. Learning to predict code-switching points. In Pro-
ceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,
pages 973–981, Honolulu, Hawaii, 2008.
[241] Thamar Solorio and Yang Liu. Part-of-speech tagging for English-Spanish code-
switched text. In Proceedings of the 2008 Conference on Empirical Methods in Natural
Language Processing, pages 1051–1060, Honolulu, Hawaii, 2008.
[242] Thamar Solorio, Elizabeth Blair, Suraj Maharjan, Steven Bethard, Mona Diab,
Mahmoud Ghoneim, Abdelati Hawwari, Fahad AlGhamdi, Julia Hirschberg, Al-
ison Chang, and Pascale Fung. Overview for the first shared task on language
identification in code-switched data. In Proceedings of the First Workshop on Com-
putational Approaches to Code Switching, pages 62–72, Doha, Qatar, 2014.
[243] Efstathios Stamatatos. A survey of modern authorship attribution methods.
Journal of the American Society for Information Science and Technology, 60(3):538–556,
2009.
[244] Wessel Stoop and Antal van den Bosch. Using idiolects and sociolects to improve
word prediction. In Proceedings of the 14th Conference of the European Chapter of the
Association for Computational Linguistics, pages 318–327, 2014.
[245] Tomek Strzalkowski, Samira Shaikh, Ting Liu, Aaron George Broadwell, Jenny
Stromer-Galley, Sarah Taylor, Umit Boz, Veena Ravishankar, and Xiaoai Ren.
Modeling leadership and influence in multi-party online discourse. In Proceedings
of COLING 2012, pages 2535–2552, 2012.
[246] Swabha Swayamdipta and Owen Rambow. The pursuit of power and its manifes-
tation in written dialog. In Proceedings of 2012 IEEE Sixth International Conference
on Semantic Computing (ICSC), pages 22–29, 2012.
[247] Maite Taboada and William C. Mann. Applications of Rhetorical Structure Theory.
Discourse Studies, 8(4):567–588, 2006.
[248] Sali A. Tagliamonte. Analysing sociolinguistic variation. Cambridge University
Press, 2006.
[249] Jenny Tam and Craig H. Martell. Age detection in chat. In ICSC ’09. IEEE
International Conference on Semantic Computing, pages 33–39, 2009.
[250] Deborah Tannen. You just don’t understand: Women and men in conversation. Ballan-
tine Books, 1990.
[251] Deborah Tannen. Framing in Discourse. Oxford University Press, 1993.
[252] Sarah Thomason. Language contact: an introduction. Edinburgh: Edinburgh Uni-
versity Press, 2001.
[253] Dolf Trieschnigg, Djoerd Hiemstra, Mariët Theune, Franciska Jong, and Theo
Meder. An exploration of language identification techniques for the Dutch folktale
50
Nguyen et al. Computational Sociolinguistics: A Survey
database. In Workshop on Adaptation of Language Resources and Tools for Processing
Cultural Heritage workshop (LREC 2012), pages 47–51, 2012.
[254] Peter Trudgill. The social differentiation of English in Norwich. Cambridge: Cam-
bridge University Press, 1974.
[255] Peter Trudgill. The Norfolk Dialect. Norfolk Origins 7. Poppyland Publishing, 2003.
[256] Khiet P. Truong, Gerben J. Westerhof, Sanne M. A. Lamers, and Franciska de Jong.
Towards modeling expressed emotions in oral history interviews: Using verbal
and nonverbal signals to track personal narratives. Literary and Linguistic Comput-
ing, 29(4):621–636, 2014.
[257] Liza Tsaliki. Globalization and hybridity: the construction of Greekness on the
Internet. In Karim Haiderali Karim, editor, The Media of Diaspora. Routledge, 2003.
[258] Benjamin Van Durme. Streaming analysis of discourse participants. In Proceedings
of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and
Computational Natural Language Learning, pages 48–58, Jeju Island, Korea, 2012.
[259] Svitlana Volkova, Theresa Wilson, and David Yarowsky. Exploring demographic
language variations to improve multilingual sentiment analysis in social media.
In Proceedings of the 2013 Conference on Empirical Methods in Natural Language
Processing, pages 1815–1827, 2013.
[260] Clare Voss, Stephen Tratz, Jamal Laoudi, and Douglas Briesch. Finding romanized
Arabic dialect in code-mixed tweets. In Proceedings of the Ninth International
Conference on Language Resources and Evaluation (LREC-2014), 2014.
[261] Yogarshi Vyas, Spandana Gella, Jatin Sharma, Kalika Bali, and Monojit Choud-
hury. POS tagging of English-Hindi code-mixed social media content. In Pro-
ceedings of the 2014 Conference on Empirical Methods in Natural Language Processing
(EMNLP), pages 974–979, 2014.
[262] Suzanne E. Wagner. Age grading in sociolinguistic theory. Language and Linguistics
Compass, 6(6):371–382, 2012.
[263] Yafei Wang, David Reitter, and John Yen. Linguistic adaptation in conversation
threads: Analyzing alignment in online health communities. In Proceedings of the
2014 ACL Workshop on Cognitive Modeling and Computational Linguistics, pages 55–
62, Baltimore, Maryland, USA, 2014.
[264] Ronald Wardhaugh. An Introduction to Sociolinguistics. Wiley-Blackwell, 2011.
[265] Li Wei. The ’why’ and ’how’ questions in the analysis of conversational
codeswitching. In Peter Auer, editor, Codeswitching in conversation: Language,
interaction and identity, pages 156–176. London: Routledge, 1998.
[266] Uriel Weinreich. Languages in contact. findings and problems. New York, Linguis-
tic Circle of New York, 1953.
[267] Uriel Weinreich, William Labov, and Marvin I. Herzog. Empirical foundations for
a theory of language change. In Winfred P. Lehmann and Yakov Malkiel, editors,
Directions for Historical Linguistics: A Symposium, pages 95–188. Austin: University
of Texas Press, 1968.
[268] Robert West, S. Hristo Paskov, Jure Leskovec, and Christopher Potts. Exploiting
social network structure for person-to-person sentiment analysis. Transactions of
the Association of Computational Linguistics – Volume 2, Issue 1, pages 297–310, 2014.
[269] Martijn Wieling and John Nerbonne. Hierarchical spectral partitioning of bipartite
graphs to cluster dialects and identify distinguishing features. In Proceedings of
TextGraphs-5 - 2010 Workshop on Graph-based Methods for Natural Language Process-
ing, pages 33–41, 2010.
[270] Martijn Wieling and John Nerbonne. Advances in dialectometry. Annual Review
of Linguistics, 1(1):243–264, 2015.
51
Nguyen et al. Computational Sociolinguistics: A Survey
[271] Benjamin Wing and Jason Baldridge. Simple supervised document geolocation
with geodesic grids. In Proceedings of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Technologies, pages 955–964, 2011.
[272] Shuly Wintner. Formal language theory for natural language processing. In
Proceedings of the ACL-02 Workshop on Effective Tools and Methodologies for Teaching
Natural Language Processing and Computational Linguistics, pages 71–76, 2002.
[273] Hiroshi Yamaguchi and Kumiko Tanaka-Ishii. Text segmentation by language
using minimum description length. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics (Volume 1: Long Papers), pages 969–978,
Jeju Island, Korea, 2012.
[274] Xiang Yan and Ling Yan. Gender classification of weblog authors. In AAAI Spring
Symposium: Computational Approaches to Analyzing Weblogs, pages 228–230, 2006.
[275] Omar F. Zaidan and Chris Callison-Burch. Arabic dialect identification. Computa-
tional Linguistics, 40(1):171–202, 2013.
[276] Faiyaz Al Zamal, Wendy Liu, and Derek Ruths. Homophily and latent attribute
inference: Inferring latent attributes of Twitter users from neighbors. In Proceed-
ings of the Sixth International AAAI Conference on Weblogs and Social Media, pages
387–390, 2012.
[277] Marcos Zampieri, Liling Tan, Nikola Ljubešić, and Jörg Tiedemann. A report on
the DSL shared task 2014. In Proceedings of the First Workshop on Applying NLP
Tools to Similar Languages, Varieties and Dialects, pages 58–67, Dublin, Ireland, 2014.
[278] Jian Zhang, Zoubin Ghahramani, and Yiming Yang. Flexible latent variable
models for multi-task learning. Machine Learning, 73(3):221–242, 2008.
52
