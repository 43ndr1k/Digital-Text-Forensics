Review of Various classification Techniques for Text Classification 
Maninder Singh1, William Perrizo2 
Department of Computer Science, North Dakota State University 
Fargo, United States 
1maninder.singh@ndsu.edu 
2 william.perrizo@ndsu.edu 
 
Abstract: - This paper present the review of various 
classification techniques along with their merits, demerits 
and scope in using them for text classification. This paper 
discuss various existing techniques like Bayesian 
classification (Na√Øve Bayes), Decision Trees, Support Vector 
Machines and K- Nearest neighbours. The paper discuss the 
applicability of above mentioned techniques for the 
classification of text especially in emails and propose a 
technique to identify any security related threat through 
emails. The malicious user does not send any plain 
information through plain text words in emails. So the 
technique described in this paper first of all consider only 
one dimensional data i.e. the technique check single email at 
a time. A new proposed approach filter the text based on 
vertical analysis of characters present in the text using P-
trees. The classification based on our proposed technique 
would be able to classify text documents which have 
threatening code words spread along the document through 
inserting the characters in different words. 
 
Keywords: - Classification, Support Vector Machine, 
Bayesian Classification, Na√Øve Bayes, Decision Trees, 
Nearest Neighbours, Machine Learning, Text Filtering, 
Security. 
 
I. INTRODUCTION 
This paper discuss the work of text classification done by 
various classification approaches. In this paper, we will be 
discussing the security threat prediction using classification of 
text in emails. But before presenting the idea, a brief 
introduction to the classification process has been presented. 
Data classification is the process in which the data is organised 
into various distinct categories in order to make the effective use 
of it [1].  
Classification and Prediction are the two areas in which models 
can be extracted to describe the present and future class of data. 
The use of classification is important in order to understand the 
large data sets [2]. Text classification is also the sub category of 
machine learning in data mining [3]. The difference here is that 
it is supervised learning that is, it learn a method for predicting 
the instance class from pre-labelled (classified) instances [4]. In 
this method, data mining function assigns items in a collection 
to target categories or classes [5].  
Classification can be further divided into two categories, model 
based and neighbour based [3]. It is begin with a dataset in which 
class assignments are known [5]. For example, a classification 
model can be used in order to assess the risk that can be originate 
from a loan application. It can classify the application based on 
persons history, his rentals, his investment or his years of 
residence etc. in this example the target class would be credit 
risk whereas, the history, rentals and residence would be 
predictors. In statistical classification, the prediction of category 
to which the newly made observation will belong depends upon 
how good the training set is? That is the correctness of training 
set is of absolute value [6]. Generally, the classification is of two 
types: supervised learning and unsupervised learning. In 
supervised learning the training set consist of the values which 
are very clearly observed or taken. In unsupervised learning the 
training set is based on grouping of data through some 
measurements or mathematical functions which distinguish 
based upon distance within the set. In classification, the training 
set is used to predict the class of the new tuple into the data set. 
So it is very much important to have a good training set. To make 
the good training set, proper classifiers are chosen and some 
feature selection approaches are followed. The feature selection 
approach is used to take out only the very important and 
considerable features from the data which are most likely to help 
in prediction of classes in classification process. There are many 
classification methods available within literature like decision 
tree induction, Bayesian classification and many other 
approaches. 
In this paper, we are proposing a technique for classifying the 
text especially in the emails. The motivation behind this has been 
that so many text classification algorithms has been proposed in 
literature, which make use of various existing techniques like 
Support Vector Machines, Na√Øve Bayes, Decision trees etc. 
These techniques do classify the large dimensional text data. The 
questions arise when we have to search for some security related 
information during email exchange. Normally, the malicious 
user does not send any plain information through plain text 
words in emails. So the technique described in this paper first of 
all will consider only one dimensional data i.e. it will be checked 
on single email at a time. As the number of email exchange 
corresponds to the very large data in volume so we are applying 
the vertical processing over the text data in our proposed 
scheme. 
II. CONTEXT 
There are many techniques used to classify the Textual data. As 
part of the literature survey for this paper, the most commonly 
used among those have been discussed in this paper. The very 
basic technique of Decision tree is used for unsupervised 
learning [2] [7]. Its description is as follows: 
 
A) Decision Tree Induction: Decision trees are unsupervised 
learning classification technique, in which the qualifier is 
checked based on the flow chart like structures. There are 
various techniques used to provide the decision tree 
algorithm like ID 3, C4.5, Na√Øve Tree (NT) and CART as 
described by upasana and Chakravarty in [7]. The decisions 
are made from the class labelled training tuples.  
Working: For example if the new tuple is X and its class 
label is not known then it is tested along the path of decision 
tree starting from root node to the leaf node. If the leaf node 
is reached it is assigned the class label of leaf node because 
the leaf nodes contains the class label of the decision taken 
at non leaf node. In this manner the classification is 
performed in decision tree induction classification. 
Text Classification Using Decision Trees: The text 
classification uses the high dimensionality of feature space 
and there is always the risk of over fitting of data features. 
So some pruning is required to select only the effective 
features. CART (Classification and Regression Tree) uses 
Gini Impurity, ID3 uses entropy based Information Gain. 
C4.5 uses rule post pruning [2] [7]. 
Advantage:  The main strength of this approach is that it 
creates very simple and understandable rules which require 
very simple computations. 
Disadvantage: for the continuous attributes the information 
gain of many points has to be computed within each variable 
[7]. Which adds to the totals computation cost. 
 
B) K-Nearest Neighbour Classification: This classification 
technique is based on the topology of learning by analogy 
which means is that any given tuple is compared with 
training tuple that are given to it [2]. 
Working: Generally, the training set is n-attributes and each 
attribute represents a point in a space making it to n-
dimensional space. K nearest neighbour classifier as 
described by [2] searches the pattern space for the k-training 
tuples that are close to unknown tuple and these k training 
tuples are the k nearest neighbours [2]. The closeness in 
terms of the distance is represented by the Euclidean distance 
between two points or tuples. 
Text Classification using KNN: Gongde guo et al in [8] 
showed the result of using kNN, SVM and kNN model based 
approaches and concluded that kNN consumed about 1200 
seconds in classifying the text on the data set of Reuters and 
20-newsgroup whereas SVM consumed 350 seconds. So this 
comparison shows that kNN is not the only best way to carry 
out text categorization. 
Advantage: it is very simple and uses distance to form the 
classification. 
Disadvantage: it give rise to large number of false positives 
 
C) Closed kNN Classification: The K nearest neighbour 
classification method the value of k has to be decided 
manually in order to predict the class of unknown sample. 
Md. Khan , willian perrizo et al. in [9] discuss the approach 
of calculating Closed nearest neighbour instead of k nearest 
neighbour. The idea behind this is that if we choose k 
manually and if the value of k is taken as 1 then prediction 
classes of the unknown sample are very much. If the k is 
chosen large then we may over look than what is in actually 
required but in closed neighbour set is build and voting is 
performed to find the predicting class. [9] Proposed a new 
approach of building nearest neighbor set, where the closure 
of k-NN set that is inclusion of all the boundary neighbors is 
taken and it is called closed-KNN. 
The definition can be summarized as  
   a). If x ‚àà KNN, then x ‚àà closed-KNN,  
   b) If x ‚àà closed-KNN and d(T,y) ‚â§ d(T,x), then y ‚àà closed-
KNN where, d(T,x) is the distance of x from target T. 
  Advantage: It takes into account the boundary value while 
considering the distance to evaluate classification. 
  Disadvantage: The problem of false positive still exists. 
 
D) Bayesian Classification:  Bayesian classifiers comes under 
the category of statistical classifier. This classification 
method can predict the membership based on probabilities. 
This classification method assume that the effect of any 
attribute value on a given class is independent of the values 
of the other attributes [2].  
Working: Let us consider X as a data tuple and it will be 
known as evidence in bayesian classification. Any 
hypothesis H is defined as to predict a data tuple X belongs 
to one of the class C. So this is denoted by P (H|X), it is called 
posterior probability. In a similar manner there is P (X|H) 
meaning that what is the probability of X if it is conditioned 
on H. There are also some prior probabilities like P (H) and 
P (X). The Bayes theorem states that  
P (H|X) = (P (X|H) P (H))/P (X) 
Text Classification using Bayesian theorem:  The 
examples in [10], [11] and [12] states that the na√Øve Bayes 
work very efficiently in text classification. The time 
complexity of na√Øve Bayes theorem is linear in time for both 
training and test. It is denoted as O (|D| L + |C| |V|) where D 
is document and L is average length, C is set of classes. 
Advantage: It is fast because of having linear execution 
time. 
 
E) Support Vector Machines: Support vector Machines are 
very good and promising to classify the linear and non-linear 
data. SVM maps the training data into higher dimensions and 
within higher dimension it searches for linear optimal hyper 
plane or in simple words a decision boundary which separate 
one class from other. 
Working: Let us say the classes are linearly separable for a 
data set D given as (Xi, Yj) where 1 ‚â§ i ‚â§ ùëõ  ùëéùëõùëë ùëó ‚àà {+1, -
1} that is there are two classes either yes or no. In the Figure 
1, it can be seen when the data is mapped on the 2-D plane, 
there are infinite number of lines that can be drawn between 
points and which distinguish them from another class. It can 
be observed from the figure 1 that there are two hyper planes, 
but we will consider the hyper plane which has the most 
separating distance. The reason for this is that it can classify 
more data variable which lies in between the lines. 
Text Classification Using Support Vector Machines: 
The text classification as stated by [16], is efficient with 
SVM and it is less prone to over fitting [2]. [2] and [16] 
discuss that the complexity of learned classifier is 
characterized by the number of support vectors rather than 
the dimensionality of data. 
 
  
Figure 1. The 2-D plane for SVM [2] 
 
III. PROPOSED IDEA 
The techniques discussed in [7], [12], [13], [14] and [15], present 
some text classification approaches towards filtering emails. The 
motivation behind proposing the idea in this paper lies in the fact 
that all these techniques classify the documents text based on 
occurrence of certain words in that text. The techniques 
discussed in [14] and [15] for filtering the spam emails works by 
identifying the keywords in the document. The techniques does 
not specify in any way to classify the data if some security threat 
or breaching information is passed over the emails by encoding 
the message character by character within words of emails. For 
example, consider the following text: 
 
‚ÄúBefore you leave to visit any Country make sure you listed the 
details. Long vocations needs planning. As a wise person one 
should study the details first before leaving. So in order to enjoy 
at the best of your moments, try to search online for visiting 
places. Then prepare your itinerary. Other than this, more 
details for information you can check on maps. Reach out to 
some of your family and friends who have visited these places 
prior to you and get feedback from them. Depending upon the 
best options make a go toward your vocation.‚Äù 
The above message if we try to classify by selecting features 
does not efficiently solve the purpose. While selecting the 
features of above text document for classification purpose, the 
existing techniques will remove prepositions, articles, 
conjunctions and pronouns etc. from the text document. But 
what if, there is some valuable information present inside the 
prepositions, pronouns or in any other words, which seems not 
so important in text during feature selection. More clearly, the 
above message includes the security breach and it is not 
classified into threat class while classifying. Let‚Äôs examine how? 
Examine the first character of every line, for more clarity, let‚Äôs 
re-write the above text in following manner. 
‚ÄúBefore you leave to visit any Country make sure you listed the 
details. Long vocations needs planning. As a wise person one 
should study the details first before leaving. So in order to enjoy 
at the best of your moments, try to search online for visiting 
places. Then prepare your itinerary. Other than this, more detail 
information you can check on maps. Reach out to some of your 
family and friends who have visited these places prior to you and 
get feedback from them. Depending upon the best options make 
a go toward your vocation.‚Äù 
Now if we extract all the first character from the text, the 
following sentence is formed: 
‚ÄúBLAST ORD‚Äù 
The ORD is code of Chicago international airport. Now this is a 
security issue and this goes unmonitored in formal classification 
techniques. The proposed techniques works in the way that it 
first of all checks for suspicious words in the form of characters 
within other words. This approach states an idea in which it 
search for the threatening pattern before applying feature 
selection as shown in figure 2. If the text successfully pass 
through first phase, then it is further classified into other classes.  
 
PHASE 1   PHASE 2 
 
 
 
 
 
Figure 2. The block diagram of Proposed Idea 
The technique is further explained through the following 
algorithm. 
PROOF: The proposed technique work in two phases. First is 
checking of the complete document using P-trees (used to 
process the data vertically). Second is it follows the conventional 
classification techniques. 
1) Applying P-Trees: in this step we make document D, which 
has elements {d1, d2, d3, d4‚Ä¶‚Ä¶. dn}. The elements of set D 
are all the words which are considered dangerous or 
suspicious or some specified code words used during war to 
denote certain things like package, item, target, bird etc. Now 
the P-Tree of text document (T) should be made which we 
want to test against out document D. One important note is 
that, the algorithm does not check all the characters in Text 
Document T in serial manner because this action would 
increase computation and time. So to further narrow the 
search, this algorithm will follow certain mathematical 
equations to search the characters inside the text. For 
example, the example at the top which uses first character of 
every word as a code word can be tested using the following 
equation. 
C= ‚àÄ (i ‚àã di && Ci ‚àà ùëã(1)) 
Where C is the outcome of the word made so far after 
checking against the text document, i is the index of 
character, di is the word in document D we are searching and 
Text 
Doc 
Heuristic/ 
Searching 
Techniques 
Conventional 
classification 
Techniques 
X is the index of line in text document T and X(1) represents 
the first word in T. 
Similarly, the algorithm can check the occurrence of words 
within the text document through applying some other 
computational equations.  
The reason why P-Tree are applied because the processing 
and searching in p-trees is very fast by applying AND 
operations. The existence of a particular character Ci , where 
1‚â§i‚â§length (di). But the working in this step follows the 
approach that Ci will be checked only if all Ci-1 matches. 
Now another important factor to be considered here is that 
what if ‚ÄúBLAST is written as BLST, KILL as KLL‚Äù. Here 
we need to apply heuristic search (either in serial or in 
parallel with the algorithm). The heuristic search will take 
input as the word which has been searched within the 
document and then look for all the stemmed, synonyms and 
phonetic occurrence of that word. If the result of heuristic is 
any threat word then the algorithm will check again word by 
word, the suspicious document only. 
2) Conventional Approach: Here we can add the results from 
the first step as one of the feature in training set which we 
are going to include for our conventional techniques. The 
na√Øve Bayes, SVM, Decision trees can be applied in the 
similar way as were applied to other text classification 
algorithms. 
 
Algorithm of the Proposed Technique: 
In the proposed algorithm, a document D containing all the 
words to be checked are contained, document T contains the text 
document which is to be checked against D, A document E 
which contains ‚Äòm‚Äô mathematical equations on which we want 
to search the words in D against T. the algorithm works like 
follows: 
Step 1:  Prepare Document DÔÉü{d1, d2, d3, d4‚Ä¶‚Ä¶. dn}. 
Step 2:  Fetch Document T (which contains email Text). 
Step 3:  ‚àÄ Xi ‚àà T, Prepare p-tree (Xi)  
             In above step Xi is the ith sentence in text document T. 
Step 4:  Fetch Document E and initialise eqÔÉüj  
  Which contains equation on which the search criteria 
   will    begin and 1‚â§j‚â§m 
Step 5:   initialise CÔÉüdi, where 1‚â§i‚â§n 
Step 6:   Loop:KÔÉü1 to numberOfLines inT 
        C= ‚àÄ (i ‚àã di && Ci ‚àà Xk (charAt(eq(k))) 
 If (di contains C) then   //so far some matches found 
            Iterate 
           Else 
 Write Results to log file 
       Break; 
Step 7:   if (j<m) then //still more equations to search 
 Increment j // text will be searched on next equation. 
 Go to Step 4 
Step 8:   Increment i // this will take the second word From D 
              JÔÉü1 // to set the search equation from the begining 
Step 9:  Go to Step 4 
Step 10:  End 
Step 11:  Pass the results to Conventional classification 
                techniques 
 
IV. CONCLUSION AND FUTURE WORK 
As traced from the algorithm and its working it can be seen that 
from the security point of view this algorithm gives better results 
than the other conventional classification technique. The 
theoretical proposal has been proposed through this paper, as the 
real output can be generated using some large data set, which are 
not available at this time to the author. So the author has tried to 
contrast the algorithms on the basis of literature so far studied. 
But there is trade-off between performance and evaluation. The 
algorithm may be suitable to detect the threats of sending vital 
information through emails. 
 The algorithm adds complexity to evaluation procedure as there 
is overhead like creation of Document D, creation of P-Trees for 
every text document and then maintain the mathematical 
equations against which the words are to be tested. Moreover, 
carefully examining the algorithm, it can be seen that the 
complexity is O (Tx * m*n) and if the Tx, m and n are 
approximately of same size then the complexity is O (N3) for the 
phase 1 of the algorithm. But if we clearly examine that the Tx 
which is total number of lines in an email to be quite less in 
number as compared to m and n, then it can be considered that 
the complexity of algorithm is O (N2). As far as space 
complexity is considered, the space requirement is more in this 
algorithm in order to make p-trees, generation of intermediate 
files like log files etc.  
This algorithm needs some future work in terms of developing 
it in more linear manner and also eliminating the use of each line 
of text in the text document. If the work could be made to do in 
linear manner, then this algorithm could be vital source to 
provide more security to the communication over emails.  
 
REFERENCES 
[1]http://searchdatamanagement.techtarget.com/definition/data-
classification 
[2] Chapter 6, Han and kamber 
[3] chapter 10, lecture slides. 
[4]http://www.cs.put.poznan.pl/jstefanowski/sed/DM-
7clusteringnew.pdf 
[5]http://docs.oracle.com/cd/B28359_01/datamine.111/  b28129/ 
classify.htm#DMCON004 
[6] http://en.wikipedia.org/wiki/Statistical_classification 
[7] Upansana and S. Chakravarty, ‚ÄúA survey of Text 
Classification Techniques for E-Mail Filtering‚Äù, Second 
international conference on Machine Learning and Computing, 
IEEE Computer Society, 2010, pages 32-36. 
[8] Gongde Guo et al., ‚ÄúUsing kNN Model-based Approach for 
Automatic Text Categorization‚Äù, Soft Computing ‚Äì Springer, 
2005.  
[9] Maleq Khan, Qin Ding and William perrizo, ‚ÄúK-Nearest 
Neighbor Classification on Spatial Data Streams Using P-Trees‚Äù, 
Computer Science Department, North Dakota State University 
[10] Text Classification, 
https://web.stanford.edu/class/cs124/lec/naivebayes.pdf 
[11] http://www.dis.uniroma1.it/~leon/didattica/webir/IR11.pdf 
[12] http://nlp.stanford.edu/IR-book/html/htmledition/naive-
bayes-text-classification-1.html 
[13] Bharath et al. , ‚ÄúShort Text Classification in Twitter to 
Improve Information Filtering‚Äù, SIGR ACM, Geneva, 
Switzerland 2010. 
[14] Ioannis Kanaris et al., ‚ÄúWords Vs Characters N-Grams for 
Anti-Spam Filtering‚Äù, International Journal on Artificial 
Intelligence Tools, world Scientific, Vol. XX, No.X(2006) I-20 
[15] Wuying Liu and Ting Wang, ‚ÄúIndex based Online Text 
Classification for SMS Spam Filtering‚Äù, Journal of Computers, 
Vol. 5, No.6, June 2010, pages 844-851. 
[16] A. Basu et al., ‚ÄúSupport Vector Machines for Text 
Categorization‚Äù, proceedings of 36th Hawaii international 
conference on System Sciences-, IEEE, 2003. 
 
