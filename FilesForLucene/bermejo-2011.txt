Expert Systems with Applications 38 (2011) 2072–2080Contents lists available at ScienceDirect
Expert Systems with Applications
journal homepage: www.elsevier .com/locate /eswaImproving the performance of Naive Bayes multinomial in e-mail foldering
by introducing distribution-based balance of datasets
Pablo Bermejo, Jose A. Gámez ⇑, Jose M. Puerta
Intelligent Systems and Data Mining Group, Computing Systems Department (I3A), Universidad de Castilla-La Mancha, Albacete, Spaina r t i c l e i n f o
Keywords:
E-mail foldering
Text categorization
Imbalanced data
Naive Bayes multinomial
Classification0957-4174/$ - see front matter  2010 Elsevier Ltd. A
doi:10.1016/j.eswa.2010.07.146
⇑ Corresponding author. Tel.: +34 967 599298; fax:
E-mail addresses: Pablo.Bermejo@uclm.es (P. Be
(J.A. Gámez), Jose.Puerta@uclm.es (J.M. Puerta).a b s t r a c t
E-mail foldering or e-mail classification into user predefined folders can be viewed as a text classification/
categorization problem. However, it has some intrinsic properties that make it more difficult to deal with,
mainly the large cardinality of the class variable (i.e. the number of folders), the different number of e-
mails per class state and the fact that this is a dynamic problem, in the sense that e-mails arrive in
our mail-folders following a time-line. Perhaps because of these problems, standard text-oriented classi-
fiers such as Naive Bayes Multinomial do no obtain a good accuracy when applied to e-mail corpora. In
this paper, we identify the imbalance among classes/folders as the main problem, and propose a new
method based on learning and sampling probability distributions. Our experiments over a standard cor-
pus (ENRON) with seven datasets (e-mail users) show that the results obtained by Naive Bayes Multino-
mial significantly improve when applying the balancing algorithm first. For the sake of completeness in
our experimental study we also compare this with another standard balancing method (SMOTE) and
classifiers.
 2010 Elsevier Ltd. All rights reserved.1. Introduction
One of the most common tasks in text-mining is classification
(Lewis, 1992), where the goal is to decide which class a given doc-
ument belongs to among a set of finite classes. In this study, we
deal with a particular task within text classification: classification
of e-mail into predefined folders (e-mail foldering Bekkerman,
McCallum, & Huang, 2005; Klimt & Yang, 2004). E-mail classifica-
tion has been studied for example in the classification or filtering
of junk mail (or spam Guzella & Caminhas, 2009; Sahami, Dumais,
Heckerman, & Horvitz, 1998) and in the analysis of e-mails col-
lected at a customer center (Sakurai & Suyama, 2005), but its appli-
cation to (semi) automatic classification of incoming mail into
folders defined by user has not received so much attention.
Recently, Bekkerman et al. (2005) presented an interesting paper
devoted to e-mail foldering. The main contributions of that paper
are: (1) identification of the challenges that e-mail classification
poses with respect to traditional document classification; (2) setting
a benchmark for subsequent e-mail foldering experiments by choos-
ing complex datasets (ENRON corpus) and giving detailed informa-
tion about the preprocessing carried out; (3) proposal of a new
evaluation method for foldering results based on the e-mail time-
line; and (4) a performance comparison of several classifiers (maxi-ll rights reserved.
+34 967 599224.
rmejo), Jose.Gamez@uclm.esmum entropy (ME), Naive Bayes multinomial (NBM), support vector
machines (SVM) and (a variant of) winnow (WW). From the compar-
ison of classifier performance over the ENRON corpus made in Bekk-
erman et al. (2005) several conclusions can be drawn: (1) SVM is the
most accurate; (2) NBM is significantly inferior to the other three
classifiers; and (3) NBM is the fastest while SVM needs much more
time (around two orders of magnitude slower than NBM).
Because NBM can be viewed as the state-of-the-art Bayesian
classifier in text-mining problems, and also because of its inner
advantages (fast, incremental, . . .), in this paper we focus on study-
ing how its performance can be considerably improved in e-mail
foldering by applying some preprocessing to the dataset. In the
case of e-mail foldering or, in general, text-based classification
we must distinguish between two types of preprocessing tasks:
(1) transformation of unstructured text documents into a data
structure capable of being used as input by a classifier learning
algorithm, and (2) classification-oriented preprocessing, that is,
the carrying out of certain modifications over the data in order to
improve the quality of the predictive model (classifier) to be
learned. With respect to the first preprocessing task, the goal is
to obtain a bi-dimensional table with rows representing docu-
ments (e-mails) and columns representing predictive attributes
or terms.1 The second preprocessing task is guided by the use we
are going to make of the dataset, that is, classification. Thus, several1 In text-mining applications, most of the attributes are words or tokens appearing
in the documents, and in general they are referred to as terms.
P. Bermejo et al. / Expert Systems with Applications 38 (2011) 2072–2080 2073supervised (class-based) preprocessing techniques such as feature
selection, feature construction and imbalancing correction can be
applied.
The goal of supervised feature selection (Guyon & Elisseeff,
2003; Liu & Motoda, 1998) and construction (Hu, 1998; Larsen,
Freitas, & Nievola, 2002) is the improvement of classifier perfor-
mance by improving the quality of the training set used to learn
the classifier. To do this, irrelevant and redundant features are re-
moved and more informative ones (with respect to the class) are
constructed. Other benefits of this process, apart from improving
accuracy and related classification-success-based measures, are:
the alleviation of the effect of the ‘‘curse of dimensionality” prob-
lem, an increase in the capacity for generalization, the speeding
up of the learning and inference process and an improvement in
model interpretability. However, a strong or aggressive reduction
of the dataset is not so important in the case of text classification,
as shown in the following studies: in Joachims (2002) it is reported
that even terms ranked in the lowest positions still contain consid-
erable information and are quite relevant; in Forman (2003) 500 or
1000 terms are usually selected; and in Montañés, Combarro, Díaz,
& Ranilla (2005) the minimum filtering level used is 20% of the
terms in the dataset.
In this paper, we focus on the remaining aforementioned super-
vised preprocessing task: dataset imbalance. The lack of balance
among classes of the training set is a well-known problem when
performing classification on a real corpus. When facing this type
of datasets, classifiers such as Naive Bayes might overfit the
learned parameters, while for non-parametric classifiers based on
neighborhood, imbalanced classes result in some invasion in the
vectorial space. This phenomenon leads to incorrect classification
for documents whose class appears in the training set just a few
times compared to the majority class documents.
The process of solving the skew in a dataset is known as balanc-
ing (Chawla, Bowyer, & Kegelmeyer, 2002), and this technique has
received a considerable amount of attention during the last few
years. However, most of the proposals in the literature are limited
to dealing with binary classification problems, that is, problems
with a binomial class variable. In the case of e-mail foldering, we
are faced with two problems: (1) the high cardinality of the class
variable (number of class states/labels = number of folders), and
(2) the imbalanced distribution of classes in the training set.
Although some approaches to text classification transform the ori-
ginal multi-class2 problem into n binary classification problems, in
this paper we follow (Bekkerman et al., 2005) and therefore only
one classifier (instead of n) is learnt to cope with the multi-class
dataset. Thus, our main contribution in this paper is the introduction
of a new distribution-based balancing algorithm, which basically
learns probability distributions from the training set and then a
new artificial and balanced training set is sampled from them and
used to train the classifier. As a result we show that the performance
of NBM clearly improves when applied over the balanced distribu-
tion, being competitive with the state-of-the-art SVM classification
algorithm. In fact, our contribution is twofold, because apart from
improving the behaviour of NBM in e-mail foldering, we think that
the proposed balancing algorithm can be of use in other multi-class
domains and for other algorithms, though this is a question that
must be carefully studied in future research.
The rest of the paper is structured as follows: the next section
briefly introduces some preliminary material about e-mail folder-
ing. Section 3 briefly reviews the imbalanced data problem. Section
4 contains our proposal for distribution-based balancing of data-
sets, and then Section 5 describes the experimental evaluation car-2 In this paper, by a multi-class problem we refer to one having more than two
class labels in the class variable, not to the problem of having more than one class
variable.ried out and its analysis. Finally, in the last section we present our
conclusions.2. E-mail foldering: preliminaries
In this section, we formally state the problem of e-mail folder-
ing, and briefly describe the preprocessing carried out, the NBM
classifier and the incremental time-based split validation used.
2.1. Statement of problem
This paper focuses on the automatic classification of e-mails,
regarding them as a set of documents and without considering
any special feature. Formally, the problem can be defined as giving
a: given a set of e-mails Dtrain = {(d1, l1), . . ., (djDj, ln)}, obtain a clas-
sifier c: D ? L, where
– di 2 D is the document which corresponds to the ith mail of the
given set of documents or e-mails D.
– li 2 L is a folder containing several documents.
– L = {l1, . . ., ln} is the set of possible folders. When referring to the
class variable we will denote it by C = {c1, . . .,cn}, understanding
that class state ci represents folder li.
As mentioned above, in this work we focus on the use of Naive
Bayes Multinomial (NBM) as the classifier to be learnt. NBM has
become the state-of-the-art Bayesian classifier in text classifica-
tion, achieving a better performance than NB binomial when
vocabulary size is not small (McCallum & Nigam, 1998).
2.2. From text e-mails to a structured dataset
The main differences between standard classification and text
classification are: the need for preprocessing the unstructured doc-
uments in order to obtain a standard data mining dataset (bi-
dimensional table) and the usually large number of features or
attributes in the resulting dataset. Two other important differences
with respect to standard text classification tasks are the large num-
ber of states in the class variable, and the usual presence of noise in
the training set, due to the fact of (almost) all users of e-mail, even
having defined topical folders, later tend to file e-mails belonging
to different concepts into the same folder. In this paper, we focus
on the bag-of-words model, that is, a document (mail) is regarded
as a set of words or terms without any kind of structure. For the
selection of the documents and terms (i.e., the vocabulary V) used
in our study we have followed the preprocessing described in
Bekkerman et al. (2005):
 Documents: Non-topical folders (inbox, sents, trash, etc.) and
folders with only one or two mails are not considered.
 Terms: We only consider words as predictive attributes (MIME
attachments are removed) and no distinction is made with
respect to where the word appears (e-mail header or body).
Stop-words and words appearing only once are removed. After
that we denote the size of V as jVj = r.
 Class: The folder hierarchy is flattened and each one of the
resulting folders constitutes a class label or state.
The representation of documents is also an important issue. The
most typical representations are frequencies and tf * idf. The former
represents a document using a vector which contains the frequen-
cies in that document of terms belonging to a predefined bag-of-
words or vocabulary. The latter also uses a vector, but in this case
the position of each term represents a mix of the frequency in that
document and its frequencies in the rest of the documents. Other
2074 P. Bermejo et al. / Expert Systems with Applications 38 (2011) 2072–2080not so usual representations are n-grams (Bekkerman, Eguchi, & Al-
lan, 2006), hypernyms (Scott & Matwin, 1999), entities (Witten,
Bray, Mahoui, & Teahan, 1999), etc. The current literature is not
able to say which representation performs best, so the decision still
depends on the case under study and the type of input accepted by
the classifier used.
In this paper, and for the sake of completeness, we have used
two different kinds of representations in our dataset. In particular,
we use the vectorial model to represent documents, thus, after
using information retrieval techniques (Salton & Buckley, 1987)
to carry out the previously described preprocessing, our datasets
can be observed as bi-dimensional matrices M[numDocs, num-
Terms], where M[i,j] = Mij is a real number representing (1) the fre-
quency of appearance of term tj in document di or (2) the tf * idf
values normalized by the cosine function.2.3. Naive Bayes multinomial
Formally, an NBM classifier assumes independence among
terms once the class they belong to is known. Furthermore, this
model lets us consider not only those terms appearing in each doc-
ument but also the frequency of appearance of each term. This is
important, because we can presume that a high appearance fre-
quency increases the probability of belonging to a particular class.
In the model considered here, the class a document belongs to is
decided by calculating the class which maximizes the Bayes rule
(Eq. (1)), computing the conditional probability as shown in Eq. (2):
PðcjjdiÞ ¼
PðcjÞPðdijcjÞ
PðdiÞ
; ð1Þ
PðdijcjÞ ¼ PðjdijÞjdij!
Yr
l¼1
PðtljcjÞMil
Mil!
; ð2Þ
PðtljcjÞ ¼
1þ
PjDj
i¼1MilPðcjjdiÞ
jV j þ
Pr
s¼1
PjDj
i¼1MisPðcjjdiÞ
: ð3Þ
Mil (the entry in our preprocessed dataset) being the number of
times that term tl appears in document di, r = jVj the size of our
vocabulary, jDj = m the number of documents to classify and jdij
the length of document i. Notice that P(cjjdi) in Eq. (3) is simply 1
if the instance corresponding to document dj is labeled with class
cj in the dataset, and 0 otherwise. Eq. (2) assumes independence
among terms, which is not realistic in real databases. Besides, this
assumption gets even more troublesome in the multinomial model
(Lewis, 1998) because it assumes not just independence among dif-
ferent terms but also among various occurrences of the same term.2.4. Incremental time-based split validation
As reported in Bekkerman et al. (2005) using training/test splits
performed at random (e.g. as in standard cross validation) for val-
idation of e-mail classification is not appropriate because e-mail
datasets depends on time, and so random splits may create unnat-
ural dependencies. Because of this fact Bekkerman et al. (2005)
proposed a new validation scheme they called: Incremental time-
based split validation.
This validation scheme consists in ordering mails based upon
their timestamp field, and then training with the first s mails and
testing using the next s. After that, training is performed with
the first 2s mails and testing with the next s, and so on until it is
trained with the first (K  1)s mails and tested with the remaining
ones, K being the number of time splits the total number of mails is
divided into, and s being the number of mails in each time split. Fi-
nally, the accuracy averaged over the K  1 test sets used is
reported.3. The imbalanced data problem and related work
Since our e-mail corpus is highly skewed, our goal is to perform
a balancing process by re-sampling the whole training datasets
from a learned distribution. Before describing our proposal in the
next Section, here we briefly review some approaches to the imbal-
anced data problem.
Imbalance appears in a dataset when the proportion of docu-
ments/sub-concepts among classes/within-classes is very unequal.
This has been a common problem in automatic classification but it
has not been properly tackled until the recent appearance of highly
skewed huge databases coming from real life sources (images,
medical diagnosis, fraudulent operations, text. . . and some other
corpora). In these databases the need for some preprocessing in or-
der to alleviate the imbalance problem is a priority (Chawla, Jap-
kowicz, & Kotcz, 2004).
The degree of imbalance or skewness refers to the ratio among
the number of documents from different classes. Thus, having a
binomial class, a ratio of 1:100 would mean that the dataset con-
tains 100 documents tagged with the majority class per each doc-
ument tagged with the minority class. This problem is even worse
when a class presents absolute rarity (Weiss, 2004), an expression
used to refer to the lack of data to properly learn a predictive mod-
el for such a class. From the literature we can learn that when deal-
ing with skewed data, the major problem is not the imbalance
itself, but the overlapping between classes or disjuncts (Jo & Jap-
kowicz, 2004; Prati, Bastista, & Monard, 2004). This problem is
known as between-class imbalance (Japkowicz, 2001) and is the
type of imbalance we deal with in this paper. Thus, the approach
we present in this study (Section 4) is expected not just to balance
classes but also to remove between-class overlapping in the space
region.
The imbalanced data problem can be approached from two dif-
ferent points of view: algorithm-level or data-level. Algorithm-level
solutions are classifier-specific and consist in the introduction of a
specific bias in the learned model (Japkowicz, 2000; Kubat & Mat-
win, 1997; Lin, Lee, & Wahba, 2002; Raskutti & Kowalczyk, 2004).
Data-level solutions are more popular and consist in the a priori
modification of the training set (Chan & Stolfo, 1998; Chawla
et al., 2002). One way to do the former is by means of adjusting
the degree of importance of each term (Liu, Loh, & Sun, 2009) or
just selecting some of them (Zheng, Wu, & Srihari, 2004). Alterna-
tively, and this is the topic we focus on, one can modify the training
data in order to balance it, by sampling from the original dataset.
Sampling-based balancing techniques can be divided into over-
sampling and under-sampling, although a combination of both can
also be used. Besides this, sampling can be performed in a directed
(intelligent) or random way. Over-sampling a training set consists
in creating new samples (from minority class) and adding them
to the training set, it being optional whether to remove the original
samples or not. On the other hand, under-sampling chooses (in a
random or directed way) samples belonging to the majority class
and then removes them until the desired balance is achieved. Di-
rected under-sampling is expected to remove documents of major-
ity class(es) from regions which belong to minority class(es), while
directed over-sampling is expected to reproduce more records of
the minority class(es) and thus to define the region of that (those)
class(es). Random under- and over-sampling only has the aim of
balancing the training set, without taking care of removing impor-
tant records. Sampling approaches were compared in Japkowicz
(2000) and the conclusions state that over-sampling and under-
sampling perform roughly the same and, moreover, directed sam-
pling did not significantly outperform random sampling. Later, in
2002 the well-known SMOTE algorithm was presented (Chawla
et al., 2002). SMOTE is a combination of over and under-sampling
whose application results in an improvement in the accuracy for
P. Bermejo et al. / Expert Systems with Applications 38 (2011) 2072–2080 2075the minority class. Anyway, the current situation is that no final
word has yet been said about what approach is best (Chawla
et al., 2004).
Another aspect that must be taken into account is the fact that
most of the approaches to the imbalanced data problem found in
the literature refers to the problem of having a binomial class while
in our problem we are faced with a multi-class, aggravated by the
fact of having a large number of possible outcomes for the class
variable. This point makes it difficult to transform the multi-class
problem into a binary one (Chawla et al., 2002), because the large
number of (binomial) models to be learned heavily increases the
time and space requirements of the process.
All the studies found in the literature which work on imbal-
anced multi-class datasets are very recent, for both algorithm-level
(Abe, 2004; Zhou & Liu, 2006) and data-level (Malazizi, Neagu, &
Chaudhry, 2008; Stamatatos, 2008) solutions. At the moment,
there are no clear statements about what imbalance solution per-
forms best, and including the multi-class paradigm adds a new
complexity level. However, we find that this case is more realistic
when the problem tackled is related to text categorization, as it is
the case in this paper.
4. Distribution-based balancing of multi-class training sets
The approach we present here to deal with imbalanced data is
based on a two step process: (1) for each predictive attribute or
term ti, i = 1, . . .,r and for each class state cj, learn a probability dis-
tribution P(tijcj); and (2) for each class state cj, sample b full in-
stances3 hf1, . . ., ,fr, cji by using the r previously learnt probability
distributions P(tijcj), i = 1, . . ., r. It is clear that if we sample the same
number of instances for each class state, then we get an artificially
generated balanced training set. Therefore, taking into account the
concepts introduced in the previous section, our method belongs
to the category of data-based algorithms that combine under- and
over-sampling with total replacement of the training set.
As mentioned in Section 3, the problems when dealing with
imbalanced data come not only from the fact of having skewed
data but also from between-class overlapping. We think that in
many cases our proposal can manage these two problems simulta-
neously. The idea is that when learning P(tijC = cj) we are trying to
represent class state cj by term ti. If cj is a majority class state, then
when the distribution is sampled, the probability of producing out-
liers is small, and so, the probability of invading other class states
also decreases. On the other hand, if cj is a minority class, as we
learn its concept for each term independently, the noise coming
from other class states is removed, and because we propose to
sample the same number of artificial instances for each class label,
then its corresponding concept will be more clearly defined with
respect to majority classes. In this reasoning, we are assuming that
no disjunction exists, that is, that, in general, a class state does not
cover different sub-concepts and so the concept can be represented
by a uni-modal distribution. If disjunctions exists, overlapping
among classes cannot be removed and so our proposal will correct
the skewed problem but not between-class overlapping. However,
we think that the existence of class states corresponding to differ-
ent sub-concepts is more likely to exist in binomial problems,
while it is not so frequent in multi-class problems. In our opinion,
experimental results confirm this expectation.
In short, our proposal performs several tasks over the training
set in a single process:
 Over-samples classes with less than b documents.3 Notice that a full instance contains the frequency fi of term ti in the sampled
document plus the class the document (e-mail) belongs to. Under-samples classes with more than b documents.
 Removes or reduces overlapping among classes.
 Fully balances all classes.
Fig. 1 shows the general scheme of the proposed distribution-
based algorithm. As we can observe there are two clearly differen-
tiated steps: learning and sampling. At learning time a probability
distribution for each pair hterm, class statei is learnt from the
unmodified training set (corresponding to the current split of the
validation) projected over the term and the class (D#ti ;Ch ). Then, at
sampling time all the distributions learnt for a class state are used
to build (by sampling the corresponding one for each term in turn)
each one of the b desired documents for such a class state. This
sampling process is repeated for all the class states in order to
get a balanced artificial training set.
Of course there are some degrees of freedom in the previous
algorithm: the number of documents to be sampled for each class
and (mainly) the kind of probability distribution used to model the
training set. In this paper, we experiment with four different prob-
ability distributions: Uniform, Gaussian, Poisson and Multinomial.
 Uniform distribution. There are several works in the literature
(e.g. Japkowicz, 2000) which, using binomial classes, conclude
that there is not much difference whether sampling by using
information extracted from the learning data, or not. Sampling
(almost) without using information about the training set can
be modeled by learning a uniform distribution. In this way
the only information we collect is the max value found for term
ti restricted to those samples belonging to class ck. Later, in the
sampling process we simply generate a uniform number in the
interval [0, maxki]. We will use this distribution as a baseline
threshold to analyze the advantages of the more informed ones.
 Gaussian distribution. The Gaussian distribution is the most fre-
quently used distribution in statistics and computer science
due to its natural capacity for correctly approximating models.
In our case it is specially appropriate when the relation class-
term represents a single concept because of its uni-modal nat-
ure.
In the univariate Gaussian distribution we assume the fre-
quency fj of term ti conditioned to class state ck as follows:f ðti ¼ fjÞ ¼
1
r
ffiffiffiffiffiffi
2p
p exp ðfj  lÞ
2
2r2
" #
:
Fig. 1. Distribution-based balancing algorithm.
Table 1
Instances, classes, attributes, and degree of imbalance in the datasets.
#I #C #A. (base:peak) (l ± r) Kurtosis
lokay-m 2489 11 18,903 (6:1159) 226.3 ± 316.3 2.75
sanders-r 1188 30 14,463 (4:420) 39.6 ± 75.6 17.05
beck-s 1971 101 13,856 (3:166) 19.5 ± 24.5 13.08
williams-
w3
2769 18 10,799 (3:1398) 153.8 ± 379.4 4.10
farmer-d 3672 25 18,525 (5:1192) 146.9 ± 255.5 7.77
kaminski-v 4477 41 25,307 (3:547) 109.2 ± 141.9 1.66
kitchen-l 4015 47 34,762 (5:715) 85.4 ± 122.8 11.75
2076 P. Bermejo et al. / Expert Systems with Applications 38 (2011) 2072–2080Thus, learning a univariate Gaussian distribution consists simply
of computing the mean and standard deviation of frequencies for
term ti from data restricted to class state ck. Sampling from a
Gaussian distribution can be done, for example, following the
well-known Box and Muller method (see for example Rubinstein
& Melamed, 1998, Chapter 2).
It should be pointed out that whenever the sampled value is less
than 0, we set it to 0 since we are working with frequencies and
negative values make no sense.
 Poisson distribution. As pointed out in Kim, Seo, & Rim (2003) ‘‘if
we think that the occurrence of each term is a random occur-
rence in a fixed unit of space (i.e. a length of document), then
the Poisson distribution is intuitively suitable to model the term
frequencies in a given document”. Because of this, the Poisson
model has been investigated in the information retrieval com-
munity and applied to text classification (Kim et al., 2003). Thus,
we think that it is a good alternative to be considered for distri-
bution modeling in our balancing algorithm.
In the Poisson distribution we assume the frequency fj of term ti
as follows:4 http://lucene.apache.org/who.html.Pðti ¼ fjÞ ¼
ekkfj
fj!
; ð4Þ
where k is the Poisson mean.
Therefore the learning step is simply a matter of computing k of
term ti restricted to class state ck. Sampling from a Poisson dis-
tribution is also a well-known process (see for example Rubin-
stein & Melamed, 1998, Chapter 2).
 Multinomial distribution. Following the generative distribution
from the Naive Bayes Multinomial Model (Kim, Han, Rim, &
Myaeng, 2006) described in formula (2), and once we have
learnt the term distribution by class following expression (3),
we are ready to generate as many documents as the parameter
b indicates.
In this case the P(jdij) distribution is assumed to follow a Pois-
son distribution, Eq. (4), which is unidimensional and indepen-
dent of the class. So by the estimation of the parameter k as the
mean number of terms in a document, that is, the mean length
of documents, we can simulate the number of terms in a newly-
generated document. Once the number of terms is given, we
pick the terms in the generated document by simulating the
probabilities P(tljcj) as many times as the number of terms has
been indicated by the Poisson distribution.
So, in the structure of the algorithm shown in Fig. 1, the changes
are:
(1) Values to compute are k and the probabilities for each term
given the class following expression (3).
(2) Each term will have the value representing the times that it
was drawn following the Multinomial distribution P(tljcj)
according to the length of document generated following a
Poisson distribution.
5. Experiments
In this section, we present our experimental study over e-mail
foldering using the proposed distribution-based balancing method.
We emphasize that our experiment is directly related with e-mail
foldering and because of the nature of this problem, perhaps the
conclusions here obtained can be extended to similar problems,
that is, those having a class variable with large cardinality and
numerical variables as predictive attributes. For the sake of com-
pleteness, apart from using our approach instantiated with Uni-
form, Gaussian, Poisson and Multinomial distributions, we also
consider the well-known SMOTE algorithm, but slightly modified
to deal with multi-class datasets. As classifier we use NBM as itwas our aim to improve its performance in the e-mail foldering
problem, but for comparison we also use support vector machines
and instance-based learning.
5.1. Test suite
As in Bekkerman et al. (2005), Klimt & Yang (2004), we have
used datasets corresponding to seven users from the ENRON cor-
pus (mail from these users and a temporal line in increasing order
can be downloaded from http://www.cs.umass.edu/ronb). The
downloaded data was preprocessed according to the process de-
scribed in Section 2.2. To do this we coded our own program in
Java, and designed it to interact with Lucene information retrieval
API4 and to output a sparse matrix M[numDocs, numTerms] codified
as an .arff file, i.e., a file following the input format for the WEKA data
mining suite (Witten & Frank, 2005).
Table 1 describes the main characteristics of the datasets ob-
tained. The last two columns are intended to show the degree of
imbalance in the datasets. As in Stamatatos (2008), we show (base,
peak), where base is the number of documents of the minority class
and peak is the number of documents in the majority class. How-
ever, we think this measure is not sufficiently representative of
the degree of imbalance because it represents different distribu-
tions in the same way, e.g., (1, 100) is valid for {1, 100} or {1,
100, 100, 100} or {1, 100, 1, 100, 1, 100}, which clearly represents
different degrees of imbalance. Because of this we have added (l,
r) to the description, l being the mean of documents per class
and r the standard deviation of the mean. We find r is a very infor-
mative value concerning the imbalance present in the dataset. A
clear indication of the degree of imbalance in the problem we are
dealing with (e-mail foldering) is that in the seven users (datasets)
considered, the standard deviation is greater than the average. Be-
sides this, we also computed the kurtosis degree for each dataset,
which in this case represents the degree of concentration of the
number of documents per class around the mean number of docu-
ments per class. A graphical representation of the imbalance in our
datasets is shown in a Box and Whisker plots (Fig. 2), where the in-
puts used were the number of instances per class state.
5.2. Experimental design
Our goal is to study whether the classifiers, specially NBM, per-
form better after balancing the datasets. To be more confident in
our conclusions, we carry out a statistical analysis in which we
compare the results with and without balancing, and we also con-
trast the results obtained using different classifiers. In the experi-
ments we deal with the following actors:
 Balancing algorithms. To balance the training sets we use the pro-
posed method instantiated with the four mentioned distribu-
tions: Uniform, Gaussian, Poisson and Multinomial. For
comparison, we also consider SMOTE (Chawla et al., 2002).
Fig. 2. Graphical representation of the imbalance degree in the seven e-mail users.
Table 2
Baseline accuracy for seven e-mail users.
NBM freq NBM tfidf SVM tfidf k-NN tfidf
lokay-m 75.27 63.65 79.12 39.54
sanders-r 55.51 48.90 66.20 35.97
beck-s 28.26 17.60 36.87 9.00
williams-w3 91.69 88.87 90.52 86.06
farmer-d 69.64 55.34 73.98 37.63
kaminski-v 45.61 34.75 54.26 9.92
kitchen-l 32.19 34.44 51.10 14.28
Mean 56.88 49.08 64.58 33.20
P. Bermejo et al. / Expert Systems with Applications 38 (2011) 2072–2080 2077SMOTE was initially proposed by its author for binomial classes
and performs the balancing by sampling synthetic documents
from a minority class in a given percentage, and then randomly
under-sampling as many majority class documents as desired.
To apply SMOTE in a multi-class problem, we select the number
(b) of documents to re-sample for each class and apply SMOTE
to over-sample, without replacement (the SMOTE algorithm
does not perform replacement), all classes with cardinality
lower than b until obtaining b documents for those classes;
then, for classes with cardinality over b, we randomly remove
as many documents as necessary to get cardinality b.
 Document representation. Documents have been represented by
using frequencies and tf * idf. Computation of tf * idf is done
incrementally, that is, trying to reproduce the fact that we have
a dynamic problem and not a static one, where tf * idf can be
computed at the beginning by using all the available informa-
tion. In our case the classifier should categorize incoming e-
mails as they are downloaded into the user inbox folder. So,
when testing using our evaluation methodology, test e-mail i
should be represented with a tf * idf value where the idf part
is computed using the training set and test e-mails from 0 to
i  1, instead of using all s test documents, since documents
i + 1 to s are not supposed to be in the inbox folder yet. In this
way, incremental time-based split validation needs more time
but is more realistic.
When balancing is carried out by learning any of the distribu-
tions presented in Section 4, the parameters needed (max, l,
r, k) are computed using frequency representation, and then
the new training set is sampled still using frequencies represen-
tation. Thus, tf * idf conversions are performed later when train-
ing and testing the classifier.
 Classifiers. We have used three different classifiers that are com-
monly considered when dealing with text categorization prob-
lems: Naive Bayes Multinomial (NBM) (McCallum & Nigam,
1998), Support Vector Machine (SVM) (Boser, Guyon, & Vapnik,
1992) and k-Nearest Neighbors (k-NN) (Fix & Hodges, 1951). For
NBM we used the implementation provided in the WEKA API.
For SVM, we used the implementation available in El-Manza-
lawy & Honavar (2005), which can be viewed as an implemen-
tation of the Chang & Lin (2001) running under WEKA
environment. For k-NN, we implemented our own code by usingWEKA API. Different values of k were tested and two distance
metrics: Euclidean and Cosine distances. In this paper we only
show the results for the best configuration found: k = 15 and
the Cosine distance which is also the standard similarity metric
in text, and it is computed by:simðu; vÞ ¼
~u ~v
j~uj  j~v j ¼
Pn
i¼1fiu  fivffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiPn
i¼1f
2
iu 
Pn
i¼1f
2
iv
q ;
where n is the number of dimensions (attributes) of each vector
and fiu the value of dimension i in vector (document) u.
Having the previous setting in mind, we ran three different
experiments:
5.2.1. Experiment 1
To compute the baseline against which to compare, we ran an
incremental time-based split evaluation (see Section 2.2) on each
user in Table 1 without balancing. With respect to the value of s
in the time-based split evaluation, in all the experiments carried
out in this paper we have used the value recommended in Bekker-
man’s work, that is, the number (s) of e-mails to classify in each
split is 100. We ran the three mentioned classifiers for both types
of document representation: frequencies and tf * idf. SVM per-
formed statistically the same in both cases so from now on we only
show its results for tf * idf, which is the configuration suggested in
the literature. For k-NN, we only show results for tf * idf because
that is the configuration which finally obtains the best results. Fi-
nally, because NBM is the main classifier under study in this paper
we show results for both representations: frequencies and tf * idf.
The results are shown in Table 2, where each entry represent the
accuracy averaged over all the folds tested in the time-split valida-
tion process.
5.2.2. Experiment 2
In this experiment, we test the behaviour of our proposal, that
is, instead of learning the classifiers from the original dataset, we
learn them from the artificially balanced dataset. Because of the
randomness introduced by the sampling process, we run each bal-
ancing algorithm five times and show averaged results. In all the
cases the number of documents b to sample per class is set to 30.
Tables 3 and 4 show the results obtained. Each entry in the tables
represents the accuracy averaged over all the folds tested in the
time-split validation and coming from the five independent execu-
tions. Comparison between baseline algorithms (Table 2) and bal-
ancing algorithms was done by using a Wilcoxon signed rank test
(Demsar, 2006; Wilcoxon, 1945) (a = 0.05), taking as input the
time-split values from the corresponding user-classifier. When
the balanced user-classifier is found to be statistically better than
the baseline (not balanced), a  is placed inside its corresponding
cell.
In Table 5 we show a paired comparison between each two
kinds of balancing methods over the four classification methods
Table 3
Results when balancing with the SMOTE algorithm.
NBM freq NBM tfidf SVM tfidf k-NN tfidf
lokay-m 67.94 67.72 68.82 57.79
sanders-r 72.21 73.28 59.59 69.82
beck-s 45.41 45.49 42.80 39. 86
williams-w3 87.20 74.43 87.75 64.84
farmer-d 61.21 44.70 72.77 37.15
kaminski-v 43.22 42.42 49.65 34.85
kitchen-l 35.92 38.54 49.72 32.68
Mean 59.02 55.23 61.59 48.14
Table 4
Results when balancing with the proposed distribution-based algorithm.
NBM freq NBM tfidf SVM tfidf k-NN tfidf
(a) Uniform distribution
lokay-m 52.02 59.20 36.70 49.24
sanders-r 59.33 68.78 71.99 66.60
beck-s 43.32 48.76 39.93 43.13
williams-w3 63.56 66.44 46.42 38.81
farmer-d 52.05 51.38 35.29 46.31
kaminski-v 40.69 51.80 28.72 49.52
kitchen-l 33.11 42.05 26.11 37.18
Mean 49.15 55.48 40.74 47.26
(b) Gaussian distribution
lokay-m 70.77 71.51 47.58 55.95
sanders-r 72.39 75.23 74.79 76.02
beck-s 45.88 50.55 44.33 45. 00
williams-w3 84.27 80.78 81.50 77.22
farmer-d 67.11 64.15 65.93 54.41
kaminski-v 52.02 56.68 55.39 52.20
kitchen-l 38.72 46.45 37.24 34.66
Mean 61.59 63.62 58.11 56.49
(c) Poisson distribution
lokay-m 70.68 73.20 69.95 60.35
sanders-r 75.40 75.88 57.09 74.09
beck-s 45.17 48.53 41.17 44.05
williams-w3 88.93 77.96 85.62 66.65
farmer-d 65.87 55.25 68.28 48.72
kaminski-v 46.54 49.36 45.67 45.07
kitchen-l 37.10 41.92 39.95 38.44
Mean 61.39 60.30 58.25 53.91
(d) Multinomial distribution
lokay-m 70.01 72.41 67.94 61.25
sanders-r 74.97 74.84 55.30 71.66
beck-s 45.09 49.33 40.48 46.19
williams-w3 89.03 78.86 85.47 66.72
farmer-d 63.22 54.49 70.25 47.55
kaminski-v 44.48 47.49 46.10 42.34
kitchen-l 35.84 41.57 40.00 37.34
Mean 60.38 59.86 57.93 53.29
Table 5
Paired comparison between balancing methods.
Uniform Gaussian Poisson Multinomial
SMOTE 15-5-8 6-7-15 4-7-17 4-11-13
Uniform 2-3-23 3-8-17 3-10-15
Gaussian 13-11-4 14-10-4
Poisson 10-16-2
2078 P. Bermejo et al. / Expert Systems with Applications 38 (2011) 2072–2080used (NBM freq, NBM tdidf, SVM tfidf and k-NN tfidf). Comparison
is in the form x  y  z, where x stands for #beats, y stands for #ties
and z stands for #loses. For example, in the first cell, the compari-
son 15-5-8 means that the SMOTE method performed statistically
better than the Uniform distribution method 15 times, statistically
the same five times and has performed statistically worse eight
times.Fig. 3. Different values for b using NBM and tf * idf docs representation.5.2.3. Experiment 3
Finally, we studied the effect of b on the performance of the dis-
tribution-based balancing algorithms. We focus on a representa-
tive case as is the case of using the Gaussian distribution for
balancing, the NBM classifier and tf * idf document representation.We run this configuration using values for b from 10 to 60, and the
results are shown in Fig. 3.
5.3. Discussion of results
5.3.1. Experiment 1: baseline
Table 2 shows the results obtained after performing time-based
split evaluation on seven users from the Enron Corpus without pre-
processing that instances set. Results show, as in Bekkerman et al.
(2005), that SVM-tfidf outperforms by far other common classifiers
such as NBM. Users with worst results are those with a lower r va-
lue in Table 1, which also corresponds with the higher cardinality
of class. Thus, we find in this dataset that high class cardinality
leads to low standard deviation in the number of documents per
class, and this is an indicator of classifiers’ performance on such
databases. We also find that Williams gets a very high performance
so it is not an easy target to improve.
5.3.2. Experiment 2: balancing
In Table 3 we present the results obtained when balancing
training sets using the SMOTE method, and in Table 4 we show re-
sults for our proposed distribution-based balancing methods. In
both tables we include a  symbol when the algorithm associated
to a cell performs statistically better than its corresponding cell
in the baseline in Table 2. Finally, a pairwise comparison between
balancing methods is presented in Table 5.
SMOTE balancing proves to be a good choice for preprocessing
skewed datasets. Its results outperform the baseline for some
users, specially for NBM-tfidf and k-NN-tfidf classifier configura-
tion. On the other hand, SVM only gets statistical improvement
in one user and it even decreases in some others.
Our distribution-based methods can be classified as one ran-
dom (Uniform) and three directed (Gaussian, Poisson and Multino-
mial). Results for Uniform re-sampling are the worst although
several statistical improvements are achieved. Comparing Gauss-
ian, Poisson and Multinomial re-samplings by looking at Table 5
we can conclude that a reasonable order from best to worst could
be: Gaussian > Poisson > Multinomial > Uniform. In particular, the
best choice seems to be Gaussian balancing with the NBM classifier
and tf * idf representation. Furthermore, as happens when using
SMOTE, the SVM classifier does not get real improvement after
P. Bermejo et al. / Expert Systems with Applications 38 (2011) 2072–2080 2079balancing, thus indicating that SVM is strong in imbalanced situa-
tions, so performing re-sampling, at least in the case of using our
methods and SMOTE, does not provide any improvement and even
worsens the learning stage. This corroborates a study by Japkowicz
& Shaju (2002) which concludes that SVM is robust against imbal-
anced datasets. Finally, comparing random and directed re-sam-
pling, we find support for our hypothesis which expected that
directed balancing alleviates the overlapping problem. Moreover,
this provides evidence to suggest that imbalance is not the only
problem in skewed datasets.
When comparing SMOTE against our proposed distribution-
based methods in Table 5, we find that SMOTE performs better than
our random distribution-based method, but worse than our three
suggestions for directed distribution-based methods. We clearly
conclude that our proposal (except for Uniform distribution) out-
performs SMOTE at least when applied to text categorization with
more than two classes.
With respect to document representation, if we focus on NBM,
whose results are shown with frequencies and tf * idf documents
representation, we can see that in the baseline NBM with frequen-
cies performs better than NBM with tf * idf but, after balancing the
data, NBM with tf * idf performs considerably better and achieves
statistically the same results as NBM with frequencies; thus, we
interpret this as proof that tf * idf representation is more imbal-
anced-sensitive than frequency representation.
Comparing users by looking at Table 1, Fig. 2 and Table 4, we
find that users which usually obtain statistical improvement are:
beck-s, kaminski-v, kitchen-l and sanders-r. These four users are
those with a larger cardinality for their class attribute and which
present lower outliers in their box and whisker plot. Furthermore,
with respect to their (l, r) representation, they are also the user
with lowest l and r values; respect to kurtosis degree, higher val-
ues clearly point to a higher need of balancing. Thus, based on
these results, we suggest that a good way to predict performance
improvement after balancing a dataset may be one of these: (1)
cardinality class, (2) outliers in box and whisker plot, (3) low (l,
r) values and/or (4) high kurtosis values. For example, by looking
at Table 1 we could predict that a good order of datasets with a
greater need for a balancing process are (from more to less):
beck-s, sanders-r, kitchen-l, kaminski-v, farmer-d, lokay-m and
williams-w3. We suggest using the kurtosis value since this is just
a single value and very related to the balancing need.5.3.3. Experiment 3: number b of documents per class
We ran the configuration Gaussian distribution with classifier
NBM and tf * idf representation using values for b from 10 to 60,
and the results are shown in Fig. 3. Choosing the value for b should
not only be based on performance but also on the computational
cost of sampling b new instances for each class. Thus, based on
Fig. 3, we suggest using values from 30 to 40, since the computa-
tional cost is quite high from 30 documents and above, while the
improvement achieved is not significant.6. Conclusions
For the NBM classifier, we achieved our goal of improving its
performance in the e-mail foldering task, which is of great interest
given that NBM is a well-known standard for text classification.
We have presented a comparison of four kinds of distributions
(one random and three directed) to fully re-sample training data-
sets for multi-class classification, applied to e-mail categorization;
we have also compared our results with the well-known SMOTE
balancing method. The results support our main hypothesis, which
stated that a directed re-sampling method not only balances the
training set but also reduces the problem of overlapping amongclasses, while random re-sampling is only capable of dealing with
the imbalance problem. Furthermore, we have found evidence that
the SVM classifier is very robust under imbalanced conditions, so
its performance does not improve after balancing. Comparing with
SMOTE, we find that our distribution-based methods statistically
outperform it, except for the Uniform distribution.
Finally, we found a strong correlation between datasets which
usually perform better after balancing; that is, it can be expected
that datasets whose performance improves after balancing will
be those which present a large cardinality class, low outliers in a
box and whisker plot, low (l, r) values and/o high kurtosis degree.Acknowledgements
This work has been partially supported by the Consejería de
Educación y Ciencia (JCCM) under Project PCI08-0048-8577574,
the Spanish Ministerio de Ciencia e Innovación under Project
TIN2007-67418-C03-01 and FEDER funds.References
Abe, N. (2004). An iterative method for multi-class cost-sensitive learning. In
Proceedings of the tenth ACM SIGKDD international conference on knowledge
discovery and data mining (pp. 3–11).
Bekkerman, R., McCallum, A., & Huang, G. (2005). Automatic categorization of email
into folders: Bechmark experiments on enron and sri corpora. Technical report,
Department of Computer Science, University of Massachusetts, Amherst.
Bekkerman, R., Eguchi, K., & Allan, J. (2006). Unsupervised non-topical classification of
documents. Technical report, Center of Intelligent Information Retrieval,
Massachusetts Univ.
Boser, B. E., Guyon, I. M., & Vapnik, V. N. (1992). A training algorithm for optimal
margin classifiers. In Proceedings of the 5th annual ACM workshop on
computational learning theory (pp. 144–152). ACM Press.
Chang, C., & Lin, C. (2001). LIBSVM: A library for support vector machines. Software
available at <http://www.csie.ntu.edu.tw/cjlin/libsvm>.
Chan, P. K., & Stolfo, S. J. (1998). Toward scalable learning with non-uniform class
and cost distributions: A case study in credit card fraud detection. In Proceedings
of the fourth international conference on knowledge discovery and data mining
(pp. 164–168). AAAI Press.
Chawla, N. V., Bowyer, K. W., & Kegelmeyer, W. P. (2002). SMOTE: Synthetic
minority over-sampling technique. Journal of Artificial Intelligence Research, 16,
321–357.
Chawla, N. V., Japkowicz, N., & Kotcz, A. (2004). Editorial: Special issue on learning
from imbalanced data sets. SIGKDD Explorations Newsletter, 6(1), 1–6.
Demsar, J. (2006). Statistical comparisons of classifiers over multiple data sets.
Journal of Machine Learning Research, 7, 1–30.
El-Manzalawy, Y., & Honavar, V. (2005). WLSVM: Integrating LibSVM into weka
environment. Software available at <http://www.cs.iastate.edu/yasser/wlsvm>.
Fix, E., & Hodges, J. L. (1951). Discriminatory analysis, nonparametric discrimination.
Technical report, USAF School of Aviation Medicine, Randof Field, Project 21-49-
004, Rept. 4.
Forman, G. (2003). An extensive empirical study of feature selection metrics for text
classification. Journal of Machine Learning Research, 3, 1289–1305.
Guyon, I., & Elisseeff, A. (2003). An introduction to variable and feature selection.
Journal of Machine Learning Research, 3, 1157–1182.
Guzella, T. S., & Caminhas, W. M. (2009). A review of machine learning approaches
to spam filtering. Expert Systems with Applications, 36(7), 10206–10222.
Hu, Y.-J. (1998). Constructive induction: Covering attribute spectrum. In Liu &
Motoda (Eds.), Feature extraction, construction and selection: A data mining
perspective. Kluwer.
Japkowicz, N. (2000). The class imbalance problem: Significance and strategies. In
Proceedings of the 2000 international conference on artificial intelligence (ICAI) (pp.
111–117).
Japkowicz, N. (2001). Concept-learning in the presence of between-class and
within-class imbalances. In AI ’01: Proceedings of the 14th Biennial conference of
the Canadian society on computational studies of intelligence (pp. 67–77). London,
UK: Springer-Verlag.
Japkowicz, N., & Shaju, S. (2002). The class imbalance problem: A systematic study.
Intelligent Data Analysis, 6(5), 429–449.
Joachims, T. (2002). Learning to classify text using support vector machines. Kluwer
Academic Publishers.
Jo, T., & Japkowicz, N. (2004). Class imbalances versus small disjuncts. SIGKDD
Explorations Newsletter, 6(1), 40–49.
Kim, S.-B., Seo, H.-C., & Rim, H.-C. (2003). Poisson Naive Bayes for text classification
with feature weighting. In Proceedings of the sixth international workshop on
Information retrieval with Asian languages (pp. 33–40).
Kim, S.-B., Han, K.-S., Rim, H.-C., & Myaeng, S. H. (2006). Some effective techniques
for Naive Bayes text classification. IEEE Transactions on Knowledge and Data
Engineering, 18(11), 1457–1466.
2080 P. Bermejo et al. / Expert Systems with Applications 38 (2011) 2072–2080Klimt, B., & Yang, Y. (2004). The ENRON corpus: A new dataset for email
classification research. In 15th European conference on machine learning (pp.
217–226).
Kubat, M., & Matwin, S. (1997). Addressing the curse of imbalanced training sets:
One-sided selection. In Proceedings of the fourteenth international conference on
machine learning (pp. 179–186). Morgan Kaufmann.
Larsen, O., Freitas, A., & Nievola, J. (2002). Constructing x-of-n attributes with a
genetic algorithm. In Proceedings of the genetic and evolutionary computation
conference (GECCO-2002) (p. 1268).
Lewis, D. (1992). Representation and learning in information retrieval. Ph.D. thesis,
Department of Computer Science, University of Massachusetts.
Lewis, D. (1998). Naive (Bayes) at forty: The independence assumption in
information retrieval. In Proceedings of ECML-98, 10th European conference on
machine learning (pp. 4–15).
Lin, Y., Lee, Y., & Wahba, G. (2002). Support vector machines for classification in
nonstandard situations. In Machine learning (pp. 191–202).
Liu, Y., Loh, H. T., & Sun, A. (2009). Imbalanced text classification: A term weighting
approach. Expert Systems with Applications, 36(1), 690–701.
Liu, H., & Motoda, H. (1998). Feature extraction construction and selection: A data
mining perspective. Kluwer Academic Publishers.
Malazizi, L., Neagu, D., & Chaudhry, Q. (2008). Improving imbalanced multidi-
mensional dataset learner performance with artificial data generation: Density-
based class-boost algorithm. In P. Perner (Ed.), Proceedings of the international
conference in data mining. LNCS (Vol. 5077, pp. 165–176). Springer.
McCallum, A., & Nigam, K. (1998). A comparison of event models for Naive Bayes
text classification. In AAAI/ICML-98 workshop on learning for text categorization
(pp. 41–48).
Montañés, E., Combarro, E. F., Díaz, I., & Ranilla, J. (2005). Towards automatic and
optimal filtering levels for feature selection in text categorization. In Proceedings of
the 6th international symposium on intelligent data analysis (IDA’05) (pp. 239–248).Prati, R. C., Bastista, G., & Monard, M. C. (2004). Class imbalances versus class over-
lapping: An analysis of a learning system behavior. In MICAI, LNAI (pp. 312–321).
Raskutti, B., & Kowalczyk, A. (2004). Extreme re-balancing for SVMs: A case study.
SIGKDD Explorations Newsletter, 6(1), 60–69.
Rubinstein, R. Y., & Melamed, B. (1998). Modern simulation and modeling (Wiley
series in probability and statistics). Wiley-Interscience.
Sahami, M., Dumais, S., Heckerman, D., & Horvitz, E. (1998). A Bayesian approach to
filtering junk e-mail. In Learning for text categorization: Papers from the 1998
workshop, AAAI technical report WS-98-05.
Sakurai, S., & Suyama, A. (2005). An e-mail analysis method based on text mining
techniques. Applied Soft Computing, 6(1), 62–71.
Salton, G., & Buckley, C. (1987). Term weighting approaches in automatic text retrieval.
Technical report, Cornell University.
Scott, S., & Matwin, S. (1999). Feature engineering for text classification. In Proceed-
ings of ICML-99, 16th international conference on machine learning (pp. 379–388).
Stamatatos, E. (2008). Author identification: Using text sampling to handle the class
imbalance problem. Information Processing & Management, 44(2), 790–799.
Weiss, G. M. (2004). Mining with rarity: A unifying framework. SIGKDD Explorations
Newsletter, 6(1), 7–19.
Wilcoxon, F. (1945). Individual comparisons by ranking methods. Biometrics
Bulletin, 1(6), 80–83.
Witten, I. H., Bray, Z., Mahoui, M., & Teahan, W. J. (1999). Text mining: A new
frontier for lossless compression. In Proceedings of data compression conference
(pp. 198–207).
Witten, I. H., & Frank, E. (2005). Data mining: Practical machine learning tools and
techniques (2nd ed.). Morgan Kaufmann.
Zheng, Z., Wu, X., & Srihari, R. K. (2004). Feature selection for text categorization on
imbalanced data. SIGKDD Explorations Newsletter, 6(1), 80–89.
Zhou, Z.-H., & Liu, X.-Y. (2006). On multi-class cost-sensitive learning. In Proceedings
of the 21st national conference on artificial intelligence (AAAI’06) (pp. 567–572).
