 
Online Information Review
Existing plagiarism detection techniques: A systematic mapping of the scholarly literature
Taiseer Abdalla Eisa Naomie Salim Salha Alzahrani
Article information:
To cite this document:
Taiseer Abdalla Eisa Naomie Salim Salha Alzahrani , (2015),"Existing plagiarism detection techniques: A systematic
mapping of the scholarly literature", Online Information Review, Vol. 39 Iss 3 pp. -
Permanent link to this document:
http://dx.doi.org/10.1108/OIR-12-2014-0315
Downloaded on: 16 May 2015, At: 00:38 (PT)
References: this document contains references to 0 other documents.
To copy this document: permissions@emeraldinsight.com
The fulltext of this document has been downloaded 7 times since 2015*
Users who downloaded this article also downloaded:
Katrin Weller, (2015),"Accepting the Challenges of Social Media Research", Online Information Review, Vol. 39 Iss 3 pp. -
Junga Kim, Chunsik Lee, Troy Elias, (2015),"Factors affecting information sharing in social networking sites amongst
university students: application of the knowledge-sharing model to social networking sites", Online Information Review, Vol.
39 Iss 3 pp. -
Dirk Lewandowski, (2015),"Living in a world of biased search engines", Online Information Review, Vol. 39 Iss 3 pp. -
Access to this document was granted through an Emerald subscription provided by 463687 []
For Authors
If you would like to write for this, or any other Emerald publication, then please use our Emerald for Authors service
information about how to choose which publication to write for and submission guidelines are available for all. Please
visit www.emeraldinsight.com/authors for more information.
About Emerald www.emeraldinsight.com
Emerald is a global publisher linking research and practice to the benefit of society. The company manages a portfolio of
more than 290 journals and over 2,350 books and book series volumes, as well as providing an extensive range of online
products and additional customer resources and services.
Emerald is both COUNTER 4 and TRANSFER compliant. The organization is a partner of the Committee on Publication
Ethics (COPE) and also works with Portico and the LOCKSS initiative for digital archive preservation.
*Related content and download information correct at time of download.
D
ow
nl
oa
de
d 
by
 A
eg
ea
n 
U
ni
ve
rs
ity
 A
t 0
0:
38
 1
6 
M
ay
 2
01
5 
(P
T
)
Existing plagiarism detection techniques:  
a systematic mapping of the scholarly literature 
 
Taiseer Abdalla Elafadil* and Naomie Salim 
Faculty of Computing 
Universiti Teknologi Malaysia 
UTM Skudai, Johor, Malaysia 
 
Salha Alzahrani 
Department of Computer Science 
Taif University 
Taif, Saudi Arabia 
 
Acknowledgement  
This work is supported by the Malaysian Ministry of Higher Education and the Research 
Management Centre at the Universiti Teknologi Malaysia under Research University Grant 
Category Vot:Q.J130000.2528.07H89.  
 
About the authors 
*Taiseer Abdalla Elfadil Eisa holds a BSc in computer science from the Sudan University 
of Science and Technology and an MSc in computer science and information from Gezira 
University in Sudan. Currently she is a PhD candidate in the Faculty of Computing at 
Universiti Teknologi Malaysia. Ms Elafadil is the corresponding author and may be contacted 
at taiseralfadil@hotmail.com. 
Naomie Salim is currently a professor in the Faculty of Computing at Universiti Teknologi 
Malaysia. She has a master’s degree in computer science from Western Michigan University 
and a PhD in information studies (chemoinformatics) from the University of Sheffield. She 
has taught at both undergraduate and postgraduate levels in subjects related to databases and 
information systems, and her research interests include information retrieval and 
chemoinformatics. Professor Salim has been involved in 28 research projects and has written 
more than 150 journal and conference papers describing research into novel techniques for 
computerised information retrieval, with particular reference to textual, chemical and 
biological information.  
D
ow
nl
oa
de
d 
by
 A
eg
ea
n 
U
ni
ve
rs
ity
 A
t 0
0:
38
 1
6 
M
ay
 2
01
5 
(P
T
)
Salha Alzahrani is an assistant professor of computer science and vice-dean of the College 
of Computers and Information Technology at Taif University. She holds a PhD and MSc in 
computer science from the Universiti Teknologi Malaysia. Dr Alzahrani also serves as web 
activities coordinator of the IEEE SMC Technical Committee on Soft Computing, and co-
organised special sessions for IEEE SMC 2012. 
 
Paper received 28 December 2014 
First revision approved 18 March 2015 
 
Abstract 
Purpose – The purpose of this study is to analyse the state-of-the-art techniques used to 
detect plagiarism in terms of their limitations, features, taxonomies and processes. 
Design/methodology/approach – The method used to conduct this study consisted of a 
comprehensive search for relevant literature via six online database repositories – IEEE 
Xplore, ACM Digital Library, ScienceDirect, EI Compendex, Web of Science and Springer – 
using search strings obtained from the subject of discussion. 
Findings – The findings revealed that existing plagiarism detection techniques require further 
enhancements as they are incapable of efficiently detecting plagiarised ideas, figures, tables, 
formulas and scanned documents. 
Originality/value – The contribution of this study lies in its examination of the current trends 
in plagiarism detection research and identification of areas where further improvements are 
required so as to complement the performance of existing techniques. 
 
Keywords Plagiarism, Detection, Taxonomy, Techniques, Processes 
Paper type Research paper 
 
Introduction 
Plagiarism is described as an unrecognised utilisation of someone’s new idea(s), 
expression(s) or text(s). It has long been considered to be a serious research, academic or 
publishing offence. Many wheels have been re-invented and so many are still being re-
invented: a term that refers to the recycling of research, ideas or texts for consumption within 
the academic or research community. However, with the advent of new technology, 
plagiarism is now being curbed and reduced since detection systems are readily available to 
D
ow
nl
oa
de
d 
by
 A
eg
ea
n 
U
ni
ve
rs
ity
 A
t 0
0:
38
 1
6 
M
ay
 2
01
5 
(P
T
)
track it. This was made possible by ensuring that billions of ideas, texts, code sources, 
images, sounds and videos are readily accessible in web databases in order to perform the 
necessary matching to detect plagiarism. The focus of this study is to investigate techniques 
used in the past for detecting plagiarism. There are various types of plagiarism: 
• Exact copy plagiarism: lifting a sentence or a substantial phrase from a source without 
using quotation marks to reference the source. 
• Modified plagiarism: taking a sentence from a source and changing the order of a few 
words. 
• Style plagiarism: copying an author’s reasoning style or concept even when the texts 
are fully paraphrased. 
• Metaphor plagiarism: copying someone else’s metaphors in describing a particular 
subject. 
• Idea plagiarism: articulating another person’s novel idea or solution as though it is 
ones’ own. 
This research therefore centres on the identification of existing plagiarism detection 
techniques, descriptions, taxonomies, limitations and features. The analysis contained in this 
paper covers the strengths and limitations of existing techniques particularly in terms of 
whether they are able to detect plagiarised tables, figures or formulae across various fields 
such as papers from chemistry, physics or mathematics amongst others. The findings of this 
research will help developers, practitioners and scholars propose optimised or more efficient 
plagiarism detection techniques with the aim of addressing the limitation(s) of existing 
techniques. 
 Plagiarism has become a crucial research area in recent years as various research, 
publishing and teaching institutions seek to drastically eliminate the re-invention of the wheel 
associated with many research studies. 
 An example of plagiarism detection can be seen in Figure 1: the shapes and direction of 
arrows were modified but the idea is actually plagiarised.  
D
ow
nl
oa
de
d 
by
 A
eg
ea
n 
U
ni
ve
rs
ity
 A
t 0
0:
38
 1
6 
M
ay
 2
01
5 
(P
T
)
 
 
Figure 1. Example of modified or idea plagiarism 
 
 The remainder of the paper is structured as follows. The next section describes the 
research method used in this review. The section after that discusses the threats to validity. 
The subsequent section presents the results and discussions while the final section concludes 
the study. 
 
Research method 
The approach proposed by Kitchenham and Charters (2007), known as review protocol as 
reflected in Figure 2, was adopted in conducting this study.  
 
 
 
 
 
 
Plagiarised figure 
D
ow
nl
oa
de
d 
by
 A
eg
ea
n 
U
ni
ve
rs
ity
 A
t 0
0:
38
 1
6 
M
ay
 2
01
5 
(P
T
)
 
 
 
 
 
 
 
 
 
 
 
Figure 2. Phases of the review protocol 
 
Research questions 
The aim of this survey was to understand and summarise the empirical proof regarding state-
of-the-art plagiarism detection software or techniques, and identify areas for further research 
in order to complement the performance of existing techniques. To achieve this aim three 
research questions were formulated: 
RQ1. What are the descriptions and limitations of existing plagiarism detection 
techniques? 
RQ2. What are the various taxonomies of existing plagiarism detection techniques? 
RQ3. What are the features used to detect plagiarism? 
These questions, which formed the basis for undertaking this research, were simultaneously 
investigated. 
Search strategy 
The search strategies utilised in this research consisted of search terms, literature resources 
and search processes as explained below. 
Search strings 
The following steps were used to build the search terms (Kitchenham and Charters, 2007): 
1) Derivation of major terms from the research questions; 
2) Identification of alternative spellings and synonyms for major terms; 
3) Identification of keywords in relevant papers or books; 
 Search strategy 
Search 
terms  
Resources   
Search 
process   
Data synthesis    
Research questions  
Study selection 
Scrutiny   
Quality assessment 
criteria    
D
ow
nl
oa
de
d 
by
 A
eg
ea
n 
U
ni
ve
rs
ity
 A
t 0
0:
38
 1
6 
M
ay
 2
01
5 
(P
T
)
4) Usage of the Boolean OR to incorporate alternative spellings and synonyms; and 
5) Usage of the Boolean AND to link the major terms. 
The resulting search terms were: (approach or method or methodology or technique) and 
(“plagiarism detection research”) or (“plagiarism descriptions”) or (“plagiarism detection 
processes”) or (“plagiarism detection software”) or (“plagiarism detection features”) or 
(“plagiarism limitations”) and (taxonomies). 
Literature resources 
Six electronic database resources were used to extract data for synchronisation in this 
research: IEEE Xplore, ACM Digital Library, ScienceDirect, Web of Science, Springer and 
Google. Title and index terms were used to conduct searches for published journal papers, 
conference proceedings, workshops, symposiums, book chapters and IEEE Bulletins. 
Search process 
A survey entails a comprehensive search of all relevant sources about a subject of discussion. 
However, the search processes employed in this research consisted of the two steps depicted 
in Figure 3:  
Stage 1: A thorough search was launched in the six electronic database sources and the 
returned results (papers) were assembled as sets of prospective papers. 
Stage 2: The reference lists of all relevant papers were perused to detect additional relevant 
papers and then, if there were any, they were combined with the ones from stage 1. This 
enabled us detect papers that had been missed in stage 1. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
D
ow
nl
oa
de
d 
by
 A
eg
ea
n 
U
ni
ve
rs
ity
 A
t 0
0:
38
 1
6 
M
ay
 2
01
5 
(P
T
)
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 3. Search and selection process 
 
Study selection 
From the first search stage, 306 prospective studies were gathered. Next, the titles of these 
studies were used to scrutinise and collate relevant studies. This task was necessary to 
eliminate duplicate and irrelevant studies. Consequently 150 relevant studies were selected. 
Thereafter, the references of each selected study were perused to identify important studies 
that might have been missed during the initial search process. This effort led to the 
identification of 15 additional studies which took the tally of the selected studies to 165. 
Finally, the quality assessment question or criteria were applied to these 165 studies. At the 
end of the exercise, 56 studies were selected and deemed capable of providing answers to the 
formulated research questions. 
Search stage 1 (Six electronic databases) 
Selection stage 1 (Identification of duplicate papers) 
100 papers 
50 papers 
50 papers 
50 papers 50 papers 6 papers 
IEEE Xplore 
ACM Digital 
Library 
ScienceDirect 
Web of Science Springer  EI Compendex  
Selection stage 2 (Application of inclusion/exclusion criteria) 
Search stage 2 (Identification of additional papers from 
references of the 99 selected papers) 
Total number of papers = 
306 papers 
200 papers 
Total = 99 papers 
105 papers  
56 [2-56] from the 
Reference list 
Selection stage 3 (Application of quality assessment criteria) 
D
ow
nl
oa
de
d 
by
 A
eg
ea
n 
U
ni
ve
rs
ity
 A
t 0
0:
38
 1
6 
M
ay
 2
01
5 
(P
T
)
Quality assessment of selected studies 
The quality assessment of selected studies was achieved by a weighting or scoring technique 
so as to obtain relevant studies capable of addressing each research question. We formulated 
five quality assessment questions to evaluate the credibility, completeness and relevance of 
the selected studies. These questions were: 
Each question has only three possible answers: “yes”, “partly” or “no”. These three answers 
are scored as follows: “yes” = 1, “partly” = 0.5 and “no” = 0. Consequently the quality score 
for a particular study is computed by finding the sum of all the scores of the answers to the 
quality assessment questions. The authors meticulously conducted quality assessment of the 
selected studies. All discrepancies in the quality assessment results were discussed among the 
authors with the aim of reaching consensus. Reliability was accomplished by considering 
only the relevant studies with an acceptable quality rating, i.e. with a quality score greater 
than 2.5 (50 percent of the potential score). As a result 93 papers were excluded from the 
initially collated studies, giving rise to 56 finally selected relevant studies. 
 These metrics guided the interpretation of the findings of the selected studies and 
determined the validity of the inferences. It also helped in ascertaining the credibility and 
coherent synthesis of the results. 
Data synthesis 
The essence of data synthesis is to summarise evidence from the selected studies in order to 
address or answer the research questions. To synthesise data the 56 selected studies were 
further perused to assess the detailed contents of each study. 
 
Threats to validity 
Publication bias, a situation where positive results in detection accuracy are more likely to be 
reported than negative results, or scholars claiming that their technique is better than others’, 
and incomplete or redundant extraction of papers, were considered as threats to the validity of 
this review (Achimugu et al., 2014). This can lead to an overestimation of the performance of 
1) Are the aims of the research clearly articulated? 
2) Is the proposed technique clearly described? 
3) Is the experimental design appropriate? 
4) Is the experiment applied on adequate project data sets or case study? [“Yes” (two or 
more
 
datasets or case studies); “Partly” (one dataset or case study); “No” (None)] 
     5) Does the research add value to the academia or industrial community? 
D
ow
nl
oa
de
d 
by
 A
eg
ea
n 
U
ni
ve
rs
ity
 A
t 0
0:
38
 1
6 
M
ay
 2
01
5 
(P
T
)
existing techniques. To avert this threat, publications that dealt with comparisons of various 
existing techniques were searched for and included in the selected studies in order to obtain 
an objective evaluation result across the various techniques. This is because in most cases this 
kind of comparative study presents unbiased reports. Also, it is possible to miss important 
studies since not all studies can be found using the search terms related to the research 
questions in their titles, abstracts or keywords. To curb this threat, a manual scrutiny of the 
references of all the extracted studies was carried out to identify those studies that were 
missed during the initial search. To eliminate redundancy of extracted data, the authors 
carried out independent valuations using the five quality assessment questions on the selected 
studies and later brainstormed in order to resolve the discrepancies and obtain similarities in 
the ordering of ratings.  
 
Results and discussion 
This section presents and discusses the findings of this review. We start by presenting a 
detailed description of the responses to each research question in separate sub-sections. The 
first research question has to do with the descriptions and limitations of existing plagiarism 
detection techniques. From the selected studies several types of plagiarism detection 
techniques were identified in line with their limitations as shown in Table 1. 
Existing techniques (RQ1) 
Bag of words models-based techniques 
Some techniques detect exact copy plagiarism using bag of words models.  
Fingerprint-based techniques detect plagiarism by matching strings in documents based 
on common fingerprint proportions, which are sequences of characters contained in the entire 
document (Barrón-Cedeño and Rosso, 2009, Barrón-Cedeño et al., 2009, Hoad and Zobel, 
2003, Menai, 2012; Aimmanee, 2011; Sindhu et al., 2013; Grozea et al., 2009). 
Microsoft SQL Server platform employs the concept of fingerprinting in comparing 
documents where the Levenstein’s metric is used to markup plagiarised fragments in the texts 
in order to detect similarity rates (Scherbinin and Butakov, 2009). 
Features based text similarity detection integrates the fingerprinting technique with four 
basic features: keyword, first sentence similarity, query phase and least common subsequence 
(Kent and Salim, 2010). 
D
ow
nl
oa
de
d 
by
 A
eg
ea
n 
U
ni
ve
rs
ity
 A
t 0
0:
38
 1
6 
M
ay
 2
01
5 
(P
T
)
Rhetorical structure theory splits texts into interconnected units to aid comparison 
where the homogenous or similar texts are analysed based on their semantics in natural 
language processing (Omar et al., 2013). 
Semantic plagiarism detection-based techniques 
These are techniques that detect plagiarism by taking the meanings of words into 
consideration.  
Stopword n-grams employ content terms to represent documents using a small list of 
stop-words to detect plagiarism based on the syntactic composition and similarities of texts in 
various documents (Stamatatos, 2011). 
The check method examines the proportion of keywords’ structural characteristics for 
texts in a document. It was developed based on an indexed structure used to parse documents 
to build the structural characteristics of text (Si et al., 1997). 
Sentence similarity establishes the degree of resemblance between two or more 
documents with reference to their sentence-to-sentence similarity calculated using pre-
defined word-correlation factors and a graphical view of sentences that show the similarity 
rates is generated (Gustafson et al., 2008). Yerra and Ng (2005) found the fuzzy-set 
information retrieval approach outperformed the three least-frequent 4-gram approaches in 
checking sentence similarity. Similarly Yuhua et al. (2006) computed the similarity between 
short texts of sentences and postulated an algorithm that considers the semantic information 
and word order in the sentences.  
The standard copy analysis mechanism algorithm is a relative measure to detect 
overlapping words by making a sentence-level comparison on a set of words that are common 
between suspicious and source documents (Anzelmi et al., 2011). To handle the rewording 
with synonyms the vocabulary of terms is expanded with synonyms through WorldNet. 
The semantic role labelling or semantic method analyses and compares text based on 
the semantic allocation for each term inside the sentence (Osman et al., 2012b). The 
technique accepts documents and segments the text; a semantic role labeller is used to detect 
the semantic term annotation of the text to detect plagiarism. 
The grammar-based method identifies plagiarised texts by considering the grammatical 
structure of a document with the help of context free grammar concepts to identify plagiarism 
from paraphrased text (Adam, 2014). This method contains a string based matching model 
which analyses the grammatical structures or patterns of sentences in order to determine 
whether the sentences have been significantly altered. It passes the final verdict by computing 
D
ow
nl
oa
de
d 
by
 A
eg
ea
n 
U
ni
ve
rs
ity
 A
t 0
0:
38
 1
6 
M
ay
 2
01
5 
(P
T
)
the extent to which the suspicious documents have altered the structure and grammar of the 
sentences in the original documents. 
The syntax-based technique identifies plagiarism with the aid of syntactical part-of-
speech (POS) tags which are utilised in representing text structures to support comparison 
and analysis (Elhadi and Al-Tobi, 2008). Similarity scores are calculated based on the 
number of corresponding POS tags existing between the suspicious and original documents. 
Other techniques that employ the syntax-based approach to detect plagiarism are syntactic 
and semantic based techniques such as duplication-gram, re-ordering and alignment of words, 
POS, phrase tags and semantic similarities of sentences. 
The semantic technology-based technique measures the semantic similarity between 
words and their meanings to confirm plagiarised text through matching the various keywords 
of the texts between the documents (Agarwal et al., 2013). 
Fuzzy-based methods detect plagiarism by representing each semantically similar text 
in a document with a corresponding fuzzy number where the similarity scores are calculated 
based on the number of fuzzy numbers attached to each text (Osman et al., 2012a; Yerra and 
Ng, 2005; Alzahrani and Salim, 2009, 2010). 
Intelligent plagiarism detection identifies different textual features for characterisation 
of patterns in a document (Ramya and Venkatalakshmi, 2013). 
ENCOPLOT derives a matrix of kernel values from texts, computed to generate a 
similarity value based on n-grams between the original and suspicious document (Grozea et 
al., 2009). This is followed by further analysis to extract the precise positions and lengths of 
the subtexts that have been copied. 
Graph-based representation  
The category of detection technique converts text into a graph to detect plagiarism by 
considering the nodes and edges in the graph (Osman et al., 2010, 2011). The nodes house 
the sentences while the attributes of the sentences are represented by the edges. The output of 
an edge corresponds to the contents of a node, which is calculated using the Jaccard measure. 
Each node is linked with a unique identity formed by considering the concept of sentence 
terms. This enables the identification of all the synonyms between the original and suspicious 
documents using WordNet. 
Tree-based representation 
The iPlag (Intelligent Plagiarism Reasoner in Scientific Publications) technique intelligently 
detects plagiarism by dividing the suspected documents into various structural components 
where the degree of similarity is assigned numerical weights and the weights are computed to 
D
ow
nl
oa
de
d 
by
 A
eg
ea
n 
U
ni
ve
rs
ity
 A
t 0
0:
38
 1
6 
M
ay
 2
01
5 
(P
T
)
determine the percentage of the similarity scores of the suspected document (Alzahrani et al., 
2011). The suspected documents are segmented into logical tree-structured representation 
using a procedure called DSEGMENT. 
In the multilayer self-organising map based approach documents are modelled with a 
rich tree-structured representation, which hierarchically includes document features such as 
pages and paragraphs to detect plagiarism (Chow and Rahman, 2009). An algorithm was 
developed to efficiently match similar texts for comparison. 
The multilevel matching approach employs a multilevel structure of document–
paragraph–sentence to detect plagiarism (Zhang and Chow, 2011). The basic idea of this 
technique is to convert the paragraphs or sentences contained in a document in a tree or graph 
form represented by nodes where traversals are performed to compare the output for 
detecting similarity. Also, traditional dimensionality reduction techniques were used to 
forecast high dimensional histograms into latent semantic space where the Earth Mover’s 
Distance is used for retrieving relevant documents and shrinks the searching domain. Two 
efficient algorithms were developed to effectively detect plagiarism based on the source of 
the documents. 
Ontology-base representation  
Techniques in this category are ontology based which means they cater for both text and idea 
plagiarism (Deepika et al., 2011). These techniques employ ontology and WordNet to detect 
paraphrased sentences or words in documents. The offline ontology technique was used to 
detect plagiarised ideas but was limited in accuracy. This type of technique can be seen in the 
work of Shet and Acharya (2012) as well. 
Cross-language or bilingual plagiarism detection deals with the automatic identification 
of plagiarised texts from different languages (Arefin et al., 2013; Barrón-Cedeño et al., 2013; 
Potthast et al., 2011; Muhr et al., 2010). This is achieved by building a bilingual dictionary of 
terms or words in different languages and a translator capable of converting the queried 
language to the target language to detect plagiarism in a multilingual setting. 
 
Table 1. Descriptive features of existing techniques 
Name of techniques  Plagiarism detection coverage Evaluation 
1 2 3 4 5 
Bag of words model / copy and paste plagiarism type 
Fingerprint-based   × × × × METER, PAN’09-13 corpus 
D
ow
nl
oa
de
d 
by
 A
eg
ea
n 
U
ni
ve
rs
ity
 A
t 0
0:
38
 1
6 
M
ay
 2
01
5 
(P
T
)
Check  × × × × Not indicated  
Features based text similarity 
detection 
 × × × × Not indicated 
Intelligent plagiarism detection   × × × × Not indicated  
Microsoft SQL Server platform   × × × × PAN’09 corpus  
Bag of words model / rewording  
Stopword n-grams   × × × PAN-PC-10 corpus 
Grammar-based   × × × Not indicated  
Syntax-based   × × × Not indicated 
Lexical, syntactic and semantic-
based  
  × × × PAN corpus in English 
SCAM algorithm   × × × Not indicated 
Rhetorical structure theory 
 
  × × × used 1000 public health papers in 
Arabic and English languages  
Sentence similarity    × × × Webis-PC-08, Meter Corpus 
ENCOPLOT   × × × Not indicated  
Fuzzy-based methods    × × × Arabic Wikipedia  
Semantic technology-based   × × × Not indicated  
Different representations 
Graph-based representation    × × × PAN-PC-09 corpus 
Ontology-base solutions   × ×  Not indicated  
iPlag: Intelligent Plagiarism 
Reasoner in Scientific Publications 
  × ×  Not indicated  
Multilayer self-organising 
map based approach  
 × × × × 1000 documents were retrieved and 
sorted by the local sorting procedures 
Multilevel matching approach   × × × Html_CityU1 
Semantic role labelling, semantic 
method 
  × × × PAN-PC- 10 datasets 
 
Hybrid similarity method  × × × × PAN-2013 
 
Bilingual or cross-language based  × ×   × PAN-PC-11 corpus 
D
ow
nl
oa
de
d 
by
 A
eg
ea
n 
U
ni
ve
rs
ity
 A
t 0
0:
38
 1
6 
M
ay
 2
01
5 
(P
T
)
Exact copy, 2 - modify copy, 3- Style plagiarism, 4- Translation plagiarism 5- Idea 
plagiarism 
 
 Also, most techniques are able to identify plagiarism from exact copy and rewording by 
synonymy replacements. Existing techniques are basically grouped based on their detection 
types (Table 2). For instance fingerprint matching has the capacity to scan and examine the 
fingerprints of two documents to detect plagiarism. It was discovered that the fingerprinting 
technique usually depends on the principles of K-grams since it deals with the partitioning of 
documents into grams of k-lengths. Fingerprints are grouped into three categories: character-
based fingerprints, phrase-based fingerprints and statement-based fingerprints (Aimmanee, 
2011; Sindhu et al., 2013; Scherbinin and Butakov, 2009). Other ways of grouping metrics 
used for detecting plagiarism are based on the documents’ semantic analysis which relies on 
the meaning of the texts in the documents. Semantic based plagiarism detection techniques 
function by using some semantic metrics calculated through statistical methods (Adam, 2014; 
Elhadi and Al-Tobi, 2008). The statistical approach of calculating similarity scores between 
documents relies on the generation of vectors which reflect values describing the frequencies 
of words existing between two or more documents. N-gram is an example of a statistical way 
of determining similarity scores between documents. It is based on the determination of the 
number of texts, categorised with sequences of N consecutive characters. Based on statistical 
analysis, texts could be described with fingerprints, where n-grams are hashed and then some 
are selected as fingerprints which can be probabilistic or stochastic. 
 Examples of metrics used to determine the extent of plagiarism between two or more 
documents are cosine, proportion and dot production functions. Others are Jaccard, Dice, 
overlap, or symmetric and asymmetric similarity functions (Alzahrani et al., 2012). The 
descriptions of each metric and the accompanied equations and ranges are depicted in Table 
2. 
 
Table 2. Metrics used for calculating similarity scores (Alzahrani et al., 2012) 
D
ow
nl
oa
de
d 
by
 A
eg
ea
n 
U
ni
ve
rs
ity
 A
t 0
0:
38
 1
6 
M
ay
 2
01
5 
(P
T
)
 
 
Limitations of existing techniques 
To identify the limitations of existing techniques, we grouped them based on their detection 
patterns. Seven major detection patterns were identified from the selected studies as 
described in Table 3. It was generally discovered that, from all the existing techniques 
proposed for detecting plagiarism, only few are able to detect similarities based on the 
meaning of texts between documents, i.e. idea plagiarism (Alzahrani et al., 2011; Deepika et 
al., 2011). Therefore improvement is required in this area.  
 
Table 3. Classification and limitations of existing techniques 
Classification name  Description  Limitations  
(i) Character-based methods  
• Fingerprinting 
• String similarity 
• N-grams 
• SCAM 
• Longest common 
subsequence  
Compare the lexical features 
such as characters, strings or 
words contained in the 
suspicious document with the 
original one  
Cannot detect plagiarism when 
texts are reworded, changed or 
paraphrased 
D
ow
nl
oa
de
d 
by
 A
eg
ea
n 
U
ni
ve
rs
ity
 A
t 0
0:
38
 1
6 
M
ay
 2
01
5 
(P
T
)
(ii) Structural-based methods  
• Tree-based 
• Graph-based 
• Rhetorical structure  
theory  
• Multilayer self-organising 
map  
• Multilevel approach 
Use structural features of text 
contained in a document such as 
keywords, headers, sections, 
paragraphs and references  
Lack methods of detecting 
plagiarised ideas or concepts 
among authors or published 
papers 
(iii) Cross language-based 
methods 
Compare the suspicious and 
original document via statistical 
models, where a probability that 
a suspected document is 
relevant to the original 
document is fully established 
irrespective of the order in 
which the texts appear in the 
suspected and original 
documents 
Requires the generation of the 
cross-lingual corpus for various 
languages, which is a difficult 
task 
(iv) Semantic-based methods 
• Semantic role labelling, 
semantic method 
• Semantic technology based  
Analyse the meaning of texts 
contained in the suspicious and 
original documents using 
information from a dictionary 
or thesaurus 
Deficient in scalability, i.e. not 
able to cover all aspects of a 
sentence  
 
(v) Fuzzy-based methods  Identify texts that are 
semantically equivalent in the 
original and suspicious 
documents through fuzzy 
numbers or sets 
Difficult to construct reliable or 
accurate fuzzy sets that will 
completely match the potentially 
similar text correctly 
 
Taxonomies for existing plagiarism detection techniques (RQ2) 
The term plagiarism generally focuses on the calculation of similarity scores from structured 
or unstructured documents, compared to the corpus database containing millions or billions 
of reference papers through querying the words or texts (Zechner et al., 2009; Potthast et al., 
D
ow
nl
oa
de
d 
by
 A
eg
ea
n 
U
ni
ve
rs
ity
 A
t 0
0:
38
 1
6 
M
ay
 2
01
5 
(P
T
)
2010, 2011; Kasprzak and Brandejs, 2010). Various detection taxonomies were identified 
based on their detection analysis or approach which typically consists of assessment of the 
local and global similarities of the original and suspicious documents (Meyer zu Eissen and 
Stein, 2006; Meuschke and Gipp, 2013; Arefin et al., 2013). Local similarity assessment 
approaches analyse matches of confined text segments in suspicious documents (Stein and 
Meyer zu Eissen, 2006) through the fingerprinting technique, while global similarity 
assessment approaches examine characteristics of longer text sections, or the complete 
document, and express the degree to which two documents are similar to each other in their 
entirety (Stein and Meyer zu Eissen, 2006). The term occurrence analysis that employs the 
use of texts operates at the global level to detect plagiarism, principally through the help of 
Vector Space Models. Figure 4 shows examples of the taxonomies that detect plagiarism by 
local and global similarity indexes. 
 
 
Figure 4. Taxonomy of existing techniques based on local and global similarities (Meuschke 
and Gipp, 2013; Meyer zu Eissen and Stein, 2006; Arefin et al., 2013) 
 
 Potthast et al. (2011), Alzahrani et al. (2011) and Meyer zu Eissen et al. (2007) 
compared the original (exact copy) and suspicious (modified copy) documents by the analysis 
of detection features such as keywords, synonyms, phrase re-ordering, syntaxes and 
semantics of words or texts contained in the documents. The encapsulation of these extensive 
features or attributes as proposed by Alzahrani et al. (2012) is shown in Figure 5. 
 
D
ow
nl
oa
de
d 
by
 A
eg
ea
n 
U
ni
ve
rs
ity
 A
t 0
0:
38
 1
6 
M
ay
 2
01
5 
(P
T
)
 
Figure 5. Taxonomy of existing techniques based on syntactic and semantic analysis 
(Alzahrani et al., 2012) 
 
 From the various taxonomies identified in the literature, most detection techniques rely 
on explicit comparisons of the document contents in a specific representation. Fingerprinting 
is among the most popular techniques in this category. It measures the similarity of two 
documents by comparing their fingerprints. Semantic analysis is a technique used to describe 
relationships between a set of documents and terms they contain. In this technique words that 
are close in meaning or close together are suspects (Bär et al., 2012). Singular value 
D
ow
nl
oa
de
d 
by
 A
eg
ea
n 
U
ni
ve
rs
ity
 A
t 0
0:
38
 1
6 
M
ay
 2
01
5 
(P
T
)
decomposition, a factorisation method of a real or complex matrix, is used to reduce the 
number of columns while preserving the similarity structure among rows (Chong, 2013). 
 The standard copy analysis mechanism identifies plagiarism by accessing the 
documents in the repository in order to compare them with the suspicious documents. This 
mechanism for splitting texts of documents is known as chunking (Zechner et al., 2009). 
Documents are compared using the relative frequency model which consists mainly of 
determining the number of occurring words in the original and suspicious documents. Hoad 
and Zobel (2003) proposed several variations of a similarity measure based on the number of 
occurrences of similar words in the documents, such as length of documents, alterations of 
word occurrences in the suspected documents, and term weighting. Their results showed that 
the term weighting similarity measure is among the best ones, particularly when stop-words 
are removed and words are reduced to their root form. Examples of plagiarism detection tools 
built around content-based methods include Turnitin, EVE2, Wcopyfind and CHECK. 
Stylometry is a statistical approach used for detecting plagiarism based on unique styles of 
writing (Stamatatos, 2009; Oberreuter et al., 2011; Meyer zu Eissen and Stein, 2006; Meyer 
zu Eissen et al., 2007; Seaward and Matwin, 2009).  
Features used for detecting plagiarism (RQ3) 
Features are the pointers or parameters used in detecting plagiarism of all types. Eight 
different types of features were identified in the literature and are listed below.  
Top keywords feature, first sentence similarity, query phrase, longest common subsequence   
The top keyword feature is executed by removing the stop-words in a document, otherwise 
known as the stemming process, where the words that appeared most are singled out in the 
document, and considered keywords that capture the ideas in the papers (Kent and Salim, 
2010). However, finding some of the frequently appearing words in the papers does not 
necessarily mean that the entire idea of a paper has been captured. The first sentence of a 
document comprises crucial ideas that capture the entire concept of the work. To use this 
feature to detect plagiarism, the first sentence of the document perceived to be original is 
matched with the first sentence of the suspected document.  
Lexical features 
These features focus on using the lexical structure of the text or documents, which operate at 
the character or the word level of the document in order to trace the plagiarism in the 
suspicious document (Alzahrani et al., 2012). This type of approach tries to enhance the 
standard string matching comparison in order to detect plagiarism. The processing techniques 
that this approach relies upon include tokenisation, lowercasing, punctuation removal and 
D
ow
nl
oa
de
d 
by
 A
eg
ea
n 
U
ni
ve
rs
ity
 A
t 0
0:
38
 1
6 
M
ay
 2
01
5 
(P
T
)
stemming. However, it can vary across techniques in terms of words, sentences, passages, 
human defined sliding window or an n-gram. 
Structural features 
Structural features (or tree-structured features), conversely, take into account the way the 
words are distributed throughout the document. Very few plagiarism detection approaches 
have been developed to handle structural or tree features (Chow and Rahman, 2009; Zhang 
and Chow, 2011; Alzahrani et al., 2011). Such studies use the representation of the 
documents such as hierarchical blocks, which is referred to as block-specific, document-
pages-paragraphs and document-paragraphs-sentences (Chow and Rahman, 2009). 
Syntactic features 
Syntactic features identify plagiarism by dividing the text into chunks, sentences and phrases 
to aid efficient comparison of the original and suspicious documents (Ramya and 
Venkatalakshmi, 2013; Elhadi and Al-Tobi, 2008; Yerra and Ng, 2005). 
Semantic features 
Semantic features identify plagiarism by detecting the synonyms, antonyms and dependency 
meanings of texts within sentences in a document (Osman et al., 2011; Deepika et al., 2011a; 
Osman et al., 2012b; Bär et al., 2012; Yuhua et al., 2006). 
 
Research findings and outlook  
In this paper we have presented an analysis of state-of-the-art plagiarism detection 
techniques. Most techniques detect plagiarism by using certain features along with fingerprint 
matching techniques. With the advancement of information technology, internet search 
plagiarism is beginning to gain considerable attention. The self-plagiarism, i.e. recycling 
one’s own ideas, can be easily detected. Also, efforts should be made to develop techniques 
that can detect stolen ideas. Operation of plagiarism detection tools is based on statistical or 
semantic methods or both to get better results. Information about methods and algorithms 
which are applied in each tool is hidden. From the available descriptions of some detection 
tools, it appears that most of the tools use statistical methods to detect plagiarism, because 
these methods are well-understood and easier to implement. It is evident that, although 
plagiarism detection tools provide excellent service in detecting matching text between 
documents, care needs to be taken in their use. Plagiarism detection tools’ inability to 
distinguish correctly cited text from plagiarised text is one of the serious drawbacks of these 
tools. Also, existing plagiarism detection techniques are incapable of efficiently detecting 
D
ow
nl
oa
de
d 
by
 A
eg
ea
n 
U
ni
ve
rs
ity
 A
t 0
0:
38
 1
6 
M
ay
 2
01
5 
(P
T
)
similarities that exist between equations, figures and tables, making human intervention 
inevitable before an author is finally accused of plagiarism. Considerable effort has been 
applied to detecting similar texts in documents; however, researchers and practitioners neeed 
to start developing techniques capable of identifying plagiarised figures, tables, equations and 
scanned documents or images. Although some authors are beginning to propose techniques 
capable of identified plagiarised figures, the accurate identification of scanned documents, 
tables or equations is still a huge challenge.  
 
Conclusion  
In conclusion the state-of-the-art plagiarism techniques have been identified and analysed 
based on their attributes, limitations, processes and taxonomies. It was discovered that the 
techniques are limited in some detection aspects. It was also discovered that some techniques 
have been implemented on an industrial scale. Delvin (2002) and Lukashenko et al. (2007) 
analysed the performance of seven plagiarism tools: Turnitin, Eve2, CopyCathGold, 
WorldCheck, Glatt, Moss and Jplag. They found that Turnitin has the overall best detection 
accuracy and is more scalable. However, robust techniques for detecting plagiarism are 
required, especially now that authors can easily modify texts, change ideas from one form to 
another or hide correct references to prevent detection. Using structural features and 
contextual information integrated with semantic similarity methods can help to detect these 
types of plagiarism.  
 
References 
Achimugu, P., Selamat, A., Ibrahim, R. and Mahrin, M.N.R. (2014), “A systematic literature 
review of software requirements prioritization research”, Information and Software 
Technology, Vol. 56 No. 6, pp. 568-85. 
Adam, A.R. (2014), “Plagiarism detection algorithm using natural language processing based 
on grammar analyzing”, Journal of Theoretical & Applied Information Technology, 
Vol. 63 No. 1, pp. 168-80. 
Agarwal, J., Goudar, R.H., Kumar, P., Sharma, N., Parshav, V., Sharma, R., Srivastava, A. 
and Rao, S. (2013), “Intelligent plagiarism detection mechanism using semantic 
technology: a different approach”, 2013 International Conference on Advances in 
Computing, Communications and Informatics (ICACCI), 22-25 August 2013, IEEE, 
Los Alamitos, CA, pp. 779-83. 
D
ow
nl
oa
de
d 
by
 A
eg
ea
n 
U
ni
ve
rs
ity
 A
t 0
0:
38
 1
6 
M
ay
 2
01
5 
(P
T
)
Aimmanee, P. (2011), “Automatic plagiarism detection using word-sentence based s-gram”, 
Journal of Science, Vol. 38, pp. 1-7. 
Alzahrani, S.M. and Salim, N. (2009), “On the use of fuzzy information retrieval for gauging 
similarity of Arabic documents”, Second International Conference on the Applications 
of Digital Information and Web Technologies, ICADIWT '09, 4-6 August 2009, IEEE, 
Los Alamitos, CA, pp. 539-44. 
Alzahrani, S. and Salim, N. (2010), “Fuzzy semantic-based string similarity for extrinsic 
plagiarism detection”, in Braschler, M., Harman, D. and Pianta, E. (Ed.), Lab Report 
for PAN at CLEF, 22-23 September, Vol. 1176, CEUR-WS.org 
Alzahrani, S.M., Salim, N. and Abraham, A. (2012), “Understanding plagiarism linguistic 
patterns, textual features, and detection methods”, IEEE Transactions on Systems, Man, 
and Cybernetics, Part C: Applications and Reviews, Vol. 42 No. 2, pp. 133-49. 
Alzahrani, S., Salim, N., Abraham, A. and Palade, V. (2011), “iPlag: intelligent plagiarism 
reasoner in scientific publications”, World Congress on Information and 
Communication Technologies (WICT), 11-14 December 2011, IEEE, Los Alamitos, 
CA, pp. 1-6. 
Anzelmi, D., Carlone, D., Rizzello, F., Thomsen, R. and Hussain, D.A. (2011), “Plagiarism 
detection based on SCAM algorithm”, in Proceedings of the International 
MultiConference of Engineers and Computer Scientists, Vol. 1, International 
Association of Engineers, Hong Kong, pp. 272-77. 
Arefin, M.S., Morimoto, Y. and Sharif, M.A. (2013), “BAENPD: a bilingual plagiarism 
detector”, Journal of Computers, Vol. 8 No. 5, pp. 1145-56. 
Bär, D., Biemann, C., Gurevych, I. and Zesch, T. (2012), “UKP: computing semantic textual 
similarity by combining multiple content similarity measures”, in Proceedings of the 
Sixth International Workshop on Semantic Evaluation, Association for Computational 
Linguistics, Stroudsburg, PA, pp. 435-40. 
Barrón-Cedeño, A. and Rosso, P. (2009), “On automatic plagiarism detection based on n-
grams comparison”, Advances in Information Retrieval, Springer, Berlin, pp. 696-700. 
Barrón-Cedeño, A., Gupta, P. and Rosso, P. (2013), “Methods for cross-language plagiarism 
detection”, Knowledge-Based Systems, Vol. 50, September, pp. 211-7. 
Barrón-Cedeño, A., Rosso, P. and Benedí, J.-M. (2009), “Reducing the plagiarism detection 
search space on the basis of the Kullback-Leibler Distance”, in Gelbukh, A. (Ed.), 
Computational Linguistics and Intelligent Text Processing, Springer, Berlin, pp. 523-
34. 
D
ow
nl
oa
de
d 
by
 A
eg
ea
n 
U
ni
ve
rs
ity
 A
t 0
0:
38
 1
6 
M
ay
 2
01
5 
(P
T
)
Chong, M.Y.M. (2013), “A study on plagiarism detection and plagiarism direction 
identification using natural language processing techniques”, PhD thesis, University of 
Wolverhampton. 
Chow, T.W.S. and Rahman, M.K.M. (2009), “Multilayer SOM with tree-structured data for 
efficient document retrieval and plagiarism detection”, IEEE Transactions on Neural 
Networks, Vol. 20 No. 9, pp. 1385-402. 
Deepika, J., Archana, V., Bagyalakshmi, V., Preethi, P. and Mahalakshmi, G.S. (2011), “A 
knowledge based approach to detection of idea plagiarism in online research 
publications”, International Journal on Internet and Distributed Computing Systems, 
Vol. 1 No. 2, pp. 51-61. 
Delvin, M. (2002), “Plagiarism detection software: how effective is it? Assessing learning in 
Australian universities”, [online], available at: http: 
//www.cshe.unimelb.edu.au/assessinglearning/docs/PlagSoftware.pdf (accessed 23 
October 2014). 
Elhadi, M. and Al-tobi, A. (2008), “Use of text syntactical structures in detection of 
document duplicates”, in Third International Conference on Digital Information 
Management, 13-16 November, London, IEEE, Los Alamitos, CA, pp. 520-5. 
Grozea, C., Gehl, C. and Popescu, M. (2009), “ENCOPLOT: pairwise sequence matching in 
linear time applied to plagiarism detection”, in 3rd PAN Workshop. Uncovering 
Plagiarism, Authorship and Social Software Misuse, CEUR-WS.org, pp. 10-18. 
Gustafson, N., Pera, M.S. and Ng, Y.-K. (2008), “Nowhere to hide: finding plagiarized 
documents based on sentence similarity”, in Proceedings of the 2008 IEEE/WIC/ACM 
International Conference on Web Intelligence and Intelligent Agent Technology - 
Volume 01, IEEE Computer Society, Los Alamitos, CA, pp. 690-6. 
Hoad, T.C. and Zobel, J. (2003), “Methods for identifying versioned and plagiarized 
documents” Journal of the American Society for Information Science and Technology, 
Vol. 54 No. 3, pp. 203-15. 
Kasprzak, J. and Brandejs, M. (2010), “Improving the reliability of the plagiarism detection 
system”, in Braschler, M., Harman, D. and Pianta, E. (Ed.), Lab Report for PAN at 
CLEF, 22-23 September, Vol. 1176, CEUR-WS.org, pp. 359-66. 
Kent, C.K. and Salim, N. (2010), “Features based text similarity detection”, Journal of 
Computing, Vol. 2 No. 1, pp. 53-7. 
Kitchenham, B.A. and Charters, S. (2007), “Guidelines for performing systematic literature 
reviews in software engineering”, Technical report, EBSE Technical Report EBSE- 
D
ow
nl
oa
de
d 
by
 A
eg
ea
n 
U
ni
ve
rs
ity
 A
t 0
0:
38
 1
6 
M
ay
 2
01
5 
(P
T
)
2007-01, Keel University and Durham University, [online], available at: 
http://www.dur.ac.uk/ebse/resources/guidelines/Systematic-reviews-5-8.pdf (accessed 
22 March 2015). 
Menai, M.E.B. (2012), “Detection of plagiarism in Arabic documents”, International Journal 
of Information Technology and Computer Science, Vol. 4 No. 10, pp. 80-9. 
Meuschke, N. and Gipp, B. (2013), “State-of-the-art in detecting academic plagiarism”, 
International Journal for Educational Integrity, Vol. 9 No. 1, pp. 50-71. 
Meyer zu Eissen, S. and Stein, B. (2006), “Intrinsic plagiarism detection”, in Lalmas, M., 
Macfarlane, A., Rüger, S., Tombros, A., Tsikrika, T. and Yavlinsky, A. (Ed.), Advances 
in Information Retrieval, Springer, Berlin, pp. 565-9. 
Meyer zu Eissen, S., Stein, B. and Kulig, M. (2007), Plagiarism Detection without Reference 
Collections, in Decker, R. and Lenz, H.-J. (Ed.), Advances in Data Analysis, Springer, 
Berlin, pp. 359-66. 
Muhr, M., Kern, R., Zechner, M. and Granitzer, M. (2010), “External and intrinsic plagiarism 
detection using a cross-lingual retrieval and segmentation system”, in Braschler, M., 
Harman, D. and Pianta, E. (Ed.), Lab Report for PAN at CLEF, 22-23 September, Vol. 
1176, CEUR-WS.org. 
Lukashenko, R., Graudina, V. and Grundspenkis, J. (2007), “Computer-based plagiarism 
detection methods and tools: an overview”, in Proceedings of the 2007 International 
Conference on Computer systems and Technologies, ACM, New York, p. 40. 
Oberreuter, G., L’huillier, G., Ríos, S.A. and Velásquez, J.D. (2011), “Approaches for 
intrinsic and external plagiarism detection”, in Notebook for PAN at CLEF 2011, 19-22 
September 2011, Amsterdam, Vol. 1177, CEUR-WS.org. 
Omar, K., Alkhatib, B. and Dashash, M. (2013), “The implementation of plagiarism detection 
system in health sciences publications in Arabic and English languages”, International 
Review on Computers & Software, Vol. 8 No. 4, pp. 915-19. 
Osman, A.H., Salim, N. and Binwahlan, M.S. (2010), “Plagiarism detection using graph-
based representation”, Journal of Computing, Vol. 2 No. 4, pp. 36-41. 
Osman, A., Salim, N., Kumar, Y. and Abuobieda, A. (2012a), “Fuzzy semantic plagiarism 
detection”, in Hassanien, A., Salem, A.-B., Ramadan, R. and Kim, T.-H. (Ed.), 
Advanced Machine Learning Technologies and Applications, Springer, Berlin, pp. 543-
53. 
D
ow
nl
oa
de
d 
by
 A
eg
ea
n 
U
ni
ve
rs
ity
 A
t 0
0:
38
 1
6 
M
ay
 2
01
5 
(P
T
)
Osman, A.H., Salim, N., Binwahlan, M.S., Alteeb, R. and Abuobieda, A. (2012b), “An 
improved plagiarism detection scheme based on semantic role labelling”, Applied Soft 
Computing, Vol. 12 No. 5, pp. 1493-502. 
Osman, A., Salim, N., Binwahlan, S., Hentabli, H. and Ali, A. (2011), “Conceptual similarity 
and graph-based method for plagiarism detection”, Journal of Theoretical and Applied 
Information Technology, Vol. 32 No. 2, pp. 135-45. 
Potthast, M., Barrón-Cedeño, A., Stein, B. and Rosso, P. (2011), “Cross-language plagiarism 
detection”, Language Resources and Evaluation, Vol. 45 No. 1, pp. 45-62. 
Potthast, M., Stein, B., Barrón-Cedeño, A. and Rosso, P. (2010), “An evaluation framework 
for plagiarism detection”, in Proceedings of the 23rd International Conference on 
Computational Linguistics, Association for Computational Linguistics,  Stroudsburg, 
PA, pp. 997-1005. 
Ramya, L. and Venkatalakshmi, R. (2013), “Intelligent plagiarism detection”, International 
Journal of Research in Engineering & Advanced Technology, Vol. 1 No. 1, pp. 171-4. 
Scherbinin, V. and Butakov, S. (2009), “Using Microsoft SQL server platform for plagiarism 
detection”, in Proceedings of the SEPLN 2009 Workshop on Uncovering Plagiarism, 
Authorship, and Social Software Misuse (PAN 09), Vol. 502, CEUR-WS.org pp. 36-7.  
Seaward, L. and Matwin, S. (2009), “Intrinsic plagiarism detection using complexity 
analysis”, Proceedings of SEPLN, Notebook for PAN at CLEF 2009, 30 September - 2 
October, Greece, Vol. 1175, CEUR-WS.org, pp. 56-61. 
Shet, K. and Acharya, U.D. (2012), “Semantic plagiarism detection system using ontology 
mapping”, Advanced Computing, Vol. 3 No. 3, pp. 59-62. 
Si, A., Leong, H.V. and Lau, R.W.H. (1997), “CHECK: a document plagiarism detection 
system”, in Proceedings of the 1997 ACM Symposium on Applied Computing, ACM, 
New York, pp. 70-7. 
Sindhu, L., Thomas, B.B. and Idicula, S.M. (2013), “A copy detection method for Malayalam 
text documents using n-grams model”, [online], available at: 
http://dyuthi.cusat.ac.in/purl/4104 (accessed 22 March 2015). 
Stamatatos, E. (2009), “Intrinsic plagiarism detection using character n-gram profiles”, 
Threshold, Vol. 2 No. 1, p. 500. 
Stamatatos, E. (2011), “Plagiarism detection using stopword n‐grams”, Journal of the 
American Society for Information Science and Technology, Vol. 62 No. 12, pp. 2512-
27. 
D
ow
nl
oa
de
d 
by
 A
eg
ea
n 
U
ni
ve
rs
ity
 A
t 0
0:
38
 1
6 
M
ay
 2
01
5 
(P
T
)
Stein, B. and Meyer zu Eissen, S. (2006), “Near similarity search and plagiarism analysis”, in 
Spiliopoulou, M., Kruse, R., Borgelt, C., Nürnberger, A. and Gaul, W. (Ed.), From 
Data and Information Analysis to Knowledge Engineering, Springer, Berlin, pp. 430-7. 
Yerra, R. and Ng, Y.-K. (2005), “A sentence-based copy detection approach for web 
documents”, in Wang, L. and Jin, Y. (Ed.), Fuzzy Systems and Knowledge Discovery, 
Springer, Berlin, pp. 557-70. 
Yuhua, L., McLean, D., Bandar, Z.A., O’Shea, J.D. and Crockett, K. (2006), “Sentence 
similarity based on semantic nets and corpus statistics”, IEEE Transactions on 
Knowledge and Data Engineering, Vol. 18 No. 8, pp. 1138-50. 
Zechner, M., Muhr, M., Kern, R. and Granitzer, M. (2009), “External and intrinsic plagiarism 
detection using vector space models”, in Proceedings of SEPLN, Notebook for PAN at 
CLEF 2009, 30 September - 2 October, Greece, Vol. 1175, CEUR-WS.org, pp. 47-55. 
Zhang, H. and Chow, T.W.S. (2011), “A coarse-to-fine framework to efficiently thwart 
plagiarism”, Pattern Recognition, Vol. 44 No. 2, pp. 471-87. 
D
ow
nl
oa
de
d 
by
 A
eg
ea
n 
U
ni
ve
rs
ity
 A
t 0
0:
38
 1
6 
M
ay
 2
01
5 
(P
T
)
