Electronic copy available at: http://ssrn.com/abstract=1522196
 
 
 
 
 
 
 
 
Brooklyn Law School Legal Studies 
 Research Papers  
Accepted Paper Series 
 
Research Paper No. 178       December 2009 
 
 
 
 
 
 
The Expert Linguist Meets the Adversarial System 
 
Lawrence M. Solan 
 
 
 
 
 
 
This paper can be downloaded without charge from the 
Social Science Research Network Electronic Paper Collection: 
http://ssrn.com/abstract=1522196 
Electronic copy available at: http://ssrn.com/abstract=1522196
1 
 
To appear in The Routledge Handbook of Forensic Linguistics (Malcolm Coulthard and Alison 
Johnson, eds.)(forthcoming 2010) 
 
The Expert Linguist Meets the Adversarial System 
Lawrence M. Solan 
 
SCIENCE IN THE ADVERSARIAL SYSTEM 
The academic world and the world of litigation produce an awkward mix.  Lawyers are in the 
business of winning their cases.  Academics are in the business of engaging in disinterested 
research in an effort to uncover truths.  Academics, including those who work in the „hard 
sciences,‟ are accustomed to such tasks as evaluating competing theories, each of which has its 
own strengths and weaknesses.  Criteria of evaluation generally include both descriptive and 
explanatory adequacy and sometimes such things as Occam‟s Razor and other measures of 
parsimony and elegance.  In linguistic theory, for example, competing syntactic accounts are 
frequently judged on the breadth of the phenomena they are able to explain without resort to ad 
hoc solutions. The more elegant solution that covers more ground wins.  In this realm, 
uncertainty is the norm.  Those engaged in scientific inquiry do not close up shop once they have 
achieved some progress.  Rather, they continue their explorations, often revising (and sometimes 
even discarding) earlier hypotheses as new data and new explanations come to light. 
The legal system also is designed to uncover truths.  But, in places that employ an 
adversarial system, it does not do so by conducting disinterested research, but rather through the 
vigorous presentation of evidence slanted toward different positions.  The assumption – more a 
matter of faith – is that the better sets of facts, arguments and theories presented in the court 
room will rise to the top, and that thereby the quest for truth will be served.  (See Landsman 
Electronic copy available at: http://ssrn.com/abstract=1522196
2 
 
1984).  For this reason, during the litigation process, lawyers are likely to exploit the uncertainty 
of opposing experts.  This can lead to serious discomfort when an expert accustomed to living 
with a level of uncertainty as a professional matter finds himself the subject of ridicule in the 
courtroom.  (See, e.g., Shuy 2006; Coulthard and Johnson 2009 for discussion).   
Philosopher/legal scholar Susan Haack, drawing on the work of Peirce, comments on the 
difference between scientific and legal inquiry: 
Distinguishing genuine inquiry, the real thing, from pseudo-inquiry or “sham 
reasoning,” C.S. Peirce - a working scientist as well as the greatest of American 
philosophers - wrote that “the spirit ... is the most essential thing - the motive”; that 
genuine inquiry consists in “actually drawing the bow upon truth with intentness in the 
eye, with energy in the arm.” For the same reason, I am tempted to write of advocacy 
"research" (in scare quotes); for it is something of a stretch to call advocacy research 
“research” at all. Advocacy “research” is like inquiry insofar as it involves seeking out 
evidence. But it is part of an advocacy project insofar as it involves seeking out evidence 
favoring a predetermined conclusion; and it is undertaken in the spirit, from the motive, 
of an advocate. In short, it is a kind of pseudo-inquiry. (Haack 2008: 1071). 
Making matters worse, lawyers are not required to be sincere in their attacks.  (Post 1987, 
Solan 2008).  They are not permitted to lie outright.  Nonetheless, the lawyer who believes the 
opposing expert‟s position to be valid remains obliged to find holes in the analysis and to exploit 
them vigorously.  If a coroner makes a computational error in computing the time of death, the 
lawyer will – in fact, must – take maximum advantage of the mistake even if the lawyer believes 
the time of death in the report to be correct.  At least that is so in the United States. 
3 
 
It is somewhat ironic that scientific investigation accepts more uncertainty than does the 
legal process, since it is the legal system‟s assumption that scientific knowledge is crisp and 
factual that makes it attractive to the legal system in the first place.  (See Berger and Solan 2008 
for discussion).  Nonetheless, that is often the case.  The imperfect match between scientific 
inquiry and the structure of the adversarial system presents a challenge for the expert witness.  
One lawyer wants the witness to act as a good team player, while the other attempts to rip him or 
her to shreds.  In this article, I bring to the attention of the forensic linguistic community a 
number of issues that have been raised more generally about expert evidence in the adversarial 
system that might be important to the field as it develops.   
A large body of literature demonstrates that in interpreting facts, people (including 
experts) tend to be biased toward confirming the result that they have already reached. People are 
aware of such tendencies in others, but deny it in themselves.  This means that even experts with 
high moral integrity will tend to cast their conclusions in a way that is helpful to the position they 
have espoused.  Moreover, people tend to view positive results without regard to underlying base 
rates.  This has led to the acceptance of forensic identification techniques that are not adequately 
grounded in science.  The proliferation of DNA analysis in legal settings makes the absence of 
such analysis in other forensic sciences particularly salient. 
The legal system has reacted to these problems in different ways.  In the United States, 
there has been a growing emphasis on the importance of valid and replicable methodology in 
court, as evidenced by the United States Supreme Court‟s 1993 Daubert decision and it progeny.  
The UK, while also considering a move in that direction (see Law Commission 2009), has 
required experts to certify that they understand their first obligation is to be straightforward and 
disinterested.  Both are positive developments. 
4 
 
I begin by discussing some of the cognitive biases that have been discussed in the recent 
psychological and legal literature.  Awareness of them can both help the expert linguist to 
understand some of the pitfalls of entering the all-or-nothing fray of litigation, and further help to 
understand the intensity of the current push toward developing reliable methodologies on which 
experts can base their testimony.  I then turn to developments in the legal system directed at 
addressing some of these problems.  Finally, this chapter turns to forensic linguistics in 
particular, and discusses various standards that the field may adopt on its own behalf. 
THE CHALLENGE FACING FORENSIC IDENTIFICATION SCIENCES 
The Need to Make Forensic Science More Scientific 
Forensic identification techniques have been under serious scrutiny and attack in the United 
States during the past decade.  To take a dramatic example, until recently bullet lead analysis had 
been a staple of law enforcement agencies in criminal cases.  For more than thirty years, 
analysts, in particular from the FBI, would testify that the lead from a questioned bullet and that 
from a reference box of ammunition associated with the defendant, “were analytically 
indistinguishable,” “came from the same batch,” “were consistent with their having come from 
the same box of ammunition,” and so on.  (Giannelli 2007: 200).  Disturbingly, difficulty in 
determining how to couch conclusions about the firmness of an identification shows itself in 
forensic linguistic identification as well.  But more disturbingly, bullet lead analysis has now 
been shown to be without scientific basis.  In 2004, the National Research Council (“NRC”), 
which is the research arm of the National Academies (formerly the National Academy of 
Science) issued a devastating report.  (NRC 2004).  Among its conclusions was that the output 
from a single “melt” of lead   “can range from the equivalent of as few as 12,000 to as many as 
5 
 
35 million 40-grain, .22 caliber longrifle bullets.” (p. 6).  Courts began jumping ship, and in 2005 
the FBI abandoned the procedure.1  Prior to that, the analysis was used in many criminal trials 
involving firearms, including death penalty cases.  The recognition that this forensic procedure is 
not provably reliable has had its consequences.  In 2008, a Florida court vacated the conviction 
of a man who had spent ten years in prison for killing his wife based on the discredited 
technology. The legitimacy of many other convictions has been brought into question.
 2  
Lead bullet analysis is not alone.  Even fingerprint identification, long considered an 
airtight method, has been questioned in recent years.  Significantly, the history of fingerprint 
identification reveals that it developed in the absence of studies demonstrating their uniqueness.  
That has always been taken as a matter of faith.  (Mnookin 2001).  This is not to say that 
fingerprinting has been unsuccessful as a tool for law enforcement.  The Innocence Project 
announces on its web page that there have been 240 post-conviction DNA exonerations in the 
U.S. courts of people who had been convicted of crimes, but of those, only two involved 
incorrect testimony about fingerprints.  In one case, an analyst testified that the comparison was 
inconclusive when in fact, it had excluded the defendant; in the other, the crime lab compared 
two sets of the defendants‟ known prints  to each other, rather than comparing the prints at the 
crime scene to the defendant‟s known prints.3  Moreover, while few disagree that fingerprint 
comparison can be very accurate in most cases (but see Cole 2004), fingerprints in forensic 
settings are often both partial and degraded and we do not know the rate at which accuracy and 
consensus diminish as the amount of information is decreased.  A case in point is the FBI‟s 
incorrect fingerprint identification of an American Muslim as the Madrid bomber. (see Dror, 
Charlton & Péron 2005). 
6 
 
 The same holds true for handwriting analysis, which has received particularly brutal 
treatment in the literature on the admissibility of expert testimony.  (See Risinger and Saks 1996, 
Mnookin 2001, but see Moeoenssens‟ 1997 defense of the field).  Here, again, the issue is not 
whether document examiners ever get it right.  Of course they do.  Rather, the issue is that 
handwriting analysis was not put to the test to determine its limits.  Even when a document 
examiner has been well-trained and is a person of integrity, absent the implementation of 
techniques that have been validated, we do not know where an expert‟s expertise begins and 
ends, other than as a matter of trust.       
 More broadly, in 2009, the National Research Council of the National Academies came 
out with a report on the status of forensic identification.  Entitled Strengthening Forensic Science 
in the United States: A Path Forward (NRC 2009), among its findings were the following: 
 Two very important questions should underlie the law‟s admission of and 
reliance upon forensic evidence in criminal trials:  (1) the extent to which a 
particular forensic discipline is founded on a reliable scientific methodology that 
gives it the capacity to accurately analyze evidence and report findings and (2) the 
extent to which practitioners in a particular forensic discipline rely on human 
interpretation that could be tainted by error, the threat of bias, or the absence of 
sound operational procedures and robust performance standards. … 
Unfortunately, these important questions do not always produce satisfactory 
answers in judicial decisions pertaining to the admissibility of forensic science 
evidence proffered in criminal trials.  (NRC 2009: S-7). 
7 
 
The report, thus, criticizes both the various forensic disciplines for not policing themselves 
adequately, and the courts for falling asleep at the job and not performing their gatekeeping 
function with adequate standards.  What should be done?  The report suggests:   
A body of research is required to establish the limits and measures of performance 
and to address the impact of sources of variability and potential bias.  Such 
research is sorely needed, but it seems to be lacking in most of the forensic 
disciplines that rely on subjective assessments of matching characteristics.  These 
disciplines need to develop rigorous protocols to guide these subjective 
interpretations and pursue equally rigorous research and evaluation programs.  
(NRC 2009: S-6).   
Cognitive Biases in Forensic Science 
What are the biases to which forensic scientists are so susceptible?  They are the same biases to 
which scientists of all sorts may succumb.  First among them are observer effects, in particular 
confirmation bias.  Observer effects refer to the long-recognized fact that “context and 
expectations influence an individual's perceptions and interpretations of what he observes.” 
(Risinger et al. 2002: 12).  Recognition of this potential drives a great deal of methodology in 
science, especially in areas of medical research where double-blind studies are the norm.  Even 
when patients are randomly selected for participation, it is only when both the doctor 
administering the treatment and the patient receiving it are unaware of whether the patient is 
receiving the experimental treatment or is a member of the control group receiving a placebo that 
the results of clinical trials are deemed reliable.  The concern is that otherwise subtle, even 
unconscious cues may lead to distortions in the results. 
8 
 
 In the case of forensic experts one of the most important cues is prior knowledge of the 
result that the party engaging the expert would like them to reach. This leads to confirmation 
bias, the “unwitting selectivity in the acquisition and use of evidence” in which people are likely 
to engage. (Nickerson 1998: 175).  In fact, even when we have no stake in the outcome, we tend 
to look at data selectively to confirm tentative conclusions we have reached.  In a famous study 
using a card game, Wason and Johnson-Laird (1972) found that people quickly see the 
significance of information that may prove them right or wrong.  In contrast, most people ignore 
evidence that can disconfirm a working hypothesis but cannot otherwise serve to strengthen the 
hypothesis. 
 Once we have taken a position on a matter, information that supports that position 
becomes more salient, and information that tends to disconfirm it becomes less so. (Nickerson 
1998, Simon (2004).  To take a classic example from the psychological literature, Darley and 
Gross (1983) showed two groups of subjects a videotape of a child taking an academic test.  One 
group of subjects was told that the child came from a high socioeconomic background, the other 
group told that the child came from a low socioeconomic background.  They were then asked to 
evaluate the child‟s academic ability based on what they observed of the child‟s behavior while 
she took the test.  The results were dramatic.  Those who were told that the child came from a 
wealthy background rated her ability as greater than did those who were told she was 
economically disadvantaged.  Both groups supported their ratings by referring extensively to 
evidence from the videotape. 
 Real-life experiences abound  It happens, for example, when the police, certain that they 
have apprehended the guilty party and acting in good faith, ignore evidence of another‟s guilt 
and of their suspect‟s innocence as they build their case.  Malcolm Coulthard discusses a number 
9 
 
of such cases in his writings.  (see, e.g.,  Coulthard 2004).  In one, convinced that they had 
apprehended four men who killed a 13-year old newspaper delivery boy, the police extracted a 
confession from one of them – Patrick Molloy. However, the police denied that the language 
contained in the confession was language that they had suggested to him and that he had 
accepted under duress, as he alleged.  The four were convicted, and Coulthard was consulted on 
appeal, at which time he found that the transcript of a police interview to be unrealistically 
identical to that of a statement attributed to him.  The Crown conceded error during the appeal 
process, based on other evidence of the defendants‟ innocence.  Such things happen when the 
police are so convinced that they have the right person that they feel justified in cutting corners.  
The doctoring of an interview record is misconduct, but the motivation to do so comes largely 
from confirmation bias.  
Confirmation bias also occurs when an expert witness focuses in a report on the 
information that bolsters the position taken, and understates or ignores information that would 
tend to lead to a contrary position.  Dror, Charlton and Péron (2005) presented five experienced 
fingerprint analysts with separate pairs of fingerprints for comparison.  In each case, the pair was 
one that the analyst had himself identified as a match sometime earlier in the ordinary course of 
business.  The experimenters also showed each of these five pairs of prints to independent 
examiners, who agreed that each pair of prints constituted a matched set.  However, a 
confederate told each of these experts that the questioned prints were the ones erroneously 
identified by the FBI as a match in the Madrid bombing case.  The experts were then told to 
ignore this contextual information in conducting their analysis.  The result was that three of the 
five experts found no match, one said he could not decide, and only one came to the same 
conclusion he had reached earlier: that there was a match.   
10 
 
Making things worse, not only are we subject to this bias, but we each have a “bias blind 
spot.”  (Pronin, Lin and Ross 2002).   That is, we have a propensity to think that our own beliefs 
are objective, while the beliefs of others are colored by various biases that influence them.  Even 
when such biases are brought to our attention, we tend to recognize them as applying to others – 
not to ourselves. In other words, we see bias in others, analytical crispness in ourselves.  Pronin 
and Kugler (2007) attribute this asymmetry to the fact that “people over-value thoughts, feelings, 
and other mental contents, relative to behavior, when assessing their own actions, motives, and 
preferences, but not when assessing others‟.” (2007: 566). They call this the “Introspection 
Illusion.”  Their studies first confirmed the bias blind spot.  When, for example, Harvard students 
were told that some people tend to be biased toward self-serving views of their academic or job 
performance, they attributed this bias more to other, similarly-situated students than to 
themselves.  They further said that they judged themselves more by evaluating their own 
thoughts and motives, and judged others based more on their actual behavior.  Whether judging 
themselves or others, the more they relied on thoughts and motives, the less bias they found.  The 
more they relied on behavior, the more bias they found.  Significantly, when participants were 
told in advance that relying on introspection instead of evaluating their own behavior can lead 
them to a biased assessment, they took heed and were no longer subject to the bias blind spot.  
(2006: 575).  This finding may have significant ramifications in the forensic arena.   
JUDICIAL REACTIONS TO EXPERT EVIDENCE IN THE US AND THE UK 
We have now identified two problems facing the expert linguist:  the brutality of the adversarial 
system, including snide and personal attacks on individuals working within standard scientific 
paradigms, and a broad concern that the forensic identification sciences lack adequate scientific 
foundation.  This section deals with how the courts have grappled with the second of these 
11 
 
issues.  The next section will deal with the tension between the two problems, and how forensic 
linguistics might develop to address them jointly.  
Linguistic expert testimony is widely accepted in both English and American courts.  
However, the courts in the US and the UK have engaged different strategies to deal with the need 
for ensuring reliable expert testimony.  While American courts have focused more on the need 
for valid and reliable methodology, British courts have concentrated on requiring experts to 
certify that they have conducted their analysis in a neutral, disinterested manner.  Both 
approaches have something to offer in response to the issues raised above. 
The Daubert Standard in American Courts: The Judge as Gatekeeper 
For most of the twentieth century, American courts admitted scientific evidence if it had “gained 
general acceptance in the particular field in which it belongs.”  The standard was set in a 1923 
court of appeals case, Frye v. United States, a case that involved a lie detector test that measured 
systolic blood pressure.  The problem with the Frye test, however, is that it is subject to 
manipulation by changing the definition of “the particular field in which it  belongs.”  Among lie 
detector analysts, the device in question in Frye might be perfectly acceptable.  Among social 
scientists and medical experts, waiting for the results of validation testing, it might be 
unacceptable.   Moreover, as noted at the beginning of this article, science is often not about 
certainty, but rather about controversy.  The “right” theory may have been articulated, but is not 
yet generally accepted, whether because of the sociology of the scientific community, or because 
the theory is in development and has not yet been shown to be able to handle crucial cases and to 
explain apparent counterexamples.   
12 
 
 These problems with the Frye standard were recognized in 1975 when the Federal Rules 
of Evidence were first adopted.  The standard under Rule 702 as originally enacted was that 
expert evidence should be admitted “if it will assist the trier of fact to understand the evidence or 
to determine a fact in issue.”  It was not clear from the rule‟s language, however, whether this 
standard was intended to replace Frye, or merely to explain the goal of the Frye standard. 
 The Supreme Court of the United States answered that question in three cases decided in 
the 1990s, which have come to be called the Daubert trilogy.  The first case, Daubert v. Merrell 
Dow Pharmaceuticals, Inc., was decided in 1993.  The issue there was whether Bendectin, an 
anti-nausea drug taken during pregnancy, caused birth defects in the plaintiff‟s children.  Most of 
the scientific literature said that it did not, but the plaintiff wished to have an expert testify to 
challenge the scientific literature and to discuss animal studies which suggested that Bendectin 
might indeed cause birth defects in children.   
Ultimately, the Supreme Court ruled that the expert testimony was not admissible 
because it lacked the indicia of scientific validity.  In determining whether proferred testimony is 
scientifically valid, the standard would no longer be whether it was generally accepted by the 
scientific community. Rather, the testimony must have a “grounding in the methods and 
procedures of science.” (p. 590).  This grounding may be evidenced by four nonexclusive 
criteria: whether the theory offered has been tested; whether it has been subjected to peer review 
and publication; the known rate of error; and whether the theory is generally accepted in the 
scientific community.  (p. 593).      Note that the fourth criterion is the Frye standard, which has 
now become one of a number of nonexclusive factors that a court will consider. 
13 
 
The second case in the Daubert trilogy, General Electric Company v. Joiner, concerned 
the standard of review for appellate courts of Daubert decisions made at trial.  The Court held 
that rulings about the admissibility of expert testimony should be overturned only if the trial 
court abused its discretion.  This is a very lax standard, and it means in essence that the decisions 
of trial judges to admit or reject expert evidence will rarely be reviewed seriously, and even more 
rarely reversed. 
Finally, in Kumho Tire Company v.Carmichael, the Supreme Court held that the Daubert 
approach applies not only to scientific testimony, but also to experts who testify based on their 
experience.  The expert in that case was called to testify on the cause of tire damage based on his 
experience in the tire industry.  The Court held that his off-the-cuff opinion based upon 
experience that cannot be tested did not meet evidentiary standards.  The determining fact is 
whether the expert “employs in the courtroom the same level of intellectual rigor that 
characterizes the practice of an expert in the relevant field.”  (p. 152). 
As a result of these three cases, the Federal Rules of Evidence were amended to permit 
expert testimony if it will “assist the trier of fact to understand the evidence or to determine a fact 
in issue,” and if 
(1) The testimony is based upon sufficient facts or data, 
(2) The testimony is the product of reliable principles and methods, and 
(3) The witness has applied the principles and methods reliably to the facts of the case.  
(Rule 702, Fed R. Evid.). 
14 
 
 It should be noted that all three cases in the Daubert trilogy were civil cases in which an 
individual was suing a corporation, and in all three, the Supreme Court held that the expert‟s 
proffered testimony did not pass muster.  It was the legal academic community, and to some 
extent criminal defense lawyers, who subsequently argued that the Daubert approach should 
apply to the forensic identification sciences, which were inadequately validated notwithstanding 
their claims.  (See, e.g., Risinger and Saks 1996).  It is still not at all clear that American courts 
apply these principles evenhandedly in civil and criminal cases alike (Risinger 2000), or for that 
matter, that the actual rulings of courts differ significantly depending upon whether a jurisdiction 
applies the Frye standard or the Daubert standard.  (Cheng and Yoon 2005).  Nonetheless, 
Daubert has colored the debate about forensic testimony and led to questions about its reliability 
absent validation studies conducted in a scientific manner. 
Expert Certification of Neutrality in the UK  
The UK has taken a somewhat different approach (see Law Commission 2009), although, at the 
time of writing, Daubert-like standards are under consideration there as well.  Rather than 
focusing on the methodology, the UK has traditionally focused on the credentials and integrity of 
the expert.  Aware of the temptation for experts to present biased evidence, Civil Procedure 
Rules have been enacted requiring experts to affirm that they are acting in a neutral manner.  The 
rules state explicitly that 
1. It is the duty of an expert to help the Court on matters within his expertise. 
2. This duty overrides any obligation to the person from whom he has received 
instructions or by whom he is paid. (Civil Procedure Rule 35.3).    
15 
 
Expert reports must contain a statement that the expert understands his duty to the court and that 
he has complied with that duty.  (Civil Procedure Rules 35.10).      
 Similarly, Appendix 11 to the Admiralty and Commercial Courts Guide imposes 
neutrality on experts.  Because the material may not be familiar to those who do not work within 
the British courts, it is worth quoting in full: 
1.  It is the duty of an expert to help the court on the matters within his expertise: rule 
35.3(1). This duty is paramount and overrides any obligation to the person from whom 
the expert has received instructions or by whom he is paid: rule 35.3(2).   
2.   Expert evidence presented to the court should be, and should be seen to be, the 
independent product of the expert uninfluenced by the pressures of litigation.   
3.   An expert witness should provide independent assistance to the court by way of 
objective unbiased opinion in relation to matters within his expertise. An expert witness 
should never assume the role of an advocate.   
4.   An expert witness should not omit to consider material facts which could detract from 
his concluded opinion.   
5.   An expert witness should make it clear when a particular question or issue falls 
outside his expertise.   
6.   If an expert's opinion is not properly researched because he considers that insufficient 
data is available, this must be stated in his report with an indication that the opinion is no 
more than a provisional one.   
16 
 
7.   In a case where an expert witness who has prepared a report is unable to confirm that 
the report contains the truth, the whole truth and nothing but the truth without some 
qualification, that qualification must be stated in the report.   
8.   If, after exchange of reports, an expert witness changes his view on a material matter 
having read another expert's report or for any other reason, such change of view should 
be communicated in writing (through the party's legal representatives) to the other side 
without delay, and when appropriate to the court. 
Admirably, the goal is to reduce the adversarial nature of scientific debate in the litigation 
context.  No doubt these standards accomplish that goal to some extent.  However, given both 
the pressure placed on experts and the bias blind spot discussed earlier, it is not likely to 
accomplish that goal up to the level of purely disinterested scientific standards.  Experts will 
continue to be tempted to write short, uninformative reports in order to keep the opposing party 
from preparing a rebuttal adequately, present few if any counterexamples to their analysis in 
their main reports; or to be as helpful to opposing parties during cross-examination as they would 
be if they believed themselves to be entirely neutral; or to raise issues on their own that might 
compromise their party‟s position, even if they would have done so in an academic climate.  As 
Sanders (2007: 1558) notes: 
 [W]hen [experts] do fail to present adequate justification for a belief, often it is 
not because they fail to present the best case for a position but that they fail to tell 
the "whole truth" about their belief and present with equal force the evidence for 
and against it. 
 
17 
 
Thus, it should not be surprising that the UK is considering a move toward focusing on valid 
methods. 
THE DIRECTION OF FORENSIC LINGUISTICS 
I have identified two problems facing the linguist who ventures into the world of litigation:  One 
is a problem that experts have with the legal system – its intolerance of uncertainty in scientific 
inquiry notwithstanding that scientists accept the fact that the current best account may not 
ultimately survive further the test of time.  This, combined with the aggressive advocacy of the 
adversarial system, can lead lawyers to ridicule even experts who are prominent researchers in 
their fields.  The result of this hostility is a reluctance on the part of many top scholars to 
participate in the system at all.  (See Coulthard and Johnson 2007 for discussion).  The second 
problem that the legal system has with experts is that the experts themselves provide inadequate 
protection against bias by failing to develop methods that have been independently validated.  
This is the main thrust of the National Research Council (2009) report on forensic science in the 
United States. 
 I would like to argue here that in fact these are the same problem, at least in large part.  
Fixing the second will reduce the severity of the first in that the best way for an expert to avoid 
bias and to reduce the stress associated with defending one‟s professional opinion in court is to 
employ reliable methods that have been proven valid.  A second, somewhat less effective 
approach is for experts to submit to proficiency testing.  This is not as good as the first method, 
because even proficient experts can succumb to bias in a forensic setting.  Nonetheless, I explore 
the benefits of proficiency testing below. 
Developing Valid Methodology 
18 
 
The basic concern in developing methods that will be acceptable in court and meet the standards 
of normal science is to develop and test those methods outside the litigation context.  Not only is 
litigation-driven research more prone to bias, but it is less highly valued by the courts for that 
reason.  In fact, as Haack (2008) points out, once Mrs. Daubert‟s case was sent back to the lower 
court for additional proceedings, the judge there commented on the reduced reliability of 
scientific evidence gathered for the purpose of the litigation itself.  Judge Kozinski commented 
there:  “[I]n determining whether proposed expert testimony amounts to good science, we may 
not ignore the fact that a scientist‟s normal workplace is the lab or the field, not the courtroom or 
the lawyer‟s office.” (Daubert 2,  1316-17 n.3)    He further contrasted the increased likelihood 
of bias when the result is tied to remuneration, with independent research conducted as normal 
science, which carries its own indicia of reliability. 
 The development of valid methodology has produced positive results in other forensic 
fields.  For example, as mentioned above, handwriting analysis has been in ill repute in the 
United States because its inability to describe a valid method with provable rates of accuracy.  In 
part because of that criticism, the government has funded research that area, resulting in 
improved technology that has been accepted under Daubert analysis.  In a 2002 case, United 
States v. Prime, the trial court summarizes some of that progress – including a greater 
understanding of the rate of error – in admitting the testimony of a handwriting expert who had 
been involved in these improvements to that field.  Academic critics have also begun to 
recognize this progress. (See, e.g., Giannelli 2003: 8-9). 
How such research is to be conducted in the linguistic arena differs from one subfield to 
another.  The forensic phonetic literature, for example, is replete with studies of what 
circumstances make it easier or harder to recognize a speaker by his or her voice.  (See Yarmey 
19 
 
forthcoming for a good summary).  Moreover, some researchers are developing automated 
systems that are being tested for rates of error in terms of both misses (failure to identify) and 
false alarms (false identification).   (See Solan and Tiersma 2005 for discussion of some of these 
developments.)  At the same time, there has been some movement toward the proficiency testing 
of those phoneticians who use both aural and acoustical information to form judgments, a trend 
to which I return below. 
 When it comes to authorship identification, some researchers have attempted to identify 
criteria which, taken in combination, can diagnose both authorship and non-authorship.  Chaski 
(2005), for example, uses aspects of punctuation, marked syntactic structures and word length in 
combination.  Her results are impressive, and she has been permitted to testify in cases after 
Daubert scrutiny, although the process has still not beentested for validity and reliability through 
independent means.  My point here is a basic one: The research upon which expert testimony is 
based is best conducted in the lab, outside the context of a particular dispute, as Judge Kozinski 
prescribes.  Other researchers have employed similar research methods, using criteria for 
identification that differ from Chaski‟s, sometimes using sophisticated mathematical modeling.  
(See, e.g., Stamatatos 2006).     
Recent research into the conventions of text messaging show promise for research along 
these lines.  Grant (this volume) discusses cases in which two possible authors of a questioned 
text message use in general quite different styles of abbreviation and ellipsis in their texting.  
Once pointed out, one‟s intuition is that such differences will predict authorship, although 
research confirming this has not yet been published.  Grant himself suggests some ways in which 
such research could be structured.  Olsson (2003) observes that while some texters are entirely 
consistent in their use of stylistic conventions, some are not, and the amount of inconsistency varies from 
20 
 
person to person.  This diversity should be taken into account in determining – based on research from 
corpora that are already available or which are gathered for research purposes – how predictive of co-
authorship and non-authorship these conventions are.   
In other subfields of forensic linguistics, the methodology may require only that the types 
of materials examined and the arguments made be standardized to the extent that a consensus 
develops about what might be useful to the legal community.  Linguists frequently testify in 
trademark disputes, for example.  Those who conduct frequent analyses, for the purpose of 
determining the confusability or the strength of a mark, might publish their approaches in order 
to set best practices for the field.  This, at least to some extent, is occurring.  (See Shuy 2002, 
Butters 2008). 
Proficiency as a Substitute for Methodology 
Those who have practiced in the area of forensic linguistics, especially in the area of authorship 
identification, might respond to the call for methodology as follows:  “I have been doing this for 
a long time, and I am very good at it. I cannot tell in advance exactly which features in a 
particular case will be diagnostic of authorship, so requiring that I develop a methodology that is 
tightly defined will require me to ignore data that might be important in an individual case.”  In 
discussing some prominent cases involving authorship identification, Peter Tiersma and I (Solan 
and Tiersma 2005) note that the absence of established methods might lead some insightful 
analysis to remain unacceptable in the courtroom. We therefore suggest that research projects be 
initiated to discover and validate reliable methods that will stand up to evidentiary scrutiny.   
Perhaps, however, proficiency testing, if done properly could serve as an intermediate 
level of validation while a field conducts research into  replicable methods.  In a recent lecture, 
21 
 
Professor Jennifer Mnookin has referred to this as the “black box” approach to forensic 
identification, and argues that it might be useful in some instances.  The concern is that in some 
instances, Daubert may be causing us to throw the baby out with the bathwater, rejecting skilled 
diagnosis based on experience.  (See Sanders 2001 for discussion).  Medical diagnosis is in part 
an art in which the most skilled diagnosticians are unable to articulate what separates them from 
the rest of the pack.  (See Groopman 2007).  It would not be surprising if practitioners of forensic 
linguistic identification also developed skills that more than meet the standard of being helpful to 
the trier of fact. 
Valid proficiency testing is difficult to accomplish.  The biggest problem is developing 
materials that are relevant to real-life forensic problems.  For example,  when a document has 
only two possible authors, rejecting one of them is sufficient to establish the other as the author 
(assuming only one person wrote the document).  In other cases, however, the question is 
whether a particular suspect wrote a document, and there is no information about who else might 
have written it otherwise.  How many other potential authors should the expert be able to reject 
before reaching an opinion that the suspect is the likely author?  These are difficult questions that 
must be resolved before proficiency testing can be designed in a meaningful way.   
Notwithstanding these difficulties, information about a practitioner‟s proficiency might 
be useful to the legal system, especially during the development of validated methodologies.  In 
fact, the success rate of proficient practitioners might be compared with that of other methods, 
particularly automated ones.  Cambier-Langeveld (2007) conducted a study in which she pitted 
phoneticians using the auditory-acoustic method against semi-automatic and automatic speaker 
recognition systems.  The result was that the machines produced fewer false alarms than the 
phoneticians, but at the same time failed to make some correct identifications that the 
22 
 
phoneticians were able to make.  Some phoneticians were extremely good and outperformed the 
machines, but disturbingly, the phoneticians varied in their level of skill, highlighting the 
importance of proficiency testing at the minimum. Absent that testing, the individual who goes 
before the jury with confidence and charisma may be the one who prevails, regardless of actual 
skill. 
Even experts who are proven to be good at what they do through proficiency testing will 
be subject to bias when they conduct their analysis in a litigation context.  Thus, I do not 
advocate this approach as a long-term goal in the development of forensic linguistics.  
Nonetheless, it might play a role in preserving insight while the field moves ahead. 
CONCLUSION 
I have explored here two related, but seemingly distinct problems that arise when the expert 
linguist enters the world of litigation: the legal system is unrealistic about what science can do, 
and the forensic community has not adequately developed valid and reliable methods.  I have 
attempted here to show how these problems can be solved together, through a single approach to 
methodology, and have suggested that proficiency testing might bridge the gap in the short term.           
      It will be up to both the academic and the forensic linguistics communities to move 
the field ahead in these directions.  To do so is particularly difficult given the dual role that many 
play as both academics and consultants. Yet some thirty years ago, it was the academic linguists 
and phoneticians who demanded that forensic use of spectrograms (voice prints) be used 
judiciously since they had not been proven accurate in forensic settings.  The field of forensic 
linguistics remains capable of moving itself forward in the early part of the twenty-first century. 
  
23 
 
Notes 
1 
Eric Lichtblau, F.B.I. Abandons Disputed Test for Bullets from Crime Scene, N.Y. Times, Sept. 2, 2005, p. A2. 
2 
Judge Overturns Jimmy Ates‟ Conviction: Use of Junk Science Leads to Release, press release of the Florida 
Innocence Project, available at   http://flainnocence.blogspot.com/search/label/Jimmy%20Ates, visited July 16, 
2009. 
3  
http://www.innocenceproject.org/docs/DNA_Exonerations_Forensic_Science.pdf, visited June 21, 2009. 
  
Further Readings 
Darley, John M. and Paget H. Gross (1983)  A Hypothesis-Confirming Bias in Labeling Effects. 
Journal of Personality and Social Psychology 44: 20-33. 
Giannelli, Paul C. (2007)  Wrongful Convictions and Forensic Science: The Need to Regulate 
Crime Labs. North Carolina Law Review 86:163-235. 
National Research Council of the National Academies (2009) Strengthening Forensic Science in 
the United States: A Path Forward.  Washington, D.C.: The National Academy Press. 
Pronin, Emily and Matthew Kugler (2007) Valuing Thoughts, Ignoring Behavior: The 
Introspection Illusion as a Source of the Bias Blind Spot. Journal of Experimental Social 
Psychology 43: 565-78. 
Sanders, Joseph (2007) Expert Witness Ethics.  Fordham Law Review 76: 1539-84. 
Shuy, Roger (2006) Linguistics in the Courtroom: A Practical Guide. Oxford: Oxford University 
Press. 
 
 
24 
 
References 
Berger, Margaret A., and Lawrence M. Solan (2008)  The Uneasy Relationship between Science 
and Law: An Essay and Introduction.  Brooklyn Law Review 73:847-54. 
Butters, Ronald (2008) A Linguistic Look at Trademark Dilution.  Santa Clara Computer and 
High Tech Law Journal 24: 507-519.  
Cambier-Langeveld, Tina (2007) Current Methods in Forensic Speaker Identification: Results of 
a Collaborative Exercise.  International Journal of Speech, Language and the Law14: 223-43. 
Chaski, Carole (2005)  Who‟s At the Keyboard? Authorship Attribution in Digital Evidence 
Investigations.  International Journal of Digital Evidence 4: 1-13.  
Cheng, Edward K. and Albert H. Yoon (2005) Does Frye or Daubert Matter? A Study of 
Scientific Admissibility Standards.  Virginia Law Review 91:471-512. 
Cole, Simon A. (2004)  Grandfathering Evidence: Fingerprint Admissibility Rulings from 
Jennings to Llera Plaza and Back Again. American Criminal Law Review 41: 1189-1276. 
Coulthard, Malcolm (2004) Author Identification, Idiolect, and Linguistic Uniqueness.  Applied 
Linguistics 25: 432-47. 
Coulthard, Malcolm, and Alison Johnson (2007) An Introduction to Forensic Linguistics: 
Language in Evidence.  Oxon: Routledge. 
Darley, John M. and Paget H. Gross (1983)  A Hypothesis-Confirming Bias in Labeling Effects. 
Journal of Personality and Social Psychology 44: 20-33. 
Dror, Itiel E., David Charlton and Elisa Péron (2005) Contextual Information Renders Experts 
Vulnerable to Making Erroneous Identifications.  Forensic Science International  156: 74-78. 
Giannelli, Paul C. (2003) Admissibility of Scientific Evidence, Oklahoma City University Law 
Review 28: 1-15. 
Giannelli, Paul C. (2007)  Wrongful Convictions and Forensic Science: The Need to Regulate 
Crime Labs. North Carolina Law Review 86:163-235. 
Grant, Tim (this volume) Txt 4n6: Idiolect Free Authorship Analysis? 
Groopman, Jerome (2007) How Doctors Think.  New York: Houghton Mifflin. 
Haack, Susan (2008) What‟s Wrong with Litigation-Driven Science? An Assay in Legal 
Epistemology.  Seton Hall Law Review 38: 1053-83. 
Landsman, Stephen (1984) The Adversary System: A Description and Defense.  Washington, 
D.C.: American Enterprise Institute. 
25 
 
Law Commission (2009) The Admissibility of Expert Evidence in Criminal Proceedings in 
England And Wales.  Consultation Paper 190.  Available at 
http://www.lawcom.gov.uk/docs/cp190.pdf. 
Mnookin, Jennifer L. (2001) Fingerprint Evidence in an Age of DNA Profiling.  Brooklyn Law 
Review 67: 13-70. 
Mnookin, Jennifer L. (2001) Scripting Expertise: The History of Handwriting Identification 
Evidence and the Judicial Construction of Reliability. Virginia Law Review 87: 1723-1845. 
Moenssens, Andre A. (1997) Handwriting Identification Evidence in the Post-Daubert World. 
UMKC Law Review 66:251-343.  
National Research Council of the National Academies (2004) Forensic Analysis: Weighing 
Bullet Lead Evidence: Washington, D.C.: The National Academy Press . 
National Research Council of the National Academies (2009) Strengthening Forensic Science in 
the United States: A Path Forward.  Washington, D.C.: The National Academy Press. 
Nickerson, Raymond S. (1998) Confirmation Bias: A Ubiquitous Phenomenon in Many Guises. 
Review of General Psychology 2: 175-220. 
Olsson, John (2003) Preliminary Observations on Author Variation in Mobile Phone Texting. 
Unpublished ms., available through www.thetext.co.uk.  
Post, Robert (1987) On the Popular Image of the Lawyer: Reflections in a Dark Glass.  
California Law Review 75: 379-89. 
Pronin, Emily and Matthew Kugler (2007) Valuing Thoughts, Ignoring Behavior: The 
Introspection Illusion as a Source of the Bias Blind Spot. Journal of Experimental Social 
Psychology 43: 565-78. 
Pronin, Emily, Daniel Y. Lin and Lee Ross (2002) The Bias Blind Spot: Perception of Bias in 
Self and Others.   Personality and Social Psychology Bulletin 28: 369-81. 
Risinger, D. Michael (2000) Navigating Expert Reliability: Are Criminal Standards of Certainty 
Being Left on the Dock? Albany Law Review 64: 99-152. 
Risinger, D. Michael and Michael J. Saks (1996) Science and Nonscience in the Courts: Daubert 
Meets Handwriting Identification Expertise. Iowa Law Review 82: 21-74. 
Risinger, D. Michael, Michael J. Saks, William C. Thompson, and Robert Rosenthal (2002) The 
Daubert/Kumho Implications of Observer Effects in Forensic Science: Hidden Problems of 
Expectation and Suggestion.  California Law Review 90: 1-56. 
Sanders, Joseph (2001) Complex Litigation at the Millenium: Kumho and How We Know. Law 
and Contemporary Problems 64: 373-415. 
Sanders, Joseph (2007) Expert Witness Ethics.  Fordham Law Review 76: 1539-84. 
Shuy, Roger (2002) Linguistic Battles in Trademark Disputes. New York: Palgrave Macmillan.  
26 
 
Shuy, Roger (2006) Linguistics in the Courtroom: A Practical Guide. Oxford: Oxford University 
Press. 
Simon, Dan (2004) A Third View of the Black Box: Cognitive Coherence in Legal Decision 
Making. University of Chicago Law Review 71: 511-84. 
Solan, Lawrence M. and Peter M. Tiersma (2005) Speaking of Crime: The Language of Criminal 
Justice. Chicago: University of Chicago Press. 
Solan, Lawrence M. (2008) Talking Like a Person, Thinking Like a Lawyer (and vice versa).  In 
Martin Solly, Michelangelo Conoscenti and Sandra Campagna, eds., Verbal/Visual Narrative 
Texts in Higher Education. Bern: Peter Lang. 
Stamatatos, Efstathios (2006) Authorship Attribution Based on Feature Set Subspacing 
Ensembles. International Journal of Artificial Intelligence Tools: 20: 1-16.  
Wason, P.C. and P.N. Johnson-Laird (1972) Psychology of Reasoning: Structure and Content. 
Cambridge, Mass.: Harvard University Press.  
Yarmey, Daniel (forthcoming) Factors Affecting Lay Persons‟ Identification of Speakers.  In 
Lawrence M. Solan and Peter M. Tiersma (eds.), The Oxford Handbook of Language and Law. 
Oxford: Oxford University Press. 
 
Laws and Cases Cited 
Daubert v. Merrell Dow Pharmaceuticals, Inc., 509 U.S. 579 (1993). 
Daubert v. Merrell Dow Pharmaceuticals, Inc., 43 F.3d 1311, (9th Cir. 1993)(“Daubert 2”). 
Frye v. United States, 293 F. 213 (D.C. Cir. 1923). 
General Electric Company v. Joiner, 522 U.S. 136 (1997). 
Kumho Tire Company v.Carmichael, 526 U.S. 137 (1999) 
United States v. Prime, 220 F. Supp. 2d 1203 (W.D. Wash. 2002) 
 
Admiralty and Commercial Courts Guide, Appendix 11 (UK) 
Civil Procedure Rule 35.3 (UK) 
Federal Rules of Evidence, Rule 702 (US)      
 
 
