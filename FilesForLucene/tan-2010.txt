Authorship Identification for Online Text
Richmond Hong Rui Tan and Flora S. Tsai
School of Electrical & Electronic Engineering
Nanyang Technological University
Singapore
ta0002nd@ntu.edu.sg and fst1@columbia.edu
Abstract—Authorship identification for online text such as
blogs and e-books is a challenging problem as these documents
do not have a considerable amount of content. Therefore,
identification is much harder than other documents such as
books and reports. The paper investigates the choice of features
and classifier accuracy which are suitable for such texts.
Syntactic features are found to be good for large data sets,
whereas lexical features are good for small data sets. The results
can be used to customize and further improve authorship
detection techniques according to the characteristics of the
writing samples.
Keywords-authorship identification, authorship detection, au-
thorship attribution, data mining, classification, blog
I. INTRODUCTION
Authorship identification (detection or attribution) is a
process of identifying the most likely author of a disputed
or anonymous document, based on a collection of known
documents. As the usage of the Web has increased over
the past few decades, more and more activities can be
done in cyberspace. Dissemination of information can be
performed using various methods with tremendous speed.
For example, websites and RSS feeds are updated regularly
where users can get a wealth of information with minimal
effort. Social networking platforms like blogs and online
discussion forums facilitate the ease of networking and
information sharing with little or no cost [11].
Not only can information posted on the Web reach out
to millions of people in a very short time, users can also
provide information anonymously. Hence, those with ill in-
tentions can utilize these powerful dissemination capabilities
to convey and influence people with threatening entries and
misinformation, which can cause serious social implications
within any community. The question of who was the author
of a digital document is often raised in the investigations of
digital crimes. Only when the author is identified will one
be able to judge whether the information given is credible
or not.
Also, online documents are often disputed in copyright or
plagiarism cases. One can claim that the disputed document
originate from oneself. There are no authentication measures
in place to identify the rightful owners.
Similar to speech recognition where speakers can be
identified by their voices, authors can be identified by their
Figure 1. Methods of Authorship Identification
style of writing. The study of different styles of writing is
referred as stylometry. A variety of stylometric approaches
for identifying the author have been proposed [10]. There are
four main methods: lexical, syntactic, semantic and content-
specific methods, as shown in Figure 1.
Lexical methods are based on the word counts and dis-
tributions in the texts. Syntactic methods extract features
relating to frequencies of words and punctuation, which
show how the author arranges and structures sentences.
Other methods include semantic and content-specific fea-
tures focusing on the vocabulary and choice of specific
words used in the texts.
Most methods proposed in past papers combine more
than one stylometric feature to identify authors. Lexical
and syntactic features are usually used together for text
classifications. Some include frequency counts of function
words, for example: “an”, “therefore”, “the” and “while”.
However, there is no standard set of extracted features to be
used for classification. These function words can indicate the
author’s preference of words and unique style of writing.
The objective of this paper to extract stylometric features
of texts and attribute these texts to its authors, investigate
the best feature set suitable for authorship identification of
online text, and study the performance of different classifi-
cation methods. Unlike the other similar work using large
feature sets, this paper focuses on developing a classifying
method that utilizes a well-defined, smaller feature set
to yield the most accurate results. Features are furthered
categorized to test their impact on the final result.
Standard text documents such as e-books are used to
test the functionality of the algorithms, before online text
documents (blog entries and forum posts) are used. Visual
C++ and MySQL database are the software development
platforms used. Datasets to be used for training and testing
2010 International Conference on Cyberworlds
978-0-7695-4215-7/10 $26.00 © 2010 IEEE
DOI 10.1109/CW.2010.50
155
Dataset Length Description
Online e-books 25 - 50 chapters e-books are
downloaded from
http://www.gutenberg.org/
which contains documents
of different genre and
authors.
Blog Entries 10 - 300 entries Blogs are online platforms
which netizens used them
to convey messages and
discussion. Potential plat-
form for security con-
cern. Blogs used for test-
ing include the BizBlogs
Database [7].
Forum Posts 20 - 50 posts Dark Web is a collection
of forum posts in various
forums which are believed
to be related to terrorism
and other purposes.
Table I
EXPERIMENTAL DATASETS
are listed in Table I.
II. LITERATURE REVIEW
The question of accuracy from authorship identification
methods is raised when being used as digital evidence in
courts of law for criminal offenses and civil disputes. The
Institute for Linguistic Evidence [6] claimed that the method
it employs has obtained 95% accuracy and was successfully
used in several digital crimes. The method involves specific
topics documents like letters of apology or writing a terrible
experience, to be written by the authors which are then
be analyzed with lexical and semantic methods to produce
various statistical results. These results can then be complied
and compared with the disputed text to obtain the final result.
The problem of this method lies in the required specific
documents to be written by the authors. In cases of au-
thorship identification in online sources, one does not have
the means to request these documents from the authors and
therefore unable to be used in intelligence and investigation
operations.
The other problem faced in the use of authorship identifi-
cation on online text is the amount of sample text. The above
mentioned method used required the sample documents
with specific lengths. This guarantees there are more than
enough information to derive the final result. In the case of
online sources, less information are available and authorship
identification cannot achieve accuracy similar to the above
mentioned method. Nevertheless, the results obtain from
authorship identification on online sources can be credible
enough for forming new leads and follow-up actions which
prove to be of paramount importance in digital and cyber
investigations.
Another method which has achieved very high success
rates is the use of Bayesian classifier with stylometric fea-
tures and function words, by the Bilkent University [3]. They
have explored various methods with lexical and syntactic
feature sets, with the Bayesian classifier with Gaussian
density yielding the best results after applying Principal
Component Analysis (PCA).
They tested their methods using published documents
on a newspaper’s website from 18 different authors, each
had written mor than 500 articles. These downloaded docu-
ments are then parsed with the help of open source java
library HTML Parser which the pure tex data are saved
in standardized XML files. Lexical and stylometric features
such as number of words, average word length, vocabulary
size and number of punctuations are extracted. From their
findings, high success rates can be achieved when there are
a lot of data available for analysis. Hence, the success rate
will increase with the number of texts and samples. This
deduction is logical as identification methods are based on
stylometric features from the texts. A larger number of texts
will yield more accurate features. Therefore, we still face a
problem when text documents and sources are scarce and
unable to yield accurate results from online forums and
discussion boards.
There are also applications of the support vector machine
(SVM) [8] to authorship attribution. Using this approach,
thousands of features can be processed and all words in
the text can be used rather than the selected function words
or content-based vocabulary. This make SVM an effective
method for authorship attribution.
Another method, Writeprints [1], developed by the Uni-
versity of Arizona, utilized the Karhunen-Loeve-transforms-
based technique to capture feature usage variance, which
can be used to identify authors and also similarity detection.
Unlike SVM, it is an unsupervised approach and is able to
yield better results over numerous methods. However, the
complexity and lack of a standard feature set may not justify
the effectiveness of this approach.
The use of function words as features for authorship
attribution were carried out in previous papers. Function
words exhibit the style and the choice of words of the
particular author regardless of the topic and content of the
specific document. Baaye [2] used 50 common function
words and 8 punctuation symbols to represent the style of
the author and achieve 88.1% accuracy.
In the conference paper published by Goswami et al. [9],
blogs adopt a very different style of writing as compared to
novels and other standard published texts. Bloggers express
their thoughts and opinions in an informal, unreserved and
unorganized manner. The language used has a mixture of
spoken and written language. It is also found that sentence
length and vocabulary are good features to classify bloggers.
Recent work by Cartright and Bendersky [5] suggested the
use of SVM for scalable data driven authorship identification
for online text. Tests are conducted with varying amount of
data set using the bag of words features approach. They
156
Training Algorithm
Classifier
Training Sets
Testing Set
Dataset
Figure 2. k-fold Cross-validation
assumed the number of potential features will increase log-
linearly with the number of potential authors. The selection
of features using information theory can reduce the number
of possible features while maintaining the predictive classi-
fication accuracy. They also suggested that feature selection
must adopt to the changing characteristics of the data set
and emphasize certain features using statistical information
present in the data set.
A. k-fold Cross-validation
The predictive accuracy of the classifier is measured using
the k-fold cross validation mentioned by Bramer [4]. In this
method, the data set is divided into k parts, as shown in
Figure 2. Each of these k parts will be in turn used as test
sets and the remaining k− 1 parts will be used for training.
For each test set which the classifier correctly classified, C,
the predictive accuracy of the classifier for that particular
test set is p = C/k. For the standard text test conducted in
this paper, the entire novel is split into its chapters, which
corresponds to the k parts. Each chapter is used as a test set
while others are used as training sets. After k iterative tests,
the predictive accuracy can be calculated.
B. Standard Error
A different value for the predictive accuracy will be
obtained if the classifier is used with other texts. This
simply means that the predictive accuracy of our tests is
an estimate of the true predictive accuracy of the classifier.
It is impossible to obtain the true predictive accuracy of the
classifier by running tests on all kinds of text and therefore
we need to use statistical methods to find an appropriate
range of values which the true predictive accuracy lies
within. The standard error method is used in this paper,
which is given by
√
p(1 − p)/k. The standard error is then
converted into percentage range.
III. DESIGN, METHODOLOGY & ALGORITHMS
This paper explored and implemented techniques for au-
thorship identification for text documents. The paper utilizes
different data mining techniques to extract features which
exhibit the writing style of the author and classify the most
likely author through sets of training data. This paper also
explores the implementation of authorship identification on
online sources such as blogs and forums.
The design and methodology are segmented into three
parts. The first part focuses on the extraction of stylometric
features and database development. The second part de-
velops and implements the classification algorithms, which
form the major part of the paper. The third part deals with
the testing of various data sets and analysis of the results.
The data sets are processed by the feature extraction
algorithms where the stylometric features are extracted and
stored in the database. The classifier uses these features to
compare with the anonymous text file before passing the
results to the next part of the program. The results generation
algorithm calculates and analyzes the information provided
by the classifier to produce the final result which is the most
probable author of that anonymous text.
1) Feature Selection: For the initial part, text documents
in the particular folder are parsed into the software for
feature extraction. Lexical and syntactic features like word
counts, unique word counts and frequencies of function
words are extracted. Some arithmetic functions are also
performed in the extraction process before the information
is saved in the MySQL database.
2) Classification: The next part focuses on the develop-
ment of classification algorithms. The classification method
selected is the Bayesian classifier, as this method yields
promising results tested in previous research papers.
3) Testing: The last part is the testing and analysis of
results. We study the performance and efficiency of the
classification algorithms with different data sets and feature
sets. From these results gathered, the most suitable feature
set and classification method can be determined.
A. Dataset Description
Datasets such as standard text from novels are used for
testing in this paper. This can be compared with other
authorship classifiers before applying it on text from online
sources. One novel from the two authors are segmented into
chapters which are stored as individual text files. Testing can
be carried out with different training sets from the chapters
of both authors and tested against chapters of the same
author to determine the predictive accuracy of the classifier.
The statistics of the two novels are shown in Table II.
Title Author Number of
Chapters
Total Word
Count
War of the
Worlds
H.G. Wells 27 59263
Journey to the
Centre of the
Earth
Jules Verne 35 66602
Table II
STATISTICS OF THE NOVELS USED
157
These novels are downloaded from the paper Gutenberg
website (www.gutenberg.org), which is the first producer of
free electronic books.
Both authors are famous for their science fiction novels,
and the two novels are also written around the same time.
Both authors have different writing styles: Verne focuses
more on the technical details, with more descriptions of
objects and people, whereas Wells is more into romance and
character driven scenes, resulting in more conversations and
characters’ relationship. Hence, Verne generally has more
chapters and longer sentence length. By extracting these
features, we are able to determine the authors if we used
one of the chapters of these two authors for testing. Some
sample text of each author are as follows:
“The descent of this obscure and narrow gallery was
very gradual and winding. Sometimes we gazed through a
succession of arches, its course very like the aisles of a
Gothic cathedral. The great artistic sculptors and builders
of the Middle Ages might have here completed their studies
with advantage. Many most beautiful and suggestive ideas
of architectural beauty would have been discovered by them.
After passing through this phase of the cavernous way, we
suddenly came, about a mile farther on, upon a square
system of arch, adopted by the early Romans, projecting
from the solid rock, and keeping up the weight of the roof.”
-Jules Verne (Journey to the Centre of the Earth, Chapter
16)
“I startled my wife at the doorway, so haggard was I. I
went into the dining room, sat down, drank some wine, and
so soon as I could collect myself sufficiently I told her the
things I had seen. The dinner, which was a cold one, had
already been served, and remained neglected on the table
while I told my story.
“There is one thing,” I said, to allay the fears I had
aroused; ”they are the most sluggish things I ever saw crawl.
They may keep the pit and kill people who come near them,
but they cannot get out of it. . . . But the horror of them!”
“Don’t, dear!” said my wife, knitting her brows and
putting her hand on mine. “Poor Ogilvy!” I said. “To think
he may be lying dead there!” My wife at least did not find
my experience incredible. When I saw how deadly white her
face was, I ceased abruptly.”
-H.G. Wells (War of the Worlds, Chapter 7)
B. Feature Description
In this paper lexical and syntactic features are extracted
for classification. Word counts and statistical information are
lexical features which show the author’s choice of sentence
length, vocabulary and the total word count of the document.
These are significant features in which different authors will
have different level of vocabulary as well as their sentence
lengths and word counts. Vocabulary can be determined by
the total number of unique words they use in the document.
This feature can be very helpful when the authors have
different nationalities and education levels.
Syntactic features used in this paper are function words
and punctuation symbols. Function words are words without
meaning by itself, and are used to construct sentences.
Different authors have their unique way of structuring their
sentences and the function words they used will be very
different from others. Commonly used function words such
as “and”, “the” and “as”, can be found in almost every
document. By counting the number of times these words
appear in the author’s documents, we are able to differentiate
the authorship of many anonymous documents.
There are other function words which can be useful for
classification. These function words are not commonly used
in many documents and only used by authors who have
special preferences or better language and grammar ability.
These words include “whereas”, “nevertheless”, “hence”,
“because” and “shall”, which can be easily substituted by
other function words. Hence, the appearance of these words
can show the style of a particular author.
Other syntactic features include the use of punctuation
symbols. Similar to uncommon function words, exclamation
marks, semi-colons and question marks are uncommon form
of punctuation, which are not commonly used in documents.
The appearance of such symbols can be preferences of a
particular author. The use of full stops or commas shows
the style and how the authors structure their sentences. The
descriptions of the various features are shown in Table III.
C. Feature Extraction
The data sets are texts from various authors, stored in
standard text format (.txt) files. These files are the training
data to be used by the classifier to determine the author
of a particular anonymous text file. Hence the stylometric
features of these text files embed the clues to the identity of
the author. A typical data set should contain at least 20 text
files or more than 4000 words, depending on the classifier
used to process these text files.
Each data set by a particular author is stored in a directory
folder. If there are data sets from two authors, two directory
folders are needed. This will help in differentiating the data
sets and reduce the complexity of the extraction algorithm
to detect the authors of these training data sets.
Using the objects for finding files in C++,
WIN32 FIND DATA and HANDLE, the algorithm
reads the files in the directory folder in sequential order.
The algorithm extracts the word counts using stringstream
class objects and uses the find function to locate and count
function words in the text file. Due to the varying total
word count in the text files, the raw function word counts
are normalized and stored into the database as percentages.
The lexical features are however directly stored into the
database. After completing the feature extraction of the text
158
Feature Type Remarks
“and” syntactic Common function word preferred by
authors to join two subjects
“hence” syntactic Uncommon function word used fre-
quently by few authors
“while” syntactic Authors’ unique style to structure
sentences
“because” syntactic Uncommon, easily substituted by
other words
“as” syntactic Shows the author’s style of sentence
structure
“shall” syntactic Uncommon word used in sentences
“but” syntactic Function word used to join two con-
trary subjects, easily substituted by
other words
“the” syntactic Very common word. Some authors
have the habit of using this word very
frequently
“of” syntactic Another common word used by au-
thors
“;” syntactic Uncommon punctuation word shows
the unique writing style of authors
“.” syntactic Full stop used to show how sentences
and objects are segmented
“?” syntactic Usually appears in dialogs. Shows
author’s style of conveying messages
through dialogs
“!” syntactic Exclamation used to show emphasis
or feelings, a clear indicator of writ-
ing style
Total Word
Count
lexical Shows how paragraphs or chapters
are structured
Unique Word
Count
lexical Shows authors’ vocabulary, choice of
different words
Number of
Sentences
lexical Shows authors’ choice of sentence
length and structure
Average
Sentence
Length
lexical Number of words in one sentence,
a good indicator of author’s writing
style
Table III
FEATURE DESCRIPTIONS
file, the algorithm will then iteratively read and process the
next text file until the last text file in the directory.
Depending on the number of authors or directories, the
feature extraction algorithm will process every directory
before passing the information to the next part of the
program.
D. Database
MySQL database is used as the main storage for this pa-
per. In this database, extracted features are stored in various
tables. All lexical features, including different authors are
stored in one table, and syntactic features are stored in four
tables. Function word counts and percentages are stored in
two separate tables which enable us to see the raw count
as well as the normalized percentages. This is similar to
the storage of the punctuations, which consist of two tables.
This database will be accessed by the classifier to retrieve
the relevant feature sets for comparison and calculation.
E. Naive Bayesian Classifier
The Naive Bayesian Classifier is classifier based on the
Bayes’ theorem which assumes all features are mutually
independent. Despite its simplicity, Naive Bayesian classifier
outperformed some of the more sophisticated classification
methods which are described in various research papers.
The main advantage of this classifier is that it requires
fewer features for training, which is ideal for our work where
blogs and forums have a smaller number of words than other
standard texts. The main approach of this classifier is by
comparing the trained feature sets and input, which is very
similar to the maximum likelihood method.
The Naive Bayesian Classifier is a conditional model
where a class variable, C, depends on a set of outcomes on
the features, Fn, which is given by the following equation:
p(C|F1..n) (1)
And using Bayes’ theorem we rewrite the equation:
p(C|F1..n) =
p(C)p(F1, ..., Fn|C)
p(F1, ..., Fn)
(2)
The denominator depends on the features and not on
C; therefore it is always constant and not of interest.
The numerator then can be written in its naive conditional
independence assumptions where every F is mutually inde-
pendent of other F s:
p(C)p(F1, ..., Fn|C) = p(C)p(F1|C)p(F2|C)p(F3|C)...p(Fn|C)
(3)
= p(C)
n∏
i=1
p(Fi|C) (4)
Based on the above equation, the class with the highest
probability will be the class to be classified:
argmaxcp(C = c)
n∏
i=1
p(Fi|C = c) (5)
In this paper, the input anonymous text goes through
the same extraction process as the training sets. Identical
features are extracted and are compared with the trained
results in the database. A fixed threshold provides a range of
percentages from the anonymous text, for example a certain
function word occur in the anonymous text is 10%, the
threshold of 5% is applied and hence provides a range of
percentages from 5% to 15%. It will be used to identify
trained documents with feature results which falls in this
range. Hence, the probability can be determined by the
number of “qualified” results against the total number of
trained documents, which is given by the following equation:
p(Fi|A) (6)
159
where Fi are the qualified feature results and A is the author.
Hence, the classifier equation can be written as:
Probability of the text by author A = p(A)
n∏
i=1
p(Fi|A)
(7)
Probabilities of all the features are calculated and multi-
plied to give the probability of the anonymous text, given an
author. The highest probability among all the authors will
be the most probable author of that anonymous text:
argmaxcp(A = a)
n∏
i=1
p(Fi|A = a) (8)
However, some of the probabilities, p(Fi|A), yield zero
results as none of the training texts falls in the range. Zero
results cause the product in the equation to zero, which do
not make any sense. Therefore, the Laplace rule is applied
to add a pseudocount of 1 to the probability and make it
negligible in the equation.
The equation may now function properly, but after running
several tests, the accuracy of the classifier decreases signifi-
cantly. Short training texts give several negligible probabil-
ities and amplify the significance of the few others. This is
undesirable as the more negligible results, the likelihood of
the particular author decreases. Therefore, a value of 0.01 is
assigned instead of 1, which make no changes to the values
except shifting the decimal place by 1.
IV. RESULTS & DISCUSSION
Numerous tests have been conducted with the Bayesian
Classifier. In this paper, we investigate the most suitable
classifier for use in authorship identification of online text
documents. Therefore, the predictive accuracy of the clas-
sifier is the most important criterion. In this paper, the
predictive accuracy of the classifier is measured using k-
fold cross validation [4].
The predictive accuracy varies with different tests. There-
fore, a standard error calculation will provide a range of
values for the true predictive accuracy. The formula for
standard error is: √
p(1 − p)/k (9)
where p is the probability of positive results and k is the
total number of samples in the test. The predictive accuracy
is calculated by the number of positive results over total
number of instances and represented in percentage.
A. Novel Results
The data sets are novels from the two authors, which
consist of around 30 chapters each, totaling of more than
60,000 words. Each chapter is used as a training set.
The predictive accuracy of the classifier is plotted on
a graph in Figure 3. The points on the curve represent
Dataset
Size
(# text
files)
Total
#
Words
#
Cor-
rect
# In-
cor-
rect
Accuracy Ratio of
Total #
to Test #
Words
62 125865 56 6 90.3%±3.75% 57.36
46 89267 42 4 91.3%±4.15% 40.69
30 58018 28 2 93.33%±4.55% 26.44
24 44037 22 2 91.6%±5.57% 20.07
18 28842 16 2 88.89%±7.4% 13.15
14 21855 12 2 85.71%±9.35% 9.96
10 16156 8 2 80%±12.26% 7.36
8 12526 6 2 75%±15.3% 5.71
6 10172 3 3 50%±20.4% 4.64
4 7961 1 3 25%±21.6% 3.62
Table IV
CLASSIFIER RESULTS
Feature Set (# Features) Accuracy Standard
Error
All Lexical Features (4) 58.06% 6.2%
All Syntactic Features (13) 88.71% 4%
All Function Words (9) 79.03% 5.2%
Common function words (5) & Vo-
cabulary / No. of sentences (2)
72.58% 5.6%
All function words (9) & Vocabu-
lary / No. of sentences (2)
82.23% 4.8%
Rare function words (4) & Vocab-
ulary / No. of sentences (2)
64.52% 6.1%
Table V
FEATURE SET COMBINATIONS RESULTS
the actual experimental results achieved using the k-fold
validation method.
From the table results (Table IV) and the accuracy graph
in Figure 3, we can observe that the accuracy decreases
when the data set size decreases. This is due to the insuf-
ficient statistical data of the stylometric features available
for comparison, which results in zero probability in many
features. Also, as the data size decreases, the accuracy
range increases, hence the uncertainty of the true predictive
accuracy increases. The results obtained from a smaller data
set will not be as accurate as the large data set used for
classification.
Therefore, in order to achieve at least 75% accuracy, we
will need to look at the result which show us 85.71%, which
its lower limit is very close to 75%. This also shows us the
total word count - test word count ratio must be above 9.96,
meaning the total word count of the data set must be 10
times larger than the word count of the test set.
B. Feature set combinations
Classification simulations using different set of features
are also conducted. Besides using all 17 lexical and syntac-
tic features, these tests will present features with varying
weights which will affect the final classification results.
Table V shows the different feature sets used.
160
Figure 3. Accuracy of Bayesian Classifier
As shown in the results, classifications based solely on
statistical features of the texts do not yield positive results.
However using syntactic feature sets give better results than
the lexical counterparts, and classification results can be
affected by the content of the data set. Similar content
or topics from two authors can have very close syntactic
features, which yields a lower classification results than two
significantly different topics.
Further tests are conducted using different combinations
of lexical and syntactic features. Common function words
such as “as”, “and” or “the” may be adequate enough
to achieve high predictive accuracy results, however the
tests conducted proved otherwise. Rarely appeared function
words such as “hence”, “shall” or “but” can differentiate the
authors, but it has been proven that they are more useful in
larger data sets.
Therefore in order to achieve high classification pre-
dictive accuracy, there must be a combination of lexical
and syntactic features. None of the above simulations had
achieve higher accuracy than the entire feature set used for
classification.
C. Blog Content Results
The classifier is also tested with BizBlogs, a database
which contains blog entries submitted by many different
users with a wide range of topics relating to business. Two
authors have been selected for the classification as they are
the few whom had more entries than the rest. The summary
Authors #
Entries
Total #
words
Range of
# words
per entry
Average
# words
per entry
FMF 230 82231 24 - 774 357.5
James Kendrick 114 19369 15 - 1342 169.9
Table VI
STATISTICS OF THE AUTHORS IN THE BIZBLOGS DATA SET
# Correct #
Incorrect
Total Accuracy Standard
Error
282 62 334 81.98% 2.01
Table VII
BIZBLOGS CLASSIFICATION RESULTS
of the statistics of the authors are shown in Table VI.
Although they have many entries in the database, each
author exhibits differences in the length of their entries.
James Kendrick’s entries are much shorter than FMF’s in
terms of word count. However, their entries span over a
wide range with entries as short as 15 words, which may
results in data insufficiency for classification since there are
few features that can be extracted.
The classifier will adopt the similar k-fold cross validation
approach to determine its predictive accuracy for blog con-
tent. All 17 lexical and syntactic features will be extracted
and used by the classifier to determine the most probable
author. The results are shown in Table VII.
161
Author # Cor-
rect
# In-
correct
Total # Accuracy Standard
Error
FMF 185 45 230 80.43% 2.61
James
Kendrick
97 17 114 85.09% 3.33
Table VIII
INDIVIDUAL CLASSIFICATION RESULTS
The predictive accuracy of the classifier is determined at
around 80% when used in the classification of blog content.
From the generated results, classification of blogs yields a
lower accuracy than the previous result with the novels. This
is probably due to the varying size of the testing data set.
Further breakdown of the results in identifying each of the
authors are shown in Table VIII.
It is shown that the classifier is more accurate in classify-
ing James Kendrick’s entries. A detailed study was carried
out to look at the features extracted from the wrongly
extracted entries. Most of these entries are much longer than
the average length of his entries. Hence, the lexical features
extracted would very much different from the other posts by
James.
In comparison, FMF’s entries are generally longer and
most of the wrongly classified entries are shorter than the
average length. Similarly, the distinct differences in lexical
features may have result in the incorrect classification.
D. Discussion
From the results, we can see that Bayesian classifier can
be used for authorship identification for text files with large
training data. It is able to achieve more than 90% accuracy
with just 17 lexical and syntactic features extracted, as com-
pared with the previous work by Baayen using 50 function
words. The limitation of the classifier is the minimum data
size of at least 10 times the word count of the anonymous
text file.
Therefore, Bayesian classifier is suitable for authorship
identification for online texts which do not have large text
files, and with large number of text files available for
training. These include blog entries and forum posts.
From the findings, syntactic features such as common stop
/ function words and vocabulary are good features for large
data sets (approx. >5000 words) and lexical features such as
number of sentences and punctuations are good for smaller
data sets (approx. < 5000 words). A good balance of both
types of features is crucial when comparing between a large
data set and a small data set.
V. CONCLUSIONS
In this paper, both lexical and syntactic features of varying
data sizes are used for authorship identification. Unlike other
similar work using large feature sets, this paper focuses
on developing a classifying method that utilizes a well-
defined, smaller feature set to yield the most accurate results.
Features are further categorized to test their impact on the
final result.
From the findings, syntactic features such as common stop
/ function words and vocabulary are good features for large
data sets and lexical features such as number of sentences
and punctuations are good for small data sets. The results
can be used to customize and further improve authorship
detection techniques according to the characteristics of the
writing samples.
REFERENCES
[1] A. Abbasi. Writeprints a stylometric approach to identity-
level identification and similarity detection in cyberspace.
ACM Transactions on Information Systems, 26(2), 2008.
[2] H. Baayen, H. van Halteren, A. Neijt, and F. Tweedie. An
experiment in authorship attribution. 6th JADT, 2002.
[3] I. Bozkurt, O. Baghoglu, and E. Uyar. Authorship attribution.
In Computer and information sciences, 2007. ISCIS 2007.
22nd international symposium on, pages 1 –5, 2007.
[4] M. Bramer. Principles of Data Mining. 2007.
[5] M.-A. Cartright and M. Bendersky. Towards scalable data-
driven authorship attribution. Center for Intelligent Informa-
tion Retrieval, 2008.
[6] C. E. Chaski. Who’s at the keyboard? authorship attribution
in digital evidence investigations. International Journal of
Digital Evidence, 4(1), 2005.
[7] Y. Chen, F. S. Tsai, and K. L. Chan. Machine learning
techniques for business blog search and mining. Expert Syst.
Appl., 35(3):581–590, 2008.
[8] J. Diederich and J. Kindermann. Authorship attribution with
support vector machines. Applied Intelligence, 19:109–123.
[9] S. Goswami, S. Sarkar, and M. Rustagi. Stylometric analysis
of bloggers’ age and gender. Third International ICWSM
Conference, 2009.
[10] E. Stamatatos. A survey of modern authorship attribution
methods. Journal of the American Society for Information
Science and Technology, 60(3):538–556, 2008.
[11] F. S. Tsai, W. Han, J. Xu, and H. C. Chua. Design and
Development of a Mobile Peer-to-Peer Social Networking
Application. Expert Syst. Appl., 36(8):11077 – 11087, 2009.
162
