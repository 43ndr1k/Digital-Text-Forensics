193
ISSN 0278-6419, Moscow University Computational Mathematics and Cybernetics, 2015, Vol. 39, No. 4, pp. 193–199. © Allerton Press, Inc., 2015.
Original Russian Text © V.V. Kulikov, 2015, published in Vestnik Moskovskogo Universiteta. Vychislitel’naya Matematika i Kibernetika, 2015, No. 4, pp. 54–61.
Some Ways of Determining Word Combinations
for Classifying Texts
V. V. Kulikov
Department of Computational Mathematics and Cybernetics, Moscow State University, Moscow, 119899 Russia
e-mail: segoon@cs.msu.ru
Received January 26, 2015
Abstract—Different ways of defining the term “word combinations” are considered. New methods
based on syntactic trees are examined, as is the use of n-grams. Terms such as n-chain, n-subtree, and
full t-subtrees are introduced. New types of word combinations can serve as the basis for start-of-text
characters in problems of computer-assisted teaching. All described types of word combinations are
compared to one another in a task of determining authorship, posed as a classification problem.
Keywords: Text processing, classification, syntactic analysis, determining text authorship.
DOI: 10.3103/S0278641915040056
1. INTRODUCTION
In problems of text classification, we can use the frequencies of detecting such features of a text as start-
of-text characters (instances of a particular word form, lemmas, parts of speech, and so on) independently
of the distribution of such items in the text. In order to consider information on the mutual combinations
of such items as the inclusion or properties of a word, we must somehow formalize the term “word com-
bination.” As a rule, patterns of word forms or their properties (referred to as n-grams) are used for this
purpose. However n-grams that consider the mutual use of consecutive words in a sentence ignore lan-
guage syntax. For example, in the sentence “Mother washed the window frame,” the word combination
“washed the window frame” is not explicitly taken into account by word bigrams.
Using a syntactic analyzer (parser) of language allows us to consider explicitly the relations between
words, which can then be used to define the term “word combination.” A MaltParser syntactic analyzer1
was used to build a root tree in which separate words of a sentence (along with punctuation marks, num-
bers, and so on) were represented as vertices and syntactic relations as edges. The incidence of tree vertices
can be used to define the mutual combination of word properties. As is shown below, using the syntactic
structure of a sentence tree to consider word combinations greatly improves the quality of classification.
Syntactic information has already been used in different ways in works by other authors on the classi-
fication of texts. For example, [1, 2] considered the possibility of using context-free grammar output rules
and obtaining start-of-text characters from them. In [2], characters were extracted automatically. The
authors of [3, 4] presented partial analyses of sentences (partial parsing [3]; chunk parsing [4]) and build-
ing start-of-text characters from the frequencies and sizes of some syntactic groups.
In this work, we introduce three new methods for formalizing the term “word combination” that can
be used to distinguish characters in such problems of computer-aided teaching as classification, clusteri-
zation, and identification. Their effectiveness is also compared to n-grams in a task of determining of text
authorship, posed as classification problem. We should note that the field for applying new types of word
combinations is much wider than determining authorship.
2. VARIANTS OF WORD COMBINATIONS
2.1. General Information. The tree of a sentence’s dependences consists of noted words connected by
syntactic relations. Using incidence, we can derive a great many characters from the tree that reflect
mutual word combinations. Let us use relatively simple classes of characters from linguistics and with clear
interpretations in the form of graph theory. In each class of characters, the term “word combination” and
1http://www.maltparser.org
194
MOSCOW UNIVERSITY COMPUTATIONAL MATHEMATICS AND CYBERNETICS  Vol. 39  No. 4  2015
KULIKOV
the size of word combinations are defined in their own way. We begin with general definitions and then
formally define all of the types of word combinations that are used. We use {…} to designate disordered
multitudes and 〈…〉 or (…) for ordered multitudes.
Oriented graph G represents a pair (V, E) where V is the multitude of vertices and E ⊆ V × V is the mul-
titude of edges. The path in graph (V, E) is a finite sequence of vertices of this graph (v1, …, vn) such that
edge (vi, vi + 1) belongs to multitude of edges E for all values of i from 1 to n – 1. We refer to the number of
edges originating from a vertex (i.e., the number of elements of multitude {(v, u)|u ∈ V}) as the outdegree
of the vertex. A vertex with a zero outdegree is referred to as a tree root: RT = Root(T). Root T with a dis-
tinguished root is an oriented graph for which three conditions are fulfilled: First, the outdegree of each
vertex is no greater than 1. Second, there is only one vertex with a zero outdegree. Third, there is a path to
zero from any vertex. We should note that there is only one path to the tree vertex from any non-root ver-
tex.
Let us define several properties for a tree with distinguished root. For each edge (v, u) ∈ E, vertex u is
referred to as a parent vertex with respect to vertex v: u = parent(T, v). It follows from the definitions
of tree and root that all tree vertices except the root have a parent vertex. We shall refer to the multitude
of vertices whose parent vertex is v as the multitude of children of vertex v: children(T, v) =
{u|parent(T, u) = v}. Subtree (V ', E') of tree T = (V, E) is thus any tree with a distinguished root such that
V' ⊆ V and E' ⊆ E.
Let us refer a tree with a distinguished root such that its vertices act as sentence words and edges rep-
resented by syntactic relations between words as syntactic tree T = ST(S) of sentence S = 〈t1, …, tk〉: T =
{{t1, …, tk}, E}. The MaltParser syntactic analyzer builds the syntactic tree for the sentence, supplemented
by the pseudo-word ROOT: S ' = {ROOT, t1, …, tk}. If syntactic relations are obtained by the MaltParser
analyzer, our definition of a syntactic tree is correct, since the results from sentence analysis are a set of
noted words, each of which has a defined parent word except root vertex ROOT, and all of the words have
paths to the root vertex. Each word has a set of preliminarily defined properties, prop1(t), …, propp(t). Cer-
tain examples are considered below. The figure shows an example of syntactic tree ST1. The root of the
tree is expressed as vertex ROOT. For simplicity, the diagram vertices are marked only by the word form
instead of group of properties prop1(t), …, propp(t). Tree ST1 is referred to below as our example.
2.2. Definition of n-grams. Let us consider an arbitrary sequence consisting of k elements 〈a1, …, ak〉.
We refer to a subsequence consisting of exactly n consecutive elements of this sequence as an n-gram.
Start-of-text characters can be represented by the frequencies of the n-grams for the sequence
〈prop(t1), …, prop(tk)〉 where prop is a property of a word.
2.3. Definition of n-chains. Let us consider unary subtree T' of a tree with distinguished root T = (V, E).
The unary tree is a set of vertices 〈v1, …, vn〉 where vertices vi and vi + 1 are connected by an edge. A simple
n-chain of tree T is any sequence 〈u1, …, un〉 such that tree ({u1, …, un}, {(ui, ui + 1)|i ∈ 1, …, n – 1}) is a
unary subtree of tree T. Different subsequences 〈prop(v1), …, prop(vn)〉 are used as start-of-text characters
where 〈v1, …, vn〉 is the simple n-chain of a text sentence and prop is a property of a word. Let us refer to
this group of characters as the simple n-chain of property prop. Some 3-chains of word forms for tree ST1
are shown below:
〈ROOT, they, call〉
〈call, nothing, but〉
〈a, bandit, .〉
Using the frequencies of simple n-chains as start-of-text characters has a great disadvantage that is also
typical of n-grams: some words of a sentence are found in n-chains more frequently than others. For
instance, leaf word v is found in only one chain: 〈v, parent(T, v), parent(T, parent(T, v)), …〉. Similarly,
Example of a syntactic tree.
ROOT
(They themselves call him nothing but a bandit.)
Сами же называютони какнеего ,иначе бандит .
MOSCOW UNIVERSITY COMPUTATIONAL MATHEMATICS AND CYBERNETICS  Vol. 39  No. 4  2015
195SOME WAYS OF DETERMINING WORD COMBINATIONS
root RT is found only in chains like {…,u, v, RT}, where v = parent(T, u) RT = parent(T, v), and the number
of such chains is no greater than the number of children of vertex RT. In contrast, the vertices farthest from
the root and leaves of the tree are found in a much greater number of chains. As a result, the most distant
vertices affect the start-of-text characters more strongly, while the vertices closer to the root and/or leaves
make relatively weak contributions. A similar problem is observed for n-grams: the first and last n – 1 ele-
ments in sequence 〈v1, v2, …, vk〉 are found in fewer numbers of n-grams than the intermediate elements.
To eliminate this imbalance, the initial sequence is extended by adding n – 1 fictive elements ε to the
beginning and the ending of the sequence: 〈ε1, …, ε, v1, …, vk, ε1, …, ε〉. As a result, the outermost elements
are found in the n-grams of the extended sequence.
Let us try to extend an arbitrary n-chain in the same way as N-grams. We shall use fictive vertices
NONE and ROOT, which are no found in the initial tree. We set the values of all properties defined for
ordinary vertices: propi(NONE), propi(ROOT). Let us define p, r-extension2 of tree T:
(1) The 0,0-extension of tree T is tree T itself.
(2) The 1,0-extension of tree T = (V, E) is tree T' = {V ∪ {R}, E ∪ (RT, R)}, where R is not found among
the vertices of tree T and has the same properties as fictive vertex ROOT: R ∉ V, propi(R) = propi(ROOT).
(3) The 0,1-extension of tree T = (V, E) is tree T' = (V ∪ { , …, }), E ∪  where v1, …,
vk denotes leaves of tree T, k is the number of leaves of tree T, and  indicates new fictive vertices not
found in the initial tree and having the same properties as fictive vertex NONE: propj( ) =
propj(NONE).
(4) The p, 0-extension of tree T at p > 1 is the (p – 1,0)-extension of the 1,0-extension of tree T.
(5) The p, r-extension of tree T at p > 0, r > 0 is the (p, r – 1)-extension of the 0,1-extension of tree T.
The p, r-extension of a tree is thus a result of the consecutive attribution of p fictive vertices to the root
of the tree and r to each leaf. We shall refer to the simple n-chain of the (n – 2, n – 1)-extension of tree T
as the extended n-chain of tree T for n > 1. In the extended chain, the intermediate vertices of tree do not
have the advantages of the leaves and root vertices, which can be found in simple chain. The number of n-
chains including a vertex depends only on the number of children of the nearest vertices. Let us demon-
strate examples of some extended 3-chains for tree ST1:
(ROOT, ROOT, call)
(ROOT, call, they)
(call, they, a)
(a, NONE, NONE)
2.4. Definition of n-subtrees. Let us refer to any arbitrary subtree of tree T with a number of vertices no
greater than n as an n-subtree of tree T with a distinguished root. For the convenient use of n-subtrees as
characters, we must apply an algorithm for coding the tree in a string of symbols. We used a recording
obtained by applying a depth-first tree-walk algorithm. Upon entering vertex v, the algorithm writes the
symbol of opening parenthesis “(”, the value of some property of the given vertex, prop(v), and symbol
“|”. Upon exiting tree vertex v, the symbol “)” is written. Child vertices are bypassed according the order
of the word in the initial sentence. For tree ST1, the result from writing is
(R00T|(call| (They|(a|))(themselves|)(him|)(nothing|(but|)(,|)(a|(bandit|(.|))))))
Some examples of 3-subtrees for the same tree are
(ROOT|(call|(Themselves|)))
(call|(Themselves|)(they|))
(a|(bandit|(.|)))
2.5. Definition of full t-subtrees. Our n-subtrees include all subtrees with numbers of vertices no greater
than n. If vertex v is the parent for u, then n-subtrees include subtrees with root vertex v, which can either
include or exclude vertex u. It is sometimes useful to study only those subtrees that consist of all vertices
within distance t from the root vertex. We shall refer to such trees as full t-subtrees. Let us formally define
the full t-subtrees of tree T with a distinguished root.
2This name for the procedure of tree extension was selected in analogy with the p,q-extension in [5]. In this work, the extension 
toward the root coincides with our procedure; however, the second part of the extension differs. The first index thus has the
same letter as in [5] (p) and second index has a new letter (r instead of q) in order to emphasize the difference from p,q-exten-
sion.
v1 v k =∪ v v1( , )
k
i ii
v i
v i
196
MOSCOW UNIVERSITY COMPUTATIONAL MATHEMATICS AND CYBERNETICS  Vol. 39  No. 4  2015
KULIKOV
(1) Tree T' = (V ', E') is the full 1-subtree of tree T = (V, E) if and only if it is a subtree of T and has
exactly one vertex: V ' = {v}, v ∈ V, E = ∅.
(2) Let T1 = (V1, E1) be the full t-subtree of tree T when t > 0. Then tree T2 = (V1 ∪ Vchildren, E1 ∪ Echildren)
is the full t + 1-tree of tree T, where Vchildren is the multitude of all children of all vertices of multitude V1
and Echildren is the multitude of all edges in tree T1 connecting Vchildren and their parent vertices.
To denote full t-subtrees, we also use a depth-first tree-walk. Some examples of the full 3-subtrees of
tree ST1 are shown below:
(ROOT|(call|(They|)(themselves|)(him|)(nothing|)))
(Themselves|(as|))
(nothing|(but|)(,|)(a|(thug|)))
(They|)
3. START-OF-TEXT CHARACTERS
Tree of syntactic dependences was obtained using a MaltParser 1.5 syntactic analyzer and a TreeTagger
3.2 morphologic analyzer3. For each word, the TreeTagger defines its lemma, general part of speech, and
morphosyntactic descriptor. For words not found in the dictionary, TreeTagger gives the pseudo-lemma
〈unknown〉, which did not allow us to consider combinations of non-dictionary lemmas. In order to obtain
more precise values of our lemmas, we also used a CSTLemma syntactic lemmatizer4. Models obtained
by S.A. Sharapov [6] were used as our Russian language models for the MaltParser, TreeTagger, and CST-
Lemma analyzers. A program5 was developed for extracting the start-of-text characters described in para-
graph 2 from the MaltParser output.
As word properties we used
—word form (i.e., the word in the form it was found in the text);
—word lemmas obtained using a TreeTagger analyzer. Such lemmas may also include the pseudo-
lemma 〈unknown〉. In order to differ these from the following type of lemmas, we shall refer to this type
as dictionary lemmas;
— and parts of speech.
—we also used a morphosyntactic descriptor in the MULTITEXT-East6 format.
Among types of word combinations, we selected n-grams along with extended n-chains, n-subtrees,
and full t-subtrees. All types of word combinations are described in paragraph 2. Word combinations with
lengths of two to four words were considered.
Our start-of-text characters included a considerable number of noise characters; it was therefore nec-
essary to select the ones we wanted. We used only those characters most commonly encountered in the
Russian language. A similar technique was applied in [7] using a frequency dictionary. For each group of
characters, we gathered statistics using the Russian language OpenCorpora program. All texts were com-
bined into a single text via simple concatenation and then processed by TreeTagger, MaltParser, and Syn-
dep analyzers. Statistics on the most frequent features were gathered for each one in the form of pairs
(name, frequency).
4. EXPERIMENTAL
We used the set of texts from [7] as our body of text. It consisted of 47 pieces of writing by 9 Russian
authors of the 18th through the 20th centuries. The different works of our body contained from 80 thou-
sand to 1.6 million symbols. To even out the difference in text sizes, we used only the first 60 thousand
symbols from each text and ignored the rest.
As our algorithm of computer-aided teaching, we used a support vector machine (SVM) [8] with a lin-
ear kernel. The Weka program7 with automatic normalization of characters and default parameter values
was also applied. We had to build multiclass classifiers that could distinguish more than two classes. Since
3http://www.cis.uni-rauenchen.de/~schmid/tools/TreeTagger/
4https://github.com/kuhumcst/cstlemma
5At the time of this work’s publication, the program was at the beta testing stage, so the author is planning to publish the source 
code under a free license at https://github.com/segoon/syndep/
6http://nl.ijs.si/ME
7http://www.cs.waikato.ac.nz/ml/weka/
MOSCOW UNIVERSITY COMPUTATIONAL MATHEMATICS AND CYBERNETICS  Vol. 39  No. 4  2015
197SOME WAYS OF DETERMINING WORD COMBINATIONS
our number of classes was low (only nine), we constructed the maximum number of binary classifiers for
each pair of authors and each classified text was denoted by the mark of the class that got the most votes
from binary classifiers (all-against-all or one-vs.-one [9]). In order to assess the qualities of different types
of word combinations, we performed a series of cross-validation experiments. The number of classified
objects was quite small (47 texts), so we selected the minimum size of a block as one text after gaining con-
trol over individual objects (leave-one-out, or LOO). The external parameters of the teaching algorithm
were the type of word combination, the word property, size of the word combination, and the size of the
vocabulary (50, 100, 500, 2000, or zero). Independent experiments were performed for each set of external
parameters.
5. RESULTS
The table shows the results from all our experiments. In some cases, word combinations longer than
3 words could not be obtained due to the lack of operative memory in the Weka program. For such cases,
the corresponding table cell contains a strikethrough.
In most properties of words and word combinations, extended n-chains and n-subtrees surpassed
n-grams in their accuracy of classification. However, morphosyntactic descriptors showed better results
for n-grams than for n-subtrees. In most cases, full t-subtrees showed the worst results. When using dic-
tionary and full lemmas, parts of speech, and morphosyntactic descriptors, the best results were produced
Results from cross-validation control
The bold font indicates the leaders among types of word combinations; underlining, word combinations that surpass the corre-
sponding bigrams; strikethroughs, the impossibility of obtaining a result due to computational complexity.
n-grams n-chains n-subtrees Full t-subtrees
1 2 3 4 2 3 4 2 3 4 2 3 4
Word form All characters 14 0 0 – 0 0 – 0 0 – 0 – –
50 87 61 4 0 78 78 78 74 76 76 31 25 4
100 87 68 21 0 74 72 70 85 82 80 42 27 4
500 78 80 48 8 80 78 78 74 76 76 55 19 6
2000 78 70 55 27 65 63 65 70 72 74 57 23 8
Dictionary 
lemma
All characters 53 2 0 – 2 0 – 0 0 – 2 – –
50 91 68 23 6 85 87 87 87 87 87 65 31 10
100 87 76 31 8 97 93 93 91 91 89 68 42 23
500 82 82 63 23 93 89 91 87 85 87 85 42 21
2000 87 80 57 27 82 80 80 76 78 78 76 40 17
Full lemma All characters 42 0 0 – 0 0 – 0 0 – 2 – –
50 85 57 2 0 91 89 89 89 91 91 48 2 0
100 87 74 14 0 100 95 97 93 93 93 65 10 8
500 80 85 46 10 95 91 89 85 82 85 74 27 12
2000 87 80 53 19 80 82 87 78 80 80 65 31 12
Part of 
speech
All characters 44 82 82 – 91 61 – 17 0 – 80 53 –
50 46 87 70 68 91 76 76 80 80 78 76 82 57
100 46 89 85 65 91 82 82 76 82 74 82 85 68
500 46 80 89 82 91 91 87 63 74 76 80 78 80
2000 46 80 80 80 91 63 65 40 44 51 80 63 76
Full mor-
phosyntac-
tic class
All characters 51 12 0 – 12 2 – 0 0 – 10 0 –
50 72 76 53 14 82 68 70 80 76 76 61 48 17
100 80 80 59 25 82 78 76 87 74 74 74 48 25
500 57 87 68 40 87 87 85 76 72 78 82 42 34
2000 51 70 53 27 76 72 76 57 59 61 70 31 23
198
MOSCOW UNIVERSITY COMPUTATIONAL MATHEMATICS AND CYBERNETICS  Vol. 39  No. 4  2015
KULIKOV
by n-chains. Extended 2-chains of lemmas showed the best results among all groups of characters. For
dictionary lemmas, only one text was classified incorrectly; for full lemmas, all texts were classified cor-
rectly.
The classification results for n-grams of all properties except parts of speech deteriorated sharply when
the size of word combinations were increased (i.e., at increments of value n). At the same time, the clas-
sification results for extended n-chains and n-subtrees of these properties remained virtually the same
when the size of word combinations were increased. Using dictionary lemmas instead of full lemmas (i.e.,
the pseudo-lemma 〈unknown〉 instead of heuristically defined lemmas) improved the results for unigrams
by 0 to 11%; for trigrams, by 0 to 21%; and for 4-grams, by 6 to 13%. However, the opposite situation was
observed for extended n-chains and n-subtrees. Although the difference was not very great, and in some
cases using pseudo-lemmas improved accuracy only slightly, the gain from using full lemmas ranged from
0 to 11%.
Using the most frequent characters encountered in the Russian language and ignoring all the rest
sharply increased the accuracy of classification in most cases when using any word property except parts
of speech, due to our sparse data. When using word forms, lemmas, or full syntactic classes, the number
of characters was very great, and rare items contained considerable amounts of noise. It should be noted
that classification without selecting word combinations of all types yields results close to zero when using
any word properties except parts of speech. We may therefore conclude that using the above types of word
combinations as start-of-text characters with no additional selecting leads to considerable repetition.
6. CONCLUSIONS
Different ways of defining the term “word combinations” were considered. Formal definitions for four
new types of word combinations were introduced: n-chains, extended n-chains, n-subtrees, and full t-sub-
trees. Extended N-chains, n-subtrees, and full t-subtrees of different word properties obtained using a
MaltParser syntactic analyzer were used as start-of-text characters for determining the authorship of texts
in a classification problem. According to our results, extended n-chains and n-subtrees in most cases
yielded better results than n-grams. At the same time, the use of 2-chains of full lemmas allowed us to rec-
ognize all test texts. Using new types of word combinations with no further selecting of characters yielded
very poor results. Further automatic selecting of most the frequent word combinations in the Russian lan-
guage allowed us to improve our results considerably. One disadvantage of the proposed method is that
determining word combinations on the basis of syntactic relations requires the use of a syntactic analyzer
in addition to a morphologic analyzer. This increases the complexity of the classifier program and intro-
duces an additional source of classification errors.
In the future, the author plans to continue his research on the behavior of new types of word combina-
tions as external parameters (the length and genre of a text) change, and using other types of word com-
binations based on syntactic trees.
REFERENCES
1. H. Baayen, H. van Halteren, and F. Tweedie, “Outside the cave of shadows: using syntactic annotation to
enhance authorship attribution,” Lit. Linguist. Comput. 11 (3), 121–132 (1996).
2. M. Gamon, “Linguistic correlates of style: authorship classification with deep linguistic analysis features,” in
Proc. Coling 2004, Geneva, Aug. 23–27, 2004 (Assoc. for Comput. Linguist., Stroudsburg, USA, 2004), p. 611–
117.
3. G. Hirst, “Bigrams of syntactic labels for authorship discrimination of short texts,” Lit. Linguist. Comput. 22
(4), 405–417 (2007).
4. E. Stamatatos, N. Fakotakis, and G. Kokkinakis, “Computer-based authorship attribution without lexical mea-
sures,” Comput. Humanit. 35 (2), 193–214 (2001).
5. N. Augsten, M. Bohlen, and J. Gamper, “The pq-gram distance between ordered labeled trees,” ACM Trans.
Database Syst. 35 (1), 1–36 (2008).
6. S. Sharoff and J. Nivre, “The proper place of men and machines in language technology: Processing Russian
without any linguistic knowledge,” in Computational Linguistics and Intellectual Technologies: Papers from the
Annual Conference “Dialogue” (RGGU, Moscow, 2011), Vol. 10, pp. 657–670.
MOSCOW UNIVERSITY COMPUTATIONAL MATHEMATICS AND CYBERNETICS  Vol. 39  No. 4  2015
199SOME WAYS OF DETERMINING WORD COMBINATIONS
7. A. S. Romanov and R. V. Meshcheryakov, “Authorship identification of short texts with machine learning tech-
niques,” in Computational Linguistics and Intellectual Technologies: Papers from the Annual Conference “Dialogue”
(RGGU, Moscow, 2010), Vol. 9, pp. 407–413.
8. C. J. C. Burges, “A tutorial on support vector machines for pattern recognition,” Data Min. Knowl. Discovery
2 (2), 121–167 (1998).
9. C.-W. Hsu and C.-J. Lin, “A comparison of methods for multiclass support vector machines,” IEEE Trans.
Neural Networks 13 (2), 415–425 (2002).
Translated by K. Gumerov
