E-Mail Authorship Attribution
applied to the
Extended Enron Authorship Corpus (XEAC)
Hendrik Neumann
Applied Computer Science
Ruhr-University Bochum
Germany
johann.neumann@rub.de
Martin Schnurrenberger
Applied Computer Science
Ruhr-University Bochum
Germany
martin.schnurrenberger@rub.de
03.04.2009
Abstract
In this paper we use the Extended En-
ron Authorship Corpus (XEAC) and writ-
ing style feature extracting tools, that we
implemented in JAVA and PHP, to create
ARFF data files, so the data can be exam-
ined with Weka. We reveal our results and
provide corpus resources for future explo-
rations.
1 About this Document
This document was written in the course of the sem-
inar Linguistische Distanz at the Department of Lin-
guistics of the Ruhr-University Bochum in Germany
as part as the seminars final exam.
1.1 General Terms
text classification, authorship attribution
1.2 Keywords
natural languange processing, e-mail, weka, feature
set subspacing, xeac, enron corpus
1.3 Referencing
Please refer to this document as follows:
H. Neumann and M. Schnurrenberger. E-Mail Au-
thorship Attribution applied to the Extended Enron
Authorship Corpus (XEAC). 2009.
2 Credits
Our work would not have been possible in the given
time, if it were not for the existance of the Enron E-
Mail Corpus, Ben Allison who contributed the En-
ron Authorship Corpus and last but not least the peo-
ple who created the Weka tool for classification and
more.
3 Introduction
Our topic is e-mail authorship attribution. That
means finding the correct author to a given e-mail
body by extracting features from it which in part
represent the author’s writing style. One could of
course read the name at the bottom of the e-mail,
but we assume that the author of the e-mail could
have pretended to be someone else and so the name
at the bottom would be fake, the writing style on the
other hand should have remained.
There are many applications for this topic. Spam
filtering, automated e-mail filing and forensic
purposes are just some of them.
To test our assumptions we use the XEAC corpus
which is an extension of the Enron Autorship
Corpus (which is a subset of the Enron E-Mail
Corpus).
We create tools (written in JAVA and PHP) for
extracting writing style features and creating
attribute-relation file format (ARFF) files, to use
them with the Weka tool for classification.
4 XEAC
The XEAC (Extended Enron Authorship Corpus)
was created in the course of this work. As the name
suggests it is an extension of the Enron Authorship
Corpus, meaning for one thing that the EAC is fully
contained within the XEAC.
The primary motivation for creating the XEAC was
for ease of use (using XML for instance), including
the original data (from the Enron E-Mail Corpus)
and providing the XEAC-split which defines a
training-set, test-set and unused-set.
The XEAC-split is meant to allow future researchers
to compare their results over this corpus to earlier
results of others.
At the time of this writing the XEAC could be
downloaded from
http://code.google.com/p/eyebachelor/downloads
Please view the README.txt contained within
the download for more details on XEAC.
5 Tools
To create an ARFF file to use with the Weka tool
and to be able to specify features that represent parts
of the authors wirting style, some tools were imple-
mented which are described in the following.
For more information on ARFF files and the Weka
tool please view Witten and Frank (2005).
5.1 Java Tool
We have implemented a graphical programm called
the authorship attribution client (AAC) which con-
sists of the following components:
• AuthorshipAttributionGUI
• AuthorshipAttributionCore
• AuthorshipAttributionSDK
• AuthorshipAttributionMetrics
• AuthorshipAttributionTagger
• AuthorshipAttributionTestSystemAnalyzer
The program is written in Java, so it is plattform in-
depedent and it comes with an easy-to-use installer
program. It also uses several third-party-tools which
are partial optional but improve the usability of the
program. The following third-party-tools are used:
• Graphviz: Graphviz (short for Graph Visual-
ization Software) is a package of open source
tools initiated by AT&T Research Labs for
drawing graphs specified in DOT language
scripts. It also provides libraries for software
applications to use the tools. Graphviz is free
software licensed under the Common Public
License.
• Weka (Waikato Environment for Knowledge
Analysis) is a popular suite of machine learn-
ing software written in Java, developed at the
University of Waikato. WEKA is free software
available under the GNU General Public Li-
cense. We have directly embedded WEKA in
our Java-program. Due to the fact, that WEKA
is licensed under the GPL we also licensed the
AAC under this license.
• Apache and PHP are needed to write and use
plugins for the AAC written in PHP. During the
installation-process the user is asked to specify
a directory which is a public folder in a web-
server with PHP support. In this directory the
installer copies the PHP-file. This file is called
from AAC during runtime to extend it’s func-
tionality.
• RASP (Robust Accurate Statistical Parsing;
only running on Unix-system) is used to ex-
tract the grammatical-structure of the sen-
tences. This informations can be used to im-
plement some interesting features.
• Stanford Log-linear Part-Of-Speech Tagger is
also directly embedded in the installer and can
be used by the extensions
The program is designed very modular. For all im-
portant components it is possible to write personal
extensions. The extensions must be packed in a
JAR-file and copied into the modules-folder of the
program. During the startup-process of the program
all modules in this directory will be loaded and dis-
played in the GUI. The JAR files for each component
is created by using the build-tool ANT. The XML-
files used by ant have also been developed exclu-
sively for the AAC.
The AAC seems to be a stable and powerful applica-
tion. Therefore it will be supported and enhanced in
the future (visit http://www.hendrik-neumann.de for
more informations).
5.1.1 Component: Graphical User Interface
The Graphical User Interface (GUI) has been cre-
ated by using the SWING-Framework. It consists of
several panels which will be displayed to the user.
The project currently uses 28 Java-files to realize the
visual output.
5.1.2 Component: Core Application
The core application is responsible for loading and
connecting all program components. It displays and
manages the GUI and manages all the plugins and
extensions. It also handles the connection to third-
party-tools like GraphViz and WEKA. The core-
applications contains 4 Java-files and over 300 lines
of code.
5.1.3 Component: Development Kit
The software development kit (SDK) is an indepen-
dent project which defines all important data and in-
terfaces for the AAC. It is embedded in all exten-
sions, the core- and the GUI-component. It consists
of 18 Java-files and over 1.300 lines of code.
5.1.4 Component: Metrics
The metrics-project contains some reference-
implementations for features we used during our
presentation.
Currently it contains the following metrics (or ”fea-
tures”):
• Categorisation: Uses Chi-square distribution to
determine the most distinctive top-words of an
author.
• GrammarStructure: Uses RASP and the Chi-
square distribution to create most distinctive
grammatical structure of an author.
• SentencesPerMail: Counts the average amount
of sentences in a mail
• WordsPerMail: Counts the average amount of
words in a mail
• WordsPerSentences: Divides the average
amount of words in a mail through the average
amount of senteces.
• PhpMetric: Is a generic component to wrap any
features implemented by the PHP-Website to
the AAC.
We have embedded this project in the AAC-installer
- so the user can directly out of the box test using
these features. If the user wants to develop individ-
ual features/metrics he needs to create a new Java-
file which implements the interface MetricInterface.
6 Java-files with over 400 lines of code where added
to this project.
5.1.5 Component: Tagger
Some features need additional informations of a
text which can be requested by the features using
taggers.
Currently we have implemented two Taggers: The
stanford tagger, which we haven’t used in any of the
current existing programs and the RASP-program
(maybe the component-name tagger isn’t very
accurate here). There is a Java-file for each if this
programs, and both files have over 150 lines of code.
New tagger-programs may be attached to the AAC
by creating a Java-file which implements the inter-
face TaggerInterface.
5.1.6 Component: TestSystemAnalyzer
A test-system-analyzer is a program which trans-
forms any kind of input data (corpora) in the AAC-
specific data model. Currently we support two types
of corpora:
• Standard mailbox files: It is possible to add any
kind of emails to the program which are stored
in the local file system by an email-program.
• XEAC-XML-files developed by Martin
Schnurrenberger
New test-system-analyzer may be developed by cre-
ating a Java-file which implements the interface
TestSystemAnalyzer.
New PHP-features can be directly implemented in
the PHP-page. All new features in the PHP-site will
be dynamically recognized during the AAC starts.
Each TestSystemAnalyzer has it’s own Java-file
which contains over 700 lines of code.
5.1.7 Component: Installer
The installer has been created by using IZPACK.
It consists of 7 XML-files which will be compiled
by the IZPACK-program. The output of this pro-
cess is a file called AAC.exe (Windows-version) and
AAC.jar (platform independent version).
5.1.8 Workflow of the AAC
The final workflow for the AAC can be described
like follows: The user starts the AAC. During this
process the core-component loads all JAR-files in
the module-folder of the program. The modules
may include taggers, features and testsystem-
analyzers. In the next step the core-component
displays the GUI of the program and registers all
extensions (modules) to it.
Now the user can either add more modules manually
or add a new corpora to the system. By clicking
on the corresponding button he now needs to select
the specific extension which should be used to
analyze the new corpora. In the next step he needs
to provide a directory which contains the corpora
data.
Now the selected test-system-analyzer iterates over
all files in this directory and converts the data to
authors and corpora. This process can be repeated
with other data - so it is possible to add multiple
corpora of different types to the AAC.
Now the user can browse all registered authors and
corpora or directly start the author-ship-attribution-
process. If he does so a new window is displayed in
which it is possible to selected any of the registered
metrics (or ”‘features”) which should be included in
this process.
Now the AAC applies each corpus to each of the
selected ”features”. The result of this process is
an arff-file for the software-suite WEKA. This arff-
file will be placed in the program-folder and directly
processed by WEKA. In the end the user is rewarded
by a new window which displays the results of this
process. If the software GraphViz is installed on the
local computer the user can also view the resulting
decision tree.
5.2 PHP Tool
Next to the Java Tool, we further implemented a
tool providing a webinterface and using PHP (let it
be noted here, that the GUI is in German).
It lets you choose one of the nine authors (supported
by XEAC), choose the preferred split (at the time of
this writing only XEAC-split is available), choose
which parts of the e-mail to use (momentarily only
the body part may be used), choose which tokenizer
to use (again only one is supported in the current
version), choose which writing style features to use
and which sets of the XEAC-split to use for training
and creating the ARFF file.
The features supported by the current version do not
need to be trained, so the second to last option can
be ignored.
At the time of this writing you may view a live
demo of this Tool at
http://dev.yomey.com/sandbox/linguistischedistanz
Unfortunately the server on which it is hosted
does not provide enough memory for processing to
finish. It stops as soon as the memory has exhausted.
Nevertheless you can get a feeling for what it does
and if it interests you feel free to contact us in this
matter.
6 Features
The writing style features that we used in our work
were rather basic but did surprisingly well.
In all the papers we sighted, the authors were rather
non-conforming as to what a good feature for this
task would be and as to how many should be used.
We continue this.
6.1 Number of Sentences
We figured that a person could have the tendency
to mostly writing many sentences or mostly writing
only few sentences.
To avoid the debate over what exactly a sentence is,
in our case roughly speaking it is any number of to-
kens followd by ”.”, ”?” or ”!”.
6.2 Mean Sentence Length
Some people may love to write long sentences (con-
taining numerous tokens), while others my hate
writing in general and always keep it short.
To account for this the lengths of all sentences
within a given e-mail are determined and summed
and divided by the number of sentences.
6.3 Type-Token Ratio
This feature is often used to describe the authors vo-
cabulary richness. For every first occurence of a
token a type variable ist generated. In the end the
number of type variables is divided by the number
of tokens in the e-mail body. The result is a num-
ber between zero and one, where one would indicate
vocabulary richness and the further we go towards
zero the less the author is capable of using different
words.
6.4 Squared Hello Distance
Yes, we know, this feature sounds rather silly but, it
surely did its part in achieving the classification re-
sult we did. At least we would like to think that.
What it does is investigate how far the author’s be-
gining five letters of the e-mail are away from the
word hello.
It does this by simultaniously going through each
letter of the word hello and for example the word
firstword, retreiving the ASCII value of the current
letters in each word, subtracting those values and
summing up all subtractions. The result is squared
to get rid of numbers less than zero. It should be
noted that if the first word realy was firstword the
algorithm only compares the first five letters, which
in this case would be first. A possible mathematical
description is shown below.
(
∑
i
ord(helloi)− ord(firstwordi))2 (1)
What is the benefit? Well, it is supposed to reveal
people who start their e-mail messages mostly in the
same manner. For instance if a person always starts
her e-mails with the word hello the distance would
always be zero, while another person who starts his
e-mails with howdy would always have a distance
grater than zero.
6.5 Single Quotationmark - Number of Tokens
Ratio
It makes sence that an author could have tendencies
to using many or little abreviations like it’s, let’s or
I’ll. The number of these is devided by the number
of tokens in the e-mail body.
6.6 Rule Type - Rule Token Ratio
Just like we considered the vocabulary richness of
the author, we know hope to capture something like
the ability of the author to use various grammatical
constructions. The rules generated by RASP were
provided by the EAC and are contained within the
XEAC as well.
7 Results and Discussion
The generated ARFF files included all of the above
features. We generated three files for each of the
nine authors in the XEAC.
The first ARFF file was created over all the sets
(training-set, test-set, unused-set) of the XEAC-split
in order to be able to compare the results to Allison
and Guthrie (2008). The mean accuracy we achieved
lies at 89.22%, which exceeds Allison and Guthrie’s
best value of 87.05% (Allison and Guthrie, 2008).
The second ARFF file was created over the training-
set and test-set to examine the improvement which
should occur by leaving out the unused-set, which
holds e-mails that contain writing style information
of other people besides the author. In some cases the
results have worsend. This may be because of some
e-mails in the unused-set may have had some for-
eign style feature which was unique for this author.
The last ARFF file was created over only the test-set.
The results of the latter should be used as compari-
son in future research over the XEAC to compensate
for algorithms that use the training-set for learning.
As a classifier we use the C4.5 decision tree learner,
which is called J4.8 in Weka, and use tenfold cross-
validation to evaluate the data.
8 Conclusion
E-mail classification versus regular text classifica-
tion has its own charm. Dealing with these short nat-
ural language texts, from a liguistics point of view,
can be quite tedious, because people writing e-mails
often forget about grammar and correct punctuation
and so on.
However, as we find, a great basic e-mail authorship
corpus has derived from this work.
Tools for feature extraction have been built, which
Figure 1: Results. This Figure shows the results of
the C4.5 decision tree learner J4.8 from Weka us-
ing tenfold cross-validation. For each author the
writing style features were derived from the three
depicted combinations of sets (sets defined by the
XEAC-split), hence the three columns. The Test-set
column should be used for comparison.
can be extended or simply be used for demonstra-
tion.
Considering the writing style features used, a highly
acceptable result has been achieved.
It should now be further investigated how to im-
prove accuracy by using different or additional writ-
ing style features or algorithms.
The only question which remains is wether the cor-
pus and especially the test-set is large enough for the
results to be of scientific value - we believe it is fine.
- cheers
References
B. Allison and L. Guthrie. Authorship Attribution of E-
Mail: Comparing Classifiers Over a New Corpus for
Evaluation. In Proceedings LREC’08, 2008.
H. Berger and D. Merkl. A Comparison of Text-
Categorization Methods applied to N-Gram Frequency
Statistics. 2003.
C. E. Chaski. Whos At The Keyboard? Authorship At-
tribution in Digital Evidence Investigations. Inter-
national Journal of Digital Evidence, vol. 4, issue 1,
2005.
M. W. Corney. Analysing E-mail Text Authorship for
Forensic Purposes. 2003.
O. de Vel. Mining Email Authorship. 2000.
O. de Vel, A. Anderson, M.Corney and G. Mohay. Min-
ing E-mail Content for Author Identification Forensics.
2001.
D. Estival, T. Gaustad, S. B. Pham, W. Radford and
B. Hutchinson. TAT: an author profiling tool with
application to Arabic emails. Proceedings of the
Australasian Language Technology Workshop: 21-30,
2007.
R. Forsyth. Notes on Authorship Attribution and Text
Classification. 2007.
S. Hill and F. Provost. The Myth of the Double-Blind
Review? Author Identification Using Only Citations.
2003.
F. Iqbal, R. Hadjidj, B. C. M. Fung and M. Debbabi. A
novel approach of mining write-prints for authorship
attribution in e-mail forensics. Digital Investigations
5: 42-51, 2008.
A. Kaster, S. Siersdorfer and G. Weikum. Combining
Text and Linguistic Document Representations for Au-
thorship Attribution. 2004.
T. Kucukyilmaz, B.B. Cambazoglu, C. Aykanat, and F.
Can. 2001. Chat mining for gender prediction, Lec-
ture Notes in Computer Science. Conference Publica-
tions, vol. 4243, pp. 274283, 2006.
K. Luyckx and W. Daelemans. Authorship Attribution
and Verification with Many Authors and Limited Data.
Proceedings of the Twenty-Second International Con-
ference on Computational Linguistics (COLING ’08):
513-520, 2008.
M. B. Malyutov. Authorship attribution of texts: a re-
view. Kluwer Academic Publishers, 2005.
C. D. Manning and H. Schuetze. Foundations of Statisti-
cal Natural Language Processing. MIT Press, 1999.
V. Nieto. Authorship attribution with help of language
engineering. 2004.
C. Sanderson and S. Guenter. On Authorship Attribution
via Markov Chains and Sequence Kernels. 2006.
C. Sanderson and S. Guenter. Short Text Authorship At-
tribution via Sequence Kernels, Markov Chains and
Author Unmasking: An Investigation. 2006.
E. Stamatatos. Authorship Attribution Based On Feature
Set Subspacing Ensembles. International Journal on
Artificial Intelligence Tools, vol. XX, no. X,: 1-16,
2006.
E. Stamatatos. A Survey of Modern Authorship Attribu-
tion Methods. 2008.
E. Stamatatos, N. Fakotakis and G. Kokkinakis.
Computer-Based Authorship Attribution Without Lex-
ical Measures. Academic Publishers, Computers and
the Humanities 35: 193214, 2001.
E. Stuckey. Methods of Token-Based Authorship Attribu-
tion for an English-Language Online Discussion Com-
munity. 2004.
I. H. Witten and E. Frank. Data Mining, Practical Ma-
chine Learning Tools and Techniques. Morgan Kauf-
mann, 2nd Edition, 2005.
No Author Given. Authorship attribution via combina-
tion of evidence. 2005.
