A Non-linear Illuminations Balancing for 
Reconstructed Degraded Scanned Text-Photo Image 
Maythapolnun Athimethphat1 and Vorapoj Patanavijit2. 
 
 
 
Abstract—Due to the advancement of digital media, a large 
number of electronic books are digitized from old paper books 
through digital cameras or scanners. The scanned image often 
contains the distractions such as noises outside the page 
boundary, skewed pages, and irregular distributions of image 
illumination that may degrade the quality of scanned images. As 
for this paper, we propose an alternative algorithm to improve 
the illuminated distribution of the scanned images. The 
algorithm can be applied on the uneven illumination where 
significant background shadows or illumination exists. It was 
tested with text-photo images including different types of 
document and desecration. The experiment results have shown 
that the proposed algorithm has produced a slightly 
improvement in performance in some cases of the text-based 
images, yet the improvement can be clearly seen for text-photo 
images as the results were compared with LLBT and SIBT 
methods. 
 
Index terms—digitization, uneven illuminations, light 
balance, object mark, and edge detection. 
I. INTRODUCTION 
Traditionally, scanning process is the first step of book 
digitization where the process often involves human labor of 
manual operations on a large quantity of pages and is prone to 
inaccuracy of page placement. Digital preservation literally 
refers to a series of activities maintaining access to digital 
materials over a period time [1]. Digitizing or digitization 
refers to a means of creating digital surrogates of analog 
materials such as books, newspapers, microfilms and 
videotapes. Transformation of the old paper books through 
digital cameras or scanners can be done by digitization. Texts 
and images can also similarly be digitized as the scanner 
captures an image or a text, and then converts it to an image 
file, such as bitmap.  The process of digitization allows 
information to be easily preserved, accessed and shared.  
Usually the sources of the image are paper-based printed such 
as journal papers, letters, newspapers, memos, and textbooks 
[2-3], [11]-[12]. However, a common problem in digitization 
is poor document image [4], [13]-[15]. 
 The poor document scanned image often contains the 
unwanted noises outside the page boundary, skewed pages, 
and irregular distributions of image illumination that may 
deform the quality of the scanned image and degrade 
document image, thus leading to poor image quality. Image 
quality is important for viewing and printing a re-mastered 
digital book. 
Binarization is normally used as a standard algorithm in 
manipulating the quality improvement of degraded document 
algorithm. Yet, several cases of proposed algorithms use 
global and local algorithm. Common algorithms are usually 
done through the utilization of single threshold to the whole 
image. Nevertheless, local thresholding algorithm uses 
different thresholds to different segments of the image. In 
1979, Otsu [16] had found a classical algorithm in image 
binarization which helps reducing the gray-level image to a 
binary image for classifying foreground and background with 
global threshold. Otsu proposed algorithm that can be 
iteratively applied to a gray scale histogram of an image for 
generating threshold candidates. In 1986, the extreme local 
thresholding algorithm [17] was proposed by calculating 
mean local thresholds. Later in 1986, Niblack [10] had 
presented the mean local threshold and the local standard 
deviation of neighborhood region. Zhang and Tan [18] 
improved Niblack’s algorithm by reducing noise sensitivity in 
2001. In 1993, Liu, Fenrish and Srihari [19] presented a 
texture feature based on thresholding algorithm that shows a 
result of the limitation of existing thresholding algorithm in 
document image binarization. Later, Liu and Srihari [20] 
proposed a texture feature that was computed from the run 
length histogram that gains benefits from both the occurrence 
matrix and Laplacian operator in 1997. This algorithm 
separated gray-level ranges of the gray-scale histogram and 
gained highly structured-stroke units from text images. Two 
new specialized algorithms, in order to determine a local 
threshold for each pixel, was proposed by Sauvola [21] in 
2000. Five typical local thresholding algorithms were 
compared in Graham’s [22] project whose came up with 
automatic selection and combination that were appropriate for 
thresholding algorithms in 2003. The document image 
binarization was presented by Ergina [23] in 2005. The 
algorithm presented applies global thresholds as well as 
histogram equalization. Yet, in 2006, Ergina and Efstathios 
[24] proposed the algorithm of hybrid binarization which was 
believed to have improved the quality of degraded document 
images by using the combination of the global and local 
threshold algorithm. 
Nevertheless, Hsia and Tsai [5] presented a novel light-
balancing algorithm (ELBT) for improving the illumination of 
text images in 2005. The adaptive signal processing including 
 
1 Department of Business Data Analysis, Faculty of Science and Technology, 
Assumption University, Bangkok, Thailand (Email: Maytha@scitech.au.edu) 
2 Department of Computer and Network Engineering, Faculty of Engineering, 
Assumption University, Bangkok, Thailand (Email: Patanavijit@yahoo.com)  
1158978-1-4244-7010-5/10/$26.00 c2010 IEEE ISCIT 2010
adaptive content, background separation, linear interpolation, 
and adaptive AGC, was used. Hsia and Tsai strongly believed 
that ELBT can overcome the disproportional light distribution 
problem from black/white text documents. As a result, Hsia 
and Tsai [6] proposed line-based light-balancing algorithm 
(LLBT) in 2006. Yet, LLBT was claimed to be ineffective for 
photo image and unable to balance the illumination of text- 
photo images. In 2009, Lee, Chen and Chang [7] proposed 
Sobel mechanism to determine text-photo object and design 
illumination-balance algorithm (SIBT) for texts and text-
photo images. Ultimately, the outcome of SIBT was believed 
to be better than ELBT and LLBT. Although SIBT was 
ineffective for text-photo images, we aim to defeat this 
particular obstacle. Because of unequal light distribution, 
scanned text-photo images emerge uneven illumination. 
Hence, we propose a new algorithm for improving the quality 
of digitization. This algorithm is based on a simulation of 
light distribution for light balancing technique, and SIBT 
algorithm to illuminate the scanned images can be done.  
 
The next section reviews SIBT, and is followed by the 
proposed algorithm (NIBT). The test procedures are described 
and the test results of various case studies to confirm the 
effectiveness of the proposed method are presented as well as 
the discussion on further refinement of the light-balancing 
algorithm is suggested. 
II. INTRODUCTION OF SIBT ALGORITHM 
The SIBT [7] algorithm portrayed in figure1 consists of 
four phases: edge detection phase, object mark phase, light 
evaluation phase, and illumination balance phase. Notations 
used all over SIBT are shown below: 
I. of images Edge        ,....,, ,2,1 neEIEIEI  
X. image of  valuepixel th i                          )( ipvX  
images.on distributiLight                                  LD  
A. Edge Detection Phase 
Edge detection is applied on the gray-scale scanned image. 
Sobel mechanisms [8]-[10] are employed in this phase. Sobel 
edge detector is mostly used in this field as it facilitates the 
implementation. Edge detection phase is used to determine 
the text and photo objects in a scanned text-photo images. The 
followings are four Sobel edge detectors utilized in SIBT. 
 
-1 0 1  -2 -1 1  -1 -1 -1  0 1 2 
-2 0 2  -1 0 1  0 0 0  -1 0 1 
-1 0 1  0 1 2  1 2 1  -2 -1 0 
Detector(1).0°  Detector(2).45°  Detector(3).90°  Detector(4).135° 
 
Average edge image is computed by: 

=
×
=
=
4
1 1
)(
4
1)(
n
wh
i
iniavg pvEIpvEI .  (1) 
The edge detection phase is for removing text and photo 
objects from a test image with square-marked object 
processing.  
 
B. Marked Object Phase 
According to the first phase which is, edge detection, we 
keep track of the location of text and photo in a test image. 
Then, trace the exterior boundaries of the objects in the binary 
image by searching only for object boundaries from edge 
information. Finally, mark them with squares to form the 
marked image.  
 
Figure1. Diagram of the proposed algorithm 
C. Light Evaluation Phase 
The discrimination method for objects and background is 
granted in order to simulate the light distribution from the 
marked image. The linear interpolation will then generate the 
marked areas to gain the information of light distribution. The 
linear interpolation is as follows: 
kmpvIpvIpvIpvLD flkfkf ×−+= −+−+−+ ]/))()([()()( 1111  (2) 
Where k is the kth pixel and m is the total number of pixels 
in marked section. 
D.  Illumination Balance Phase 
At the final phase, the test image adjusts the light 
distribution in illumination balance phase. The processed 
image is generated by fusing both light interpolated 
background image and the test image with adaptive gain 
control (AGC) equation. 
The AGC can be expressed as follows: 
),(ImageTest
),(onDistributiLight
),( yx
yx
levelBalanceyxoutput ×=  (3) 
III. THE  LIGHT MODEL USING NONLINEAR  
Figure 2 demonstrates the flowchart of the entire proposed 
algorithm. According to the given flowchart, at the first stage, 
we try to locate text and photo objects by using edge detection 
algorithm to separate the content and background. Then the 
light modeling is used to evaluate light distribution from the 
marked image. In light evaluation phase, we try to simulate a 
new value instead of the marked pixels in order to get the 
appropriate light distribution. The accurate generation of light 
distribution is very important as the quality of light balancing 
depends on the accuracy of light distribution. 
In figure 3, the large object overlaps the shadow regions 
can be noticed. This problem affects light distribution, which 
is insufficient to generate favorable results. The accurate 
generation for light distribution is very important. To obtain 
accurate light distribution or background image from the 
SIBT algorithm, it is essential that light distribution data have 
1159
to be well approximated. In the assessment process, we search 
for each pixel from the marked image to find the marked 
sections. Vertically search the marked section in the marked 
image column by column, and if a marked section is found, 
then record the position. The global threshold is adopted to 
filter out outlier from the record then it will calculate the 
mean of the unmarked pixel value. 
 
Edge
 
Marked Object
LD image
 
Figure 2 Processing Flowchart 
 
Global threshold is computed by  
pixel unmarked of number Total
yxupv
GTH
all
x
all
y

= == 1 1
),(
 (4) 
Where upv(x,y) is the unmarked pixel. 
 
The discrimination method for objects and background 
is granted in order to simulate the light distribution from the 
marked object image, and partition the uneven illumination 
region. Hence, we can form the light distribution from the test 
image as presented in figure 3(b).  
In order to determine the cast shadow regions, we 
utilized polynomial equation to approximate the shadow 
regions by using the information from the unmarked region in 
figure 3(b). Find the coefficients of a polynomial of degree n 
that fit the data from the unmarked region to mimic the 
illumination distribution. As a result, the polynomial curve 
fitting algorithm (Polyfit) would belong to MATLAB library 
to formulate shadow boundary (m1) and light intensity (m2) 
for each location as shown in figure 4(a). 
In figure 3(d), recasting light distribution by linearly 
interpolate pixels of the marked section, shows the light 
intensity distribution that was simulated from nonlinear 
equation and SIBT.  
 
3(a)Test Image 3(b)Partition Image
 
3(c) SIBT Algorithm 3(d)  Our Algorithm 
Figure3 Light Distribution Image 
 
4(a) Test Image 
 
4(b) SIBT Algorithm 
1160
 
4(c) Our Algorithm 
Figure4 Illumination Intensity Distributions  
Subsequently, the synthesized images seem to agree with 
our visual intuition as shown in figure5 and 6. We found that 
the equation performed well in terms of both computational 
and efficiency and solution accuracy.  
IV. EXPERIMENTS 
In this section, the experiments and outcomes obtained by 
the proposed algorithm are presented. The experiments were 
performed in MATLAB. In order to demonstrate the proposal 
performance of our algorithm, we generated the degraded 
document images by adding natural shadows to the scanned 
images from different direction. We have conducted several 
experiments to test and validate our algorithm. Most of the 
sample images used in our experiment is consisted of two 
categories of image: text image and text-photo image. For this 
paper, PSNR is used as the performance indicator. 
 
A. Performance Evaluation 
We have compared the original image with computed 
images by human visual perception and used the peak-signal 
to noise rate (PSNR) in order to evaluate the image qualities 
of result images. PSNR is most commonly used as a measure 
of quality of reconstruction of lossy compression. 
PSNR is defined as: 
       PSNR = 10log10 (MAX2/MSE) dB.  (5) 
Where MAX = is the maximum possible pixel value of the 
image. The pixel presented on the gray-scale image was 255. 
The mean square error (MSE) for two m × n images is as 
follows: 
 

= =
−
×
=
m
i j
jiji POnm
MSE
1
n
1
2
,, )(
1
         (6) 
Therefore, we can calculate the PSNR of each computed 
image via (5). 
B. Experiment results 
The outcomes of the experiment are shown in PSNR. 
Presented in Table I. is the result of benchmarking value that 
has been performed to the text-image. Figure 3(c), 3(d), and 
3(e) compare three results of the output of the our proposed 
algorithm, LLBT and SIBT algorithm. As shown in the Figure 
3 and Table I, the overall results demonstrate favorable 
performance to the proposed algorithm, and the others. Both 
SIBT and NIBT achieve almost similar PSNR rate; yet, there 
are no differences between the three algorithms.   
 
Table I. Compression of performance using proposed algorithm and other 
algorithm for Text Images 
  Image(DB)   
Methods image1 image 2 image 3 image 4 
LLBT 26.8573 26.4670 11.8712 11.8694 
SIBT 29.3183 28.4974 11.6752 11.6878 
NIBT 29.4886 28.4263 11.6719 11.6857 
 
Table II. shows the comparisons between the performance 
of SIBT algorithm, LLBT, and the proposed algorithm with 
various text-photo images. Candidly, our algorithm beats 
SIBT, and LLBT algorithm in terms of improving the quality 
of text-photo images. Figure 4(c), 4(d) and 4(e) show the 
significant improvement of image quality. 
 
Table II. Compression of performance using proposed algorithm and other 
algorithm for Text-Photo Images 
  Image(DB)   
Methods image 5 image 6 image 7 image 8 
LLBT 14.8303 13.2847 14.4766 13.1067 
SIBT 21.3176 18.4962 14.7962 12.1919 
NIBT 22.9583 20.6904 17.8839 13.7854 
 
Experimental studies have shown that appropriate light 
interpolation background provides excellent result images. 
V.  CONCLUSIONS 
Document quality is an important basic resource needed for 
most online libraries. The quality of the scanned images affect 
subsequent digital document. In this algorithm, a light balance 
algorithm is proposed using signal processing where this 
algorithm aims to remove of uneven light background. 
 
 The main algorithm consists of edge detection, light 
modeling, polynomial, linear interpolation, and light 
balancing. This algorithm succeeds in good light balancing 
for text-photo image.  The outcomes have been compared 
with other methods and are gratifying. The algorithm has the 
following delightful advantages such as simplicity, and is 
based on basic technique; low computational cost due to its 
simplicity. To succeed the higher quality, the light modeling 
should be improved. The machine learning may improve the 
quality of light modeling, which can help achieving higher 
performance. 
Future work will concentrate on light modeling by using 
machine learning and combinations of thresholding 
algorithms for separate shadow region to gain the best light 
modeling. 
REFERENCES 
[1] “What is Digital Preservation”. Library Technology Reports 
44:2 (Feb/March 2008): 5. 
[2] Antonacopoulos A., and Karatzas D., “Document image 
analysis for World War II personal records”, First International 
Workshop on Document Image Analysis for Libraries 
(DIAL’04), p.336-341, 2004. 
1161
[3] Marinai, S., Marino E.,and Cesarini, F.; Soda G., “A general 
system for the retrieval of document images from digital 
libraries”, First International Workshop on Document Image 
Analysis for Libraries (DIAL’04), p.150-173, 2004. 
[4] H. S. Baird.,“Difficult and urgent open problems in document 
image analysis for libraries”, First International Workshop on 
Document Image Analysis for Libraries (DIAL’04), p.25.32, 
2004. 
[5] S. C. Hsia, and P. S. Tsai, “Efficient light balancing techniques 
for text images in video presentation system”, IEEE Trans. 
Circuits Syst. Video Technol., vol. 15, no. 8, pp.1026-1031, 
Aug 2005. 
[6] S. C. Hsia, and P. S. Tsai,” A cost-effective line-based light-
balancing technique using adaptive processing”, IEEE Trans 
Image Process., vol. 15, no. 9, pp.2719-2729, Sep 2006. 
[7] Jung-San Lee, Chin-Hao Chen and Chin-Chen Chang, “A novel 
illumination-balance technique for improving the quality of 
degraded text-photo images”, IEEE Trans. Circuits Syst. Video 
Technol., vol. 19, no. 6, pp.900-905, June 2009. 
[8] N. Kazakova, M. Margala, and N. G. Durdle, “Sobel edge 
dection processor for a real-time volume rendering system”, in 
Proc. 2004 Int. Symp. Circuits Syst., vol. 2. Canada, pp. II913-
916, May 2004. 
[9] N. Kanopoulos, N. Yasanthavada, and R. Baker, “Design of an 
image edge detection filter using the sobel operator,” IEEE J. 
Solid State Circuits, vol. 23, no. 2, pp. 358-367, Apr. 1988. 
[10] Y. D. Qu, C. S. Cui, S. B. Chen, and J. Q. Li, “ A fast subpixel 
edge detection method using sobel-zernike moments operator”, 
Image Vision comput., vol. 23, no. 1, pp. 11-17, 2005. 
[11] Rangachar Kasturi, Koichi Kise “Camera/Video-Based 
Document Analysis Systems”,International Workshop on 
Document Analsysis Systems (DAS’05). 
[12] Jian Lian, David Doermann, Huiping Li “Camera-based 
analysis of text and documents: a survey”,IJDAR 2005. 
[13] Z. Zhang and C. L. Tan, “Restoration of images scanned from 
tick bound document”, Proc. Int. Conf. Image Processing., vol 1, 
2001, pp. 1074-1077.  
[14] E. Kavallieratou, “A Binarization Algorithm specialization on 
document images and photos”, Eighth International Conference 
on Document Analysis and Recognition (ICDAR’05), p.463-
467, 2005. 
[15] E. Kavallieratou and E. Stamatatos, “Improving the quality of 
degraded document images”, International Conference on 
Document Image Analysis for Libraries (DIAL’06). 
[16] Otsu N., “A threshold selection method form gray-level 
histograms”, IEEE Trans. Systems Man Cybernet. pp. 62-66, 
9(1), 1979 
[17] J. Bernsen, “Dynamic thresholding of grey level images”, Proc. 
Int. conf. Patt. Recogn., 1986, pp. 1251-1255, Paris, France. 
[18] W. Niblack, “An introduction to digital image processing”, 
pp.115-116, Prentic Hall, 1986. 
[19] Y. Liu, R. Fenrich, and S.N. Srihari, "An Object Attribute 
Thresholding Algorithm for Document Image Binarization," 
Proc. Second Int'l Conf. Document Analysis and Recognition, 
pp. 278-281,Tsukuba Science City, Japan, 1993. 
[20] Y. Liu, and S. N. Srihari, “Document image binarization based 
on texture features”, IEEE Transactions on PAMI, vol. 19, no. 5, 
May 1997, p. 540-544. 
[21] J. Sauvola, M. Pietikainenm, “Adaptive document image 
binarization”, Pattern Recoginition, pp. 225-236, 33(2000). 
[22] G. Leedham, C. Yan, K. Takru, J. H. Nata, and L. 
Mian,”Comparison of some thresholding algorithms for 
text/background segmentation in difficult document images”, 
Seventh International Conference on Document Analysis and 
Recognition (ICDAR’03). 
[23] E. Kavallieratou, “A Binarization Algorithm specialization on 
document images and photos”, Eighth International Conference 
on Document Analysis and Recognition (ICDAR’05), p.463-
467, 2005. 
[24] E. Kavallieratou and E. Stamatatos, “Improving the quality of 
degraded document images”, International Conference on 
Document Image Analysis for Libraries (DIAL’06). 
 
1162
 
                                          (a) Original scanned image                           (b)  Test image 
 
                            (c) SIBT(TH = 25, BL = 260)                                  (d) NIBT                                                   (e) LLBT 
Fig.5 Scanned text image 
 
                                          (a) Original scanned image                           (b)  Test image 
 
                            (c) SIBT(TH = 25, BL = 260)                                  (d) NIBT                                                   (e) LLBT 
Fig.6 Scanned text-photo image
 
1163
