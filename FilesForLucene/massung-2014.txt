Natural Language Engineering 1 (1): 1–23. Printed in the United Kingdom
c© 2014 Cambridge University Press
1
Non-Native Text Analysis: A Survey
SEAN MASSUNG and CHENGXIANG ZHAI
Department of Computer Science
College of Engineering
University of Illinois at Urbana-Champaign
{massung1,czhai}@illinois.edu
( Received 25 November 2014 )
Abstract
Non-native speakers of English far outnumber native speakers; English is the main lan-
guage of books, newspapers, airports, air-traffic control, international business, academic
conferences, science, technology, diplomacy, sports, international competitions, pop music,
and advertising (British Council 2014). Online education in the form of MOOCs (mas-
sive online open courses) is also primarily in English—even teaching English. This creates
enormous amounts of text written by non-native speakers, which in turn generates a need
for grammar correction and analysis.
In this paper, we provide a survey of existing work on non-native text analysis. We start
with an overview of common datasets used by researchers, comparing their attributes and
potential uses. Then, an introduction to native language identification follows: determining
the native language of an author based on text in the second language. This section
is subdivided into various techniques and a shared task on this classification problem.
Next, we discuss non-native grammatical error correction—finding and modifying text to
fix errors or to make it sound more fluent. Again, we discuss different methods before
investigating a relevant shared task. Then, we take a brief look at the developing field of
text simplification for non-native speakers. Finally, we end with conclusions and potential
future directions. While this survey primarily focuses on detecting and correcting non-
native English text, most approaches are general and can be used across any language
pairing.
1 Introduction
By the year 2020, British Council (2014) estimates that there will be two billion
English language learners. Some learn in the classroom; some learn online. Some
may even learn through their phone or in an online class. Regardless of the medium,
computational tools to enhance this educational experience will be valuable. Auto-
matic scoring of essays—not only for grammar, but also fluency—would contribute
greatly to second-language learners’ understanding. User personalization for online
services (including search engines and social networks) would benefit from improved
user profiling. More relevant books or news articles could be recommended if the
user’s background and competency of English were known.
Due to these many motivating examples, research in non-native text analysis has
2 Sean Massung and ChengXiang Zhai
Fig. 1. Dichotomy of non-native text analysis as part of the general text mining
and NLP domain.
prospered. This field encompasses any textual task that deals with words written
in a language other than the writer’s native tongue. We call the native language
L1 and the second, learned language L2. Throughout this survey, we will usually
assume that L2 is English, though most techniques discussed are general and could
function with any pair of L1 and L2.
Existing work in non-native text analysis is somewhat scattered, appearing in
workshops and shared tasks more frequently than standard proceedings. In this
survey, we provide a systematic overview of these scattered works to make it easier
for researchers to digest the state of the art in this emerging area.
1.1 Scope
The current work on non-native text analysis generally falls into three categories:
1. Native language identification (NLI): classifying L1 based on text written
in L2. Techniques can be categorized into feature-based (using a classifier)
or likelihood-based (using a probabilistic model).
2. Non-native Grammatical Error Correction (GEC): detecting and cor-
recting grammatical errors in L2 text. Techniques can be categorized into
targeted (correcting specific errors) or general (correcting all errors).
3. Text simplification (TS): providing a better experience for users interacting
with text in their L2. Techniques are much more varied in this field.
These three areas can be regarded as special cases of three more general categories
of text mining and NLP as shown in Fig 1.
A good overview of grammatical error correction with a focus on non-native
learners can be found in Leacock, Chodorow, Gamon, and Tetreault (2010). In
contrast, this survey is a more concise collection on the topic and consists of many
recent advances since 2010. Most importantly, we categorize work under the label
non-native text analysis, not just grammatical error correction.
Spelling correction described in Kukich (1992) is a potentially relevant task,
though we choose to focus on the grammar correction aspect instead, since spelling
Non-Native Text Analysis: A Survey 3
correction is practically L1-agnostic. That’s not to say, however, that spelling fea-
tures don’t play a role in (e.g.) NLI; rather, the method of spelling correction is not
relevant to non-native text analysis. Correcting speech is also an important issue:
besides the obvious speech recognition challenges in computer science and electrical
engineering, speakers tend to voice their words much differently than when writing.
Therefore, we do not include any investigation into these works in this paper.
Shared tasks provide a common goal and dataset to a wide array of researchers.
This enables quick and accurate comparison of different methods, while simultane-
ously increasing interest and producing exposure for the problem at hand. Thus, we
detail two relevant shared tasks in this survey that deal with non-native writers of
English. Author profiling, authorship attribution, and plagiarism detection at PAN
2013 with Pardo, Rosso, Koppel, Stamatatos, and Inches (2013) and PAN 2014
with Rangel, Rosso, Chugur, Potthast, Trenkmann, Stein, Verhoeven, and Daele-
mans (2014) are also related, but we do not focus on those here, instead considering
them related works since there is no specific component of non-native analysis.
1.2 Organization
We now review the structure of this paper. In each section, a figure briefly sum-
marizes the papers discussed and the methods used. If applicable, a comparison of
performance is also included.
First, in section 2, we introduce common datasets used in works described
throughout the rest of this survey. They should give some perspective on the task
at hand and create a mental baseline for the reader.
In section 3.1, we start with an introduction to NLI, usually formulated as a
classification problem. Observed data is text written in L2 (where L2 means En-
glish: the second or foreign language), and a classifier predicts the writers’ L1 (the
native language). Although the classification problem itself is relatively simple, the
features used distinguish different approaches. These range from simple unigram
word counting to parse tree reranking descriptions. In future sections, we will see
how classifiers themselves may be adapted or configured to take advantage of this
specific domain.
Section 3.2 discusses the NAACL HLT (Human Language Technologies) 2013
shared task in NLI using a corpus detailed in section 2.
Section 4.1 discusses correcting grammatical errors, such as articles and prepo-
sitions. These techniques are usually framed as classification problems; a classifier
determines if there is an error or not, and if so, predicts what the best solution
would be given a fixed set of candidates. Some methods go beyond a simple clas-
sification setup and rely on more NLP-heavy techniques such as language models
and grammatical parse trees to find the “best” corrected sentence.
Then, section 4.2 covers the CoNLL 2013 shared task on grammatical error cor-
rection. This task also uses a corpus described in section 2, and systems find and
correct errors in text written by non-native English speakers.
Section 5 briefly outlines how text is simplified or summarized for non-native
speakers. This subfield is the least mature of those discussed in this survey.
4 Sean Massung and ChengXiang Zhai
Dataset Docs Avg Len L1 Languages
CEEAUS 1,008 128 Chinese, English, Japanese (3)
FCE 1,244 200 Catalan, Chinese, Dutch, French,
German, Greek, Italian, Japanese,
Korean, Polish, Portuguese, Russian,
Spanish, Swedish, Thai, Turkish (16)
ICLE 3,640 700 Bulgarian, Czech, Dutch, Finnish
French, German, Italian, Polish,
Russian, Spanish, Swedish (11)
ICLEv2 6,085 617 Bulgarian, Chinese, Czech, Dutch,
Finnish, French, German, Italian,
Japanese, Norwegian, Polish, Russian,
Spanish, Swedish, Tswana, Turkish (16)
ICNALE 5,600 129 Chinese, English, Filipino, Hong Kong,
Indonesian, Japanese, Korean, Pakistani,
Singaporean, Taiwanese, Thai (11)
NUCLE 1,397 863 Unknown: data from NUS undergrads
TOEF11 12,100 348 Arabic, Chinese, French, German,
Hindi, Italian, Japanese, Korean,
Spanish, Telugu, Turkish (11)
Fig. 2. Comparison of non-native English datasets.
Finally, we end with a discussion and conclusions in section 6, reflecting on what
issues may still be unaddressed.
2 Non-Native Corpora
All the work on non-native text analysis uses some kind of non-native text corpus.
In this section, we give an overview of the major datasets used in existing work,
which we will survey in the next few sections. As shown in Fig 2, the sizes of these
corpora vary from around one thousand to twelve thousand documents with an
average length of 128 to over 800 words. They cover a wide L2 range, geographically
focused in Europe and Asia. We now give a detailed description of each dataset.
ICLE, by Granger (2003), the International Corpus of Learner English, is an
early popular dataset used to investigate NLI. It is a collection of essays on “ex-
Non-Native Text Analysis: A Survey 5
tremely varied” topics written by undergraduate students studying English. Addi-
tional metadata for each essay is part of the ICLE—such as gender and age—though
these features are rarely used by researchers. There are roughly the same number
of essays per L1. Commonly, a five-language subset of Russian, Czech, Bulgarian,
French, and Spanish languages is used, popularized by Koppel, Schler, and Zigdon
(2005). Thus it is important to note if this subset of the ICLE is used so accurate
comparisons may be made.
ICLEv2, by Granger, Dagneaux, Meunier, and Paquot (2009), is an expanded
version of the ICLE. There are no major differences in the text files themselves, since
the ICLE is a subset. Rather, version two additionally comes with a concordancer,
a graphical interface that allows easy search and simple exploratory analysis over
the dataset. However, when running experiments detailed in future sections of this
paper, researchers tend to use the raw text data from the corpus instead of any
built-in software.
CEEAUS, by Ishikawa (2009), is the Corpus of English Essays written by Asian
University Students. Unlike the ICLE, the essays are restricted to two topics: It
is important for college students to have a part-time job and Smoking should be
completely banned at all restaurants in the country. A common vocabulary for each
prompt emerges due to this fact, perhaps allowing a more meaningful word usage
analysis to occur. Further restrictions attempt at creating the most uniform testing
conditions as possible: spell checking is required, but dictionary use is prohibited;
the writers have twenty to forty minutes to complete their task. Aside from its small
size, the main downside to this corpus is that 77% of the essays are from native
Japanese speakers, establishing a rather high classification baseline for NLI.
FCE, created by Yannakoudakis, Briscoe, and Medlock (2011)—the First Cer-
tificate of English dataset—is a corpus of essays written by students studying En-
glish that took the Cambridge Assessment’s ESOL (English as a second or other
language) exam. The dataset was compiled as a benchmark for automated essay
scoring systems, and contains about eighty types of correction annotations in each
essay. Like the ICLE, additional metadata is also included, such as grade, age, and
native language. There are ten different prompts, each with a varying number of
responses. This corpus may be downloaded freely online1.
ICNALE, by Ishikawa (2013), the International Corpus Network of Asian Learn-
ers of English, is an expanded version of CEEAUS. Like the ICLEv2, the previous
version is a subset of this one. Aside from adding more Asian L1 languages, the lan-
guage distribution of the dataset is much more balanced. Professional proof readers
corrected the part-time job essays, and their revised versions are also included. This
provides an opportunity to see how the non-native text is translated into sounding
more native—with this information it is possible to do much more than NLI! The
last segment of the ICNALE is a collection of 1,900 recorded speeches and their
transcripts. After registering online, the corpus is available2 to freely download.
1 http://ilexir.co.uk/applications/clc-fce-dataset/
2 http://language.sakura.ne.jp/icnale/download.html
6 Sean Massung and ChengXiang Zhai
NUCLE, by Dahlmeier, Ng, and Wu (2013), the National University of Singa-
pore Corpus of Learner English, is also a collection of essays on a wide variety of
subjects. The essays were written by students studying English at the Center for
Language Communication at NUS and annotated and corrected by English instruc-
tors. Overall, nearly 47,000 errors were detected by the instructors with good rater
agreement. Findings on the corpus revealed that overall, errors in non-native text
are actually quite rare (at least in this corpus), on the order of around four per one
hundred words. Since this corpus was not intended for NLI, there is no information
about the L1 language of each essay. This is the dataset used by the CoNLL 2013
shared task described in section 4.2, and is available for download online3.
TOEF11, by Blanchard, Tetreault, Higgins, Cahill, and Chodorow (2013), is a
collection of essays from the Test of English as a Foreign Language, a component
of the Educational Testing Service. It is a standardized test measuring English
proficiency for students seeking to enroll in college. There are 1,100 essays per L1,
sampled from eight prompts. The purpose of the corpus was to further enable work
in NLI, and was used by the NAACL-HLT 2013 shared task described in section 3.2.
From our comparison, we have two datasets that are subsets of another (CEEAUS
and ICLE). There is likely no reason to prefer CEEAUS over ICNALE or ICLE
over ICLEv2, since the newer versions are simply expanded. This leaves five main
datasets: FEC, ICLEv2, ICNALE, NUCLE, and TOEF11. Of these five, FCE, IC-
NALE, NUCLE, and TOEF11 could be used for error analysis since they contain
annotations. All except NUCLE can be used for native language identification since
the L1 information is part of the datasets. All are publicly available, but the ICLEv2
and TOEF11 are not free and require a license.
As for content, all are essays written by university students, but only ICNALE
and TOEF11 have the essay topics strictly controlled—though the former has two
prompts and the latter has eight. ICNALE L1s are clearly focused in Asia; NU-
CLE L1s are unknown, while FCE, ICLEv2, and TOEF11 L1s are more evenly
distributed across the globe.
Considering these differences, we need to think about the following questions
when interpreting results. What is the baseline accuracy? How difficult is it to
distinguish L1s? Does the number of documents (and document length) have any
bearing on an impression of the results?
In the next three sections, we systematically review the three areas of non-native
text analysis.
3 Native Language Indentification
NLI is usually the first step in any second language error correction or author
profiling system. Identifying the native language of an anonymous text was first
popularized by (Koppel et al. 2005). Brooke and Hirst (2012) do an extensive survey
of NLI feature efficacy, and develops a robust model that works well when used
3 http://www.comp.nus.edu.sg/~nlp/corpora.html
Non-Native Text Analysis: A Survey 7
across corpora. NLI tasks are most commonly evaluated solely on a small learner
corpus usually consisting of student essays (as shown in section 2). It was previously
thought that lexical features would be biased or overfit towards essay topics, but a
cross-corpus evaluation showed that this was not the case.
For a more in-depth discussion of authorship attribution, we recommend the
reader consult Stamatatos (2009) or Koppel, Schler, and Argamon (2009). Many
techniques common to authorship attribution and author profiling are also relevant
to NLI.
As mentioned in section 2, the ICLE was an early popular dataset to evaluate NLI
tasks, especially using a subset of five European languages partitioned by (Koppel
et al. 2005). A table has been created (Fig 3) to portray the summary described
in Brooke and Hirst (2012). In addition to the results displayed in the table, Wong
and Dras also attempted to perform dimensionality reduction with LDA by Blei,
Ng, and Jordan (2003) as feature generation; however, this was not a successful
method.
Techniques for NLI can be categorized into two methods: feature-based and
likelihood-based. Feature-based methods rely on informative features derived from
the text and are fed to standard machine learning algorithms (usually SVM or Max-
Ent). Likelihood-based methods learn a probabilistic model (usually a grammar or
language model) for each L1 and assign a label based on the maximum likelihood
model. We now move onto specific techniques applied in NLI.
3.1 Techniques in Native Language Identification
The first feature-based method used by Tsur and Rappoport (2007) found that
incredibly simple top two hundred frequent bigram character features fed to SVM
led to 66% accuracy on the five native languages. They claimed that word choice
of non-native speakers is influenced by the phonology of their native language (as
evidenced by the effectiveness of the character features). This is compared to a un-
igram words baseline which achieved only 47% accuracy. They finally hypothesized
that using a spoken-language corpus would achieve even stronger results favoring
character bigrams since conscious effort put into speaking words is much less than
writing them. For analyzing transcripts of spoken words, the ICNALE corpus may
be applicable, described in section 3.
NLI has also been approached through contrastive analysis from Wong and Dras
(2009): the idea that errors in text are influenced by the native language of the
author. They investigated three error types as features: subject-verb disagreement,
noun-number disagreement, and determiner misuse. These error types are then used
as “stylistic markers” for NLI features with an SVM classifier. To find these errors
in text, they used an open source grammar checker4, as opposed to professionally
edited text. Interestingly, ANOVA showed that the features had a measurable effect,
but after combining their contrastive features with existing methods, they were not
able to significantly increase the classification accuracy from (Koppel et al. 2005).
4 http://queequeg.sourceforge.net/index-e.html
8 Sean Massung and ChengXiang Zhai
Paper Method Accuracy
(Tsur and Rappoport 2007) character n-grams∗ 66%
(Wong and Dras 2009) syntactic errors∗ 74%
(Wong and Dras 2011) syntactic rules∗ 80%
(Wong et al. 2012) adaptor grammars∗+ 76%
(Swanson and Charniak 2012) tree substitution grammars∗ 78%
Fig. 3. Summary of NLI results listed in (Brooke and Hirst 2012) for the ICLE
corpus; accuracies added to chart. ∗ indicates feature-based methods and + indicates
likelihood-based methods.
The previous authors, Wong and Dras (2011), follow their work on contrastive
analysis, attempting to amend its shortcomings. Instead of error types, they use
two different features obtained from grammatical parse trees: horizontal slices (pro-
duction rules) and parse rerankings. They claim these are the first pure syntactic
features used in NLI. For the production rules, they immediately applied infor-
mation gain dimensionality reduction. The reranking features are those contained
in the Charniak parser5 and Stanford Parser6 trained on the Wall Street Journal.
Unlike the previous two attempts, the authors found MaxEnt to outperform SVM
as the classifier. Additionally, five-fold cross validation was performed (as opposed
to ten-fold), which means the accuracies can’t be precisely compared with previous
work. In any event, they report a final accuracy of 80%, which was the highest
reported as of 2012.
Wong, Dras, and Johnson (2012) explore the last author’s—Mark Johnson’s—
adaptor grammars by Johnson, Griffiths, and Goldwater (2006) to generate features.
Simply, adaptor grammars are a non-parametric extension to PCFGs (probabilistic
context free grammars). They can learn arbitrary-length word sequences (collo-
cations); for example, gradient descent and cost function were learned under a
machine learning topic. These adaptor grammars are used in two ways: in the first,
collocations are used as features in a MaxEnt classifier. In the second, the gram-
mar is trained on each class (representing native language). At test time, the most
probable grammar to have generated the text is selected. For both tasks, the au-
thors use five-fold cross validation on seven native languages. In the feature-based
classification, they achieved 76%; in the language model-based classification, they
achieved only 50%, a performance similar to the unigram word baseline from Tsur
and Rappoport (2007).
Swanson and Charniak (2012) made use of tree substitution grammars (TSGs)
created by Blunsom and Cohn (2010). Various tree induction methods are compared
5 http://cs.brown.edu/~ec/
6 http://nlp.stanford.edu/software/lex-parser.shtml
Non-Native Text Analysis: A Survey 9
to generate features, and five-fold cross validation on seven native languages is
performed. All TSG features outperformed the CFG baseline (at 73%). The highest
TSG induction method was Bayesian induction at 78%.
Massung, Zhai, and Hockenmaier (2013) also make use of grammatical parse tree
features, but mainly focus on their structural aspects as opposed to the syntactic
category labels. In one classification task, they found these features to work well
in determining the nationality of student essay writers from the CEEAUS dataset.
These structural parse tree features may be applicable in other tree-based objects
such as adaptor grammars, tree substitution grammars, and dependency parses,
but this has yet to be explored.
Although not directly tackling NLI, authorship attribution from Kim, Kim,
Weninger, Han, and Kim (2011) does use grammatical parse tree features similarly
to the above papers. They defined a new tree-based feature, k-embedded-edge (ee)
subtrees: subtrees that share a set of k ancestor-descendant subtrees. Therefore, a
0-ee subtree would be one arbitrarily-sized subtree, and a 1-ee subtree would be
one subtree and one descendant subtree anywhere in the parse tree. This creates an
exponential number of potential patterns, and the authors define frequent pattern
mining algorithms to prune the number of ee tree features. As with the last paper,
this approach would be feasible for other tree structures, but is also unexplored.
In summary, Fig 3 lists the comparable accuracies from experiments run on the
ICLE subset of five European languages. In general though, accuracies between
70% to 80% are standard for a wide variety of techniques and corpora.
3.2 Shared Task in Native Language Identification
The NAACL Human Language Technologies workshops held a shared task dedi-
cated to NLI and recorded by Tetreault, Blanchard, and Cahill (2013). Its goal was
to increase visibility of the problem and standardize results on a larger dataset:
TOEF11. In total, twenty-nine teams participated in the shared task.
There were three separate subtasks, differentiated by what data was allowed for
training:
• Closed Training: use only the TOEF11 corpus for training. All twenty-nine
teams attempted this task.
• Open Training I: use any data except the TOEF11 corpus for training; the
evaluation is still done on the TOEF11 test set. Only three teams attempted
this task.
• Open Training II: Use any data including the TOEF11 corpus. Only four
teams attempted this task.
Overall results for the closed training were relatively close; the top thirteen teams
had between 80 − 83.6% accuracy. The open training I subtask teams performed
much lower (probably as expected) at around 33, 38, and 57% accuracy. Finally, in
the open training II subtask, none of the four teams were able to beat the highest
closed score. The most useful results for evaluating models are probably those of
the closed task, since all training data is uniform, though it is interesting to see
10 Sean Massung and ChengXiang Zhai
Classifier Count
SVM 13
Ensemble 4
MaxEnt 3
DFA 1
String kernel 1
PPM 1
k-NN 1
Fig. 4. Number of teams using particular n-gram features (left) and number of
teams using a particular machine learning algorithm (right). DFA and PPM refer
to discriminant function analysis and prediction by partial matching.
approaches employed to increase the amount of training data. For example, in the
open subtasks, other NLI datasets were used such as ICLE, FCE, and ICNALE
(all described in section 2). Additionally, some teams crawled Lang-87, a site where
users write in a second language and get corrections from native speakers.
The most common features used were n-grams of lexical tokens such as words or
part-of-speech tags; Fig 4 compares the n-grams used by all teams. Aside from these,
some less common features were grammatical: dependency parses, TSGs from Blun-
som and Cohn (2010), parse tree rules, and adaptor grammars from Johnson, Grif-
fiths, and Goldwater (2006) were used. Spelling error features were also captured
by three of the teams. These features are all described in work in section 3.1. Below,
we outline a few of the more unique feature representations.
Skipgrams by Guthrie, Allison, Liu, Guthrie, and Wilks (2006) were a n-gram
variant feature used by several teams. Instead of considering an n-gram to consist
of n adjacent words in the text, a k-skip n-gram allows up to k words total to
be skipped in a sequence of tokens. Given the sentence “They all studied statistics
Monday evening”, normal 3-grams would be {They all studied, all studied statistics,
studied statistics Monday, statistics Monday evening}. A few possible 2-skip 3-
grams are {They all studied, They studied statistics, They statistics Monday}. This
attempts to capture important phrases without regard to interspersed function
words. Using skipgrams was shown to reduce perplexity on a testing set, as well as
offer a viable alternative to increasing the corpus size (which is usually not feasible).
Since most NLI datasets we have examined are relatively small, using skipgrams
could help improve language modeling.
For one feature type, the system by LIMSI used a form of machine translation
7 http://www.lang-8.com
Non-Native Text Analysis: A Survey 11
called “back translations” by Lavergne, Illouz, Max, and Nagata (2013). A back
translation attempts to capture a writer’s lexical preference for a word sense. For
example, LIMSI found that the English word sense for awkward is more likely to
be written as clumsy by native Spanish speakers. These word preferences were
used as features in addition to other features: word n-grams, spelling mistakes, and
grammatical mistakes. Adding the back translation features slightly increased the
task’s overall classification accuracy.
The CMU-Haifa team used a ratio of passive to active verbs in their sys-
tem: Tsvetkov, Twitto, Schneider, Ordan, Faruqui, Chahuneau, Wintner, and Dyer
(2013). They operated under the common assumption that English uses passive
voice more frequently than other languages, and the amount of passive use may
vary based on the writer’s L1. Recall that passive voice is a literary technique that
shifts attention away from the one performing an action. “The passive voice is of-
ten used” is a sentence in the passive voice; “English speakers often use the passive
voice” is a sentence in the active voice. The passive voice is characterized by the
verb to be and the past participle of the verb. In the previous sentences, we have
is used and use conjugated differently. Using the passive ratio feature exclusively
yielded a 12% accuracy on a 9% baseline. In combination with four other main fea-
tures, the accuracy was not significantly improved. However, further investigation
could be warranted to examine the usefulness of such stylistic composition features.
SVM was by far the most prevalent classifier employed by the teams, as displayed
in Figure 4. Below, we outline some of the more uncommon classifiers.
Discriminant function analysis (DFA) from statistics was used by Kyle, Crossley,
Dai, and McNamara (2013) for increased interpretability over other methods. Using
this model, they were able to find correlations between different L1s. For instance,
Japanese and Korean L1s were highly correlated in their underuse of the words
{all, any, but, different, or, person, this, your}. Unsurprisingly, languages of similar
origin were also correlated—the Romance languages were often misclassified as one
another due to increased correlation of n-gram tokens.
Originally designed to operate on DNA sequences, the string kernel model used
by Popescu and Ionescu (2013) is given a stream of characters. Specifically, a kernel
based on local rank distance is used for native language identification. The simplest
string kernel function f(w1, w2) counts the number of substrings of a particular
length that w1 and w2 share. It is also quite intuitive to introduce a normalized
version that is not biased by long words. Local rank distance is then an extension
that counts the sets of similar n-grams between w1, w2. In their NLI context, each
word was actually a character n-gram. This method has the advantage that no
syntactic information is needed; no parsing or even sentence or word segmentation
is required. In training, they found that n ∈ [5, 8] gave the best results, allowing
them to take third place overall in the closed task.
Similar to the string kernels previously, Bobicev (2013) uses a method which
requires very little text processing: prediction by partial matching (PPM), a statis-
tical compression method. The output of PPM is a language model which can be
used to find the highest likelihood L1 given some unknown text. Both words and
characters were used as tokens, though character features performed much worse
12 Sean Massung and ChengXiang Zhai
than words, obtaining a precision of 37% (compared to 70% from the word tokens)
on the 9,900-document training set.
In conclusion, most methods used by teams in the NLI shared tasks were very
standard (e.g. n-grams of words with SVM). However, a small number of teams
investigated some unique text representations and classification techniques. Given
options between feature-based and likelihood-based strategies, most teams focused
on features. While not always successful, these new methods can be further explored
and analyzed in future work. As another result of the shared task, an extensive
baseline for the TOEF11 dataset has been created.
4 Non-Native Grammar Correction
We now transition from native language identification to non-native grammar cor-
rection. While the former is often tackled as a one-step task, the latter usually relies
on classification as a component, but also consists of other parts that are able to
capture a deeper syntactic meaning. Correcting machine translated text is a related
issue, but we do not discuss it here; instead, please see Corston-Oliver, Gamon, and
Brockett (2001) or Gamon, Aue, and Smets (2005). Fig 6 compares the different
methods discussed in this section.
Some grammar correction methods are targeted towards a very specific subset of
errors, often categorized by the corpus; others attempt to solve more general errors
concerning word sense or collocations. Evaluation for grammar correction is much
more varied and unstandardized in comparison to the configurations from NLI seen
in the previous section. Which non-native corpus is used also dictates the types of
errors that can be corrected.
4.1 Techniques in Non-Native Grammar Correction
Lee and Seneff (2006) train a trigram language model on a lattice of alternatives,
where “alternatives” are prepositions, articles, and auxiliaries that may or may not
occur between words in the original text. For example, the sentence I want flight
Monday can be corrected by inserting two tokens as such: I want a flight on Monday.
Their algorithm first strips all such alternatives from the original sentence. So far,
this is not much different from the article and preposition corrections. However,
they additionally change each remaining word in the input sentence to be a set of
related words to the base form: want → {want, wants, wanted, wanting}. Their
language model then outputs the k-best candidates. Next, these candidates are
given to a PCFG and reranked. The final output is the top-ranked sentence from
the PCFG. Across all experiments, they found that reranking the language model
candidates significantly increased the F measure.
Brockett, Dolan, and Gamon (2006) used statistical machine translation to trans-
late non-native speech into native speech. They first identified common errors in
a Chinese learner’s English corpus, and used regular expressions to convert target
English from Reuters articles into ungrammatical English. This approach is very
similar to one by Rozovskaya and Roth (2010), which is an error insertion method;
Non-Native Text Analysis: A Survey 13
Paper Method Target
(Lee and Seneff 2006) LM with PCFG scoring articles, prepositions,
and word forms
(Brockett et al. 2006) machine translation common errors
(West et al. 2011) bilingual random walk word sense
(Dahlmeier and Ng 2011a) machine translation collocation errors
(Dahlmeier and Ng 2011b) structure optimization articles and prepositions
Fig. 5. Comparison of GEC strategies.
instead of articles, it uses generic grammatical errors such as I knew many infor-
mations about Christmas. It is unclear how extensive or comprehensive the regular
expressions were to introduce grammatical errors since no examples are given or ref-
erenced. Additionally, the regular expressions uniformly distribute errors through-
out the source language, which is not how errors naturally occur. Despite this, they
did find that their processed training data was able to be used in the MT system
to successfully correct errors. This shows that—given a source model—statistical
machine translation may be used to correct grammatical errors.
West, Park, and Levy (2011) use bilingual random walks between L1 and L2
word senses. For example, on one side of a bipartite graph are L1 words. There are
connections from a word w ∈ L1 to a word w′ ∈ L2 if a w could be translated into
w′. w could be the English word head, and be translated into a physical head, head
of an organization, or the verb to head. This model was used to correct non-native
sounding phrases such as entire stranger to the more natural complete stranger.
This bipartite graph was combined with a language model to correct non-native
sentences. In these experiments, the native language was Korean. Evaluation was
performed with Amazon Mechanical Turk 8 where workers chose between the cor-
rected sentence and the original sentence. Results were not strongly positive, since
sometimes the corrected errors changed the meaning of the sentence or made it un-
grammatical. In future work the authors suggest using a richer probabilistic model
such as a PCFG.
Dahlmeier and Ng (2011a) use the NUCLE corpus to find and correct collocation
errors via machine translation. Here, a collocation is a phrase commonly used by
native speakers. The authors propose that when a writer mentally translates from
L1 to L2, some unnatural phrases result due to word choice. They give an example,
“I like to look movies” that might be written by a native Chinese speaker since watch
and look are very similar in the L1. It would be possible to correct this to the more
grammatical “I like to look at movies”, but it still doesn’t sound natural. Instead,
look is replaced by watch, resulting in the more fluent collocation watch movies.
8 https://www.mturk.com/mturk/welcome
14 Sean Massung and ChengXiang Zhai
For their experiments, they assume the unnatural collocations have already been
identified; this mimics a system where a user may ask for improvement suggestions
for a snippet of writing. They train a statistical machine translation model on a
parallel Chinese-English corpus to correct collocation errors in the NUCLE corpus.
A log-linear model was used to score the candidate phrases which allows additional
spelling, homophone, and synonym features to be incorporated. They evaluated
their method as a retrieval task, where they returned the top k suggestions to fix
each collocation error. Two native-English speakers judged results from five hundred
corrections with good rater agreement. Finally, they performed an analysis of errors
and found that the main reason top-ranked phrases were not correct was due to
out-of-vocabulary words.
Dahlmeier and Ng (2011b) introduce an alternating structure optimization (ASO)
approach to GEC. In short, ASO is able to leverage a common structure between
multiple related problems; see Ando and Zhang (2005) for a more detailed descrip-
tion. In this case, the related problems are selection (find features from native text)
and correction (fix the errors in non-native text). Targets were article and prepo-
sition errors, again using the NUCLE corpus. It was shown that ASO significantly
outperformed a simple linear classifier as well as two unnamed commercial grammar
checkers. Features included part-of-speech tags, hypernyms from WordNet, named
entities, and shallow parsing tags.
4.2 Shared Task in Non-Native Grammar Correction
We continue our discussion on GEC with the introduction of the CoNLL-2013
shared task. The training data was the NUCLE corpus, containing annotations
categorized by five error types along with the corrected text. Some systems first
classified potential errors by error type and then corrected errors while others made
no such distinction. Additionally, the teams received preprocessed input: sentence
segmentation, word tokenization, and part-of-speech tagging were all performed.
In all, there were 1,397 essays consisting of 57,151 total sentences. Article and
determiner errors were most prevalent, with the other four classes appearing roughly
equally. Testing data was considerably smaller with only fifty essays. Teams were
also allowed to use any external resources that were non-proprietary and publicly
available. Fifty-four teams registered for the task, but only seventeen submitted
final results. Fig 6 shows the methods of the top five teams. The five error types
are listed below:
• Article or determiner: “In late nineteenth century, there was a severe air
crash happening at Miami international airport.” Correction: replace late with
the late.
• Preposition: “Also tracking people is very dangerous if it has been controlled
by bad men in a not good purpose. Correction: replace in with for.
• Noun number: “I think such powerful device shall not be made easily avail-
able.” Correction: replace device with devices.
• Verb form: “However, it is an achievement as it is an indication that our so-
Non-Native Text Analysis: A Survey 15
Team P R F1
UIUC (Rozovskaya et al. 2013) 46.45 23.49 31.20
averaged perceptron and Näıve Bayes
NTHU (Kao et al. 2013) 23.80 26.35 25.01
n-gram and dependency language model
HIT (Xiang et al. 2013) 35.65 16.56 22.61
MaxEnt and rules
NARA (Yoshimoto et al. 2013) 27.39 18.62 22.17
phrase SMT and treelet language model
UMC (Xing et al. 2013) 28.49 17.53 21.70
rule filter, MaxEnt, language model scorer
Fig. 6. Top five teams from the CoNLL-2013 shared task on grammatical error
correction (Ng et al. 2013).
ciety is progressed well and people are living in better conditions.” Correction:
replace progressed with progressing.
• Subject-verb agreement: “People still prefers to bear the risk and allow
their pets to have maximum freedom.” Correction: replace prefers with prefer.
While the contest setup and evaluation does not completely mimic real-life sce-
narios, it does provide useful information for those interested in GEC. Since the
errors are categorized into five groups, it is possible to perform an error analysis on
the results.
As mentioned previously, even performing the evaluation is not a trivial task;
suggested corrections—while completely valid—may not actually match the “of-
ficial” corrections. To mitigate these issues, the contest organizers permitted the
contestants to submit their own gold standard corrections if they disagreed with
the provided answers. Of course, the corrections were reviewed. It is also worth
noting that the training and test data contained errors other than those in the five
categories, but these additional errors were not taken into consideration in the final
scoring.
The fifth-place team, Xing, Wang, Wong, Chao, and Zeng (2013) from University
of Macau (UMC), had a separate module for each error type. First though, they
ran a simple spelling correction tool on the corpus. Additionally, they performed
some preprocessing to overcome the data sparsity problem (errors vs non-errors
were roughly 1:100). They referred to this as error label propagation: they simply
used k-NN to match labeled sentences with unlabeled ones. They then propagated
16 Sean Massung and ChengXiang Zhai
the errors from the source to the destination sentences by optimizing a loss func-
tion based on the induced edges between sentences. This is perhaps the most novel
contribution from their work. They added in some handmade rule-based filters to
select error types, and then shuttled each sentence through their system. They used
a language model scorer trained on the Google 5-gram corpus to score their cor-
rections. The maximum entropy classifier was only used in detecting noun number
and subject-verb agreement errors, where the rule-based filters were used for all
error types except the preposition errors. In the preposition errors module, each
preposition was simply replaced with those from the set {in, for, to, of, on, } and
the best was selected by the highest-scoring according to the language model.
In fourth place, Yoshimoto, Kose, Mitsuzawa, Sakaguchi, Mizumoto, Hayashibe,
Komachi, and Matsumoto (2013) from the Nara Institute of Science and Technology
(NARA), also took a hybrid approach depending on error types. They ran each
portion separately and then merged the modified sentences together to form the
final answer. To correct subject-verb agreement and verb form, they used a treelet
language model. A treelet refers to some subtree of an entire parse tree. To correct
noun number errors, they used a simple binary classifier to distinguish between
whether a noun should be singular or plural. To correct preposition and determiner
errors, they used phrase-based statistical machine translation. The treelet language
model was used to rank candidate corrections. The candidates were generated by
replacing verbs with a list of potential verbs: for example, be → {be, being, been}.
In the machine translation unit, they generated candidate translations and have
the language model score the results in a similar way to the treelets.
The third-place team, Xiang, Yuan, Zhang, Wang, Zheng, and Wei (2013) from
Harbin Institute of Technology (HIT), split their system into two parts: a ma-
chine learning module (article vs determiner, prepositions, and noun errors) and a
rule-based module (verb form and subject-verb agreement). As indicated in Fig 6,
the classifier used was a Maximum Entropy classifier with additional feature selec-
tion via genetic algorithms and confidence tuning based on the maximum entropy
score. The rules were first preprocessed with the frequent pattern mining algorithm
FP-growth to find common incorrect phrases. These common phrases were then
removed from the candidate set to be corrected. A list of these common phrases
was provided and may prove useful for future work. In their results, they found that
their models were very sensitive to the parameters and that the confidence tuning
had the largest positive effect in their pipeline.
Kao, Chang, Chiu, Yen, Boisson, Wu, and Chang (2013) from National Tsing
Hua University (NTHU) finished in second place with a system that was very
similar to theirs from the previous year. In the CoNLL-2012 shared task, subject-
verb agreement was not a labeled error type so the NTHU system ignored these
errors. In the seemingly current trend, a module was created for each error type,
though their modules were more similar to each other than the previous systems.
Each module used a moving window of words up to length five—five here because
Non-Native Text Analysis: A Survey 17
they leverage the Google 5-gram corpus as well as their previous tool Linggle9.
Linggle is a linguistic search engine in which the user can specify wildcards for
nouns, verbs, and adjectives Boisson, Kao, Wu, Yen, and Chang (2013). Results
were returned with those wildcards filled in ordered by increasing likelihood in the
dataset. These likelihoods were used to score candidates generated by their system
in each module. Their candidate voting system was created using a backoff model
of lower-order n-grams. Verb form errors were really the only significantly different
module by including pointwise mutual information.
Rozovskaya, Chang, Sammons, and Roth (2013) from the University of Illinois
at Urbana-Champaign (UIUC) had the best-performing system. They made use of
previous work for article errors (with averaged perceptron) and preposition errors
(with Näıve Bayes). The remaining three error classes were dealt with Näıve Bayes
with priors trained on the same Google corpus that NTHU used. Although the
CoNLL corpus already contained part-of-speech tags, they reparsed the dataset
with their own tools in addition to running a shallow parser for feature generation.
There was no pipeline in the system, meaning that corrections of each model were
simply pooled together to create the final output sentence. In their error analysis,
they found that an incorrect verb form can cause parsers to break, which greatly
hindered rule-based methods. Since the errors were so sparse, they hypothesize,
simpler machine learning algorithms would be more robust to the large amount of
noise. This analysis provided much insight and would be useful for future work.
After the contest ended, Rozovskaya and Roth (2013) (from UIUC) returned to
the CoNLL-2013 shared tasks with a joint learning and inference approach. Based
on their observations from the shared task, they reasoned that individual (read:
independent) classifiers per word do not capture interactions between errors and
word choice. They accomplish this by combining individual classifiers using integer
linear programming—a model which is able to jointly learn the error occurrences.
They thus increased the F1 score on the CoNLL-2013 data to 42%, significantly
higher than their first place score. They do note though, that F1 may not be an
appropriate measure; it is indeed more intuitive to measure the increase in correct-
ness of the original data since it is quite likely that the grammar correction systems
actually introduce errors themselves.
In conclusion, we saw a variety of techniques for correcting grammatical errors,
which can be categorized into targeted vs general strategies. Targeted strategies fo-
cus on errors of only specific types (such as the shared task) which general correctors
try to improve the overall fluency of the L2 text.
5 Text Simplification for Non-Native Speakers
Summarization, simplification, and readability go hand in hand to help a non-native
speaker understand text. Unlike NLI and GEC, most algorithms operate on solely
on well-formed L2 passages. Simplification can be seen as an easy-to-understand
9 http://linggle.com
18 Sean Massung and ChengXiang Zhai
summary of a more difficult text; simplification usually “translates” one sentence
to another, in efforts to make the result have a better readability. It is a form
of monolingual machine translation when using a parallel corpus of advanced and
simple language. For a detailed description of general text simplification, we direct
the reader to Siddharthan (2014).
Unfortunately, not much work has been done in text simplification specifically for
non-native speakers. A typical use case is simplifying medical texts so the common
reader can make sense of them (e.g. see Abrahamsson, Forni, Skeppstedt, and Kvist
(2014)). Other use cases could be helping younger readers or users with learning
disabilities.
Wikipedia and Simple Wikipedia 10 are popular parallel corpora. Both Wubben,
van den Bosch, and Krahmer (2012) and Zhu, Bernhard,, and Gurevych (2010) use
them as corpora for sentence simplification via monolingual machine translation.
The former uses non-native speakers to judge sentences from their system, but the
system itself doesn’t take into account the users’ native language when forming
the simplifications. The latter defines sentence splitting, deletion, reordering, and
substitution operations on complex parse trees in order to simplify them into more
understandable sentences. They evaluate with standard readability measures as well
as perplexity from an English language model.
Lappas and Vlachos (2012) show how to rank documents in a search engine to
favor both relevance and readability for non-native speakers. The readability score is
determined based on the user’s native language, although this is not automatically
detected. Each document is then assigned a (relevance, readability) pair at query-
time, and it can be imagined that documents are plotted in this 2D space. A
document is said to dominate another document if it is more understandable and
more relevant. In the 2D document space, documents that are not dominated by
any other document are on the “skyline” (or perimeter) of the space. These are the
documents that are browsed by the user.
As the text simplification field continues to evolve, we hope to see more simpli-
fication tasks specifically aimed at helping second-language learners. The “teddy
bear principle” states that language learners tend to stick with a relatively small
set of learned syntactic patterns when speaking or writing in L2. Depending on
the L1, a sentence simplification task could translate the complex sentences into
a format more comfortable to the user. Peterson and Ostendorf (2007) analyze
changes made to professionally abridged versions of newspaper articles to deter-
mine common translations. These common modifications could be incorporated in
a monolingual translation model.
6 Discussion and Conclusions
We realize that support for dealing with non-native text is becoming increasingly
important; many applications benefit from automatic author profiling and gram-
matical error correction. User experiences in any online domain can be enhanced
10 http://simple.wikipedia.org/wiki/Main_Page
Non-Native Text Analysis: A Survey 19
with text simplification for non-L2 writers and speakers. We call this collection of
challenges and tasks non-native text mining. This survey detailed common datasets
and covered three main applications in non-native text mining: native language
identification, grammatical error correction, and text simplification.
In section 2, we saw the main corpora used by researchers for NLI and GEC.
Compared to other text datasets, these are relatively small and only consist of
essays written by students. A larger corpus would be beneficial, as would a corpus
of text other than essays such as blogs, social media, or research papers. Finally,
the availability of such datasets are directly proportional to the amount of impact
they may have; freely available corpora are much more likely to be used than their
proprietary counterparts.
Section 3 detailed NLI as a classification problem. We also saw how a shared task
was able to elicit many quality works from the community in an organized fashion.
We subdivided NLI approaches into two techniques: feature- and likelihood-based.
Much more effort has been put into feature-based methods using standard machine
learning algorithms. Instead of classifiers and probabilistic models, is it possible to
use other text mining algorithms? For example, can clustering methods be opti-
mized for use with NLI? What is the significance of outliers and dense subclusters?
How do collaborative filtering systems change when additional information about
learning styles of users are known?
Grammatical error correction was discussed in section 4. We saw how most ap-
proaches used a targeted strategy: first classify errors from a limited list and then
attempt to fix them. In future work, we would like to see more general approaches
applied that are not restricted to specific error types. Aside from grammaticality,
fluency can also be considered: exactly what phrases or syntactic structures make
an essay sound “native”? Are specific topics handled with different grammar? Or
are some differences inherently cultural? How are ethnicity, culture, and race re-
lated in our context? Just because one’s text may seem like native English, does
that mean it is American English? If American, is it Northeastern or Southern?
Dialect plays a very important role in fluency.
Compared with NLI and GEC, Section 5 showed very limited work related to
simplifying text for non-native speakers. We anticipate to see a more “personal”
approach in this field, related to the user’s background. Is it possible to pair com-
plementary students together for a better online learning experience? How can a
user’s L1 influence what kind of summarization suits them best?
In time, the field non-native text analysis will evolve into non-native text min-
ing. Aspects from the three areas discussed will be combined into unified algorithms
and intelligent applications. For example, an NLI system can determine a user’s L1,
and use that knowledge to offer sophisticated annotations and corrections to their
own text in L2 while simultaneously summarizing and condensing text from other
sources. Indeed, other areas aside from these three will arise to further the under-
standing of non-native writing or to help non-native speakers better understand
their second language.
20 Sean Massung and ChengXiang Zhai
References
Abrahamsson, E. and Forni, T. and Skeppstedt, M. and Kvist, M. 2014. Medical Text
Simplification using Synonym Replacement: Adapting Assessment of Word Difficulty
to a Compounding Language. In Proceedings of the 3rd Workshop on Predicting and
Improving Text Readability for Target Reader Populations (PITR), 57-65
Ando, R. and Zhang, T. 2005. A Framework for Learning Predictive Structures from
Multiple Tasks and Unlabeled Data. In the Journal of Machine Learning Research,
1817-53
Blanchard, D. and Tetreault, J. and Higgins, D. and Cahill, A and Chodorow, M. 2013.
TOEF11: A Corpus of Non-Native English. Technical Report, Educational Testing Ser-
vice
Blei, D. and Ng, A. and Jordan, M. 2003. Latent Dirichlet Allocation. In the Journal of
Machine Learning Research, 993-1022
Blunsom, P. and Cohn, T. 2010. Unsupervised Induction of Tree Substitution Grammars
for Dependency Parsing. In Proceedings of the 2010 Conference on Empirical Methods
in Natural Language Processing, 1204-13
Bobicev, V. 2013. Native Language Identification with PPM. In Proceedings of the Eighth
Workshop on Innovative Use of NLP for Building Educational Applications, 180-87
Boisson, J. and Kao, T. and Wu, J. and Yen, T. and Chang, J. 2013. Linggle: a Web-
scale Linguistic Search Engine for Words in Context. In Proceedings of Association for
Computational Linguistics (Conference System Demonstrations), 139-44
British Council 2014. How Many People Speak English? From http://www.
britishcouncil.org/learning-faq-the-english-language.htm
Brockett, C. and Dolan, W. and Gamon, M. 2006. Correcting ESL errors Using Phrasal
SMT Techniques. In Proceedings of the 21st International Conference on Computa-
tional Linguistics and the 44th annual meeting of the Association for Computational
Linguistics, 249-56
Brooke, J. and Hirst, G. 2012. Robust, Lexicalized Native Language Identification. In
Proceedings of the International Conference on Computational Linguistics, 391-408
Corston-Oliver, S. and Gamon, M. and Brockett, C. 2001. A Machine Learning Approach
to the Automatic Evaluation of Machine Translation. In Proceedings of the 39th Annual
Meeting on Association for Computational Linguistics, 148-55
Dahlmeier, D. and Ng, H. 2011. Correcting Semantic Collocation Errors with L1-induced
Paraphrases. In Proceedings of the Conference on Empirical Methods in Natural Lan-
guage Processing, 107-17
Dahlmeier, D. and Ng, H. 2011. Grammatical Error Correction with Alternating Struc-
ture Optimization. In Proceedings of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Technologies - Volume 1, 915-23
Dahlmeier, D. and Ng, H. and Wu, S. 2013. Building a Large Annotated Corpus of Learner
English: The NUS Corpus of Learner English. In Proceedings of the 8th Workshop on
Innovative Use of NLP for Building Educational Applications, 22-31
Gamon, M. 2010. Using Mostly Native Data to Correct Errors in Learners’ Writing: A
Meta-classifier Approach. In Proceedings of Human Language Technologies: The 2010
Annual Conference of the North American Chapter of the Association for Computational
Linguistics, 163-71
Gamon, M. and Aue, A. and Smets, M. 2005. Sentence-level MT Evaluation Without
Reference Translations: Beyond Language Modeling. In Proceedings of the European
Association for Machine Translation (EAMT, 103-11
Granger, S. 2003. The International Corpus of Learner English: A new resource for foreign
language learning and teaching and second language acquisition research. In Teachers
of English to Speakers of Other Languages Quarterly, 538-46
Granger, S. and Dagneaux, E. and Meunier, F. and Paquot, M. 2009. The International
Learner Corpus of English, Version 2. Presses Universitaires de Louvain.
Non-Native Text Analysis: A Survey 21
Guthrie, D. and Allison, B. and Liu, W. and Guthrie, L. and Wilks, Y. 2006. A Closer
Look at Skip-gram Modelling. In Proceedings of the Fifth International Conference on
Language Resources and Evaluation (LREC’06), 101-11
Han, N. and Chodorow, M. and Leacock, C. 2006. Detecting errors in English article usage
by non-native speakers. Natural Language Engineering, 115-29
Hermet, M. and Désilets, A. 2009. Using First and Second Language Models to Correct
Preposition Errors in Second Language Authoring. In Proceedings of the Fourth Work-
shop on Innovative Use of NLP for Building Educational Applications, 64-72
Ishikawa, S. 2009. Vocabulary in Interlanguage: A Study on Corpus of English Essays
Written by Asian University Students (CEEAUS). In Phraseology: Corpus Linguistics
and Lexicology, 87-100
Ishikawa, S. 2013. The ICNALE and Sophisticated Contrastive Interlanguage Analysis of
Asian Learners of English. In Learner corpus studies in Asia and the world. 91-118
Johnson, M. and Griffiths, T. and Goldwater, S. 2006. Adaptor Grammars: A Framework
for Specifying Compositional Nonparametric Bayesian Models. In Neural Information
Processing Systems, 641-8
Kao, T. and Chang, Y. and Chiu, H. and Yen, T. and Boisson, J. and Wu, J. and Chang, J.
2013. In Proceedings of the Seventeenth Conference on Computational Natural Language
Learning: Shared Task, 20-25
Kim, S. and Kim, H. and Weninger, T. and Han, J. and Kim, H. 2011. Authorship Classi-
fication: A Discriminative Syntactic Tree Mining Approach. In Proceedings of the 34th
International ACM SIGIR Conference on Research and Development in Information
Retrieval, 455-64
Koppel, M. and Schler, J. and Zigdon, K. 2005. Determining an author’s native language
by mining a text for errors. In Proceedings of the eleventh ACM SIGKDD international
conference on Knowledge discovery in data mining, 624-8
Koppel, M. and Schler, J. and Argamon, S. 2009. Computational Methods in Author-
ship Attribution. In the Journal of the American Society for Information Science and
Technology, 9-26
Kukich, K. 1992. Techniques for Automatically Correcting Words in Text. In ACM Com-
puting Surveys, 377-439
Kyle, K. and Crossley, S. and Dai, J. and McNamara, D. 2013. Native Language Identifi-
cation: A Key N-gram Category Approach. In Proceedings of the Eighth Workshop on
Innovative Use of NLP for Building Educational Applications, 242-50
Lappas, T. and Vlachos, M. 2012. Customizing Search Results for Non-native Speakers. In
Proceedings of the 21st ACM International Conference on Information and Knowledge
Management, 1829-33
Lavergne, T. and Illouz, G. and Max, A. and Nagata, R. 2013. LIMSI’s participation to
the 2013 shared task on Native Language Identification. In Proceedings of the Eighth
Workshop on Innovative Use of NLP for Building Educational Applications, 260-65
Leacock, C. and Chodorow, M. and Gamon, M. and Tetreault, J. 2010. Automated Gram-
matical Error Detection for Language Learners. Morgan and Claypool (Synthesis lectures
on human language technologies), edited by Graeme Hirst.
Lee, J. and Seneff, S. 2006. Automatic grammar correction for second-language learners.
In Proceedings of the 9th International Conference on Spoken Language Processing,
1978-81
Massung, S. and Zhai, C. and Hockenmaier, J. 2013. Structural Parse Tree Features for
Text Representation. In Proceedings of the International Conference on Semantic Com-
puting, 9-16
Ng, H. and Wu, S. and Wu, Y. and Hadiwinoto, C. and Tetreault, J. 2013. The CoNLL-
2013 Shared Task on Grammatical Error Correction. In Proceedings of the Seventeenth
Conference on Computational Natural Language Learning: Shared Task, 1-12
22 Sean Massung and ChengXiang Zhai
Pardo, F. and Rosso, P. and Koppel, M. and Stamatatos, E. and Inches, G. 2013. Overview
of the Author Profiling Task at PAN 2013. In Proceedings of the Conference and Labs
of the Evaluation Forum (Working Notes)
Petersen S. and Ostendorf, M. 2007. Text Simplification for Language Learners: a Corpus
Analysis. In Proceedings of the International Speech Communication Association Special
Interest Group on Speech and Language Technology in Education, 69-72
Popescu, M. and Ionescu, T. 2013. The Story of the Characters, the DNA and the Na-
tive Language. In Proceedings of the Eighth Workshop on Innovative Use of NLP for
Building Educational Applications, 270-78
Rangel, F. and Rosso, F. and Chugur, I. and Potthast, M. and Trenkmann, M. and Stein,
B. and Verhoeven, B. and Daelemans, W. 2014. Overview of the 2nd Author Profiling
Task at PAN 2014. In Proceedings of the Conference and Labs of the Evaluation Forum
(Working Notes)
Rozovskaya, A. and Roth, D. 2010. Training Paradigms for Correcting Errors in Grammar
and Usage. In Proceedings of Human Language Technologies: The 2010 Annual Confer-
ence of the North American Chapter of the Association for Computational Linguistics,
154-62
Rozovskaya, A. and Roth, D. 2013. Joint Learning and Inference for Grammatical Error
Correction. In Proceedings of Empirical Methods in Natural Language Processing, 791-
802
Rozovskaya, A. and Chang, K. and Sammons, M. and Roth, D. 2013. The University
of Illinois system in the CoNLL-2013 shared task. In Proceedings of the Seventeenth
Conference on Computational Natural Language Learning: Shared,13-19
Siddharthan, A. 2014. A survey of research on text simplification. In the International
Journal of Applied Linguistics, 259-98
Stamatatos, E. 2009. A survey of modern authorship attribution methods. In the Journal
of the American Society for Information Science and Technology, 538-56
Swanson, B. and Charniak, E. 2012. Native Language Detection with Tree Substitution
Grammars. In Proceedings of the 50th Annual Meeting of the Association for Compu-
tational Linguistics: Short Papers - Volume 2, 193-97
Tetreault, J. and Blanchard, D. and Cahill, A. 2013. A Report on the First Native Lan-
guage Identification Shared Task. In Proceedings of the Eighth Workshop on Innovative
Use of NLP for Building Educational Applications, 48-57
Tsur, O. and Rappoport, A. 2007. Using Classifier Features for Studying the Effect of
Native Language on the Choice of Written Second Language Words. In Proceedings of
the Workshop on Cognitive Aspects of Computational Language Acquisition, 9-16
Tsvetkov, Y. and Twitto, N. Schneider, N. Ordan, N. and Faruqui, M. and Chahuneau,
V. and Wintner, S. and Dyer, C. 2013. Identifying the L1 of non-native writers: the
CMU-Haifa system. In Proceedings of the Eighth Workshop on Innovative Use of NLP
for Building Educational Applications, 279-87
West, R. and Park, A. and Levy, R. 2011. Bilingual Random Walk Models for Automated
Grammar Correction of ESL Author-produced Text. In Proceedings of the 6th Workshop
on Innovative Use of NLP for Building Educational Applications, 170-79
Wong, J. and Dras, M. 2009. Contrastive analysis and native language identification. In
Australasian Language Technology Association Workshop 2009, 53-61
Wong, J. and Dras, M. 2011. Exploiting Parse Structures for Native Language Identifi-
cation. In Proceedings of the Conference on Empirical Methods in Natural Language
Processing, 1600-10
Wong, J and Dras, M. and Johnson, M. 2012. Exploring Adaptor Grammars for Na-
tive Language Identification. In Proceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Computational Natural Language Learn-
ing, 699-709
Non-Native Text Analysis: A Survey 23
Wubben, S. and van den Bosch, A. and Krahmer, E. 2012. Sentence Simplification by
Monolingual Machine Translation. In Proceedings of the 50th Annual Meeting of the
Association for Computational Linguistics: Long Papers - Volume 1, 1015-24
Xiang, Y. and Yuan, B. and Zhang, Y. and Wang, X. and Zheng, W. and Wei, C. 2013.
A Hybrid Model For Grammatical Error Correction. In Proceedings of the Seventeenth
Conference on Computational Natural Language Learning: Shared Task, 115-22
Xing, J. and Wang, L. and Wong, D. and Chao, L. and Zeng, X. 2013. UM-Checker:
A Hybrid System for English Grammatical Error Correction. In Proceedings of the
Seventeenth Conference on Computational Natural Language Learning: Shared Task,
34-42
Yannakoudakis, H and Briscoe, T and Medlock, B. 2011. A New Dataset and Method for
Automatically Grading ESOL Texts. In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human Language Technologies, 180-89
Yoshimoto, I. and Kose, T. and Mitsuzawa, K. and Sakaguchi, K. and Mizumoto, T. and
Hayashibe, Y. and Komachi, M. and Matsumoto, Y. 2013. NAIST at 2013 CoNLL
Grammatical Error Correction Shared Task. In Proceedings of the Seventeenth Confer-
ence on Computational Natural Language Learning: Shared Task, 26-33
Zhu, Z. and Bernhard, D. and Gurevych, I. 2010. A Monolingual Tree-based Translation
Model for Sentence Simplification. Proceedings of the 23rd International Conference on
Computational Linguistics, 1353-61
