1
2 A state of the art on computational music performance
3 Miguel Delgado, Waldo Fajardo, Miguel Molina-Solana *
4 Department of Computer Science and Artificial Intelligence, Universidad de Granada, Daniel Saucedo Aranda s/n, 18071 Granada, Spain
5
7
a r t i c l e i n f o
89 Keywords:
10 Computational music
11 Expressive performance
12 Machine learning
13
1 4
a b s t r a c t
15Musical expressivity can be defined as the deviation from a musical standard when a score is performed
16by a musician. This deviation is made in terms of intrinsic note attributes like pitch, timbre, timing and
17dynamics. The advances in computational power capabilities and digital sound synthesis have allowed
18real-time control of synthesized sounds. Expressive control becomes then an area of great interest in
19the sound and music computing field. Musical expressivity can be approached from different perspec-
20tives. One approach is the musicological analysis of music and the study of the different stylistic schools.
21This approach provides a valuable understanding about musical expressivity. Another perspective is the
22computational modelling of music performance by means of automatic analysis of recordings. It is known
23that music performance is a complex activity that involves complementary aspects from other disciplines
24such as psychology and acoustics. It requires creativity and eventually, some manual abilities, being a
25hard task even for humans. Therefore, using machines appears as a very interesting and fascinating issue.
26In this paper, we present an overall view of the works many researchers have done so far in the field of
27expressive music performance, with special attention to the computational approach.
28 2010 Published by Elsevier Ltd.
29
30
31 1. Introduction
32 Imagine the scene. You switch on your hi-fi system, select Clair
33 de Lune by Claude Debussy and Sergei Rachmaninoff as performer.
34 After that, you hit the ‘Play’ button, sit on your favourite armchair
35 and enjoy the music. The situation sounds perfectly normal . . . but
36 for a small detail: Rachmaninoff never recorded Debussy’s Clair de
37 Lune!
38 To listen to a performance we need a performer. So far, this role
39 has always been assumed by humans but, why can’t the hi-fi sys-
40 tem (more generally, a computer) be the performer and play the
41 music as it was Rachmaninoff himself? All it needs is enough
42 knowledge of how to play.
43 When skilled musicians play a piece of music, they do not do it
44 mechanically, with constant tempo or loudness, exactly as written
45 in the printed music score. Rather, they speed up at some places,
46 slow down at others and stress certain notes. The most important
47 parameters available to a performer are timing (tempo variations)
48 and dynamics (loudness variations). The way these parameters
49 ‘should be’ varied during the performance is not precisely specified
50 in the printed score. So that, it is performer’s duty to use them
51 properly.
52It is a fact that student musicians spend more time practicing
53than almost any other activity. Weekly music lessons, endless
54scales, nightly rehearsals, recitals for friends and family, and juries
55with faculty members are commonplace in their lives. Hours of
56practicing will help them learn to interpret a piece of music as
57the composer envisioned it, as well as to develop their own signa-
58ture sound – one that is unique to each of them. In other words,
59what makes a piece of music come alive is also what distinguishes
60great artists from each other.
61Other questions arise at this point: how should those expres-
62sive resources be employed? What is that which makes Rach-
63maninoff an outstanding pianist? And those simple questions,
64which many people have asked for many years, do not have
65still a clear answer from musicologists. Even when those ques-
66tions will eventually find an acceptable answer, another will be
67posed: can a computer take advantage of that knowledge, being
68able to substitute a famous performer? As we will see in this
69paper, many attempts have been made and several computa-
70tional models have been proposed during the last century to
71do so.
72This work is organized as follows: first of all, Section 2 describes
73what is musical performance and its parameters, and how they can
74be used to distinguish between performers; Section 3 presents
75some works where computers were used for extracting informa-
76tion about those parameters from the music itself, for representing
77that knowledge in a computer, and for applying it to generate new
78performances; Section 4 introduces some difficulties to be faced in
0957-4174/$ - see front matter  2010 Published by Elsevier Ltd.
doi:10.1016/j.eswa.2010.06.033
* Corresponding author.
E-mail addresses: mdelgado@ugr.es (M. Delgado), aragorn@ugr.es (W. Fajardo),
miguelmolina@ugr.es, miguems@gmail.com (M. Molina-Solana).
Expert Systems with Applications xxx (2010) xxx–xxx
Contents lists available at ScienceDirect
Expert Systems with Applications
journal homepage: www.elsevier .com/locate /eswa
ESWA 4823 No. of Pages 6, Model 5G
10 July 2010
Please cite this article in press as: Delgado, M., et al. A state of the art on computational music performance. Expert Systems with Applications (2010),
doi:10.1016/j.eswa.2010.06.033
79 the future when using computers in this domain; and the Section 5
80 summarizes the work.
81 2. Music performance
82 Most people would judge the literal execution of a musical
83 score to be significantly less interesting than a performance of that
84 piece by even a moderately skilled musician. Why is that so? Be-
85 cause what we hear is not a literal rendition of the score. Of course,
86 the principal vehicle for the communication of musical composi-
87 tions is the music score in which the composer codifies his inten-
88 tions. However, the information written in the score does not
89 represent an exhaustive description of the composer’s intentions.
90 It carries information such as the rhythmical and melodic structure
91 of a certain piece, but there is not yet a notation able to describe
92 precisely the timing and timbre characteristics of the sound.
93 When speaking, we use several voice resources such as chang-
94 ing velocity, tone or loudness. All this effects are not explicitly in
95 the text we are reading. In fact, when several people read a text,
96 resulting sounds are not the same, even though words in the sheet
97 remain unchanged. So does in music. In the same way that in a
98 written poem there is no explicit notation for how to pronounce,
99 in musical scores there is also such a lack of information. This com-
100 parison is actually quite appropriate because former research on
101 music performance has revealed interesting analogies in the com-
102 munication of emotions in singing and speech (Bresin & Friberg,
103 2000; Sundberg, 2000)Q1 .
104 Performing is a crucial activity in music. In many kinds of music
105 the performer acts as a kind of mediator: a mediator between com-
106 poser and listener, between written score and musical sound. It is
107 the performer who renders each note in the score in terms of inten-
108 sity, duration and timbre by movements of fingers, arms or mouth.
109 This results in different performances of the same piece reflecting
110 each performer’s culture, mood, skill and intention. These vari-
111 ances also contribute to determining the performing styles of dif-
112 ferent musicians. So that, the music we hear has two main
113 sources: the score and the performance, and they both need from
114 the other.
115 Briefly, Widmer and Goebl (2004) define expressive music perfor-
116 mance as ‘‘the deliberate shaping of the music by the performer, in
117 the moment of playing, by means of continuous variations of
118 parameters such as timing, loudness or articulation”. Changes in
119 tempo (timing) are non-linear warping of the regular grid of beats
120 that defines time in a score. It is also possible to change only the
121 duration of certain notes. Changes in loudness (or dynamics) are
122 modifications of the intensity of notes with respect to the others
123 and to the general energy of the fragment in consideration. Articu-
124 lation consists in varying the gap between contiguous notes by, for
125 instance, making the first one shorter or overlapping it with the
126 next.
127 Music performance is a deep human activity which requires
128 emotional, cognitive and artistic aptitudes. At the same time, it is
129 also a complex task involving physical, acoustic, physiological, psy-
130 chological, social and artistic aspects. Several factors determine the
131 rendition of a musical piece. One of the most obvious is the phys-
132 ical condition of the performer. Not in vain, performer’s mood,
133 health and fatigue play a crucial role in the process of playing an
134 instrument. Some studies (see those from Gabrielsson (1995) and
135 from Rigg (1964)) have shown major variations in renditions by
136 the same performer when he is in different moods.
137 Manual abilities are also an important point that is especially
138 visible when comparing a beginner with an expert. With practice,
139 musicians can improve their velocity and precision, reducing the
140 amount of unintended deviations with respect to the score (com-
141 monly known as errors). Other factors that affect the rendition
142are the location where it takes place and the instrument being
143used. The acoustics of the place are important because they estab-
144lish the sounds that can be made. So does the instrument, which
145has an evident influence on the character of the work.
146Because the conventional score is quite inadequate to describe
147the complexity of a musical performance, and since the literal syn-
148thesis of notes from a score is flat and unappealing, there is an
149opportunity for learning systems that can automatically produce
150compelling expressive variations. Hence, methods for automati-
151cally ‘‘bringing life” to musical scores become useful and interest-
152ing. Research in this field ranges from studies aimed at
153understanding expressive performance to attempts at modelling
154aspects of performance in a formal, quantitative and predictive
155way, so that a computer might be able to perform them.
1562.1. Functions of expressivity
157Since the very first moment that some deviations exist in the
158way of playing a score, we can ask for the motives of their exis-
159tence. Two main aims can be identified in a first sight.
160In first place, expressivity is used as an instrument for commu-
161nicating emotions. Meyer (1956) stated that meaning (be it emo-
162tional or aesthetic) arises in music when expectations raised by
163the music are not realized. It was Rigg’s paper (Rigg, 1964) one
164of the pioneer works which tackled the relation between emotions
165and musical structure. Some interesting and typical regularities
166found throughout the years were described there: solemn music
167tend to be slow, low pitched, and without irregularities; happy mu-
168sic is fast, major mode and high pitched.
169Gabrielsson (1995) and Lindström (2006) studied the relation
170between motional intentions and musical microstructure (for in-
171stance, tempo deviations, changes in intensity or articulations).
172Canazza, Poli, Drioli, Rodà, and Vidolin (2000) studied how physical
173parameters in musical recordings (tone, articulations or global
174tempo) were affected by the modification of performer’s expressive
175intentions. In their experiments, the performer was asked to ex-
176press, by her rendition of the musical score, sensorial concepts
177such as ‘bright’, ‘light’ or ‘dark’. The sonological analysis of the
178recordings made it possible to relate certain values to given con-
179cepts (e.g., a ‘light rendition’ was found to be in fast tempo, with
180shortened note durations and soft attacks).
181The significance of various performance parameters in the iden-
182tification of emotional qualities of a performance has been tested
183in synthesis experiments. Automatic performances were obtained
184by setting certain expressive cues to greater or lesser values and,
185in formal listening tests, listeners were able to recognize and iden-
186tify the intended emotions. In the computer program developed by
187Canazza et al. expressiveness was applied both to a ‘neutral’ per-
188formance played by a musician with no intended emotion, and to
189a computer-generated ‘deadpan’ performance. Juslin (1997), on
190the other hand, manually adjusted the values of some previously
191identified cues by means of ‘‘appropriate settings on a Roland JX1
192synthesizer that was MIDI-controlled” by a Synclavier III.
193For more information regarding research in musical perfor-
194mance, including the role expressivity plays in the communication
195of emotions, Gabrielsson’s work (Gabrielsson, 2003) might be
196consulted.
197In second place, expressivity clarifies the musical structure,
198understanding within this term the metrical structure, phrasing
199and harmonic structure. In the work by Sloboda (1983), one could
200observe that performers tend to play louder and more legato the
201notes at the beginning of measures. It was also reported that the
202more expert the pianist was, the more frequent those resources
203were employed and the easier to transcribe the music for the
204audience.
2 M. Delgado et al. / Expert Systems with Applications xxx (2010) xxx–xxx
ESWA 4823 No. of Pages 6, Model 5G
10 July 2010
Please cite this article in press as: Delgado, M., et al. A state of the art on computational music performance. Expert Systems with Applications (2010),
doi:10.1016/j.eswa.2010.06.033
205 Musical structure has its influence on the expressivity of perfor-
206 mances too. It has been discovered that the beginning and the end
207 of phrases tend be slower than the rest. For instance, Todd (1989)
208 proposed a model to predict the final rubato in musical works.
209 Harmonic progressions in a work also have an influence on the
210 expressivity of its renditions. In particular, Palmer (1996) demon-
211 strated that melodic expectation—the degree in which an expected
212 note is finally realized—was related to the energy with which notes
213 are played.
214 3. Computational music performance achievements
215 Advances in digital sound synthesis and in computational
216 power have enabled real-time control of synthesized sounds.
217 Expressive control of these becomes then a relevant area of re-
218 search in the Sound and Music Computing1 field. Empirical research
219 on expressive music performance has its origin in the 1930s, with
220 the pioneering work by Seashore (1938). After a 40-years gap, the
221 topic experienced a real renaissance in the 1970s, and music perfor-
222 mance research is now highly productive. A comprehensive over-
223 view of this research can be found in Gabrielsson (2003).
224 As said before, research in musical performance has a multidis-
225 ciplinar character, with studies that veer from understanding
226 expressive behaviour to modelling aspects of renditions in a formal
227 quantitative and predictive way. Historically, research in expres-
228 sive music performance has focused on finding general principles
229 underlying the types of expressive ‘deviations’ from the musical
230 score (e.g., in terms of timing, dynamics and phrasing) that are a
231 sign of expressive interpretation. Works by Poli (2004) and
232 Widmer and Goebl (2004) contains recent overviews on expressive
233 performance modelling.
234 Three different research strategies can be distinguished: (1)
235 acoustic and statistical analysis of performances by real musicians
236 –the so-called analysis-by-measurement method; (2) making use
237 of interviews with expert musicians to help translate their exper-
238 tise into performance rules –the so-called analysis-by-synthesis
239 method; and (3) inductive machine learning techniques applied
240 to large databases of performances.
241 Studies by several research teams around the world have shown
242 that there are significant regularities that can be uncovered in
243 these ways, and computational models of expressive performance
244 (of mostly classical music) have proved to be capable of producing
245 truly musical results. These achievements are currently inspiring
246 new research into more comprehensive computational models of
247 music performance and also ambitious application scenarios.
248 One of the issues in this area is the representation of theway cer-
249 tain performers play by just analyzing some of their renditions (i.e.,
250 study the individual style of famous musicians). That information
251 would enable us to identify a performer by only listening to their
252 rendition. These studies are difficult because the same professional
253 musician can perform the same score in very different ways (com-
254 pare several commercial recordings by Sergei Rachmaninoff or
255 Vladimir Horowitz). Recently, new methods have been developed
256 for the recognition of music performers and their style. Among
257 them, the most relevant are the fitting of performance parameters
258 in rule-based performance models, and the application of machine
259 learning methods for the identification of performing style of musi-
260 cians. Recent results of specialized experiments show surprising
261 artist recognition rates (for instance, see those from Saunders,
262 Hardoon, Shawe-Taylor, & Widmer (2008) or Molina-Solana, Arcos,
263 & Gomez (2008)).
264 So far, music performance research has been mainly concerned
265 with describing detailed performance variations in relation to mu-
266sical structure. However, there has recently been a shift towards
267high-level musical descriptors for characterizing and controlling
268music performance, especially with respect to emotional charac-
269teristics. For example, it has been shown that it is possible to gen-
270erate different emotional expressions of the same score by
271manipulating rule parameters in systems for automatic music per-
272formance (Bresin & Friberg, 2000).
273Interactive control of musical expressivity is traditionally a con-
274ductor’s task. Several attempts have been made to control the tem-
275po and dynamics of a computer-played score with some kind of
276gesture input device. For example, Friberg (2006) describes a
277method for interactively controlling, in real-time, a system of per-
278formance rules which contains models for phrasing, micro-level
279timing, articulation and intonation. With such systems, high-level
280expressive control can be achieved. Dynamically controlled music
281in computer games is another important future application.
282Recently, some efforts have been made in the direction of visu-
283alizing expressive aspects of music performance. Langner and
284Goebl (2003) have developed a method for visualizing expressive
285performances in a tempo-loudness space: expressive deviations
286leave a trace on the computer screen in the same way as a worm
287does when it moves, producing a sort of ‘fingerprint’ of the perfor-
288mance. This method has been recently extended by Grachten,
289Goebl, Flossmann, and Widmer (in press). This and other recent
290methods of visualization can be used for the development of new
291multi-modal interfaces for expressive communication, in which
292expressivity embedded in audio is converted into visual represen-
293tation, facilitating new applications in music research, music edu-
294cation and Human–Computer Interaction, as well as in artistic
295contexts. A visual display of expressive audio may also be desirable
296in environments where audio display is difficult or must be
297avoided, or in applications for hearing-impaired people.
298For many years, research in Human–Computer Interaction in
299general and in sound and music computing in particular was ded-
300icated to the investigation of mainly ‘rational’ abstract aspects. In
301the last ten years, however, a great number of studies have
302emerged which focus on emotional processes and social interac-
303tion in situated or ecological environments. The broad concept of
304‘expressive gesture’, including music, human movement and visual
305(e.g., computer animated) gesture, is the object of much contempo-
306rary research.
3073.1. Data acquisition
308In this interdisciplinary research field, the obtention of informa-
309tion on musical expressivity can be approached from different per-
310spectives. One approach is the musicological analysis of music and
311the study of the different stylistic schools. This approach provides a
312valuable understanding about musical expressivity.
313Another perspective is the computational modelling of music
314performance by means of automatic analysis of recordings. This
315sound analysis perspective can be raised by the (studio specific)
316recording of several performers where several expressive resources
317are emphasized. That information can be gathered by using aug-
318mented instruments (i.e., instruments provided with sensors of
319pressure or movement). Proceeding this way, the data on obtains
320is very precise, but it is necessary a complex setup and those spe-
321cial instrument are anything but cheap.
322An alternative approach is to directly use commercial record-
323ings for the analysis of expressivity, extracting all the relevant data
324from the audio signals themselves. This approach has several
325advantages: there are tons of recordings available (and often some
326performers have several ones); and the performances are ‘real’ and
327gather the decisions taken by the performers without any external
328influence. Nevertheless, working with commercial recordings has
329some important drawbacks too: some information (consider, for1 http://smcnetwork.org.
M. Delgado et al. / Expert Systems with Applications xxx (2010) xxx–xxx 3
ESWA 4823 No. of Pages 6, Model 5G
10 July 2010
Please cite this article in press as: Delgado, M., et al. A state of the art on computational music performance. Expert Systems with Applications (2010),
doi:10.1016/j.eswa.2010.06.033
330 instance, the bow speed in a violin) cannot be easily gained from
331 the audio; these recordings do not come from a controlled scenario
332 and the sound analysis may become more difficult.
333 Computers are important in both approaches, because they al-
334 low us to store and to process all the gathered data. This informa-
335 tion is huge in size and it is impossible to deal with it in a manual
336 way.
337 3.2. Computational models for artistic music performance
338 The use of computational music performance models in artistic
339 contexts (e.g., interactive performances) raises a number of issues
340 that have so far only partially been faced. The concept of a creative
341 activity being predictable and the notion of a direct ‘quasi-causal’
342 relation between the musical score and a performance are both
343 problematic. The unpredictable intentionality of the artist and
344 the expectations and reactions of listeners are neglected in current
345 music performance models. Surprise and unpredictability are cru-
346 cial aspects in an active experience such as a live performance.
347 Models considering such aspects should take account of variables
348 such as performance context, artistic intentions, personal experi-
349 ences and listeners’ expectations.
350 In the past, this problem has been tackled by using machine
351 learning techniques. For instance, Juslin, Friberg, and Bresin
352 (2002) described the main sources of expressivity in musical rendi-
353 tions and expressed the necessity of integrating some of this as-
354 pects in a common model they started to sketch.
355 Ramírez, Maestre, Pertusa, Gómez, and Serra (2007) proposed a
356 model for identifying saxophonists from the way of playing by
357 using very precise information about deviations in parameters
358 such as pitch, duration and loudness. They measure those devia-
359 tions both in inter and intra note level.
360 de Mántaras and Arcos (2002) studied the expressivity of sev-
361 eral AI-based systems for music composition. They compared this
362 expressivity with the one that exists in human recordings. More-
363 over, they introduced SAXEX, a system capable of generating
364 expressive performances of jazz ballads by using examples from
365 human performers and a case-based reasoner.
366 Hong, on the other hand, studied howmusical expressivity is af-
367 fected by tempo and dynamics variations (Hong, 2003). He em-
368 ployed cello recordings for the experiments. He extended
369 previous work by Todd (1992), by applying newmusical ideas from
370 the 20th century to Todd’s model.
371 Dovey (1995) proposed an attempt to use inductive logic in or-
372 der to determine the rules that pianist Sergei Rachmaninoff may
373 have used in their performances with an augmented piano. The
374 aim was to extract general rules (in the form of universal predi-
375 cates) about each note’s duration, tempo and pressure. All that
376 information was obtained from the way of playing the piano.
377 The group led by Gerhard Widmer has worked in the automatic
378 identification of pianists. In Widmer, Dixon, Goebl, Pampalk, and
379 Tobudic (2003), they studied how to measure several aspects of
380 performances by applying machine learning techniques; whereas
381 in another work (Stamatatos & Widmer, 2005), they proposed a
382 set of simple features that could serve to represent performer’s
383 expressivity from a rendered musical work.
384 Moreover, in a recent paper, Saunders et al. (2008) represent
385 musical performances as string of symbols from an alphabet. Those
386 symbols contain information about changes in timing and energy
387 within the song. After that, they use Support Vector Machines to
388 identify the performer in new recordings.
389 Sapp’s work is also an interesting proposal, as it represents mu-
390 sical renditions by means of those sketches are based on the corre-
391 lation between time and energy (Sapp, 2007).
392 Most of the modelling attempts in performance research, try to
393 capture common performance principles, that is, they focus on
394commonalities between performances and performers. However,
395the ultimate goal of this kind of research and of many of the works
396is not the automatic style replication or the creation of artificial
397performers, but to use computers to teach us more about the elu-
398sive artistic activity of expressive music performance. While it is
399satisfying to see that the computer is indeed capable of extracting
400information from performance measurements that seems to cap-
401ture aspects of individual style, this can only be a first step. In order
402to get real insight, we will need learning algorithms that, unlike
403nearest-neighbour methods, produce interpretable models.
404Although, it may sound odd, there are concrete attempts at
405elaborating computational models of expressive performance to a
406level of complexity where they are able to compete with human
407performers. The Rendering Contest (Rencon)2 (Hiraga, Bresin, Hira-
408ta, & Katayose, 2004) is an annual event first launched in 2002. It
409tries to bring together scientist from all over the world for a compe-
410tition of artificially created performances. It uses an human judge to
411evaluate music performances automatically generated by comput-
412ers. Participants are asked to generate a rendition of a musical work
413by using a predictive level. In a wider sense, we can somehow see
414this paradigm as an expressive performance Turing test.3 In other
415words, the best systems are those than manage to generate perfor-
416mances which sounds indistinguishable from human ones.
417As can be seen, music performance is an interesting research to-
418pic which enables the study of human’s emotions, intelligence and
419creativity. These are precisely the issues Marvin Minsky referred to
420when he wrote about music as a human activity (Minsky, 1992).
4213.3. Automatic music performance
422The principal characteristic of an automatic performance sys-
423tem is that it converts a music score into an expressive musical
424performance typically including time, sound and timbre deviations
425from a deadpan realization of the score. Mostly, two strategies
426have been used for the design of performance systems, the analy-
427sis-by-synthesis method and the analysis-by-measurement
428method.
429The first method implies that the intuitive, nonverbal knowl-
430edge and the experience of an expert musician are translated into
431performance rules. These rules explicitly describe musically rele-
432vant factors. A limitation of this method can be that the rules
433mainly reflect the musical ideas of specific expert musicians. On
434the other hand, professional musicians’ expertise should possess
435a certain generality, and in some cases rules produced with the
436analysis-by-synthesis method have been found to have a general
437character.
438Rules based on an analysis-by-measurement method are de-
439rived from measurements of real performances usually recorded
440on audio CDs or played with MIDI-enabled instruments connected
441to a computer. Often the data are processed statistically, such that
442the rules reflect typical rather than individual deviations from a
443deadpan performance, even though individual deviations may be
444musically highly relevant.
445Many authors have proposed models of automatic music per-
446formance. Todd (1992) presented a model of musical expression
447based on an analysis-by-measurement method. Rule-based sys-
448tems have been proposed by Zanon and Poli (2003), Friberg
449(1991) and Friberg, Colombo, Frydén, and Sundberg (2000).
2 http://www.renconmusic.org.
3 The Turing test is a proposal for a test of a machine’s ability to demonstrate
intelligence. Described by Alan Turing in the 1950 paper ‘‘Computing Machinery and
Intelligence”, it proceeds as follows: a human judge engages in a natural language
conversation with one human and one machine, each of which try to appear human.
All participants are placed in isolated locations. If the judge cannot reliably tell who
the machine and the human are, the machine is said to have passed the test.
4 M. Delgado et al. / Expert Systems with Applications xxx (2010) xxx–xxx
ESWA 4823 No. of Pages 6, Model 5G
10 July 2010
Please cite this article in press as: Delgado, M., et al. A state of the art on computational music performance. Expert Systems with Applications (2010),
doi:10.1016/j.eswa.2010.06.033
450 Performance systems based on artificial intelligence techniques
451 have been developed too. Widmer (2003) proposed a machine
452 learning based system extracting rules from performances. Ishika-
453 wa, Aono, Katayose, and Inokuchi (2000) developed a system for
454 the performance of classical tonal music; a number of performance
455 rules were extracted from recorded performances by using a multi-
456 ple regression analysis algorithm. Arcos, de Mántaras, and Serra
457 (1998) developed a case-based reasoning system for the synthesis
458 of expressive musical performances of sampled instruments.
459 Delgado, Fajardo, and Molina-Solana (2009) developed a multi-
460 agent approach to music composition and generation.
461 4. Future challenges
462 Since the literal synthesis of notes from a score is bland and
463 unappealing, there is an opportunity for learning systems that
464 can automatically produce compelling expressive variations. The
465 problem of synthesizing expressive performance is as exciting as
466 challenging. Musical performance is one of the many activities that
467 trained people do very well without knowing exactly how they do
468 it. This is, precisely, one of the main problems to be faced because
469 there is no model that accurately tells us how to perform.
470 When referring to artistic domains, it is hardly possible to find a
471 ‘correct’ model which predictions always correspond with what
472 humans do and what they think is acceptable. We cannot forget
473 that evaluation in these domains is often subjective and heavily-
474 dependent on who is speaking.
475 Many aspects are involved within expressive performance and
476 it is almost impossible to use them all. Moreover, there are some
477 parameters and dimensions which are commonly considered as
478 non-relevant but that, in fact, might be. Only a portion of the whole
479 problem is tackled by current techniques. One future challenge is
480 to address the problem by using as much dimensions as possible.
481 It could also be possible that some important patterns are hidden
482 and we haven’t still discovered them.
483 Moreover, to obtain very precise data about all those parame-
484 ters is a challenging problem that cannot still be done in a auto-
485 matic way. Annotating all this information is a very time-
486 consuming task and requires a lot of effort from several humans.
487 Early systematic investigations in the field have dealt with this
488 problem either by reducing the length of the music (to just some
489 seconds) or by controlling the size of the collections.
490 Recent approaches try to avoid this task by the use of some sta-
491 tistical learning techniques and by focusing in a more abstract rep-
492 resentation of the real notes and their values. Statistical
493 musicology has not historically received much attention, but it is
494 increasing its popularity as problems are getting more and more
495 complex, and the amount of available data grows, even though col-
496 lect large amount of quantitative data is a really hard task. Temper-
497 ley (2007) tackles musical perception from a probabilistic
498 perspective in his recent book Music and Probability. Apart of pro-
499 posing a Bayesian network model, the author carries out an inter-
500 esting survey of works that use statistical tools to solve problems
501 in the Sound and Music Computing area.
502 Despite some successes in computational performance model-
503 ling, current models are extremely limited and simplistic regarding
504 the complex phenomenon of musical expression. It remains an
505 intellectual and scientific challenge to probe the limits of formal
506 modelling and rational characterization. Clearly, it is strictly
507 impossible to arrive at complete predictive models of such com-
508 plex human phenomena. Nevertheless, work towards this goal
509 can advance our understanding and appreciation of the complexity
510 of artistic behaviours. Understanding music performance will re-
511 quire a combination of approaches and disciplines–musicology,
512 AI and machine learning, psychology and cognitive science.
513For cognitive neuroscience, discovering the mechanisms which
514govern the understanding of music performance is a first-class
515problem. Different brain areas are involved in the recognition of dif-
516ferent performance features. Knowledge of these can be an impor-
517tant aid to formal modelling and rational characterization of higher
518order processing, such as the perceptual differentiation between
519human-like and mechanical performances. Since music making
520and appreciation is found in all cultures, the results could be ex-
521tended to the formalization of more general cognitive principles.
522Finally but not least, it is the problem of the individuality of
523each work. Even though there is a huge amount of available data,
524every song is different from the rest. Hence, it would not be ade-
525quate just to apply the way of playing Beethoven’s Ninth Sym-
526phony to Brahms’ Symphonies. A deep study of the work is
527needed in order to understand the author, the context and the mu-
528sic. One should always keep in mind that artistic performance is far
529from being predictable.
5305. Conclusions
531At this point, the question in the beginning of the paper strikes
532again: can the computer play like a human? This work has tried to
533offer a comprehensive overview of the current research that is
534going on in the field of computational expressive music perfor-
535mance. As shown, there is still plenty of room for new research
536in the area, and the field is currently very active. We have shown
537the problems been faced as well as the most promising directions
538for further work.
539Studies in music performance have a particular value in our
540time. The art of performing music is the result of several years of
541training. At the same time, contemporary information technology
542offers the possibility of automatic playing of music specially com-
543posed for computers or stored in large databases. In such case, the
544music is typically played exactly as nominally written in the score,
545thus implicitly ignoring the value of a living performance and its
546underlying art and diversity.
547As seen, research on music performance ranges from studies
548aimed at understanding expressive performance to attempts at
549modelling aspects of performance in a formal, quantitative and pre-
550dictive way. This research can provide expressive tools that tradi-
551tionally have been hiding in musicians’ skill and musical intuition.
552When explicitly formulated, these tools will give the user the possi-
553bility to play music files with different expressive colouring.
554Even though we are sceptical about a machine completely
555replacing a human performer, we are sure that this technology will
556be available in a not very far future for certain tasks. Scenes like the
557one in the beginning of this paper will not be science-fiction any-
558more and it is only a matter of time that they will become com-
559monplace. We have also shown that there are currently some
560attempts in this direction, like the Rencon contest.
561We strongly believe that it is time for computer science to work
562in the music domain. This research will make a great impact in
563both the arts and sciences. Not in vain, music is more than an inter-
564esting and, somehow, odd domain; it is part of our human essence.
565Acknowledgements
566This research has been partially supported by the Spanish Min-
567istry of Education and Science under the project TIN2006–15041-
568C04–01. M. Molina-Solana is also supported by FPU Grant
569AP2007–02119.
570References
571Arcos, J. L., de Mántaras, R. L., & Serra, X. (1998). Generating expressive musical
572performances with SaxEx. Journal of New Music Research, 27(3), 194–210.
M. Delgado et al. / Expert Systems with Applications xxx (2010) xxx–xxx 5
ESWA 4823 No. of Pages 6, Model 5G
10 July 2010
Please cite this article in press as: Delgado, M., et al. A state of the art on computational music performance. Expert Systems with Applications (2010),
doi:10.1016/j.eswa.2010.06.033
573 Bresin, R., & Friberg, A. (2000). Emotional coloring of computer-controlled music
574 performances. Computer Music Journal, 24(4), 44–63.
575 Canazza, S., Poli, G. D., Drioli, C., Rodà, A., & Vidolin, A. (2000). Audio morphing
576 different expressive intentions for multimedia systems. IEEE MultiMedia, 7(3),
577 79–83.
578 Delgado, M., Fajardo, W., & Molina-Solana, M. (2009). INMAMUSYS: Intelligent
579 multiagent music system. Expert Systems with Applications, 36(3), 4574–4580.
580 de Mántaras, R. L., & Arcos, J. L. (2002). AI and music: From composition to
581 expressive performance. AI Magazine, 23(3), 43–57.
582 Dovey, M. J. (1995). Analysis of Rachmaninoff’s piano performances using inductive
583 logic programming. In Proceedings of the eighth european conference on machine
584 learning (ECML95) (pp. 279–282). London, UK: Springer-Verlag.
585 Friberg, A. (1991). Generative rules for music performance: A formal description of a
586 rule system. Computer Music Journal, 15(2), 56–71.
587 Friberg, A. (2006). pDM: An expressive sequencer with real-time control of the KTH
588 music performance rules movements. Computer Music Journal, 30(1), 56–71.
589 Friberg, A., Colombo, V., Frydén, L., & Sundberg, J. (2000). Generating musical
590 performances with director musices. Computer Music Journal, 24(3), 23–29.
591 Gabrielsson, A. (1995). Music and the mind machine. Expressive intention and
592 performance. Berlin: Springer [pp. 35–47].
593 Gabrielsson, A. (2003). Music performance research at the millennium. The
594 Psychology of Music, 31(3), 221–272.
595 Grachten, M., Goebl, W., Flossmann, S., & Widmer, G. (in press). Phase-plane
596 representation and visualization of gestural structure in expressive timing,
597 Journal New Music ResearchQ2 .
598 Hiraga, R., Bresin, R., Hirata, K., & Katayose, H. (2004). Rencon 2004: Turing test for
599 musical expression. In Proceedings of the 2004 conference on new interfaces for
600 musical expression (NIME04) (pp. 120–123). Singapore: National University of
601 Singapore.
602 Hong, J.-L. (2003). Investigating expressive timing and dynamics in recorded cello.
603 Psychology of Music, 31(3), 340–352.
604 Ishikawa, O., Aono, Y., Katayose, H., & Inokuchi, S. (2000). Extraction of musical
605 performance rules using a modified algorithm of multiple regression analysis.
606 In Proceedings of the 2000 international computer music conference, San Francisco
607 (pp. 348–351).
608 Juslin, P. N. (1997). Perceived emotional expression in synthesized performances of
609 a short melody: Capturing the listeners judgment policy. Musicae Scientiae, 1(2),
610 225–256.
611 Juslin, P. N., Friberg, A., & Bresin, R. (2002). Toward a computational model of
612 expression in performance: The GERM model. Musicae Scientiae(special issue),
613 63–122.
614 Langner, J., & Goebl, W. (2003). Visualizing expressive performance in tempo-
615 loudness space. Computer Music Journal, 27(4), 69–83.
616 Lindström, E. (2006). Impact of melodic organization on perceived structure and
617 emotional expression in music. Musicae Scientiae, 10, 85–117.
618Meyer, L. (1956). Emotion and meaning in music. Chicago, IL: University of Chicago
619Press.
620Minsky, M. (1992). Machine models of music. Music mind and meaning. Cambridge,
621MA, USA: MIT Press.
622Molina-Solana, M., Arcos, J. L., & Gomez, E. (2008). Using expressive trends for
623identifying violin performers. In Proceedings of ninth international conference on
624music information retrieval (ISMIR2008) (pp. 495–500).
625Palmer, C. (1996). Anatomy of a performance: Sources of musical expression. Music
626Perception, 13(3), 433–453.
627Poli, G. D. (2004). Methodologies for expressiveness modelling of and for music
628performance. Journal of New Music Research, 33(3), 189–202.
629Ramírez, R., Maestre, E., Pertusa, A., Gómez, E., & Serra, X. (2007). Performance-
630based interpreter identification in saxophone audio recordings. IEEE
631Transactions on Circuits and Systems for Video Technology, 17(3), 356–364.
632Rigg, M. G. (1964). The mood effects of music: A comparison of data from former
633investigators. Journal of Psychology, 58, 427–438.
634Sapp, C. (2007). Comparative analysis of multiple musical performances. In
635Proceedings of eighth international conference on music information retrieval
636(ISMIR 2007) Vienna, Austria (pp. 497–500).
637Saunders, C., Hardoon, D., Shawe-Taylor, J., & Widmer, G. (2008). Using string
638kernels to identify famous performers from their playing style. Intelligent Data
639Analysis, 12(4), 425–440.
640Seashore, C. E. (1938). Psychology of music. New York: McGraw-Hill.
641Sloboda, J. A. (1983). The communication of musical metre in piano performance.
642Quarterly Journal of Experimental Psychology, 35, 377–396.
643Stamatatos, E., & Widmer, G. (2005). Automatic identification of music performers
644with learning ensembles. Artificial Intelligence, 165(1), 37–56.
645Sundberg, J. (2000). Emotive transforms. Phonetica, 57, 95–112.
646Temperley, D. (2007). Music and probability. The MIT Press.
647Todd, N. P. (1989). A computational model of rubato. Contemporary Music Review,
6483(1), 69–88.
649Todd, N. P. (1992). The dynamics of dynamics: A model of musical expression.
650Journal of the Acoustical Society of America, 91(6), 3540–3550.
651Widmer, G. (2003). Discovering simple rules in complex data: A meta-learning
652algorithm and some surprising musical discoveries. Artificial Intelligence, 146(2),
653129–148.
654Widmer, G., Dixon, S., Goebl, W., Pampalk, E., & Tobudic, A. (2003). In search of the
655Horowitz factor. AI Magazine, 24(3), 111–130.
656Widmer, G., & Goebl, W. (2004). Computational models of expressive music
657performance: The state of the art. Journal of NewMusic Research, 33(3), 203–216.
658Zanon, P., & Poli, G. D. (2003). Estimation of parameters in rule systems for
659expressive rendering in musical performance. Computer Music Journal, 27(1),
66029–46.
661
6 M. Delgado et al. / Expert Systems with Applications xxx (2010) xxx–xxx
ESWA 4823 No. of Pages 6, Model 5G
10 July 2010
Please cite this article in press as: Delgado, M., et al. A state of the art on computational music performance. Expert Systems with Applications (2010),
doi:10.1016/j.eswa.2010.06.033
