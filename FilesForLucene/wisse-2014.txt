Authorship Identification and Verification
of JavaScript Source Code
An Evaluation of Techniques
Wilco Wisse

Authorship Identification and Verification
of JavaScript Source Code
THESIS
submitted in partial fulfillment of the
requirements for the degree of
MASTER OF SCIENCE
in
COMPUTER SCIENCE
by
Wilco Wisse
born in Borsele, the Netherlands
Software Engineering Research Group
Department of Software Technology
Faculty EEMCS, Delft University of Technology
Delft, the Netherlands
www.ewi.tudelft.nl
Netherlands Forensic Institute
Knowledge and Expertise Centre for
Intelligent Data Analysis
The Hague, the Netherlands
www.forensicinstitute.nl
c© 2014 Wilco Wisse.
Cover picture: Fragment famous old Dutch text “Hebban olla vogala” by Unknown -
Personal collection of Paul Kempeneers. Licensed under Public domain via Wikime-
dia Commons – http://commons.wikimedia.org/wiki/File:Hebban_olla_vogala_
fragment.jpg
Authorship Identification and Verification
of JavaScript Source Code
Author: Wilco Wisse
Student id: 4034171
Email: w.c.wisse@student.tudelft.nl
Abstract
The increasing number of criminals that exploit the speed and anonymity of
the Web has become of increasing concern. Little effort has been spent to trace
the authors of malicious code. To that end we investigated authorship identifi-
cation and verification of JavaScript source code. We evaluated three character
based approaches and propose a new domain specific approach. What is new in
the domain specific analysis approach, is that it represents code by a parse tree to
extract structural features. The evaluation of the techniques with open source code
from GitHub, turned out that the approaches that use character n-gram features
achieved the best performance. However, the combination of n-gram and domain
specific features turned out to be complementary, resulting in a higher performance.
Techniques that used similarity based classification were especially successful if a
limited amount of training data were available, while feature vector based tech-
niques were mainly successful when a large amount of training data were available
and in an authorship verification context. By means of code minification we evalu-
ated how the classification accuracy is affected by removing authorship information
from the source code. Code minification has shown to significantly deteriorate the
performance of the authorship analysis methods. Especially the compression based
technique is robust against code minification.
Thesis Committee:
Chair: Dr. A.E. Zaidman, Faculty EEMCS, TU Delft
Company supervisor: Dr. C.J. Veenman, Netherlands Forensic Institute
Committee Member: Dr. A. Bacchelli, Faculty EEMCS, TU Delft
Committee Member: Dr. C. Hauff, Faculty EEMCS, TU Delft

Preface
The master thesis that is lying in front of you is the result of almost one year research
at the NFI. Although this writing is only the last stage of a journey, the work as a whole
could be expressed by the words of novel writer E.L. Doctorow: “Writing is like driving
at night in the fog. You can only see as far as your headlights, but you can make the
whole trip that way.” I am using this opportunity to express my gratitude to everyone
who helped and supported me to keep the show on the road. First, this thesis would
not happen to be possible without the help of Andy Zaidman. I appreciated his useful
feedback and the enthusiasm and patience in which he supported me thoughout my
thesis. It is also with immense gratitude that I acknowledge the help of Cor Veenman.
The valuable brainstorm sessions provided me many new insights and I liked the high
level of freedom he gave me during the project. Besides Andy and Cor, I am grateful
to the other committee members for reviewing my work. I would also like to thank my
colleagues at Kecida. It was a real pleasure to do my research in such an interesting
team. In particular, I would like to thank Gert Jacobusse for offering suggestions on
the thesis topic and his ideas on the work in progress. Last but not the least, I would
like to thank my family. None of this would have been possible without their love,
encouragement and prayer.
Wilco Wisse
The Hague, the Netherlands
December 11, 2014
iii

Contents
Preface iii
Contents v
1 Introduction 1
1.1 Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
1.2 Authorship analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
1.3 Research objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
1.4 Thesis organization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
2 Preliminaries and Related Work 7
2.1 The authorship analysis process . . . . . . . . . . . . . . . . . . . . . . . . 7
2.2 Source code authorship identification . . . . . . . . . . . . . . . . . . . . . 15
2.3 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
3 Approach 23
3.1 Character based approaches . . . . . . . . . . . . . . . . . . . . . . . . . . 23
3.2 Domain specific approach . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
3.3 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
4 Experimental Setup 39
4.1 Validation procedure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
4.2 Source code collections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
5 Results 49
5.1 Authorship identification . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
5.2 Authorship verification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
6 Discussion 59
6.1 Comparing classification techniques . . . . . . . . . . . . . . . . . . . . . . 59
6.2 Comparing the effectivity of features types . . . . . . . . . . . . . . . . . . 60
6.3 Applying authorship verification . . . . . . . . . . . . . . . . . . . . . . . 61
v
Contents
6.4 Addressing authorship analysis complications . . . . . . . . . . . . . . . . 61
6.5 Overall effectivity of the techniques . . . . . . . . . . . . . . . . . . . . . . 62
6.6 Limitations and threats to validity . . . . . . . . . . . . . . . . . . . . . . 63
7 Conclusions and Future Work 67
7.1 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67
7.2 Research results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68
7.3 Main conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
7.4 Future work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
Bibliography 71
A Abstract Syntax of JavaScript 79
A.1 Syntax definition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79
A.2 Refactorings to the AST . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82
A.3 Layout nodes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83
B Domain Specific Features 85
B.1 Layout . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85
B.2 String patterns . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87
B.3 Comments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88
B.4 String length . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88
B.5 List length . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89
B.6 Descendant count . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90
C Dataset Description 91
C.1 Dataset A . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91
C.2 Dataset B . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91
D Graphs of Experimental Results 95
vi
1
Introduction
The cover of this work shows one of the oldest known sentences in western old Dutch:
“Hebban olla uogala nestas hagunnan hinase hic anda thu uuat unbidan uue nu.” Proba-
bly the fragment dates to 11th centrury. Although the author of the fragment is unknown,
the writing style still reveals some clues of authorship, such as the origin of the writer.
Recent research concluded that the fragment was probably written by a woman or from
a female perspective. Clearly, the writing style of a text conveys much information of the
respective author. Similar to texts, a ‘fingerprint’ of writing style can be extracted from
program source code. In this thesis the focus is on authorship analysis of JavaScript
source code.
1.1 Background
The activity of authorship analysis can at least be dated back to the middle ages when
the scholasticism dominated the academics in Europe. In that time, knowing the au-
thorship of a text was essential to determine the text its veracity [48]. A typical example
of authorship analysis is the investigation whether all works of Shakespeare have cor-
rectly been claimed to be written by himself, or that there is evidence that William
Shakespeare is the same person as William Shakspere [23]. Others tried to use author-
ship identification techniques to determine the author of the letter to the Hebrews by
comparing different texts of the New Testament [69].
The basis of authorship analysis is that language is flexible enough in format to allow
humans to create certain habits in how the language is used. If texts are handwritten,
the analysis of physical characteristics such as the shape of dots and regularity provide
clues of authorship [72]. Also printed texts contain much authorship information, such
as the frequency of individual words, the preference to use short or long sentences and
punctionation style. Programming languages share many linguistic concepts with natural
languages [81]. Both can be expressed by textual symbols that have semantics. Also,
both are systematically constructed according to a grammar that describes the formation
1
1. Introduction
and composition of words. Although the syntax definition of programming languages
is more strict than natural languages, there is still a certain amount of flexibility that
enables programmers to develop their own programming style [33, 19].
In general, authorship analysis has numerous applications. It may be used in cases
of academic dishonesty and to resolve issues in litigation and theft. The focus on au-
thorship analysis on JavaScript source code is mainly of interest due to the rapid growth
in information technology and the increasing popularity of the Internet. The Web is
relatively unprotected and an increasing number of criminal activities are exploiting
its convenience and anonymity. Much effort has been spend to detect malicious con-
tent. However, less attention has been paid to identification of the respective authors.
JavaScript is commonly used on webpages. The identification of the developer of frag-
ment of JavaScript code forms a supplementary source of authorship information that
can play a crucial role in a forensic investigation, such as tracing the authors of fraud-
ulent websites. As such, source code authorship analysis is part of the field of software
forensics, which comprises a multitude of program analysis tasks such as malicious code
analysis and source code plagiarism detection [72].
1.2 Authorship analysis
We first introduce the authorship analysis problem. In general, the term authorship
analysis embodies a multitude of methods for examining the characteristics of a docu-
ment to draw conclusions on its authorship [84]. This can be divided in at least three
subproblems, namely authorship identification, authorship characterization and author-
ship verification. Because a generally accepted definition is lacking and these terms are
used inconsistently in literature, we first define clearly how we will use these terms.
1.2.1 Authorship identification
The aim of authorship identification is to determine the most likely author of a document
with unknown authorship by examining other writings of the candidate authors [84]. In
this work we define the term authorship identification as follows:
Definition 1 (Authorship identification). Given a set of authors A = {a1, a2, . . . , an}, a
set of training documents D = {d1, d2, . . . , dm}, and a surjective label function l : D → A
which defines the authorship of each training document. The authorship identification
problem is then to predict author(d), the author of an unlabeled document d.
A distinction can be made between closed and the open set authorship identification
[83]. In the closed set problem, the author of document d is a priori known to be one of
the authors in A. In contrast to the closed set problem, the open set problem does not
assume that d neccesarily has been written by one of the candidate authors. It can thus
be written by a previously unseen author for which no training documents have been
provided.
2
1.2. Authorship analysis
Juweel der Civ-
itas Uw diepe
warme gloed En
schitterende glans
Ontroeren ons
gemoed Als de
mooiste diaman-
ten Zend gij licht
naar alle kanten
Johannes Calvijn
?
Who wrote d?
d
a1
a2
a3
(a)
Juweel der Civ-
itas Uw diepe
warme gloed En
schitterende glans
Ontroeren ons
gemoed Als de
mooiste diaman-
ten Zend gij licht
naar alle kanten
Johannes Calvijn
?
Did a1 write d?
d a1
(b)
Figure 1.1: The difference between (a) closed set authorship identification and (b) authorship verifi-
cation. In this example ai are authors and d is an unlabeled document.
Definition 2 (Open vs. closed set authorship identification). The authorship identifi-
cation problem is a closed set problem if and only if it is given that author(d) ∈ A.
Plagiarism detection is not the same as authorship identification, but is related.
Plagiarism detection is concerned with the detection of replication of the work from
another author’s work in a document, and hence is concerned with authorship analsyis
within a document. Similarly, source code authorship identification is different from
identifying code clones in software.
1.2.2 Authorship verification
Authorship verification (also known as authorship discrimination or similarity detection)
involves determining whether two documents have been written by the same author,
without necessarily identifying the author. In other words, for two documents d1 and
d2 the problem is to decide whether author(d1) equals author(d2) [6, 84]. Basically,
this is a special case of the open set authorship identification problem with only one
author and with only one available training document. Others however also used the
name verification problem for the one author open set problem with multiple training
documents available for the target author [48, 83]. Figure 1.1 conceptually shows the
authorship identification and authorship verification problem. The main difference be-
tween verification and closed set identification is that verification is a ‘one versus the
rest of the world’ problem, so there is no candidate author set established in advance,
as is the case in identification. We define the authorship verification problem as follows:
Definition 3 (Authorship verification). The authorship verification problem is an open
set authorship identification problem with one candidate author, i.e. |A| = 1.
1.2.3 Authorship characterization
Another field of authorship analysis is authorship characterization [84], which has been
called authorship profiling in literature as well [48]. This problem adresses the question
how characteristics of the author of a document can be identified, for example the gender,
the educational background and language familiarity. This field does not aim to answer
3
1. Introduction
the question which person wrote a piece of writing, but to predict personal characteristics
of the author.
1.3 Research objectives
Authorship identification has been applied on several programming languages, includ-
ing Pascal [64], C [53], Java [55, 70, 19] and C++ [45, 57]. In this work the focus is
in particular on authorship analysis of JavaScript source code. JavaScript is an inter-
preted object-oriented scripting language which shares many syntactic concepts from
C and Java. The investigation of authorship analysis of JavaScript code is mainly of
significance because of its wide availability in its source code form. First, JavaScript is
an interpreted language, which means that the source code is executed directly, without
compiling the program into machine-language instructions. Secondly, JavaScript is very
commonly embedded within webpages and runs client-side. Overall, the source code is
widely available on the Web so that authorship analysis of JavaScript can be crucial in
cybercrime investigations, such as tracing the authors of malicious websites. This brings
us to our main research question:
Main Research Question. To what extent are authorship analysis techniques effective
on JavaScript source code?
Authorship analysis is a broad term. Therefore, we narrowed the main research ques-
tion down to a number of more specific questions. These are detailed in the remainder
of this section.
1.3.1 Reviewing the literature
Source code authorship analysis has not been investigated quite extensively. Neverthe-
less, a number of authors have addressed this problem. An overview of existing research
in the form of a literature research should give an understanding of state of the art
software analysis techniques. The results of the literature study also form the basis for
the remaining research questions. The objective of the literature study is formulated in
the first sub-question:
Research Question 1. What is the state of the art in software authorship analysis?
1.3.2 Comparing authorship analysis techniques
Authorship analysis is a special case of the categorization of text documents into a
predefined number of classes, and is thus a text classification problem. Broadly speaking,
authorship analysis techniques can be divided into similarity based approaches where a
similarity metric is employed for classification, and feature vector based approaches
that employ machine learning algorithms for classification. The different techniques
are evaluated on various problem complexities by regulating the number of candidate
authors and the amount of available training data per author. In our study we evaluate
4
1.3. Research objectives
one similarity based approach and three feature based approaches that use a Support
Vector Machine (SVM) as classification algorithm. This is summarized in the second
sub-question:
Research Question 2. What is the difference in performance between similarity based
and feature vector based techniques, by considering the number of candidate authors and
the number of available training samples per author?
The characteristics that are used in the analysis process are known as features. Next
to the used classification technique, a fundamental difference between analsyis tech-
niques is the syntactical complexity of the language the features are extracted from.
Some approaches extract features from a character based level and are thus language
independent. Other approaches, however, use a higher level language representation to
analyze the documents. Since high level features are specific to a certain language, we
refer to these methods as domain specific techniques. In this work we propose a new
domain specific approach dedicated to JavaScript and compare it with three existing
(language independent) character based approaches from literature. New to the domain
specific approach is that we consider structural parse tree features. In natural language,
syntactical patterns in the parse tree have found to be strong markers of writing style.
In contrast to natural languages, programming languages can be parsed unambiguously.
Nevertheless, we have not found techniques that mined syntactical features this way.
This brings us to the next sub-question:
Research Question 3. What is the effectivity of domain specific JavaScript features
compared to character based features?
1.3.3 Applying authorship verification
We will investigate both the authorship identification and verification problem. Several
studies in literature considered authorship identification of source code, but no studies
have been found that addressed authorship verification. This study aims to contribute to
the field by embedding the implemented authorship analysis techniques in an authorship
verification context.
Research Question 4. What is the effectivity of authorship analysis techniques in
authorship verification compared to authorship identification?
1.3.4 Addressing authorship analysis complications
Existing approaches report promising results, but various assumptions are done to guar-
antee the feasibility of the approach. For instance, most studies assume the code under
investigation was written by a single author, that the coding style is not influenced by
external factors and that the coding style of the author is consistent over time and across
projects. A question that needs to be asked, if whether such assumptions are realistic.
There are various complications which may deteriorate authorship analysis techniques,
5
1. Introduction
such as the reuse of code and external constraints placed by managers or tools [53, 55].
As a result the analysis techniques may not generalize well to code that is more likely to
be found in a forensic investigation or in industrial practice. It is far beyond the scope of
this work to address all the situations which may negatively influence the effectivity of
authorship analysis. Therefore we only address two problems, which are related to the
external impact on coding style. These are addressed in the following research question:
Research Question 5. What is the influence of code collaboration and code minification
on the classification performance?
Code minification is the process of removing all redundant characters from source
code, for instance by stripping the layout and shortening variable names. This is usually
done for data compression purposes. Next, we investige the ability to identify authors
when they collaborated in the same project. Obviously, this may be problematic as
they influence each other, for instance by using the same variables, objects and other
constructs.
1.4 Thesis organization
This thesis has been organised in the following way. We begin in Chapter 2 by a liter-
ature review that should give an understanding of state of the art authorship analysis
techniques. The results of the literature study will form the basis for the remaining
chapters. In Chapter 3, the authorship analysis techniques are introduced that will be
investigated. The experimental setup to evaluate the techniques and the results are
described in Chapters 4 and 5, respectively. Subsequently, the main findings and limi-
tations are discussed in Chapter 6. Finally, we conclude and give recommendations for
further research in Chapter 7.
6
2
Preliminaries and Related Work
In this chapter we aim to establish a scientific background of source code authorship
analysis. We start by the the general process of authorship analysis. The second part
moves on to authorship identification of source code and we investigate how this task
has been approached in literature. Because many concepts discussed in this chapter are
not limited to software but also apply to texts, we use the word document to refer to a
text in natural language as well as a piece of source code.
2.1 The authorship analysis process
In general, authorship identification involves the categorization of documents into a
predefined number of classes, and is thus a classification problem. Figure 2.1 depicts
some important aspects of most authorship identification studies [79]. In order to be able
to classify unlabeled documents, the documents in the collection are uniquely identified
by performing a number of measurements on them, which are known as features. In
general, if documents are represented by n different features, they form a feature vector
xi ∈ Rn and identify a document di in an n dimensional feature space. The role of the
classifier is then to divide the feature space of the training samples into regions that
correspond to the different classes, which is known as training the classifier. If a feature
vector of an unknown document falls in the region of one of the classes it is classified
to that class. This prediction can be either correct or incorrect. In the latter case a
misclassification has occurred which will deteriorate the performance of the classifier.
In an experimental setting, a part of the documents from the document collections are
hold out as test set to be able to compare the predictions of the classifier with the actual
class labels. A resulting performance measure pC gives an indication of the performance
of the classifier on the used dataset. Overall, there are large differences in the choice
of features extracted from the document, the classification model used for classification
and the performance evaluation of the model. These aspects will briefly be discussed
in the remainder of this section. Section 2.1.1 addresses different choices for feature
7
2. Preliminaries and Related Work
Documents
Feature
extraction
Training Evaluationdi xi =


x1
x2
...
xn


Feature
vectors
Classification
model C
Trainset Testset Performance
pCC
Figure 2.1: General outline of the authorship identification process. In this figure the documents di
are labeled by class label ci. The documents are used for training and validation purposes.
extraction. Section 2.1.2 examines some classification methods. Finally, Section 2.1.3
discusses some classifier evaluation measures.
2.1.1 Feature extraction
In the feature extraction phase, a feature set is composed. A feature set is a set of
predefined characteristics of the document which should be able to characterize the
particular writing style of an author. Such features are refered to as stylometric features,
style markers, or style features in literature. In authorship identification of natural
language, Stamatatos distinguished five feature types based on the syntactic complexity
of the language representation [73]. We briefly review the taxonomy in Table 2.1. In
Section 2.2 we will see that feature extracted from source code can be organized on the
used language representation in a similar manner. We now pay some attention to two
feature types that are relevant for the remainder of this thesis, namely character n-gram
features and syntactical features.
Character n-gram features
The application of character n-grams has proven to be quite successful in both author-
ship identification of texts and of source code [9]. In general, an n-gram is a consec-
utive sequence consisting of units of length n. By using a sliding window of length n,
a document of t units will generate t − n + 1 n-grams. For instance, the code frag-
ment ‘function(){}’ has 12 characters and generates the following set of t − n + 1 = 5
character 8-grams: function, unction(, nction(), ction(){, tion(){}. Although a
large number of n-grams in a large document will be redundant, many of the n-grams
will implicitly capture stylistic information. For instance, the frequency of the n-gram
‘function’ may determine the number of function declarations per lines of code, while
‘unction(’ also captures that no space is used between the identifier and the parenthesis.
The n-gram ‘nction()’ further indicates that zero parameters have been defined in this
particular function definition. A risk of character n-grams is that they may also capture
contextual information that is closely associated with the particular domain. Thus if
the training documents are not representative for the domain, the classifier may become
overtrained [48]. In general, the features should be as independent as possible with
regard to the context of the training samples to avoid overtraining of the classifier.
8
2.1. The authorship analysis process
Syntactical features
In source code, the structure of a program can conveniently be expressed by the Abstract
Syntax Tree (AST), which is the result of parsing the code. Although programming
languages can be parsed unambiguously (in contrast to natural languages), we have found
no techniques that mined structures from the AST. In natural language, syntactical
features have been proven to be good features for authorship identification [46]. A
wide range of basic syntactical features have been used, of which most notable function
words and Part of Speach (POS) tags [59]. POS tags are grammatical word and sentence
categories such as nouns, noun phrases and verb phrases. However, since the performance
of natural language processing tools greatly improved, syntactic trees can be generated
with high accuracy nowadays. Syntactic trees show how the POS tags are structured
by a rooted and ordered tree. Baayen [3] was the first who used rewrite-rule frequencies
extracted from the parse tree. Rewrite rules represent the combinations of a node and
its immediate constituents in the tree. A rewrite rule such as A → B,C results in
a parent node A with children B and C somewhere in the syntactic tree. Related to
this is the frequency of every pair of tags (A,B) in the tree [63] and to consider the
depth of the syntax trees in the document [41]. There are also a number of works that
considered more complex syntactic structures in the parse trees, such as feature sets of
Text representation
Character A very simple text representation is to consider it as a sequence of characters. A basic
example of a measure based on this representation is the frequency distributions
of characters. A character n-gram representation is a very well-known character
based representation. Also data compression based methods are character based.
The intuition behind compression based techniques is that a unlabeled document
compresses the best with the training documents of its author because they have a
high degree of similar patterns.
Lexical Lexical features are extracted from a document represented by a sequence of tokens,
with each token corresponding to a word, number or other unit. Examples are ba-
sic language properties such as the average sentence length, and word occurrence
frequencies. Another typical example is the vocabularly richness of a text [34].
Syntactic Syntactic features provide code measures of a higher level of abstraction than lexical
features. Syntactic features have the advantage that they are less topic dependent
than lexical features and are related to how an author syntactically organizes his
sentences. [38]. The most common syntactical features are function words, gram-
matical elements such as Part of Speach (POS) tags and rewrite rules frequencies in
the grammar [46].
Semantic Semantic analysis of a text can provide clues of authorship as well. An example is
the degree of usage of synonyms.
Domain-
specific
Domain specific features are not related to a particular text representation, but use
features typical for a specific text domain. Examples are the analysis of greetings in
emails or the layout in WYSIWYG’ed documents [38].
Table 2.1: Feature extraction in authorship identification based on different text representations
abstraction levels.
9
2. Preliminaries and Related Work
tree fragments [46], closed and maximal frequent subtrees [56] and skeleton structure
representations that only contain the structure of the subtrees without the syntactic
categories [59]. Additionally, one approach in literature expressed how the elements
appear in syntactic trees by extracting n-grams within the tree structure [80]. Others
use syntactic relations in the parse tree to extract n-grams from the original text, by
taking the order into account the elements appear in the parse tree [71].
Feature selection
Feature selection methods show that a subset of features may lead to significant improve-
ments in classification performance in comparision to using the full feature set [61]. The
goal of feature selection is to find the features that make a significant contribution and
to eliminate features which only add noise in the classification process. Feature selection
is not a trivial process however. A feature may seem to be irrelevant when examined
individually, but in combination with other features it may be of interest.
2.1.2 Classification
Having considered feature extraction, we now look at the classification process. A clas-
sifier predicts the authorhip of previously unseen documents based on the extracted
features from the training documents. The purpose of training is to capture the various
regularities from the training data as well as possible. There is however often a tradeoff
between the bias and the variance of a classifier, which both contribute to the prediction
error [30]. With a low bias and a high variance the learning algorithms will discover the
regularities of the particular training set very well and as a consequence performance
of the classifier on the training set is high. However, with a high variance the learner
may be sensitive to idiosyncrasies of the training set and does not generalize well to
previously unseen data. In such cases the learner is overtrained (also called overfit).
Clearly, there is a tension between minimizing both the the bias and the variance of a
learner. We now discuss some classification approaches that have commonly been used
in authorship analysis studies.
Profile based versus instance based methods
Stamatatos distinguished two frequenly used classification approaches in authorship
identification, which are referred to as profile based1 and instance based methods [73].
Profile based and instance based methods differ in whether the training documents are
treated individually (per document), or cumulatively (per author). This is explained in
Figure 2.2. The key distinction between the methods is how the training data is sam-
pled: as a general style for each author or a separate style for each document. In the
profile based approaches all available training documents are concatenated per author in
one big file and the features of this cumulative file forms the profile that characterizes
1This term should not be confused with authorship characterization which is sometimes called pro-
filing.
10
2.1. The authorship analysis process
Juweel der Civ-
itas Uw diepe
warme gloed En
schitterende glans
Ontroeren ons
gemoed Als de
mooiste diaman-
ten Zend gij licht
naar alle kanten
Johannes Calvijn
Juweel der
Civitas Uw
diepe warme
gloed En
schitterende
glans On-
troeren ons
gemoed Als
de mooiste
diamanten
Zend gij
licht naar
alle kanten
Johannes
Calvijn
Nooit
klinkt het
slotakkoord
Steeds volgt
een nieuwe
zin Het spel
gaat altijd
door Dus
zetten wij
weer in Als
de mooiste
diaman-
ten Toont
gij telkens
nieuwe
kanten
Johannes
Calvijn
+
+
Hoog rijst zij voor
ons op De univer-
siteit Daar brand
het technisch vuur
Op elke faculteit
De techniek zal
morgen zorgen En
een toename waar-
borgen Van de en-
tropie
Nooit klinkt het
slotakkoord Steeds
volgt een nieuwe
zin Het spel gaat
altijd door Dus
zetten wij weer
in Als de mooiste
diamanten Toont
gij telkens nieuwe
kanten Johannes
Calvijn
=
Nucleus van het
Woord Die het dis-
puut fixeert In een
begrensde baan
Door Dordt gefor-
muleerd Amicitia
en studie Leven
door het Evangelie
De kernenergie
author(u) = ai
ta1,1
u Classifier
xa1 =

xa1,1
xa1,2

ta1,2
ta1,3
Juweel der Civ-
itas Uw diepe
warme gloed En
schitterende glans
Ontroeren ons
gemoed Als de
mooiste diaman-
ten Zend gij licht
naar alle kanten
Johannes Calvijn
Juweel der
Civitas Uw
diepe warme
gloed En
schitterende
glans On-
troeren ons
gemoed Als
de mooiste
diamanten
Zend gij
licht naar
alle kanten
Johannes
Calvijn
Nooit
klinkt het
slotakkoord
Steeds volgt
een nieuwe
zin Het spel
gaat altijd
door Dus
zetten wij
weer in Als
de mooiste
diaman-
ten Toont
gij telkens
nieuwe
kanten
Johannes
Calvijn
+
+
Hoog rijst zij voor
ons op De univer-
siteit Daar brand
het technisch vuur
Op elke faculteit
De techniek zal
morgen zorgen En
een toename waar-
borgen Van de en-
tropie
Nooit klinkt het
slotakkoord Steeds
volgt een nieuwe
zin Het spel gaat
altijd door Dus
zetten wij weer
in Als de mooiste
diamanten Toont
gij telkens nieuwe
kanten Johannes
Calvijn
=
ta2,1
xa2 =

xa2,1
xa2,2

ta2,2
ta2,3
xu =

xu,1
xu,2


ai
xa1
xa2
xu
i = argmini(d(xu, xai))
→
→
→
x1
x2
(a) Profile based classification.
Classifier
Juweel der Civ-
itas Uw diepe
warme gloed En
schitterende glans
Ontroeren ons
gemoed Als de
mooiste diaman-
ten Zend gij licht
naar alle kanten
Johannes Calvijn
Hoog rijst zij voor
ons op De univer-
siteit Daar brand
het technisch vuur
Op elke faculteit
De techniek zal
morgen zorgen En
een toename waar-
borgen Van de en-
tropie
Nooit klinkt het
slotakkoord Steeds
volgt een nieuwe
zin Het spel gaat
altijd door Dus
zetten wij weer
in Als de mooiste
diamanten Toont
gij telkens nieuwe
kanten Johannes
Calvijn
Nucleus van het
Woord Die het dis-
puut fixeert In een
begrensde baan
Door Dordt gefor-
muleerd Amicitia
en studie Leven
door het Evangelie
De kernenergie
ta1,1
u
ta1,2
ta1,3
Juweel der Civ-
itas Uw diepe
warme gloed En
schitterende glans
Ontroeren ons
gemoed Als de
mooiste diaman-
ten Zend gij licht
naar alle kanten
Johannes Calvijn
Hoog rijst zij voor
ons op De univer-
siteit Daar brand
het technisch vuur
Op elke faculteit
De techniek zal
morgen zorgen En
een toename waar-
borgen Van de en-
tropie
Nooit klinkt het
slotakkoord Steeds
volgt een nieuwe
zin Het spel gaat
altijd door Dus
zetten wij weer
in Als de mooiste
diamanten Toont
gij telkens nieuwe
kanten Johannes
Calvijn
ta2,1
ta2,2
ta2,3
xu =

xu,1
xu,2


→ xa1,1
→ xa1,2
→ xa1,3
→ xa2,1
→ xa2,2
→ xa2,3







xa1,1 xa1,2 xa1,3



xa2,1 xa2,2 xa2,3


ai
x1
x2
xu
→
(b) Instance based classification.
Figure 2.2: Profile based versus instance based authorship identification. Document u is the unlabeled
document which has to be attributed to either author a1 or author a2. For both authors three training
documents are available. In both approaches two features x1 and x2 are generated. The profile based
model has two data points (for each cumulative profile one). The author of the suspect document
then corresponds to the author whose profile has the smallest distance to suspect document xu. The
instance based approach on the contrary has data points for all individual training examples xai,j and
a classification algorithm is employed to create a model of the two authors.
the writing style of each author. In contrast, the instance based paradigm considers
the documents of the candidate authors as a set of distinct training documents. The
profile based method has a straightforward classification process: a similarity metric is
used to measure the distance between the profile of the unlabeled document and the
cumulative profiles of the candidate authors. The unlabeled document is attributed to
the author whose writing style (considered collectively in a single document) is most
similar [49]. In the instance based approaches machine learning classifiers are typically
used for classification.
Feature vector versus similarity based methods
Till now, we assumed all the instances (whether being represented per author or per
document) are represented in the same numerical vector space, i.e. in Rn. In this
11
2. Preliminaries and Related Work
representation, each dimension corresponds to the same quantitive measurement on the
document and the instances are all modeled in the same feature space, such that each
dimension reflects a single measured pattern. We refer to these approaches as feature
vector based approaches. In feature vector based approaches, a wide range of feature
types can be combined to create an expressive representation, such as the mean sentence
length, frequency of particular word-occurences and syntactical features. The instance
based methods in literature that are feature vector based usually employ a machine
learning classifier. One of the best performing machine learning classifiers in the feature
vector based approach is the Support Vector Machine (SVM). Support vector machines
separate classes by defining hyperplanes between the classes in such a way that the
hyperplanes are as far as possible from the closest members of the different classes. For
a more elaborate description we refer to Section 3.1.2.
In contrast, in the similarity based approaches the instances are typically represented
by character or lexical based features and classification takes place with a similarity func-
tion that quantifies the degree of shared information between the unlabeled document
and the training samples. Most often, the similarity based approaches stick to the pro-
file based paradigm. One key aspect of similarity based approaches is that instances
may be modeled in a non-numerical, not well defined feature space. For example, some
similarity based approaches use text compression algorithms to quantify the similarity
between the samples [54, 76]. Hence, the samples are represented in their original tex-
tual form, and no feature vectors are extracted. Common n-grams (CNG) is perhaps
the most representative similarity based approach, where the feature set of each author
is formed by the most frequently used n-grams [15]. In this case each sample is modeled
with a unique set of most frequently occuring character n-grams. This makes it difficult
to transform it to a compact numerical represention, as in the case with feature vector
based approaches. In addition, in similarity based methods it may be difficult to com-
bine different kinds of stylometric features, since a similarity measure usually can only
handle data that is homogeneous enough to determine the degree of similarity.
Information retrieval approaches
If the method is both similarity and instance based, the documents are represented
individually, and the identification is done by obtaining the documents most relevant to
the unlabeled document by means of a similarity measure. It is notable that in general,
this is exactly one of the problems that information retrieval deals with. In general,
the aim of information retrieval techniques is to obtain the documents most relevant
to a certain information need. Similarly, the instance and similarity based methods
obtain the samples and corresponding class labels (i.e. the information) most relevant
to the unlabeled document (i.e. the information need). Therefore, these methods are
sometimes refered to as information retrieval approaches [11, 50]. Stamatatos [73] seems
to pertain the described properties of similarity based classification to the profile based
paradigm, such as difficulty to combine different feature types. However, the information
12
2.1. The authorship analysis process
retrieval approaches show that this is not limited to the profile based classification2.
Classification in authorship verification
So far, we considered classification models for authorship identification. In authorship
verification training data is available of only one author (the target author). Classifica-
tion techniques in authorship verification can be distuinguished into two main categories
[65]:
• Intrinsic verification models. In this category, authorship verification is seen as a
one-class classification problem, where the only class, the target class, is determined
by instances in the training data. So training exclusively takes place by the data
in the set of documents of the known author. Class prediction is done by setting
a certain decision line as a decision bounary around the training samples in the
target class.
• Extrinsic verification models. In the former category, setting the decision boundary
is difficult, because it is hard to decide how tightly the boundary should be fit
around the target class [44]. It is also hard to find representative features that
are sufficiently unique for the author. In such cases it can be advantageous to use
not only positive data but to use additional documents from external sources to
model an outlier class. In this way, the verification problem is transformed from
a one-class to a binary class classification problem. The outlier class represents
all documents not belonging to the target. Hence the decision boundary is based
on the similarity of an unlabeled document to both the target and outlier class
instead of only considering the target class. Due to the diversity in the outlier
class (it represents all other authors not being the target author) a large number
of representative negative training samples should be available.
2.1.3 Classifier evaluation
The classifier is usually evaluated with a quantitative performance measure. It is not
difficult to see that it is not fair to use the same data set for training and then for
testing. In the holdout method the data is partitioned into a training set used to learn
the classifier and a separate test set for evaluating the classifier. A drawback of this
method is that it reduces the number of training or testing instances. In cross validation
techniques, both a large training set can be used while the dataset is tested well at the
same time and the independence between train and test set is maintained [47]. Cross
validation refers to validation techniques which repeatedly select test and train sets and
then average the performance over multiple test/train set combinations. In k-fold cross
validation the dataset is split into k equally sized subsets (the folds) and the classifier
is trained on k − 1 folds and tested with the remaining fold. This procedure is then
2An example of an information retrieval approach in source code is the technique proposed by
Burrows, which will be discussed in Section 2.2.1
13
2. Preliminaries and Related Work
repeated for all k combinations test and train set. Because each fold is used once as
test set, every instance will be used for validation. In the extreme case, k is chosen very
large such that each fold comprises only one instance. This is known as the leave-one-out
method. Although leave-one-out cross validation will test the dataset very thoroughly,
a major drawback is its high computational complexity.
The eventual performance of the classifier can be expressed by several evaluation
measures [66]. For multiclass problems such as authorship identification, the accuracy
is typically used, which is the proportion of correctly classified instances:
a = TP + TN
P +N ,
where P is the total number of positive samples in the test set, N the total number
of negative samples in the test set, and TP and TN are the number of correctly pre-
dicted positives samples (true positives) and correctly predicted negative samples (true
negatives) respectively.
In authorship verification two objectives can be distinguished: a high precision p and
a high recall r,
p = TP
TP + FP , r =
TP
P
,
where the false positive rate (FP ) is the fraction of incorrectly predicted positive sam-
ples. The precision is the proportion of correctly predicted postives. To achieve a high
precision, the FP -rate should be low. Consequently a document should be predicted to
be positive only if there is a very high confidence that this decision is correct. However,
in this case the proportion of real positives that are predicted as such (i.e. the recall)
may be low. A high recall can be achieved when a document is predicted to be positive
also if there is low confidence. The tradeoff between precision and recall is equivalent to
how tighly the decision line is set around the target class (or between the target and out-
lier class). The F1-score is a measure that combines precision and recall by expressing
the performance as the harmonic mean between them:
F1 =
2 · p · r
p+ r
In authorship verification a Receiver Operating Characteristic (ROC) curve can give
insight in the relation between the false positive rate (FPR) and the true positive rate
(TPR) when varying the ratio between precision and recall. The TPR is identical to
the recall: it is the fraction of positives that are predicted as such. The FPR is known
as the specificity and is the fraction of negatives that is predicted as such. A common
quantitative final performance measure is the area under the ROC curve (AUC). It can
be shown that the AUC of a classifier is equivalent to the probability that a randomly
chosen positive instance is ranked higher than a randomly chosen negative instance [24].
14
2.2. Source code authorship identification
2.2 Source code authorship identification
Thusfar we considered the process of authorship identification without focussing on
source code. Fortunately, programming language concepts are based on many linguistic
concepts from natural languages and as a result the authorship identification process is
not very different. Features can be generated from a character based language represen-
tation. One can also consider more abstract language representation levels. The first
phase in most compilers is lexical analysis. In this step the source code is converted from
a sequence of characters into a sequence of tokens. These tokens form a lexical source
code representation. In subsequent compilation steps, the tokens are converted into a
syntax tree by following a grammar, which defines how tokens can form a syntactically
correct program. As is the case in ordinary linguistics, computer languages have a gram-
mar with a corresponding syntax and semantics. Hence, just as with natural languages,
we can generate character, lexical, syntactic and domain specific features.
In literature, several closed set source code identification studies are found. However,
we have not found authors that addressed the verification or open set identification
problem. Among the source code authorship identification studies in literature, Burrows
compared the different techniques most thoroughly [12]. He presented a comparative
study that includes a technique developed by himself [10] and a reimplementation of
most of the existing techniques. Burrows observed that the technique developed by
himself and a technique by Frantzeskou [29] outperformed the other approaches. The
techniques of Franzeskou and Burrows deviate on some essential aspects from most other
contributions. First, they are similarity based. Secondly, the features are extracted from
a low level language representation (character and lexical n-gram features respectively).
The similarity based approaches will be described in Section 2.2.1. Most other techniques
found in literature are feature vector based and employ machine learning classification
algorithms. These will be described and compared to the Franzeskou and Burrows
method in Sections 2.2.2 and 2.2.3. Many of the feature vector based approaches use
domain specific features, however, also character based document representations are
used.
2.2.1 Similarity based approaches
The Frantzeskou approach
Frantzeskou [29, 28] developed a Common n-gram (CNG) based identification technique.
This technique is presented as the Scap approach. This approach is profile and similarity
based and uses character n-grams features3. The first step in the approach is the con-
struction of an author profile for each candidate author. Figure 2.3 systematically shows
how a profile is constructed. First all the sample files of an author are concatenated into
one large file. The next step is the extraction of the L most frequent n-grams from the
3In the approach byte level n-grams are used as features, which are in essence identical to character
n-grams, but also include all non-printing characters such as cariage returns.
15
2. Preliminaries and Related Work
Source files
author c Concatenate
Count frequencies of character
level n-grams
Keep the L most frequent n-grams
n
L
Scapc = {(x1, f1), (x2, f2), . . . , (xL, fL)}
Figure 2.3: Construction of a Scap profile for each candiate author c.
concatenated file. A profile is then defined as the ordered set of pairs consisting of the
L most frequent character n-grams xi with the corresponding normalized frequencies fi.
The classification method is similarity based. Code with unknown authorship is
classified by comparing the similarity of the candidate author profiles with the profile of
the unlabeled code (the latter profile is derived in the same way). Frantzeskou evaluated
two different choices for the similarity measure. The first measure is the relative distance
measure, which was taken from [42]:
RD =
∑
g∈programprofile
(
f1(g)− f2(g)
1
2 · (f1(g) + f2(g))
)2
,
where f1(g) is the normalized frequency of an n-gram g in the author profile, and f2(g)
the normalized frequencies of g in the program profile. A drawback of this measure is
that if a limited number of training samples are available per author, the profile length
may be smaller than L, and hence become positively biased towards the authors with a
small profile. That is why a second similarity measure is introduced that uses simplified
profiles, without relative frequencies: SP = {x1, x2, . . . , xL}. The similarity measure, is
then defined as the size of the intersection between the author and program profile:
SPI = |SPA ∩ SPP | ,
where SPA and SPB are the simplified profiles of the author and the program respec-
tively. Franzeskou examined that the SPI similairty performed consistently better or
equal compared to the RD measure for different choices of n and L.
The Burrows approach
Burrows [10] proposed an instance based authorship identification method with features
on the lexical language level. The method is similarity a based and uses n-grams of the
types of lexical token as features. Figure 2.4 depicts an overview of the approach. First,
the programs of an author are subject to a lexical analysis in which the documents are
tokenized. Based on the type of the tokens, a subset of the tokens is then selected to
model the training documents. The token types, such as operators, keywords, identifiers
16
2.2. Source code authorship identification
Tokenize
Represent as sequence of codes of
selected token types
Extract all token n-grams
if(answer==42)break;
if ( answer == 42 ) break ;
Documents
di of author
a1
if bc id eo li bc br sc
1. if bc id eo
2. bc id eo li
3. id eo li bc
4. eo li bc br
5. li bc br sc
Index token n-grams
with search engine
Suspect text q
Index
Term Inverted lists
ifbcideo → 1: (a1, 1)
bcideoli → 1: (a1, 1)
ideolibc → 1: (a1, 1)
...
ai
Query
Figure 2.4: Diagram of the information retrieval technique adopted by Burrows. In this example the
documents of the author a1 are indexed and queried by a previously unseen document q. The right
part of the figure illustrates each step at the left side with a piece of source code.
and whitespace can be denoted by two alphanumeric characters, e.g. ‘op’ for an operator
and ‘wh’ for whitespace. Every document is subsequently represented by a contiguous
sequence of token types corresponding to the selected tokens. The final model is then
formed by extracting all the n-grams of token types from the document.
The similarity measure for classification is adopted from the field of information re-
trieval. Analogous to information retrieval, the modeled training documents are indexed
in a search engine. The resulting index then allows to find the labeled training docu-
ments that are most relevant to the given query, that is formed by the token type n-gram
representation of the unlabeled document. The most popular index structure, which is
also used in this approach, is the inverted file, which is a dictionary of terms [58] (in
this case the terms are token n-grams). The index records for each distinct term the
term frequency tf , which represents its total number of occurences in the data. Addi-
tionally, an inverted list is formed as a set of tuples (d, idfd) where d is the document
identifier and idfd is the inverse document frequency. The inverse document frequency is
the frequency of the term within the document d. Once the index has been constructed,
classification can be done by finding the most relevant document with respect to the
unlabeled document that is used as query. The common notion of similarity measures
in information retrieval is that the more often the query terms are in the document, the
more relevant the document is. Secondly, the relevance of a document increases with the
rarity of the term accross the whole corpus. These conditions can be met by assigning
17
2. Preliminaries and Related Work
to each term a weight which is proportional to its importance both within the document
and in the entire document collection. As such, the measure can roughly be described
by
wi,d = tfi,d × idfi
where wi,d is the weight of document d of term i, tfi,d is the term frequency and idfi
is the inverse document frequency. Often, weighting also compensates for differences in
document length, since larger documents tend to have a higher term frequency as they
tend to repeat the same words multiple times. In the work of Burrows, five different
similarity measures are compared. Burrows found that the Okapi similarity measure was
most effective [11]. This measure involves the term frequency, inverse document length,
and inverse document frequency and is defined as
Okapi(q, d) =
∑
t∈Q
wt ·
(k1 + 1)tft,d
K + tft,d
· (k3 + 1)tft,q
k3 + tft,q
wt = ln
(
N − idft + 0.5
idft + 0.5
)
, K = k1
(
(1− b) + b · ld
lavg
)
,
where Q is the query document, Dd is the document, t is the term and N is the number of
documents in the collection, ld is the document length and lavg is the average document
length. Burrows further investigated the length of varying query document length. More
importantly, the role of different token types (sixty-three in total) were examined. The
best classification accuracy is achieved by selecting operators, keywords and whitespace
characters features together.
2.2.2 Feature vector based appoaches
The feature vector based approaches can be divided in terms of the syntactic complexity
of the document representation where the features are extracted from. Most approaches
use domain specific features. However, also some character based approaches can be
found in literature. In general, the feature vector based approaches follow the classifica-
tion approach as outlined in Figure 2.1 of Section 2.1, where the data sampling is instance
based and the classifier is an off-the-shelf machine learning algorithm. Therefore, these
approaches are sometimes referred to as machine learning approaches.
Text based features
Some feature vector based author identification techniques in source code use features
from a low level language representation. A representative example is the character
n-gram based approach, where each dimension of the feature vectors correspond to
the frequency of a particular n-gram [15]. Also Kothari [52, 70] used character based
features. Kothari compared the performance of two types of features, namely character
based stylometric features and character n-grams. The stylometric features refer to
features such as the distribution of line sizes, the distribution of leading spaces and the
distribution of words per line. It was shown that classification with character 4-grams
18
2.2. Source code authorship identification
produced better results than classification with the stylometric features. More recently,
character distribution frequencies and character based stylometric features were used for
the identification of HTML code associated with malware [82].
Domain specific features
Most of the instance based approaches found in literature use language dependent do-
main specific features. One of the earliest source code authorship identification contribu-
tions considered the layout of the code [64]. Later, Krsul and Spafford identified a large
number of other domain specific features which can be useful in source code authorship
identification [53]. They identified layout, style and structural features respectively.
• Layout features are related to typographic aspects such as the indentation style.
• Style features concern style related aspects such as naming conventions and the
degree of usage of global variables. These are less susceptible to be changed auto-
matically by pretty printers and code formatters than layout metrics.
• Structural features measure the structural decomposition of the code and are more
dependent on experience of the programmer, such as mean lines of code per func-
tion and software quality measures.
Several works in source code authorship identification use domain specific features and
a machine learning classification algorithm, e.g. [22, 55, 52, 19, 53, 57]. In [19] 56
of the features identified by Krsul and Spafford were evaluated to classify Java source
code by means of discriminant analysis. In some works domain specific features are
combined with features generated from a character based document representation, such
as the mean number of characters per line [57] and the number of bits in the zipped
(compressed) source code [22]. Also [55] used a wide range of text based and domain
specific features, such as the number of words on a line and the number of membership
operators in a singe logical identifier in Java code. In some cases, code metric tooling
is employed for feature generation. Code metrics incorporate all kind of quantitive
measures of code, such as code maintainability, coupling and cohesion. An example of a
feature generated by a code tool is the cyclomatic complexity [57]. Others employed a
lint static analyzer to gather information on the number of warnings in the code and used
a dynamic analysis tool to determine the testability of code [35]. More complex features
have also been examined in binary code authorship identification, such as subgraphs in
the control flow graph (CFG) [67]. In [45], more subjective features are suggested that
are identified by an expert’s judgements, such as the degree to which comments match
the actual source code’s behavior.
2.2.3 Comparison of the existing approaches
Above we regarded a number of approaches to source code authorship identification.
Since most works use different training set corpora and are based on different program-
19
2. Preliminaries and Related Work
ming languages, a reasonable comparison between the techniques is difficult. Neverthe-
less, we now attempt to compare the approaches in terms of obtained performance.
Classification technique
One of the first comparisons was done by Franzeskou [29], who showed the effectiveness
of his common n-gram method by validating on the same dataset as the domain spe-
cific approach in [57]. As already mentioned earlier, the most prominent comparison
was done by Burrows [12]. Burrows implemented most of the existing techniques and
compared them with four different collections with code from student assignments and
open source code. Burrows showed that the similiarity based methods of himself and
Franzeskou perform better than the feature vector based methods that employ a machine
learning classifier4. In an independent study in [77], the accuracy of the similarity based
approaches of Burrows and Franzeskou were further compared with each other. However,
the authors asked themselves whether comparing the accuries of the two methods is fair.
The Burrows method inherently anonymizes the data as it uses lexical types as tokens
and thus neglects values of literals and comments. Therefore they anonymized all data
by removing comments and string literals and found that in this case both approaches
performed approximately equally well.
Document representation
Burrows seems to attribute the difference in accuracy between the techniques in his study
mainly to the similarity based vs. machine learning classification paradigm. Since the
similarity based approaches both use n-grams from a low level language representation
and the feature vector based approaches mostly use domain specific features from a
high level language representation, it is however questionable whether the difference in
performance only traces back to the classification paradigm: it may also relate to the
used features. Research results indeed indicate that the latter may be the case. First, in
the comparative study of Burrows, the performance of the similarity based approaches
were followed by a machine learning approach by Kothari, that used character n-gram
features [52]. Also latter results in literature suggest that the similarity based approaches
are not generally better. In [15] is shown that compared to the Scap approach, a feature
vector based approach with character n-grams performs approximately equally well. One
main conclusion of that study was that the machine learning approach was more accurate
in balanced training sets. Such results suggest that it are mainly the approaches that
use n-gram features that achieve a high accuracy. Of course, this does not imply that
the classification paradigm has no importance.
In reviewing the literature, few studies were found that sought to determine which
high level program characteristics are of most significance for authorship analysis. Juola
noted that text formatting and layout features lend itself in particular to source code,
4Burrows refers in his study to the similarity based methods of himself and Franzeskou as informa-
tion retrieval approaches. In contrast, according to our definition, only the approach of Burrows is an
information retrieval approach (see Section 2.1.2).
20
2.3. Conclusion
because of the importance many authors place on the style in the source code [38].
Indeed, literature shows that layout is one of the most relevant indications of authorship
in source code. This is the case in either domain specific and character based methods.
In the domain specific approach of [19] was observed that for Java, the layout features
played a more important role than the style and structure metrics. Similar conclusions
followed from analysis of the character n-gram based Scap method on Java source
code [27, 26]. Since character n-grams are perfectly able to capture the formatting and
layout, this is perhaps one of the reasons that character n-grams perform well in source
code authorship identification. The JavaScript language is highly influenced by Java.
Therefore the layout is expected to play an important role in the classification.
2.3 Conclusion
In this chapter we first explored the authorship analysis problem in general. We then fo-
cussed on existing source code authorship analysis techniques in literature. All works we
encountered focus on the authorship identification problem with a closed set of program-
mers and no studies are found that investigate the authorship verification problem. The
employed classification techniques can broadly be divided in similarity based techniques
that employ a similarity metric for classification and and feature vector based methods
that employ machine learning algorithms. Different language representations are used
for feature extraction. In general, the works that use features from a low language ab-
straction level in the form of n-grams, have shown to achieve the highest accuracies. The
layout of source code is among the most significant indications of source code authorship.
21

3
Approach
In this chapter the implemented source code authorship identification and verification
approaches are presented. First, we detail the developed authorship identification and
verification techniques that use character features in Section 3.1. These techniques are
adopted from literature. Then we propose a new domain specific technique in Section
3.2. The domain specific approach is our own work and based on features specific to
JavaScript. The most significant aspect is that the approach employs structures from the
Abstract Syntax Tree (AST). In all the discussed approaches, the authorship verification
problem is modeled as an extrinsic verification model and thus trained with positive and
negative samples.
3.1 Character based approaches
We implemented three techniques employing features from a text based language rep-
resentation. These are approaches from literature and not our own work. However, we
embed the approaches in authorship verification, which is different. The implemented
text based authorship identification studies include the Scap appoach (Section 3.1.1),
machine learning classification with character n-grams (Section 3.1.2) and classification
by compression distances (Section 3.1.3).
3.1.1 The Scap approach
The Scap method is a Common n-gram (CNG) technique that achieved promising
results in several studies, and can be considered as state of the art. As such, the results
of this method will enable us to use it as base line to compare the performance with
the other implemented techniques. The identification process is described in Algorithm
3.1. In this algorithm the function getProfile(d) returns the L most frequent n-grams
x1, x2, . . . , xL from document d. An unlabeled document is classified to the author whose
profile is most similar to the unlabeled document profile. Since the Scap method has
been described elaborately in Chapter 2 we refer to Section 2.2.1 for further details.
23
3. Approach
In our study, we set L to 1500 as was originally suggested by Franzeskou [29]. Other
studies, however, reported a different optimal choice for n and L, such as n = 14 in
combination with an unlimited profile size [12]. We leave it as future work to tune the
values of n and L. Since computing the intersection between profiles is computationally
heavy, we sticked to the (rather small) proposed value of L. This allowed us to validate
the method more extensively and to test with a larger number of candidate authors.
Scap and authorship verification
We adapted the algorithm in order to be able to employ it in the context of authorship
verification. To accomplish this, we adopted two different approaches. What distin-
guishes them, is how the outlier class is modeled. First, the outlier class may be repre-
sented by one cumulative profile P1, which consists of the most frequent n-grams in all
concatenated negative documents. Intuitively, an unlabeled document which belongs to
the target is likely to have a higher proportion of n-grams that match the target profile
than a document of an arbitrary other author. This approach is shown in Figure 3.1(a).
In this figure, the decision line is defined in terms of the similarity between the unla-
beled profile with respect to the outlier profile P1 and the target profile Ptarget. More
precisely, an instance is classified as target iff d(Pu, Ptarget)−d(Pu, P1) < k, where k is the
threshold value and d is the distance function. Note that a document is not neccessarily
classified as target if its distance to the target class is smaller than the distance to the
outlier class. The second approach to model the outlier class is by using multiple outlier
profiles Pi, each corresponding to a different author i. This approach is shown in Figure
3.1(b). In this case an instance is classified as target iff d(Pu, Ptarget)− d(Pu, Pi) < k ∀i.
Input:
C: set of candidate authors
Dc: training samples for each candidate author c ∈ C
u: unlabeled sample
L: profile size
n: gram length
1: function Scap
2: Pu ← getProfile(u, L, n)
3: for c ∈ C do
4: Pc ← getProfile(concat(Dc),L, n)
5: Sc ← |Pu ∩ Pc|
6: end for
7: return argmaxc(Sc)
8: end function
Algorithm 3.1: The Scap algorithm. This is a common n-gram (CNG) approach. Classification
takes place by determining the most common character n-grams between the unlabeled code and the
training data of each author.
24
3.1. Character based approaches
Pu
P1Ptarget
(a)
P1
Ptarget
Pu
P2
P3
P4
P5
P6P7
(b)
Pu
P1
P2
P
u ∩
P
1
Pu
∩ P
1
(c)
Figure 3.1: Authorship verification with Scap. The profile Pu represents the unlabeled profile. In (a)
the outlier class is modeled with one profile, while in (b) the outlier class is modeled with separate
profiles of different authors. Figure (c) shows the rationale behind the SPI measure: the intersection
between an unlabeled profile and two author profiles determines the code similarity.
The theshold value k defines the required confidence that an instance is positive
to be classified as such and determines in this way the bias to the target class. By
regulating the required confidence that an instance is positive to be classified as such,
we aim to be able vary ratio between the precision and recall of the classifier. In the
Scap method, this confidence is completely determined by the value returned by the
similarity function. Also the Receiver Operating Characteristics curve (ROC-curve) is
obtained by varying the threshold, and tracking the FPR and TPR at several choices
for the value of k. Clearly, in such a cases the distance metric is of great importance.
The SPI similarity is not a proper distance metric and has some other weaknesses.
That is why we adopted the Jaccard index instead of the SPI measure for authorship
verification. We now explain the rationale behind this decision by taking a closer look
at the similarity metric.
A closer look at the similarity metric
In the Scap approach the similarity between profiles is defined by the SPI measure,
which is the cardinality of the intersection between profiles, i.e. the similarity between
the unlabeled profile u and the author profile of an author i is SPIi = |Pu ∩ Pi|. This
metric can be considered as a precision measure. Remember the precision is the frac-
tion of retrieved instances that are relevant. Now the similarity measure SPI ′i = SP Ii|Pu|
expresses the fraction of relevant n-grams in the author profiles (and thus is a preci-
sion measure) and is monotonic with SPI. Consequently it will result in an identical
authorship identification process.
In the SPI measure it is important that the author profiles Pi are assumed to have
the same length L, so that the measure is not biassed towards authors with a larger
profile. There may however, not be enough data to model the outlier classes with L
n-grams, such that this assumption cannot be made. This is depicted in Figure 3.1(c),
where the two author profiles P1 and P2 have different sizes. In this case, not only the
precision is relevant but also the recall. The precision pi is the fraction of matching
25
3. Approach
n-grams in the unlabeled profile, and the recall ri is the fraction of matched instances
in author profile i:
pi =
|Pu ∩ Pi|
|Pu|
, ri =
|Pu ∩ Pi|
|Pi|
.
In Figure 3.1(c) the precision p1 and p2 is approximately equal, while the r2 > r1.
Both the precision and recall are of importance, so we define the new similarity measure
as the harmonic mean of precision and recall, the F1-score. This measure addresses the
problem with author profiles with unbalanced sizes. Now, the obtained F1 score is equal
to the Sørensen–Dice coefficient. This metric, Dice(Pu, Pi) = 2|Pu∩Pi||Pu|+|Pi| , is a commonly
used similarity measures in information retrieval [14] and taxonomy and bio-associational
studies [16].
F1 = 2 ·
pi · ri
pi + ri
= 21
pi
+ 1ri
= 2|Pu|
|Pu∩Pi| +
|Pi|
|Pu∩Pi|
= 2|Pu ∩ Pi||Pu|+ |Pi|
= Dice(Pu, Pi)
Note that the Dice coefficient is monotonic with the SPI measure, when |Pi| = L for
all i. Consequently, using the Dice coefficient will not lead to a different classification, if
all author profiles are of length L. It can however be shown that the according distance
measure 1−Dice(Pu, Pi), is not proper distance function as it does not satisfy triangle
inequality [14]. The Jaccard index J(Pu, Pi) = |Pu∩Pi||Pu∪Pi| is not very different from the
Dice coefficient1, but is a proper distance metric. Furthermore, a nice property is that
0 < J < 1. The confidence C that an unlabeled document is a target object can then
be expressed by:
C = 12 +
J(Pu, Ptarget)− J(Pu, Pi)
2 ,
where Pi is the most similar outlier author profile. The unlabeled document is then
predicted to belong to the target class if C > k. Different choices for theshold k will
result in a different ratio between the precision and recall.
3.1.2 The n-gram based approach
The n-gram approach is feature vector based. Together with the similarity based Scap
method, this method has shown to be quite effective in literature [15]. Each document
is described by a vector xi ∈ Rm, where each dimension corresponds to the relative
frequency of the occurences of an n-gram in the document. A problem in n-gram based
techniques is the curse of dimensionality. The ASCII table includes 95 different printable
characters, which will results in 958 ≈ 6.6 · 1015 character 8-grams. Obviously, this
dimensionality cannot be handled well. What can be done is to decrease n or to select
a subset of n-grams. Most techniques in literature limit the number of dimensions by
selecting only the most frequent character n-grams that are present in the sample data
collection. We also adopted this strategy. Not all the selected n-grams are neccessarily
1Note that it can be written as J(Pu, Pi) = |Pu∩Pi||Pu∪Pi| =
|Pu∩Pi|
|Pu|+|Pi|−|Pu∩Pi|
26
3.1. Character based approaches
observed in a particular code sample. Consequently, the feature vector may be sparse.
Therefore we chose a larger number of n-grams than was done in the Scap method and
utilized the 7000 most frequent n-grams. For classification we follow the instance based
paradigm and classify by means of a Support Vector Machine (SVM). Since classification
by an SVM is much faster than classification by the SPI measure in the Scap method,
this dimensionality was computationally feasible. The SVM classifier was chosen, since
SVMs are among the best text classification algorithms to date [1]. SVMs have many
properties that make that they perform well on text classification. For instance, they
can handle a high dimensional input space and they are robust when document vectors
are sparse [37]. The SVM is a binary classification algorithm. Authorship identification
with more than 2 authors requires a multiclass classifier, which can be constructed by
combining several binary classifiers [36]. In authorship verification the two classes are
modeled with the positive and negative data. By estimating the posterior probabilities
of a class prediction, the threshold can be varied.
The Support Vector Machine
We now briefly give a theoretical background behind the SVM classifier [25]. Consider
the binary classification problem in a two dimensional space in Figure 3.2. Each training
sample xi is labeled by class label yi, such that yi = +1 for positive samples and yi = −1
for negative samples. An SVM separates the positive samples from the negative examples
by defining a hyperplane between the instances. This decision boundary can be described
by w · x + b = 0, where w is (by definition) normal to this hyperplane. In Figure 3.2(a),
the line is intuitively a good choice for the decision boundary, since it is far away from the
data of both classes. An unlabeled document u is then predicted positive it falls in the
region above the decision line, described by w · u + b > 0, and negative otherwise. The
question remains how the direction and location of the decision line can be determined.
We first assume that the data points of the classes are linear separable such that
xi ·w + b ≥ +1 for yi = +1
xi ·w + b ≤ −1 for yi = −1.
Thanks to the definition of the two label values, these two equations can then con-
veniently be combined into
yi(xi ·w + b) ≥ 1 ∀i.
This constraint is however not enough to pick a particular value for w and b, since
there are many possible orientations of the decision boundary. Hence, another constraint
needs to be defined, that boils down to selecting w and b such so that the decision
line is as far as possible away from the closest members (the support vectors) of both
classes. This notion can be formalized by defining two additional hyperplanes H1 and H2
perpendicular to the decision boundary on which the closest members are. By rescaling
w and b appropriately, these planes can be described by:
xi ·w + b = +1 for H1,
xi ·w + b = −1 for H2,
27
3. Approach
x1
x2 +
+
+
+
– –
–
(a)
w
· x
+
b
=
0
x1
x2
w
· x
+
b
=
+
1
w
· x
+
b
=
−
1
w
2
||w||
x+
x−
(b)
Figure 3.2: Classification by an SVM.
which can again be combined into a single formula:
yi(xi ·w + b) = 1 ∀i.
With this equation we are now able to calculate the distance between the two hy-
perplanes. Consider two points x+ and x− on the margins. The length of the difference
between these vectors is x+ − x−. Since we know that w is perpendicular to the hyper-
planes, the width of the margin can now be expressed by the dot product between the
unit normal vector and the difference vector, (x+ − x−) · w||w|| . By using the defined for-
mula for the margin hyperplanes, we can rewrite it to x+ ·w = 1−b and −x− ·w = 1+b,
so that the width of the hyperplane can be expressed as 2||w|| . This is exactly the value
we want to optimize to find the maximum-margin hyperplane:
max 2||w|| s.t. yi(xi ·w + b) ≥ 1 ∀i.
It can be shown that this optimization problem can be solved by expressing it as the
equivalent quadratic programming problem
min12 ||w||
2 s.t. yi(xi ·w + b) ≥ 1 ∀i.
We will not go through the mathematical details of this. By using the so called kernel
trick, the SVM can also be efficiently used in non-linear classification problems. In this
case a kernel function is used to map the data to a higher dimensional space. The data in
the higher dimensional space may be linearly separable, even if this is not the case in the
original lower dimensional space. However, it can still not be guaranteed that all data
is linearly separable. To make the algorithm work for non-linearly separable datasets as
well, we reformulate the minimization problem by introducing a cost function, to pay a
cost for points that fall in the margin (and thus have a distance to the decision boundary
smaller than 1):
min12 ||w||
2 + C
∑
i
ξi s.t. yi(xi ·w + b) ≥ 1− ξi ∀i, ξi ≥ 0∀i.
28
3.1. Character based approaches
The soft margin parameter C controls the balance between two goals: maximizing
the width of the hyperplane and ensuring that most samples have functional margin at
least 1. By tuning this parameter a tradeoff can be made between the bias and variance
of the SVM. The strengths of an SVM are that they scale well to high dimension data
and they have nice theoretical properties: unlike neural networks, an SVM does not
suffer from local optima. Furthermore the SVM can be controlled by tuning margin
parameter C. In our approach we will use a linear kernel function.
3.1.3 Compression based approach
Compression based approaches employ text compression distances between documents.
The basic idea is that documents that have similar characteristics should be compressed
with each other more easily than documents with dissimilar characteristics. This idea is
not new and has been used in various domains, such as text categorization and language
identification [76]. In our work we employed the compression based technique that was
reported in [54]. The compression distance metric that will be used is:
CDM(d1, d2) =
C(d1d2)
C(d1) + C(d2)
where C(d) is the size of the compressed document d and d1d2 is the concatenation of d1
and d2. If the two documents are similar then CDM approaches 0.5, while it approaches
1 if they are completely unrelated.
For classification, the nearest neighbour classification method could be applied, where
the unlabeled document is classified to the author whose source code the best compresses
jointly. However, we follow the approach of [54] that reduces the risk of overfitting. In
this approach, for an unlabeled document a vector is created with the distances of this
document to all the training texts. Then an of-the-shelf classification algorithm is capable
to attribute the unseen document to one of the authors. However, in case the training
set is large, the computational burden can become too heavy, since the feature vectors
will become as long as the number of documents in the training set. To overcome this
problem, a subset of documents of each author in the training set can be selected. The
selected samples are indicated with prototypes of the training set. Figure 3.3 depicts the
construction of such a vector. In this example two prototypes are selected per author,
so that a 6-dimension feature vector x ∈ R6 is created for the unlabeled document.
Subsequently, the distance of the training documents to the prototypes are determined
in a similar manner. An SVM classifier is trained by the feature vectors corresponding to
the training documents. Algorithm 3.2 describes the explained approach in more detail.
The text compression algorithm to compute the CMD metric is performed by a
variant of the Prediction by Partial Matching (PPM) algorithm, namely PPMd [18].
PPM is a lossless compression algorithm, which assumes that every next symbol is
dependent on previous symbols. The key issue is to let the alphabet be the alphabet of
sequences and not alphabet of single symbols [40]. The algorithm maintains frequencies
for past character sequences, and compresses the text by predicting the next symbol
29
3. Approach
x1
x2
x3
x4
x5
x6
Figure 3.3: Feature vector construction in the compression based approach in a 3 class classification
problem and two prototypes per class.
Input:
C: set of candidate authors
Dc: training samples for each candidate author c ∈ C
du: unlabeled sample
1: Select prototypes P = (p1, p2, . . . , pr) from the training samples in Dc
2: for each c ∈ C do
3: for each dt ∈ Dc do
4: x(c)t ← (x1, x2, . . . , xr) where xi = CDM(pi, dt)
5: end for
6: end for
7: x(u) ← (x1, x2, . . . , xr) where xi = CDM(pi, du)
8: Train an SVM with x(c)t with all t and c
9: Predict class membership c ∈ C of x(u) with the trained classifier
Algorithm 3.2: Compression based authorship identification. This method has either aspects of
similarity and feature vector based techniques, and can thus be regarded as a hybrid approach.
using these frequencies. Thus each symbol is encoded according to the context provided
by the preceding symbols [76].
3.2 Domain specific approach
The approaches discussed so far, use character based features. In this section we present
a feature vector based approach that that employs features specific to JavaScript, and
classifies with an SVM. The domain specific features used in literature can be divided
into layout, style and structural features respectively2. We also extracted these features,
however, we propose to generate a much larger feature space, by parsing the code into an
AST prior to feature extraction. Most important of this representation is that enables
an easy way to capture structural patterns in the tree. By our knownledge no prior
studies in authorship analysis have considered such structures in the parse tree. The
2See Section 2.2.2.
30
3.2. Domain specific approach
Exp → Num
Exp → Exp “+” Exp Exp
Exp Exp
2 + 3
Plus
2 3
Grammar:
Source: 2 + 3 + 5
CST: AST:Exp
Num Num
+ 5
Num
Plus
5
Figure 3.4: The difference between the Abstract Syntax Tree and Concrete Syntax Tree. The grammar
describes the language for the addition of numbers. Note that the grammar is ambigous as it is possible
to derive two different parse trees from the source code. Hence, in practice additional rewrite rules
are neccesary, which will result in an even bigger CST (while retaining the same AST).
extraction the domain specific JavaScript features was caried out by a tool we developed3
in JavaScript by using Node.js. The parse tree of JavaScript programs is obtained by the
Esprima ECMAScript parser4, which generates a Mozilla compatible AST. Before we
further detail the feature extraction and selection process, we describe the AST program
representation.
3.2.1 AST program representation
Parse trees are usually employed in compilers and language tools for code analysis and
allow to select detailed structural features from the program. Crucial is the difference
between the Concrete Syntax Tree (CST) and Abstract Syntax Tree (AST) [78]. Figure
3.4 illustrates the difference between AST and CST in more detail. The CST represents
the concete syntax of the source language defined by the concrete grammar for parsing
the source code. This tree includes many of the punctuation tokens from the syntax and
extra nonterminal symbols for technical purposes such as elimination of ambiguity and
left recursion. The AST is the result of simplifying the CST down to the real content
and meaning of the code. Therefore, it may reveal variation among authors and is the
prefered tree to be used in authorship analysis. For JavaScript program analysis, we
adhere to the Abstract JavaScript Syntax specification as described in Appendix A. An
example of a parse tree obtained by this grammar is depicted in Figure 3.5. In the
remainder of this section we describe the rationale behind the different types of domain
specific features which are generated from the parse tree. The different features are
described exhaustively in Appendix B.
3.2.2 Domain specific features
As stated above, we extracted layout, style and structural features from the parse tree.
We will now discuss how we extracted these feature types from the AST.
3https://github.com/wilcowisse/v.js
4http://esprima.org
31
3. Approach
Program
ExpressionStatement
CallExpression
MemberExpression
Identifier Literal
Identifier Identifier
“foo” “bar” “a” 1
bo
dy
ex
pr
es
sio
n
callee
arguments
objec
t property
na
m
e
na
m
e
1 2
na
m
e
va
lu
e
Figure 3.5: AST corresponding to the JavaScript program foo.bar(a,1). Note that this is a
syntactically valid JavaScript program, although it will produce an error during run-time since foo
and bar are not defined.
Layout features
Programming layout features deal specifically with the layout of the program, such as
indentation and spacing. A problem in the AST program represenation is that this
kind of information is disregarded, since it is inessential for program analysis during
compilation or interpretation. That is why we performed a number of refactorings on
the AST to re-add this kind of layout information from the source code to the tree.
This is done by introducing additional nodes of the type Layout, BlockComment and
LineComment as children to the nodes in the AST, as is shown in Figure 3.6. For instance,
in a member expression the object and the property are separated by a dot. The author
is free to insert spacing before and behind this punctuation mark. In the example in
Figure 3.6, two additional child nodes will be appended to the MemberExpression-node.
We describe the layout slots for this node type as object 1 . 2 property, where the numbers
represent the locations in the source code the layout nodes correspond to. Similarly,
layout nodes are added to many other programming constructs. These are elaborated in
Appendix A. We grouped different layout slots with roughly the same meaning, such as
the layout slots before closing parenthesis and the layout slots before different operators
in binary expressions. For each defined layout feature we defined 12 histogram intervals.
These intervals record whether zero, one, two or many spaces, tabs and carriage returns
are used in each layout slot. Layout with comments are described by the introduced
comment nodes. Using these nodes we identify the ratio between line comments and
block comments. Furthermore, we track the length of the comments and the node types
of the parent nodes, which is an indication of the prefered location to put comments in
the source code.
32
3.2. Domain specific approach
“foo” “bar”
“a” 1
args
ob
jec
t
property
cal
lee
C
M I L
I I
foo . bar ( a , 1 )
Figure 3.6: Adding layout nodes to the AST of Figure 3.5.
Style features
Programming style features deal with characteristics of the program that are not easily
being changed by pretty printers, such as variable length. The following list describes
the style features we extracted from the AST.
• String patterns. The nodes in the AST convey much textual information, such
as the name of identifiers. The frequency of matches of regular expression form
the histogram groups for each feature. For instance, various naming conventions
may be adopted in the code, such as CamelCase and Pascal Case. These are
captured by the first character of identifiers related to function identifiers and
objects. Furthermore type information can be checked. Although JavaScript is
a loosely typed language we can match quotes in literal values (i.e. strings) and
numeric values.
• String length. A similar approach is used to match the length of strings in the
AST nodes. This enables to identify features such as the length of identifiers and
the length of literal values.
• Number of children Some node types have a list of child nodes. The length of
different kind of these lists are recorded. Think of the number of parameters in
function declarations and the number of elements in the in initialization of an
array.
Structure features
The program structure is captured by program structure features. The structural de-
composition of the tree, is defined with two types of structural features, which are listed
below. Figure 3.7 depicts these feature types.
• Descendant node count. We identify the complexity of programming constructs by
counting the number of descendent nodes of the child nodes belonging to partic-
ular node types. The size of child nodes determines the complexity programming
33
3. Approach
“foo” “bar”
“a”
args
ob
jec
t
property
cal
lee
C
M I
I I
(a)
“foo” “bar”
“a” 1
args
ob
jec
t
property
cal
lee
C
M I L
I I
(b)
Figure 3.7: Example of structural features extracted from the AST of the code fragment we considered
before. Figure (a) shows an example of the number of descendants of a function call argument. Figure
(b) shows a node 2-gram.
instructions in many cases, such as the complexity of the parameters and argu-
ments. For instance, the following JavaScript fragments clearly shows an increased
complexity of passed function arguments in comparison to foo.bar(a,1), see also
Figure 3.7(a).
foo.bar(a, function(a,b){
...
});
Similarly, many other node types hold such information, such as the size of return
arguments and the size of binary expressions.
• Node n-grams. With node n-grams we aim to capture various structural charac-
teristics. They capture the used rewrite rules in the JavaScript abstract grammar,
as described in Appendix A. First, we track the occurence frequency of uni-node-
grams. This is in essence the frequency distribution of individual node types, i.e.
the expression and statement types. The variation between node types may for
instance indicate the preference for While, For, ForIn and DoWhile loops. Fur-
thermore, the frequency of constructs such as TryStatements, NewExpressions
and MemberExpressions may be a strong indicator of defensive programming or
an object-oriented or functional programming style. Next to unigrams we con-
structed node 2-grams and node 3-gram features. A node 2-gram is represented
by two node types connected by an attribute. This boils down to the determina-
tion of the frequency of every unique pair (A →attribute B), where node A is the
parent of node B in the parse tree. Similarly, a node 3-gram is represented by
three nodes connected by two attributes. Refering to the parse tree in Figure 3.5,
examples of 2-node-grams are (MemberExpression →object Identifier), (Member-
Expression →property Identifier) and (CallExpression →callee MemberExpression).
Node 2-grams should be able to determine structural characteristics in the parse
tree. For instance, the constructs that are returned from a function, parameter
34
3.2. Domain specific approach
types, the kind of update expressions in a for loop and many other characteristics.
Considering the code fragment foo.bar("a",1) of Figure 3.5, another author may
prefer to call the function like (function(){ ... })("a",1); This will be cap-
tured by a higher frequency of the (CallExpression →callee FunctionExpression)
bigram. Node 3-grams contain more contextual information than node 2-grams.
An example is the following fragment, where functions are defined in an array that
is passed as function argument. They are described by the node 3-gram (CallEx-
pression →arguments ArrayExpression →elements FunctionExpression). The use of
such structures may strongly vary per author.
bar([
function(a,b){...} ,
function(a,b){...}
]);
3.2.3 Refactorings to the AST
To extract structural patterns, only the node types in the tree are considered. As a result,
information that is enclosed as attributes of the nodes is possibly neglegted. Therefore,
we performed some refactorings in te parse tree, by representing some attribute values
as child nodes. Another refactoring is that we introduced some new node types. The
reason is that some nodes types had multiple concrete representations in the source code.
For instance, a MemberExpression can be either a member accessed by an identifier
(i.e. foo.bar, where bar is an identifier), or a member accessed by an expression (i.e.
foo[bar], where bar is an expression). Also the use of parenthesis in expressions are
implicit in the tree structure. Such syntactical differences have a different meaning in
the source code and also contain different layout slots. The refactorings performed on
the AST are detailed in Appendix A.
3.2.4 Feature extraction
For every defined feature we maintain a histogram distribution of the relative frequency
of occurences, as was done in several other studies, e.g. [41, 55, 70]. In this appoach
the range of the feature values is splitted into a number of bins. Then for each bin,
the number of occurrences (matches) in the sample data are counted. The approach is
described in Algorithm 3.3. For instance, we define multiple intervals (i.e. the histogram
distribution) for the number of function arguments (i.e. the feature). We then track the
number of occurences of arguments in all function definitions in the source code. The
different features and histogram values for each feature are listed in Appendix B. After
the occurences have been counted, we flatten X to a i · j × 1 vector x and normalize its
values. We evaluate three different types of normalization:
• Normalization of the vector x, such that the sum of its elements becomes 1.
• Feature wise normalization, such that the sum of occurences in the histogram
groups for each different feature becomes 1 (unless the feature is not observed).
35
3. Approach
Input:
F ← a set of features.
P ← a set of AST program root nodes belonging to the author.
X← an i×j empty matrix which contains for each feature fi ∈ F a set of j histogram intervals.
1: visit(pt) for each pt ∈ P
2: normalize(X)
3: function visit(n : node)
4: for each fi ∈ F do
5: if fi matches n then
6: determine feature value and increment Xi,j accordingly.
7: end if
8: end for
9: visit(n.children)
10: end function
Algorithm 3.3: Domain specific feature generation.
Node 1-gram Count
DebuggerStatement 7
LabeledStatement 39
WithStatement 79
DoWhileStatement 110
ContinueStatement 685
. . .
ExpressionStatement 245615
CallExpression 306206
Literal 395096
MemberExpression 471884
Identifier 1607897
Node 2-gram Count
IfStatement→testArrayExpression 0
. . .
IfStatement→testThisExpression 1
ForStatement→updateLiteral 1
. . .
WhileStatement→bodyContinueStatement 2
IfStatement →consequentEmptyStatement 2
. . .
CallExpression→argumentsLiteral 132337
CallExpression→calleeMemberExpression 205447
MemberExpression→objectIdentifier 297063
Table 3.1: Some of the least and most frequent node 1 and 2-grams in the dataset, with their absolute
number of observations in ascending order. For the details about the dataset we refer to Section 4.2.
• Binary discretization, such that each histogram value becomes 1 if it is observed
at least once and 0 otherwise.
3.2.5 Feature selection
The resulting JavaScript AST syntax has 38 different statement and expression types.
Consequently, the number of unique 1, 2 and 3-grams becomes computationally heavy.
Therefore we removed a large number of node n-grams. First, the abstract grammar
defines for the child attributes whether an expression or statement is expected, which
eliminates a large number of possible node 2 and 3-grams. Also, not all node n-grams
that can be derived from the abstract syntax are valid in JavaScript. For instance, the
code new 1+1() can be parsed without errors, since the callee in a NewExpression can
36
3.3. Conclusion
Technique Document representation Classification Document sampling
Scap Character (n-gram) Similarity based, SPI Profile based
n-gram based Character (n-gram) Feature vector based, SVM Instance based
Compression based Character (plain text) Hybrid, SVM Instance based
Domain specific AST Feature vector based, SVM Instance based
Table 3.2: The techniques under investigation in this study.
be of any type of expression. In practice, this code will result in a runtime error, and
consequently the node 2-gram (CallExpression→calleeBinaryExpression) is not expected
to be observed in JavaScript code collections. Moreover, some structures are valid,
but were expected to be observed very rarely in the code, such as while(...)break,
with the associated bigram (WhileStatement→bodyBreakStatement). To eliminate such
infrequent n-grams we empirically evaluated what were the most frequent node n-grams
in all the JavaScript code in our dataset. This selection process was caried out recursively
as follows. First, we selected the node n-grams that were observed 18 times or more,
starting at n = 1. Then the node n-grams that occurred 4374 times or more were
extended to node n + 1-grams. The same process then was performed for node n + 1
grams. The numbers 18 and 4374 originate from the dataset: 10% of the 182 authors
which owned together 4374 repositories. To get an idea of the selection process, Table
3.1 lists the most frequent and least frequent 1 and 2-grams found in the dataset. After
the node gram selected procedure we ended up with 1015 node gram features. For the
other features we then selected the numerical histogram intervals as they have been
documented in Appendix B. These intervals were chosen by intuition while counting the
absolute number of occurences in the entire dataset, such that empty or over-full bins
were avoided.
3.3 Conclusion
In this chapter we presented the approaches that will be used in authorship identification
and verification of JavaScript source code. We presented three existing approaches that
employ low level character-based features. The Scap approach is similarity based, while
the other ones are feature vector based. Furthermore, the similarity measure used in
the Scap method was adapted in order to be employed in authorship verification. The
domain specific approach differs from the character based approaches in the sense that
it extracts features from the AST and is thus language specific to JavaScript. Table 3.2
summarizes the techniques.
37

4
Experimental Setup
In this chapter we describe the experimental setup and the methodology by which the
research question will be answered. We use source code from GitHub repositories to
validate the authorship identification and verification techniques. Remember that the
task of authorship identification is to predict the most likely author in a set of authors,
while in authorship verification task is to predict whether a document was written by
an author or not. In Section 4.1 we describe the validation procedure. Then we address
the construction of the JavaScript source code collection in Section 4.2.
4.1 Validation procedure
To validate the proposed authorship analysis techniques, they were implemented in Mat-
lab. For classification we adopted PRTools [20]. PRTools is a Matlab toolbox for pat-
tern recognition that defines many basic pattern recognition concepts and routines. The
evaluated feature vector based approaches employ an SVM for classification. Although
PRTools provides a Matlab SVM implementation, we chose libsvm [13] for speed consid-
erations. Libsvm is a C++ implementation of the SVM classifier. Fortunately, this library
has a Matlab interface and is compatible with PRTools. To compute one-class classifica-
tion related concepts such as ROC-curves and AUC-values, we employed dd_tools [75],
which is a one-class classification toolbox extending the PRTools toolbox. Next, we sep-
arately describe the validation procedure for authorship identification and verification.
4.1.1 Authorship identification
The process for validating the identification techniques is described in Algorithm 4.4.
We measured the accuracy of the techniques with a varying number of training samples
per author and a varying number of candidate authors. In this algorithm we repeatedly
select a random subset of authors from the whole dataset. To reduce variance in the test,
10 iterations are performed using a different set of authors. Per iteration, the trained
classifier is tested with at least ntest samples per author. Because some authors had
39
4. Experimental Setup
a very large number of samples, we limited the number of test samples per author to
16 in practice. Limiting the maximum number of test samples per author prevented
an imbalanced testing set and a too high computational burden. In the algorithm, the
function logspace(a, b, n) generates n logarithmically spaced integers between a and b
with no repetitions. We express the performance of the classifier by the accuracy, since
most other works used this measure, which makes it easy to compare them. The accuracy
is defined as
a = TP + TN
P +N ,
where TP and TN are the number of true positives and true negatives, and P and N
are the total number of postive and negative samples [24].
To prevent topic biasses, we defined a code sample to be an entire JavaScript reposi-
tory. This ensures that the same repostiory is not used either for training and validation.
As a result, the evaluation results will be more representative for varying coding style
between projects, since classification will less likely be based on characteristics of a par-
ticular project, such as variable names specific to a project.
4.1.2 Authorship verification
For authorship verification we used the same datasets as were used for authorship iden-
tification. One author is selected as target author which is modeled by a varying number
of training samples. Since we solved the authorship verification as a binary class classifi-
cation problem, we modeled also the outlier class by selecting (a possible heterogeneous)
set of samples from the other authors in the dataset. To reduce the expected variance in
the test results, the outlier class is validated with 5-fold cross validation. The validation
approach is detailed in Algorithm 4.5. Significant is that the outlier samples used for
Input:
C: candidate authors in the dataset
Dc: training samples for each candidate author c ∈ C
1: for nauthor ←logspace(2, 75, 10) do
2: for ntrain ←logspace(1, 25, 6) do
3: for i← 1 . . . 10 do
4: Select a random C ′ ⊆ C, such that |C ′| = nauthor and |Dc| > ntrain +ntest ∀c ∈ C ′.
5: Train with ntrain randomly selected samples from Dc for each c ∈ C ′
6: Test with t remaining samples in Dc for each c ∈ C ′ such that ntest ≤ t ≤ nmax
7: Track accuracy pi of the instances in the test set.
8: end for
9: p =meani(pi) . Performance with nauthor authors and ntrain training samples.
10: end for
11: end for
Algorithm 4.4: Validation procedure for authorship identification techniques.
40
4.2. Source code collections
testing are selected from different authors than the outlier samples for training. In this
way we modeled the case that the negative test samples belong to an author we never
observed before. Computing the AUC values and coordinates of the ROC curve is caried
out by the dd_tools Matlab toolbox.
4.2 Source code collections
A labeled set of JavaScript code is needed in order to validate the proposed authorship
analysis techniques. Unfortunately, no such JavaScript datasets are readily available.
Therefore, we developed a code collection tool for mining source code from GitHub.
GitHub is a web based service for hosting repositories of the distributed Version Control
System Git1. It extends Git by an online interface and several collaboration features,
such as forking, wikis and pull-requests. It has become the largest code host in the world,
with more than 5M developers collaborating across 10M repositories [32]. Many of those
repositories are open source software projects, which makes GitHub a very adequate
source to mine source code. We constructed two different datasets:
• Dataset A1 and A2 consists of a number of authors with many repositories
developed solely by the authors. Dataset A2 is identical to dataset A1, but in
this dataset all the code is modified by a JavaScript minification algorithm. The
1https://github.com/
Input:
C: candidate authors in the dataset
Dc: training samples for each candidate author c ∈ C
1: for ntrain ←logspace(1, 25, 6) do
2: Select a set of authors C ′ ⊆ C such that |Dc| ≥ ntrain + ntest ∀c ∈ C ′
3: for each c ∈ C ′ do
4: Compose target train set Dt ⊆ Dc with |Dt| = ntrain
5: Compose target test set Du ← Dc \Dt
6: Divide the remaining authors C \ {c} at random into 5 equally sized sets C1 . . . C5
7: Select for each author a set prototypes Pc ⊆ Dc with |Pc| ≤ nprotos
8: for k ← 1 . . . 5 do
9: Train with target samples Dt and outlier samples
⋃
c∈Ci Pc with i 6= k
10: Test with target samples Du and outlier samples Pc with c ∈ Ck
11: Track estimated labels and posterior probabilities.
12: end for
13: end for
14: Compute ROC-curve and the corresponding AUC-value from the tracked information
15: end for
Algorithm 4.5: Validation procedure for authorship verification techniques.
41
4. Experimental Setup
aim of the minified code is to investigate the influence of removing all redundant
characters from the source code on the classification performance.
• Dataset B1 and B2 both consist of a cluster of repositories developed together
by a team of authors. The aim of this dataset is to validate the authorship iden-
tification approaches on code that was developed collaboratively.
The remainder of this section describes the data mining process we followed for estab-
lishing these datasets.
4.2.1 Git and GitHub authorship concepts
First, the different authorship concepts in Git and GitHub are of interest in order to
understand the rationale behind the data collection approach. Different terms such as
contributor, collaborator and commiter are used and have a slightly different meaning.
Figure 4.1 shows how these authorship concepts relate to each other. A GitHub user
account is identified by a unique login name. The single user who has full control of
a particular repository, is the owner of that repository. The owner can grant read and
write access to other users, which are collaborators on the repository. A collaborator
is able to commit directly to the repository. In addition, the concept contributor is
important. A GitHub user is a contributor whenever he contributed to a project by
committing to a project’s default branch or the gh-pages branch2, opening an issue or
proposing a pull-request. Note that the project’s set of contributors is not neccessarily
equal to the set of collaborator, since contributions such as pull requests or opening issues
can be done by every arbitrary GitHub user (and not only collaborators). Consequently,
the code in the repository can be written by authors who are not collaborator of the
project. Moreover, a contributor does not necessarily have to be author of code in the
project, as he may for instance only have reported an issue. Oddly enough, we will see
that in the non-ideal case, code can have been written by an author than can not be
linked to a GitHub user. Consequently, it does even not hold that each author in the
repository is a contributor. GitHub furthermore introduced the concept of organizations.
Organizations are GitHub accounts that are designed for bussiness and large open-source
projects that need multiple owners and adminstrators. Within an organization, groups
of members (teams) can be created and access to repositories can be controlled. The
latter concepts are not shown in Figure 4.1.
In Git, a developer is represented by an identity, which is formed by the combination
of a name and an e-mail address. Every commit includes the identity of the developer. In
order to construct the labeled datasets, the identity of the commits is crucial to determine
the author of the code. In principle it is possible that a single person uses multiple
identities in one repository. This is the case when the developer has multiple checkouts
of the project configured with a different name or e-mail address. The committer is not
neccessarily identical to the author of the commit. The code is written by the author and
can be committed on behalf of the original author by another person. If a Git repository
2The gh-pages branch is meant to generate a webpage for the project.
42
4.2. Source code collections
Github repository
User account
User Organization
owns
contributes
collaborates
is member
Git repository Commitrecords
committer
Identity
hosted as
nameemail address
linked to
is a
1 n
1
1
1
1
1
n
n n
n
n
n
n
n
n
authorn 1
login
Figure 4.1: ER diagram of relationship between Git and GitHub repositories and authors.
is hosted on GitHub, the email-address in the commit is used to link the commit to a
GitHub user. A GitHub user can add multiple email addresses to his account, so that
multiple Git identities can be traced back to one Github account. If the identity is
unknown to GitHub, the commits are not linked to any user.
4.2.2 Data collection tool
We developed an application3 that enables exploratory repository collection on GitHub
by using the GitHub API4. Figure 4.2 depicts the GUI of the application, which visualizes
the relation between GitHub developers and repositories in a developer network. We
define a developer network as a directed graph, where a directed edge (u, v) is considered
to be a developer u linked to repository v. The repositories and developers in such a
network displayed with blue and green vertices respectively.
We aimed to be able to determine the GitHub users that definitely contributed code
to a repository, and vice versa, the repositories a GitHub user contributed code to. The
tool determines the contributors of a specified repository by the GitHub API REST call:
GET /repos/: owner/:repo/contributors
However, as was explained, a contributor does not neccessarily have authored code to
a project. Therefore the tool may associate users with a repository to which they did
not contribute code. Besides the users belonging to a project, we are interested in all
the repositories associated to a user. Unfortunately, the GitHub API does not offer
functionality to query the repositories a user contributed to. Therefore, the tool is only
able to identify the repositories a user owns or is member of. This is possible by the
REST call:
GET /users/: username/repos
3https://github.com/wilcowisse/Shibboleth
4https://developer.github.com/v3/
43
4. Experimental Setup
Figure 4.2: Graphical user interface of the source code mining application.
Clearly, looking up users of a project and projects of a user is assymetric and produces
data that should be validated. Therefore, the tool eventually determines the real authors
of a repository by cloning a GitHub project to the local hard drive. Then the code
is analyzed by employing the Git blame command, which annotates each line in the
given file with information that includes the identity of the person who last modified
the line. The blame information is used to assign a class label to the code in the
cloned repositories, needed to construct labeled datasets. A possible improvement on
our approach is not to consider the contributors to repositories, but instead to query all
the commits done to a repository:
GET /repos/: owner/:repo/commits
The resulting commits enclose all the authors who committed to the project. Moreover,
an advantage is that for each commit it is indicated to which GitHub user the committer
and author of the commit are linked. In our approach the Git user is manually linked
to a GitHub user by his email-address. This is however not possible, when a GitHub
author has not made his email address publicly available or committed by another e-mail
address. In such cases we use a heuristic that links the Git username and email-address
to a Github account: we evaluate the measure of similarity between the Git and Github
usernames and login name by the Jaro-Winkler string similarity metric [17]. In practice
a record is linked if the string similarity is above the threshold of 0.9, which roughly
implies a nearly exact match.
The application respects the API rate limit of 60 requests per hour. Using basic
authentication this limit can be extended to 5000 requests per hour by supplying a
44
4.2. Source code collections
personal OAuth token. By using caching the number of requests are further limited.
The application relies on a number of third pary libraries. They include the Google
HTTP Client Library used to access the REST interface of the GitHub API. To clone
and analyse Git repositories, JGit5 is employed. Furthermore, the Gephi Toolkit6 is
used to display the relation between repositories and developers.
4.2.3 Mining repositories
The core functionality of the data collection application involves the construction of a
dataset from given GitHub repositories. What has not been discusses so far, is how
appropriate projects have been discovered. For instance, collections A1 and A2 require
projects which have been written by exactly one author. Furthermore, since the valida-
tion methods need multiple samples per author, developers are wanted that developed a
large number of projects. GitHub does not offer search functionality satisfactory for these
needs. Hence, in order to find appropriate repositories we used data from the GHTor-
rent project [32]. The GHTorrent project offers an offline mirror to a data collection
that has been gathered by event streams and data from GitHub. We used the rela-
tional repository metadata that is offered in a MySQL database7. Figure 4.3 shows the
database tables which are relevant for our work. The database table project_members
refers to users with commit access to the referenced project [31], so we concluded that
project members equivalent with GitHub collaborators. Again, this is not exactly the
information we would like to have (i.e. we target contributions), however, it is a good
approximation of a number of repositories a user contributed to. The used database
contained 1.1M JavaScript repositories and users. We decided to remove projects that
were forked from other projects, since those forked projects ‘inherited’ all the project
members of the upstream repository. As a consequence some popular projects appeared
a very large number of times in the database, with the only difference that the project
had another owner. After removing forks, 361554 JavaScript repositories remained,
which were imported in Gephi [5]. Gephi is open source software for the analysis of
graph structures and networks. Using Gephi we removed all users that were member of
less than 5 projects. This ensured that there were multiple samples per author. The
resulting users and repositories are shown in Figure 4.4 (a).
4.2.4 Data selection
Using Gephi, appropriate users and projects were selected from the mined data to con-
struct the eventual datasets. To construct dataset A, we first queried the repositories
that had one author (e.g. the repositories without other collaborators). Then we listed
the corresponding users in descending order on the number of such repositories they
owned. Since the GHTorrent data may be outdated we determined the currently owned
5http://www.eclipse.org/jgit
6http://gephi.github.io/toolkit/
7We used the database dump from of 10-12-2013. The size of the compressed dump is approximately
4.2GB while the uncompressed size is more than 15GB.
45
4. Experimental Setup
projects
id : int
owner_id : int
url : varchar
language : varchar
forked_from : int
project_members
repo_id : int
user_id : int
users
id : int
login : varchar
name : varchar
email: varchar
type : varchar
Figure 4.3: GHTorrent MySQL database tables used in our study. Projects are equivalent to Github
repositories.
repositories on Github by our data collection tool. Then we validated whether the owner
was the only GitHub user who contributed to the owned projects. If that was the case,
the repository was included in the dataset.
Dataset B consists of a cluster of repositories developed by the same set of authors.
This dataset was constructed by querying k-core networks by Gephi in the GHTorrent
dataset. A k-core network is a subgraph in the original graph where all nodes have degree
of at least k. Consequently a repository in such a network is developed by at least k
authors, and these authors collaborated in least k of these repositories. Figure 4.4 (b)
depicts a number of such k-core networks from the dataset. By manually inspecting a
number of such clusters we selected two of them with the property that most contributors
did a significant contribution to the project. Since a k-core network is a subset of a larger
network, we then identified all the project members and collaborators of the repositories
in the core network. A resulting developer network is shown in Figure 4.4 (d).
4.2.5 Data cleaning and preparation
After selecting the data, we created a clean subset of these data. The cloned data of
dataset A was 29.1GB, distributed over 826k files. Obviously, only JavaScript files are
relevant, and thus other data in the repository were removed. The remaining datasize
was 185.6MB. Unfortunately, we encountered that many repositories included source
code of JavaScript libraries such as JQuery in their code base. Since the libraries were
written by different authors, the libraries would introduce noise in the extracted features
to characterize the writing style of the particular author. Thus without removing the
libraries the results of the authorship analysis are in question. We first attempted to re-
move libraries by a predefined blacklist of file names. However, by manually inspecting
the data, we found that this was not sufficient, since the file names had some varia-
tion, and obviously, we were not aware of all existing JavaScript libraries. Therefore
we adopted the following strategy to eliminate libraries from the code base. The first
observation we did was that in general libraries were much larger than other JavaScript
files. Therefore we removed all files that were larger than 100kB (this already eliminates
the JQuery libraries). The second observation was that the code in library files was com-
mitted in a single commit. Therefore, we used Git blame to determine whether all lines
in a were committed in one single commit, and removed the file if this was the case. We
were confident that this approach was satisfactory after manually searching for libraries
in the remaining code. After removing libraries, we removed all JavaScript files that
46
4.2. Source code collections
(a) (b)
(c)
knowuh
MySystem-Wise-Integration-WIP
scytacki
rklancer sfentress
biologica.jspsndcsrv
ddamelin
stepheneb
lab
pjanik
wday
kamalpaul
willy-vvu
jengoree
catoangelo
jacksonhenry3
mysystem_sc
pjmorse
gigamorph
babakofi
hirokiterashima
smartgraphs-authoring
smartgraphs-generator
SuhasZeus
meghavipatelmayankdedhianehathakur
pawanbagweAnishGhumara
sparks
jonahelgart
Jigar24
(d)
Figure 4.4: The GHTorrent dataset imported in Gephi. Figure (a) shows the users with 5 or more
JavaScript repositories. Figure (b) shows the 200 authors that had the largest number of repositories
without other collaborators in this dataset. Figure (c) depicts a number of k-core networks in the
GHTorrent dataset. One of these k-core networks including all the collaborators of each repository is
shown in figure (d).
could not be parsed. In dataset A we then removed the authors which had less than
5 repositories. Finally, we removed repositories that were smaller 1kB or larger than
50kB. This was done because there was a high variance in the size of repositories. By
removing the smallest and largest samples, the size of the samples are better balanced.
Table 4.1 shows the size of dataset A1 before and after the described cleaning process.
Dataset A2 consists of the same repositories as dataset A1, but all the code was
47
4. Experimental Setup
Before cleaning After cleaning
Authors 216 182
Repositories 5846 4374
Files 15092 10039
Total datasize 185.6 MB 29.3MB
Table 4.1: Dataset A1, before and after detecting and removing all invalid JavaScript files, authors
and repositories.
0
0.2
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1
F
n
Compression (kB %)
(a)
0
0.2
0.4
0.6
0.8
1
0 5 10 15 20 25 30 35
F
n
Size (kB)
Repository size
(b)
Figure 4.5: Result of minifying data set A1. Figure (a) depicts the degree of compression per
JavaScript file. Figure (b) depicts the empirical cumulative distribution of the repository size of
dataset A2. In this graph, the dashed line corresponds to dataset A1.
minified. Minification is done by using the Esmangle8 and Escodegen9 projects, with
the compact=true option. Figure 4.5 shows the percentage of compression per file, and
how this affected the eventual size of the samples. In dataset B we removed the edges
between authors and reposities that represented a contribution of less than 1kB. Also
the authors that contributed to only 1 project, and the projects with only one author
were removed. The amount of code in dataset B was limited, so we manually inspected
the code for JavaScript libraries. Appendix C details the graphs of the resulting authors
with the corresponding size of their contributions.
8https://github.com/Constellation/esmangle
9https://github.com/Constellation/escodegen
48
5
Results
In this chapter we present the experimental results of the four different authorship anal-
ysis techniques that have been introduced in Chapter 3. The fundamental difference
between the techniques under investigation can be summarized as follows:
• Broadly speaking, there are two basic classification approaches in authorship anal-
ysis: similarity and feature vector based approaches. The Scap method is a sim-
ilarity based common n-gram (CNG) technique, while the other techniques are
feature vector based techniques and employ an SVM classifier.
• Next, the syntactical complexity of the used features is different. In our study
we evaluate one language specific approach that uses layout, style and structure
features from the AST. The other techniques use language independent character-
based features. These include character n-grams and text compression features.
The evalation procedures have been described in Section 4. A different evaluation pro-
cedure was designed to investigate the authorship identification and verification task.
Correspondingly, we describe the experimental results of these two distinct authorship
analysis tasks in Section 5.1 and Section 5.2.
5.1 Authorship identification
The task of authorship identification is to predict the most likely author in a set of
candidate authors. We first evaluated the identification techniques on dataset A1. The
samples in this dataset consist of GitHub repositories each developed by a single au-
thor. Figure 5.1 (a) and (b) provide the results obtained by validating the identification
techniques by training with n = 7 and n = 25 samples per author on this dataset1.
The results show that the authorship identification techniques are quite effective. In a
1In Appendix D, the results for other values of n are presented.
49
5. Results
0
0.2
0.4
0.6
0.8
1
0 10 20 30 40 50 60 70
Ac
cu
ra
cy
Number of authors
Scap
Domain specific features
Compression based
n-gram based
Domain specific and n-gram features
(a) n = 7
0
0.2
0.4
0.6
0.8
1
0 5 10 15 20 25 30
Ac
cu
ra
cy
Number of authors
Scap
Domain specific features
Compression based
n-gram based
Domain specific and n-gram features
(b) n = 25
Figure 5.1: The authorship identification techniques trained with 7 and 25 samples respectively, with
a varying number of candidate authors. Note that in figure (b) the maximum number of authors on
the x-axis is 34 in contrast to 75 authors in figure (a). This is due to the validation procedure, which
selects at random a set of authors from the corpus, such that the number of samples for each author
is equal to or larger than the number of required train and test samples. Table 5.1 shows that an
identification problem with 50 or 75 candidate authors cannot be trained with 25 samples per author.
closed set identification problem with 34 authors, a random guess would have an ex-
pected accuracy of 1/34 ≈ 2.9%. In contrast, the best performing technique achieved an
accuracy of 65.7% when training with 7 samples per author. The accuracy increases to
85.1% when trained with 25 samples per author. Overall, the n-gram based techniques
achieved the highest accuracies, while the compression based method achieved the least
high accuracy.
Unfortunately, the domain specific approach did not outperform the n-gram based
approaches. However, we observed that the domain specific approach is able to improve
the machine learning approach that uses character n-grams. Since both techniques are
feature vector based, the feature space of these two approaches can be combined into a
single higher dimensional representation. The two feature spaces may express different
authorship information, since they are extracted from a different language representa-
tion, e.g. the domain specific features may be better able to describe the structual
aspects, while the n-grams may be better able to describe stylistic properties. As a
consequence, the two feature spaces can complement each other, resulting in a more
expressive characterization of the writing style. The purple line in Figure 5.1 indeed
shows that the concatenation of the feature set is not redundant, resulting in a higher
accuracy.
50
5.1. Authorship identification
Number of authors 171 168 155 134 91 46
Number of train samples (n) 1 2 4 7 13 25
Table 5.1: Number of authors in the dataset that have n or more training samples. This is the result
after reserving 5 samples per author for validation.
5.1.1 Comparison of the classification techniques
We now zoom in on the accuracy of the different classification techniques presented in
Figure 5.1. The identification techniques are evaluated on various problem complexities
by regulating the number of candidate authors and the amount of available training
data per author. It is straightforward that the accuracy substantially decreases with
a larger number of candidate authors. More candidate authors make it difficult to
discriminate between the coding characteristics of the developers. Figure 5.1(a) shows
that the accuracy declines sharply when the number of candidate authors is limited,
while the decline is less fast with a larger number of candidate authors. Apparently, a
part of the features in the feature space is not sufficiently unique accross the authors.
Those features can only be exploited when the number of candidate authors is small.
With a larger number of training samples, classification is based on the features that are
to a larger extent unique for each author. In that case the decline is more gradual.
Besides the number of candidate authors, the number of training samples influences
the classification accuracy. A smaller number of training data allows the learning al-
gorithm to generalize from the training data less well. An important observation in
Figure 5.1 is that it rougly indicates that with a larger number of training samples,
the accuracy of the machine learning classification approaches increase proportionally
more than the similarity based approach (i.e. Scap). To investigate this relationship
further, we compared the influence of the number of training samples on the Scap tech-
nique and the machine learning technique with the combination of domain specific and
n-gram features. This is shown in Figure 5.2. With 7 training samples, the methods
approximately perform equally well. With a small training set size, the Scap method
is substantially better. However, Scap performs slightly worse with a large number of
training samples. The most likely cause for these results is that the data sampling in
the feature vector based approaches is instance based, which means that each training
document is considered as a distinct unit that contributes separately to the classification
model. Therefore, multiple training samples per class are required for creating a reli-
able model. In contrast, Scap is profile based, which means that the differences between
training documents by the same author are disregarded and that the training documents
are treated cumulatively per author. Consequently, a profile can be constructed with a
limited number of training samples. Stamatatos [73] pointed out that segmenting long
samples into multiple parts may improve the instance based approaches. Although data
segmentation may be a good way to improve the accuracy, we did not investigate it.
First, the documents should be parsable for the domain specific approach. This makes
it difficult to split the files at arbitrary positions. Secondly, to prevent topic biasses, we
strictly separated repositories in the training and test data so that no code of a project
51
5. Results
0
0.2
0.4
0.6
0.8
1
2 3 4 5 6 7 8 9 10
Ac
cu
ra
cy
Number of authors
n = 4
n = 7
n = 13
n = 25
(a)
0
0.2
0.4
0.6
0.8
1
0 10 20 30 40 50
Ac
cu
ra
cy
Number train samples
Features and n-grams
SCAP
(b)
Figure 5.2: Figure (a) shows the accuracy of the Scap approach (dashed line) compared to the
machine learning approach with n-gram and domain specific features (solid line). In figure (b) the
two approaches are compared with a varying number of train samples. The results were obtained
by a set of 15 candidate authors and a training set size of 13 samples per author. To decrease the
variance, we tested with a larger number of validation iterations than the results in the previous figure
(100 instead of 10).
had been seen before. In the current validation procedure splitting the repositories in
multiple parts would violate this condition.
5.1.2 Comparison of the feature types
Having analyzed the different classification techniques, we now focus on the different
feature types that were used. The techniques in our study that use character n-gram
features include machine learning classification with n-grams, and similarity based clas-
sification with n-grams (i.e. the Scap method). As explained in the literature review in
Chapter 2, character n-grams are one of the most effective feature types in authorship
identification. Figure 5.1 shows that also in our case the accuracy of the n-gram based
techniques achieved the best performance.
Next to the n-gram based techniques, the compression based approach is character
based. However, in the experiments it turned out that the compression based method is
the worst performing identification technique. By inspecting the compression distances
between samples, we encountered that the differences of the values were very small. This
may be due to the limited amount of the content of the document that is characteristic for
the coding style of a developer. Consequently, the difference in compression distances
that originates from coding style may be concealed by noise introduced by the other
content in the document. For example, frequently used words or sentences in comments
52
5.1. Authorship identification
0
0.2
0.4
0.6
0.8
1
All together
Layout
Layout excluded
Node 2-gram
Node 3-gram
Node children count
Node descendant count
Comment parent node types
Comment length
Statement types
Expression types
Naming style
Literal data types
Comment types
Ac
cu
ra
cy
Feature type
Figure 5.3: Results of testing different types of domain specific features. We did this by excluding
feature from the feature space. We tested with 15 candidate authors and a training set size of 13.
Omitting all features corresponds to a random guess, 1/15 ≈ 0.067, this is indicated with the dashed
line in the bottom of the graph.
may determine the compression distance to a great extent. As a result, the contribution
of the smaller portion of characters in the document that describes the layout, may be
negligible.
To investigate the contribution of the individual domain specific feature types, we
tested the domain specific approach by exclusively using one feature type and removing
the other feature from the feature space. This is shown in Figure 5.3. In this figure ‘Ex-
pression types’ and ‘Statement types’ relate to the corresponding type of node 1-grams.
From the graph above we can see that the structural features obtained a high accuracy.
This supports our hypothesis that structures in the AST are effective in distinguish-
ing developer styles. Another important observation is that the accuracy of using only
layout features is higher than the accuracy of all the remaining features together (the
layout excluded). The significance of the layout features is also shown in Figure 5.4.
This figure shows in descending order the accuracy of all 212 = 4096 combinations of
domain specific feature types listed in the bar chart in Figure 5.3 (excluding the first
and third bar). All the combinations that include layout features, correspond to the first
half of the graph, before the accuracy drops in the middle of the graph.
Having discussed the different feature types, we now briefly look at the normalization
of the feature values. As was discussed in Chapter 3, we maintain for every feature
a histogram distribution of the relative frequency of occurences. We observed that
the domain specific and n-gram based approaches achieved a better performance with
binary normalization compared to representing features values by the relative frequency
of occurrences. Therefore, the presented results use binary feature normalization. It is
53
5. Results
0
0.2
0.4
0.6
0.8
1
0 2000 4000
Figure 5.4: Accuracy of all combinations
of domain specific feature types.
Technique A1 A2 ∆(%)
Scap 0.77 0.52 33
Domain
specific
0.76 0.51 33
Compression
based
0.66 0.54 18
n-gram
based
0.79 0.63 20
Combined 0.84 0.60 28
Table 5.2: Relative difference in accu-
racy between dataset A1 and A2.
0
0.2
0.4
0.6
0.8
1
0 10 20 30 40 50 60 70
Ac
cu
ra
cy
Number of authors
Scap
Domain specific features
Compression based
n-gram based
Domain specific and n-gram features
Figure 5.5: Authorship identification accuracies on
dataset A1 (solid lines) and A2 (dashes lines). In these
experiments 13 training samples per author were used.
Table 5.2 presents the deviation in accuracy between the
two datasets when using 13 training samples per author,
and a set of 15 candidate authors.
difficult to explain this result, but it may be related to the dependency of each bin-value
on other bins in the histogram. In the case feature-wise normalization is used, the value
of each bin is relative to the values of other bins, which may have undesired effects. For
instance, missing observations may cause the relative frequency of other bins to become
higher. Conversely, bins with a very high number of occurences may cause the value of
bins with more scarcely observed patterns to become too small to be significant. Clearly,
the absence of an observation or a high variance of a bin may introduce noise in this
way. If binary normalization is used, the value of a bin indicates whether a feature is
observed or not in the sample. The values are thus independent of other bin-values
in this case and may consequently be more consistent across the samples. A possible
drawback of binary normalization is that relevant information from the input data may
be disregarded. For example, the ratio between the use of different sorts of control flow
statements is ignored. Furthermore, more vector values will become 1 with an increased
document size. This suggests that more sophisticated normalization may improve the
classification performance. For instance, several methods exists to deal with missing
feature values by estimating them from the data that are present [68].
54
5.1. Authorship identification
5.1.3 The influence of source code minification
The techniques discussed so far were validated on dataset A1. This dataset contains the
original source code that is developed by a single author. We addressed two problems
which may deteriorate the authorship analysis techniques, namely source code minifi-
cation and collaboration within projects. First, we focus on the effect of source code
minification. Code minification is the process of removing all redundant characters from
source code, for instance by stripping the layout and shortening variable names. To
validate this dataset, we removed all layout related features from the feature space in
the domain specific approach. Figure 5.5 provides the corresponding classification accu-
racies tested with 13 training samples, compared to dataset A1. Table 5.2 describes the
relative performance reduction with an instance of 13 training samples and 15 authors.
The accuracy of the authorship identification was considerably worse when validating
with the minified JavaScript code in dataset A2.
The techniques that had the highest accuracies on the original code also had the high-
est accuracies on minified code. Surprisingly, the compression based technique seems to
be the least influenced by the minification of the code, and obtained at a small number of
candidate authors even a better performance than the Scap approach. This is remark-
able, since the Scap method is substantially better with the original source code. The
observation may be linked to the minification procedure, which makes the code more
uniform by removing layout and by renaming variables2. As a consequence, minifica-
tion produces many re-occuring character patterns in the code, which the compression
algorithm may take advantage of: the compression takes place by identifying repeated
character sequences. Aspects such as the number of parameters and other coding style
aspects may become better visible due to the uniformity of the code. The latter can also
be exploited by the n-gram based techniques. Among the most frequent n-grams are
‘ion(a,b)’, ‘;});});}’ and ‘var b=th’. Such patterns are not mixed with style charac-
teristics and hence carry much contextual information in the code such as the number
of parameters and nesting depth, etc.
5.1.4 Authorship identification within projects
As well as code minification, the collaboration of multiple authors within projects may
deteriorate the authorship analysis techniques. To test this, the same authorship iden-
tification techniques were then evaluated on dataset B1 and B2, which both consist of
a cluster of repositories developed together by a team of authors. Unfortunately, none
of the techniques were effective on these datasets, as the accuracies did not significantly
surpass the random-guess baseline. Due to time constraints we did not further inves-
tigate the cause of the negative results. First, it may be the case that it is simply too
difficult to detect authors if they closely worked together. However, it may also be the
case that the way the sample data is collected is not accurate. We used Git blame in-
formation to identify the lines of code written by each author. The blame information
indicates who was the last one who edited the line. As a consequence, the line of code
2The minifier tries to shorten the code by renaming variables in a consistent way.
55
5. Results
may often have been introduced by another author, and later be edited by the person
we considered as author. In such cases it is hard to define who is the actual author of
the concerning code.
5.2 Authorship verification
In Section 5.1 we analyzed the results of the authorship identification techniques. The
results of authorship verification are discussed in this section. The authorship verification
task involves determining whether two documents have been written by the same author,
without necessarily identifying the author. Figure 5.6 shows the ROC-curves of the
verification techniques, in case the target user is trained with 7 samples. An ROC-curve
describes the compromises between the true positive rate (TPR) and the false positive
rate (FPR) when the decision threshold is varied [60]. Obviously, the TPR should be
high, while the FPR should remain low. A better classification performance is indicated
by an ROC curve that is higher to the left in the graph. By choosing a threshold value,
the classifier can operate at any desired point that lies on the ROC curve. Since it is hard
to compare the differences between ROC-curves, often the Area Under the Curve value
(AUC-value) is taken as indication of performance. The AUC-value can be interpreted
as the probability that the classifier will rank a randomly chosen positive instance higher
than a randomly chosen negative instance [24]. Random guessing produces a diagonal
line between (0,0) and (1,1), therefore a realistic classifier should always have an AUC
higher than 0.5. Figure 5.7 shows the AUC values of the authorship analysis techniques
for a varying number of training samples. The corresponding ROC curves can be found
in Appendix D.
5.2.1 Comparison of the classification results
We first consider the techniques applied on the original source code in dataset A1,
presented in Figure 5.7 (a). The performance results of the authorship verification tech-
niques have a number of aspects in common with the authorship identification results.
First, classification with an SVM using n-gram features was the most successful tech-
nique. Furthermore, the performance of the domain specific approach was lower, but
combining the domain specific feature space with the n-gram feature space resulted in
a higher performance. The compression based method is also in authorship verification
the least well performing technique.
The authorship verification and identification results also deviate on some essential
points. In contrast to authorship identification, the Scap approach performs consider-
ably worse compared to the machine learning approaches. As was pointed out in the
previous section, the feature vector based approaches are better able to identify the au-
thorship of unlabeled samples if much training samples are available. Now, in authorship
verification, an abundant amount of code samples were available to model the outlier
class, so that the outlier class can be modeled pretty well. Therefore, the performance
of the instance based methods may be attributed to the well modeled outlier class. Fur-
56
5.2. Authorship verification
0
0.2
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1
TP
R
FPR
Scap (one outlier per author)
Scap (one outlier profile)
Domain specific features
n-gram features
Domain specific and n-gram features
Compression based
(a) Dataset A1
0
0.2
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1
TP
R
FPR
Scap (one outlier per author)
Scap (one outlier profile)
Domain specific features
n-gram features
Domain specific and n-gram features
Compression based
(b) Dataset A2
Figure 5.6: ROC curves for the different verification techniques. The target author is trained with 7
samples.
thermore, the outlier class is very diverse and heterogeneous, as it represents ‘the rest
of the world’. The SVM classifier may be better able to discriminate between the two
classes than the similarity measure that is used in the similarity based approaches.
We adopted two different approaches to use the similarity based Scap method in
a verification context. In the first approach the outlier class is represented by a single
cumulative profile of most frequent n-grams and the distance between the unlabeled
code and the cumulative profile is used to predict the class label. In the second approach
the outlier class is modeled by using multiple outlier profiles, each corresponding to a
different author. Then, the minimal distance of a piece of unlabeled code to an author
is taken to predict the class label. As Figure 5.7 shows, the difference between the
two evaluated representations of the outlier class in the similarity based approach are
minimal.
5.2.2 The influence of source code minification
The effect of code minification was also tested in the verification problem context. Figure
5.6 (b) and Figure 5.7 (b) show the verification results on the dataset with minified source
code. As was the case with authorship identification, the compression based approach
performs surprisingly well with minified JavaScript code. Especially with a large number
of training samples the compression based technique was quite effective. In the previous
section we already argued that this may be attributed to the uniformity of the source
code after minification, which the compression algorithm may take advantage of.
It is remarkable that all the feature vector based approaches show a drop down in the
57
5. Results
0.5
0.6
0.7
0.8
0.9
1
5 10 15 20 25
AU
C
Training samples
Scap (one outlier profile)
Scap (one outlier per author)
Compression based
Domain specific features
n-gram features
Domain specific and n-gram features
(a) Dataset A1
0.5
0.6
0.7
0.8
0.9
1
5 10 15 20 25
AU
C
Training samples
Scap (one outlier profile)
Scap (one outlier per author)
Compression based
Domain specific features
n-gram features
Domain specific and n-gram features
(b) Dataset A2
Figure 5.7: AUC-values for various classifiers and number of train examples
AUC-values at two training samples. This is particularly the case for minified code in
dataset A2. These observations are reflected by the ROC curves that show an unexpected
shape at two training samples (see also Appendix D). We did not investigate the origin
of these unexpected results, but it may relate to the way the posterior probabilities (the
probability that a predicted label is correct) are obtained from the SVM classifier. Since
the SVM is not based on probability density estimates, the confidences are estimated
from the distance of the samples to the decision boundary [21]. This is done by scaling
the distances on the [0,1] interval by a sigmoid function, such that the likelihood of
the posterior probabilies is maximized3. The peculiar shape of the ROC curve may be
attributed to an inadequate estimation of the confidence intervals.
3In PRTools this normalization is performed by the cnormc.m function.
58
6
Discussion
The experimental results have been presented in the previous chapter. In this chapter
we discuss the main findings and implications obtained by the experiments. First we
compare the authorship analysis approaches in terms of the classification technique and
the feature choice in Sections 6.1 and 6.2 respectively. Then we discuss the effectivity
of authorship analysis techniques applied to authorship verification in Section 6.3. We
also address the investigated complications in authorship analysis in Section 6.4. Subse-
quently, the overall effectivity of the techniques are summarizid in Section 6.5. Finally,
we point out the limitations and threads of validity of the current work.
6.1 Comparing classification techniques
The investigated authorship analysis techniques include one similarity based technique
which employs a distance metric for classification, and three feature vector based tech-
niques that employ an SVM for classification. The authorship identification results
confirm the high performance of the similarity based technique by common n-grams
that has been reported in literature [28]. We found that the similarity based technique
can especially handle cases where a very limited amount of training data per author
is available, while the feature vector based techniques were more effective with a large
number of training samples. Different aspects may explain these observations. First,
the machine learning techniques are instance based which means that each document
is treated separately and a sufficient amount of documents is needed to create a rep-
resentative model of the classes. Secondly, the observations may be inherent to the
difference between similarity based and feature vector based classification. In the fea-
ture vector based approaches, the classes are modeled in the same feature space, where
each dimension represents a single measured pattern in the source code. With a small
number of training samples, we perceived that the representation of the feature vectors
was sparse, because many of the features were not observed. In contrast, the similarity
based approaches classify by means of similarity function that quantifies the degree of
59
6. Discussion
shared information between the instances. This information can also be extracted with
a limited number of training samples, i.e. we are not tied to a predefined set of n-grams
to be observed in a document. Therefore, in the similarity based techniques a more
adequate set of information can be extracted if only a small amount of training data is
available.
In contrast to authorship identification, the feature vector based approaches turned
out to be substantially better than the similarity based techniques. This may be at-
tributed to the abundant amount of training samples that were available to model the
outlier class. Like the classes in authorship identification could be modeled well with
many training samples, the instance based techniques may create a very accurate model
of the outlier class with the large number of available training samples. The profiles
used in the similarity based method, in contrast, disregard the differences between the
repositories in the outlier class. Since the outlier class is very heterogeneous, it may be
not possible to create an accurate model in a profile based representation.
6.2 Comparing the effectivity of features types
One of the observations is that the approaches that use character n-gram features had
the best performance. A possible explanation of the good performance n-gram features
is their ability to implicitly capture much stylometric characteristics. Character n-grams
may capture many little nuances of style, which are not considered in the domain spe-
cific feature space. Therefore we presume a good classification is mainly based on small
textual differences in the code, such as frequently occuring variable and function names,
text in the comments, and small stylistic variations in the layout. This may also be an
explanation for the observation that the compression based technique is the least well
performing approach. The PPM compression algorithm compresses the text by predict-
ing the next symbol by identifying repeated character sequences in the original text. If
the author specific style information mainly lies in small stylistic variation, only a small
number of author specific symbol sequences may contribute to a good text compres-
sion. Consequently, the proportion of authorship relevant content that contributes to
the compression distance may be minimal and thus be concealed by noise introduced by
other content of the document. Small deviations such as commonly used variable names
and layout are less well captured by the domain specific document representation, which
may explain its lower accuracy.
One of the main contributions of this work is the investigation of structural features
extracted from the Abstract Syntax Tree (AST). The structural aspects were described
by node n-grams extracted from the parse tree. We showed that node 2 and 3-grams
are effective markers of the coding style of an author. However, our results agree with
the findings of other studies, which show that the layout is a more relevant indication
of authorship in source code than style and structure features [27, 19]. Furthermore,
the performance of the domain specific techniques have shown to be less effective than
the n-gram based techniques. These findings confirm the high accuracy of character n-
gram based approaches in comparison to domain specific approaches that were found in
60
6.3. Applying authorship verification
literature as was discussed in Chapter 2. However, the results do not imply the domain
specific approach is not of interest. First, we showed that the combination of n-gram
and domain specific features are complementary, since they can be combined in such
a way that they enhance each other. Secondly, because the layout is one of the most
important markers of style, it is at te same time also the most susceptible to be changed
to circumvent authorship analysis techniques [53]. In such cases, the domain specific
approach may better allow to constrain the features that are used in the classification
process. Additionally, some works pointed out that character n-gram based methods may
be less robust in cross-topic and cross-genre conditions, since n-grams may be closely
associated to a particular domain. Thus if the training data is not representative for
an author, the n-gram model may capture particular topics and may perform poorly
when the training and test corpora are on a different topic [74]. Further research should
be done to investigate the effect off such situations in source code authorship analysis.
Clearly, it is not unlikely that the domain specific approach is the preferred technique
in such situations.
6.3 Applying authorship verification
The previous studies on authorship analysis of source code focussed on problems with
small and closed candidate sets. However, no attention has been paid to authorship ver-
ification of source code, which is a one-versus-the-rest-of-the-world problem. Although
some results indicate that verification is considerably more difficult than authorship
identification [2], we obtained quite high levels of performance in the verification prob-
lem. We followed the extrinsic verification paradigm where the one-class problem is
transformed to a multi class classification problem. In the similarity based paradigm we
adopted two strategies to model the outlier class, first by representing the outlier class
by one single profile, and secondly by representing the outlier class by distinct profiles
belonging to different authors. The difference between these two approaches turned out
to be minimal. The first approach is quite similar to a recently proposed profile based
approach for authorship verification in [65]. The second is more or less comparable to the
impostors method proposed in [51]. However, in the latter work the adopted similarity
metric is different, since the average of the distance to the outlier profiles is taken instead
of the minimal distance. The mentioned similarity based authorship verification meth-
ods in natural language have shown to be very effective, e.g. the winning submission of
the PAN 2013 contest was a modification of the impostors method [39]. However, the
findings of the current study do not support that this is also the case in source code. As
was explained above, the feature vector based approaches achieved performances that
were substantially higher than the performance of the similarity based approach.
6.4 Addressing authorship analysis complications
We addressed two problems which are related to external impact on coding style, namely
the impact of code minification and the impact code collaboration. Code minification has
61
6. Discussion
Original code Minified code
Few samples Similarity based,
Scap
Similarity based,
Scap
Many samples Feature vector
based, n-gram∗
Feature vector
based, n-gram∗
Table 6.1: The authorship analysis techniques that obtained the highest accuracies in authorship
identification. Techniques with an asterisk indicate that the accuracy can be improved by combining
the technique with domain specific features.
Original code Minified code
Few samples Feature vector
based, n-gram∗
Compression
based∗∗
Many samples Feature vector
based, n-gram∗
Compression
based∗∗
Table 6.2: The authorship analysis techniques that obtained the highest AUC-values in authorship
verification. Techniques with an asterisk indicate that the accuracy can be improved by combining
the technique with domain specific features. Two asterisks indicate that if feature vector based clas-
sification with n-grams and domain specific features are combined, they outperform the compression
based technique.
shown to significantly deteriorate the performance of the methods, but the techniques
remain quite effective. In authorship identification the n-gram based techniques per-
formed the best. In authorship verification, especially the compression based techniques
are robust to minified code. Also in identification, the performance of the compression
base technique is the least influenced by the minification procedure. This is likely due
to the large number of repeated character sequences in minified code that are the result
of stripping the layout and renaming variables in a predefined way. Consequently, the
proportion of authorship relevant content in minified code that contributes to a good
compression may be larger than in the code that has not been not minified. Unfortu-
nately, we were not able to verify that the identification techniques can be applied when
authors collaborated in the same project. The reason for these results is an important
issue for further research.
6.5 Overall effectivity of the techniques
We now have discussed a number of aspects of the experimental results. The extent to
which the different authorship analysis techniques are effective depend on the problem
context. In the current authorship identification results, it is mainly the number of
training samples that determine the choice for a particular authorship analysis technique.
In authorship verification it is mainly the minification of the code that determines the
choice for a particular authorship analysis technique. These observations are summarized
in Table 6.1 and Table 6.2.
62
6.6. Limitations and threats to validity
6.6 Limitations and threats to validity
We made a number of assumptions to guarantee the feasibility of the approach. This may
cause the results to not generalize well to source code found in a forensic investigation
or in industrial practice. This section describes some threats to the validity of our
experiments.
6.6.1 Data representativeness
First, there may be a selection bias in the construction of the datasets. To construct
dataset A we selected those authors from the GHTorrent database, which owned a large
number of JavaScript repositories. Consequently, the dataset may be mainly represen-
tative for active and experienced JavaScript developers that are active on GitHub. In
addition, we assumed that it is a priori known which parts of the code were developed
by a single author. If there is a number of developers of a piece of code, the whole team
of developers should be identified. This has not been addressed in this study, and by
our knowledge no other authors addressed this problem.
We also do not know to what extent the external influence on coding style plays a
role in the classification accuracy. In practice, there are various complications which
may deteriorate authorship analysis techniques:
• Reuse of code. Among the most serious problems is the reuse of code. The popu-
larity of question and answer websites such as Stack Overflow1 makes that a vast
amount of solutions for programming solutions can readily be copied. Reuse of
programming examples is not generally seen as a serious ethical offense as is the
case in plagiarism of written texts, but has been considered as a key notion in soft-
ware development [4, 7]. The source code that was gathered for our experiments
is existing open source code, and is in this sense representative for this problem,
as some parts may have been copied. However, we do not known the influence of
copied code precisely.
• Evolution of style and adversarial attacks. Another problem is a varying program-
ming style of an individual programmer over time, between languages and between
projects. Although a number of individual characteristics have shown to be corre-
lated to programmers [35], the results reported in [62] indicate that typographical
style can change over the lifetime of a program. Also, an adversary may intention-
ally change the programming style to circumvent authorship analysis techniques.
This can be an obfuscation attack which is the attempt to write in such a way
that the style will not be recognized, or an imitation attack which is the attemt
to write in a style that is similar to another author. It has been shown that such
adversarial attacks succesfully can be applied in written texts [8].
• External constraints. The developers may also be influenced by external constraints
such as naming conventions, or the code may be pretty printed by IDEs. Moreover,
1http://stackoverflow.com
63
6. Discussion
the used framework or library may also have significant influence on aspects such as
the complexity of the code, the number of functions and whether the developer uses
an imperative or a functional programming paradigm. Additionally, a JavaScript
library such as Prototype2 enables the JavaScript developer to define classes and
inheritance which are not part of the genuine JavaScript language. It is not unlikely
that such frameworks highly change the programming style of the programmer.
To mitigate the last two complications we defined the samples in the training and
validation data to be entire repositories. As a result the projects used for training and
validation are strictly separated, such that they may be written in a different context
or domain. However, further investigations should give insight into the severity of the
mentioned complications.
Subsequently, we should note that the minification of JavaScript code strips a lot of
authorship information from the code, but that code minification as it was applied in
our investigation is not representative for an adversarial attack or for code formatting
conventions imposed by an IDE. In these cases, not all the code is modified in the
same way, like we did. For instance, an author may use different IDEs, which format
the code in different ways. Equivalently, an adversary may intentionally use different
ways to change the layout of the code. Therefore, to investigate such attempts, the
techniques should be trained and evaluated with distinct coding formats and different
naming conventions used by the same author.
6.6.2 Limitations of the techniques and the validation procedure
The evaluated techniques have some limitiations. The character based approaches that
were adopted from literature need to be tuned in a better way. First, the PPM compres-
sion algorithm used in the compression based technique is especially successful at the
compression of natural language text [40]. In source code, other compression algorithms
may be more appropriate. Next, we have not considered tuning the optimal n-gram
length in the n-gram based approaches. Also we did not investigate the optimal profile
length (L) in the Scap technique. Some authors suggest that the truncation of the
author profiles, as it was originally proposed, can be safely ignored [12]. However, we
have our doubts whether this is generally advisable. Other research pointed out that in
cross-topic attribution low frequency n-grams should be avoided, since they are closely
relation to the context of the training data [74]. This may also be also the case in source
code.
We now consider some limitations of the domain specific authorship technique which
is based on an AST language representation. The major drawback of this approach
relates to the way the feature space was established. Therefore we suggest some ways to
improve this technique:
• In the domain specific approach, the feature values are splitted into a number of
histogram intervals. Literature shows that the particular choice for bin intervals
2http://prototypejs.org
64
6.6. Limitations and threats to validity
can improve the classification [70]. Therefore, optimizing the discretization of
the interval ranges may be a way to improve the performance. Furthermore, in
the current feature representation, the boundaries of the histogram intervals are
non-overlapping. This may conceal the relation between the feature values. For
instance, consider the case that candidate authors a1 and a2 use variable names
of character length 1–2 and 10–12 respectively. Then, from the current feature
representation it can not be derived that a third author that used variable length
3–4 is most similar to author a1, since the bins are considered as distinct features
by the SVM classifier.
• The experimental results showed the high impact of layout related features. How-
ever, we only counted the number of spaces, tabs and cariage returns at each layout
slot. As a result, the order of these characters was not tracked. Describing the lay-
out by character n-grams may be a better way, since they also capture the lexical
order of the characters.
Finally, the accuracy performance measure that was used to evaluate the performance
of the identification problem may be not the adequate one in a forensic investigation.
Some studies considered the ranking of authors by likelihood, rather than only identifying
the most likely author, e.g. [63]. A ranked result would help to obtain the confidence of
the authorship of a document for each individual author in the investigation.
65

7
Conclusions and Future Work
The main aim of this thesis has been to investigate different authorship analysis tech-
niques on JavaScript, and to develop a domain specific technique that is based on the
AST program representation. In this chapter we summarize the main results obtained
in this study and suggest some future research directions.
7.1 Contributions
The findings from this study make several contributions to the current literature:
• Most previous contributions on source code authorship analysis targeted program-
ming compiled languages. In contrast, JavaScript is an interpreted language, which
means that no pre-runtime translation takes place and the source code is executed
without translating the source code to machine-language. Consequently, source
code is more likely available than the compiled languages that were under invesi-
gation in other studies. Moreover, JavaScript is widely used on the Web. Hence,
suspect code can likely be found in practice.
• We investigated different classification techniques. The present study confirms
several findings of previous studies that compared similarity based and machine
learning authorship analysis techniques. We showed the effectivity of n-gram based
techniques and compared similarity and feature vector based techniques.
• Authorship identification has been addressed in a number of previous studies. How-
ever, little attention has been paid to authorship verification, which we addressed
in our study.
• Unlike natural languages, the definition of programming languages is much more
formal, and can unambiguously be parsed. Therefore, we proposed to represent
the source by an Abstract Syntax Tree prior to feature extraction. A document
representation by an AST enables to extract structural features from the parse
67
7. Conclusions and Future Work
tree easily. By our knownledge no studies have considered such structures in the
parse tree.
• By means of JavaScript code minification we evalated how the classification accu-
racy is affected by removing authorship information from the source code.
• We determined for four different authorship analysis techniques what was the most
accurate technique with original and minified code, and with a different amount
of training data.
7.2 Research results
The present study was designed to compare four different JavaScript source code author-
ship analysis techniques. The investigated authorship identification techniques include
one similarity based technique, where classification is done by a similarity metric, and
three feature vector based techniques that employ an SVM for classification. The simi-
larity based technique is based on character n-gram features. The feature vector based
techniques consist of a character n-gram based technique, a compression based technique,
and a domain specific technique which is based on an AST source code representation.
In the introduction we defined five research questions that would bring us closer to an
answer on the main research question. We now review each of these questions.
Research Question 1. What is the state of the art in software authorship analysis?
We addressed this research question in Chapter 2 by reviewing the literature. In
summary, we explored similarity and feature vector based classification techniques in lit-
erature, and investigated different feature types that have been used in the classification
process. In general, the works that use features from a low language abstraction level in
the form of n-grams, have shown to achieve the highest accuracies.
Research Question 2. What is the difference in performance between similarity based
and feature vector based techniques, by considering the number of candidate authors and
the number of available training samples per author?
The approaches that use character n-gram features achieved the best performance
in either authorship identification and verification. The similarity based technique can
especially handle cases where a very limited amount of training data per author is
available, while the feature vector based techniques that were based on character n-
grams and domain specific features were slightly more effective with a large number of
training samples. We have not encounted striking differences between the techniques
related to the number of candidate authors in the classification problem.
Research Question 3. What is the effectivity of domain specific JavaScript features
compared to character based features?
68
7.3. Main conclusions
The techniques that used character n-gram features achieved a higher performance
than the domain specific approach. However, we showed that the combination of n-gram
and domain specific features is complementary, resulting in a higher performance. The
compression based technique turned out to be the least effective authorship analysis
technique. The investigation of the domain specific approach has shown that the layout
features were the strongest markers of writing style. Also structures in the parse tree
have shown to be effective markers of the coding style of an author.
Research Question 4. What is the effectivity of authorship analysis techniques in
authorship verification compared to authorship identification?
The character n-gram based techniques obtained the highest accuracies in authorship
verification as was also the case in authorship identification. In contrast to authorship
identification, the feature vector based approaches turned out to be substantially better
than the similarity based approach.
Research Question 5. What is the influence of code collaboration and code minification
on the classification performance?
Code minification has shown to significantly deteriorate the performance of the au-
thorship analysis methods. Still, the effectivity of the techniques is reasonable. Espe-
cially the compression based technique is robust against code minification. In authorship
verification, the compression based technique was the second best technique with mini-
fied code, following the combination of domain specific and n-gram based features. The
current study was unable to analyse authorship identification within projects. This is
an important issue for further research.
7.3 Main conclusions
This work has demonstrated that JavaScript source code of unknown authorship can
effectively be attributed to the respective developer. The main research question of our
research relates to the effectivity of the analysis techniques on JavaScript source code:
Main Research Question. To what extent are authorship analysis techniques effective
on JavaScript source code?
The results indicate that an adequate coding style signature of JavaScript developers
can be constructed if a sufficient amount of sample code is available. The extent to which
the different authorship analysis techniques are effective depend on the problem context,
and can be summarized as follows:
• In authorship identification, the n-gram based techniques obtain the highest accu-
racies. If a limited number of training samples are available the similarity based
technique is better, while with a large number of training samples the feature vector
based technique is better. In the latter case, the domain specific features can help
to improve the classification accuracy. With minified code the same techniques are
effective.
69
7. Conclusions and Future Work
• In authorship verification a distinction should be made between original and mini-
fied JavaScript code when considering which techniques are most effective. In
the original code, the n-gram based techniques obtained the highest AUC-values.
With minified code, the compression based technique was the best. However, if fea-
ture types are combined, the combination of character n-gram and domain specific
features achieve an even higher performance.
7.4 Future work
The research presented in this thesis has raised several questions that provide the basis
for further research. First, the techniques evaluated in our study should be validated on
datasets that are more representative for malicious source code, as our dataset consisted
of open source software. Next, the similarity based technique we evaluated is also the
only technique that uses a profile based data sampling. The similarity based technique of
Burrows [11] that was presented in Section 2.2.1 is instance based. A comparison between
the presented techniques and this technique would give more insight in the difference
between similarity and feature vector based classification. Moreover, the variability in
the validation results should be investigated. To reduce variablility, we averaged the
results over multiple rounds. Further research is needed to determine the origin of the
variability. For instance, what code is hard to identify, and what is the influence of the
used frameworks on the classification accuracy? Additionally, in Chapter 6 we indicated
a number limitations of the authorship analysis techniques. Addressing these limitations
could be a good way to improve the performance of the current techniques. Especially
the domain specific technique, that was developed by us, provides several opportunities
for future research. Since little work has done in tree structures in the AST of source
code, the selected tree structure features were defined in a rather intuitive way. There
is a large volume of published studies that consider the classification of tree patterns
[43]. Although the current study shows the effectivity of using structures in the parse
tree, more thorough structural features could be investigated. Finally, a domain specific
authorship identification technique could especially be of interest in a cross language
setting. In this case, the syntax definitions of the languages can be quite different,
while the underlying structures may remain stable per author over different languages.
Further research regarding cross-language authorship analysis in source code would be
worthwhile.
70
Bibliography
[1] Ng Andrew. Support vector machines. http://cs229.stanford.edu/notes/
cs229-notes3.pdf.
[2] Shlomo Argamon and Patrick Juola. Overview of the international authorship iden-
tification competition at pan-2011. In CLEF (Notebook Papers/Labs/Workshop),
2011.
[3] Harald Baayen, Hans Van Halteren, and Fiona Tweedie. Outside the cave of shad-
ows: Using syntactic annotation to enhance authorship attribution. Literary and
Linguistic Computing, 11(3):121–132, 1996.
[4] Ohad Barzilay, Orit Hazzan, and Amiram Yehudai. Characterizing example embed-
ding as a software activity. In Proceedings of the 2009 ICSE Workshop on Search-
Driven Development-Users, Infrastructure, Tools and Evaluation, pages 5–8. IEEE
Computer Society, 2009.
[5] Mathieu Bastian, Sebastien Heymann, Mathieu Jacomy, et al. Gephi: an open
source software for exploring and manipulating networks. 2009.
[6] Ruud Bolle. Guide to biometrics. Springer, 2004.
[7] Joel Brandt, Philip J Guo, Joel Lewenstein, Mira Dontcheva, and Scott R Klemmer.
Two studies of opportunistic programming: interleaving web foraging, learning,
and writing code. In Proceedings of the SIGCHI Conference on Human Factors in
Computing Systems, pages 1589–1598. ACM, 2009.
[8] Michael Robert Brennan and Rachel Greenstadt. Practical attacks against author-
ship recognition techniques. In IAAI, 2009.
[9] Steven Burrows. Source code authorship attribution. PhD thesis, Ph. D. thesis,
School of Computer Science and Information Technology, RMIT University, Mel-
bourne, Australia, 2010.
71
Bibliography
[10] Steven Burrows and Seyed MM Tahaghoghi. Source code authorship attribution
using n-grams. In Proceedings of the Twelth Australasian Document Computing
Symposium, Melbourne, Australia, RMIT University, pages 32–39, 2007.
[11] Steven Burrows, Alexandra L Uitdenbogerd, and Andrew Turpin. Application of
information retrieval techniques for source code authorship attribution. In Database
Systems for Advanced Applications, pages 699–713. Springer, 2009.
[12] Steven Burrows, Alexandra L Uitdenbogerd, and Andrew Turpin. Comparing tech-
niques for authorship attribution of source code. Software: Practice and Experience,
44(1):1–32, 2014.
[13] Chih-Chung Chang and Chih-Jen Lin. LIBSVM: A library for support vector ma-
chines. ACM Transactions on Intelligent Systems and Technology, 2:27:1–27:27,
2011. Software available at http://www.csie.ntu.edu.tw/~cjlin/libsvm.
[14] Moses S Charikar. Similarity estimation techniques from rounding algorithms. In
Proceedings of the thiry-fourth annual ACM symposium on Theory of computing,
pages 380–388. ACM, 2002.
[15] Evangelos Chatzicharalampous, Georgia Frantzeskou, and Efstathios Stamatatos.
Author identification in imbalanced sets of source code samples. In Tools with Arti-
ficial Intelligence (ICTAI), 2012 IEEE 24th International Conference on, volume 1,
pages 790–797. IEEE, 2012.
[16] Alan H Cheetham and Joseph E Hazel. Binary (presence-absence) similarity coef-
ficients. Journal of Paleontology, pages 1130–1136, 1969.
[17] William Cohen, Pradeep Ravikumar, and Stephen Fienberg. A comparison of string
metrics for matching names and records. In KDD Workshop on Data Cleaning and
Object Consolidation, volume 3, pages 73–78, 2003.
[18] Ramon de Graaff and Cor J Veenman. Bootstrapped authorship attribution in
compression space. In CLEF (Online Working Notes/Labs/Workshop), 2012.
[19] Haibiao Ding and Mansur H Samadzadeh. Extraction of java program fingerprints
for software authorship identification. Journal of Systems and Software, 72(1):49–
57, 2004.
[20] R Duin, P Juszczak, P Paclik, E Pekalska, D de Ridder, D Tax, and S Verzakov.
Prtools 4.1. A Matlab Toolbox for Pattern Recognition, Software and Documentation
downloaded May, 2010.
[21] Robert PW Duin and David MJ Tax. Classifier conditional posterior probabilities.
In Advances in pattern recognition, pages 611–619. Springer, 1998.
[22] Bruce S Elenbogen and Naeem Seliya. Detecting outsourced student programming
assignments. Journal of Computing Sciences in Colleges, 23(3):50–57, 2008.
72
Bibliography
[23] Ward EY Elliott and Robert J Valenza. And then there were none: Winnowing the
shakespeare claimants. Computers and the Humanities, 30(3):191–245, 1996.
[24] Tom Fawcett. An introduction to roc analysis. Pattern recognition letters,
27(8):861–874, 2006.
[25] Tristan Fletcher. Support vector machines explained. Tutorial paper., Mar, 2009.
[26] Georgia Frantzeskou, Stephen MacDonell, Efstathios Stamatatos, and Stefanos
Gritzalis. Examining the significance of high-level programming features in source
code author classification. Journal of Systems and Software, 81(3):447–460, 2008.
[27] Georgia Frantzeskou, Stephen G MacDonell, Efstathios Stamatatos, Stelios Geor-
giou, and Stefanos Gritzalis. The significance of user-defined identifiers in java
source code authorship identification. 2011.
[28] Georgia Frantzeskou, Efstathios Stamatatos, Stefanos Gritzalis, Carole E Chaski,
and Blake Stephen Howald. Identifying authorship by byte-level n-grams: The
source code author profile (scap) method. International Journal of Digital Evidence,
6(1):1–18, 2007.
[29] Georgia Frantzeskou, Efstathios Stamatatos, Stefanos Gritzalis, and Sokratis Kat-
sikas. Source code author identification based on n-gram author profiles. In Artificial
Intelligence Applications and Innovations, pages 508–515. Springer, 2006.
[30] Jerome H Friedman. On bias, variance, 0/1—loss, and the curse-of-dimensionality.
Data mining and knowledge discovery, 1(1):55–77, 1997.
[31] Georgios Gousios. The ghtorent dataset and tool suite. In Proceedings of the 10th
Working Conference on Mining Software Repositories, pages 233–236. IEEE Press,
2013.
[32] Georgios Gousios, Bogdan Vasilescu, Alexander Serebrenik, and Andy Zaidman.
Lean ghtorrent: Github data on demand. In Proceedings of the 11th Working Con-
ference on Mining Software Repositories, pages 384–387. ACM, 2014.
[33] Andrew Gray, Philip Sallis, and Stephen MacDonell. Software forensics: Extending
authorship analysis techniques to computer programs. 1997.
[34] Jack Grieve. Quantitative authorship attribution: An evaluation of techniques.
Literary and linguistic computing, 22(3):251–270, 2007.
[35] Jane Huffman Hayes and Jeff Offutt. Recognizing authors: an examination of the
consistent programmer hypothesis. Software Testing, Verification and Reliability,
20(4):329–356, 2010.
[36] Chih-Wei Hsu and Chih-Jen Lin. A comparison of methods for multiclass support
vector machines. Neural Networks, IEEE Transactions on, 13(2):415–425, 2002.
73
Bibliography
[37] Thorsten Joachims. Text categorization with support vector machines: Learning
with many relevant features. Springer, 1998.
[38] Patrick Juola. Authorship attribution. Foundations and Trends in information
Retrieval, 1(3):233–334, 2006.
[39] Patrick Juola and Efsthathios Stamatos. Overview of the author identification task
at pan 2013. In Information Access Evaluation. Multilinguality, Multimodality, and
Visualization. 4th International Conference of the CLEF Initiative, CLEF, pages
23–26, 2013.
[40] Arkadi Kagan. Ppm - prediction by partial matching. http://compressions.
sourceforge.net/PPM.html. Accessed: 2014-08-22.
[41] Andreas Kaster, Stefan Siersdorfer, and Gerhard Weikum. Combining text and
linguistic document representations for authorship attribution. In SIGIR workshop:
Stylistic Analysis of Text For Information Access, 2005.
[42] Vlado Kešelj, Fuchun Peng, Nick Cercone, and Calvin Thomas. N-gram-based
author profiles for authorship attribution. In Proceedings of the conference pacific
association for computational linguistics, PACLING, volume 3, pages 255–264, 2003.
[43] Nikhil S Ketkar, Lawrence B Holder, and Diane J Cook. Empirical comparison of
graph classification algorithms. In Computational Intelligence and Data Mining,
2009. CIDM’09. IEEE Symposium on, pages 259–266. IEEE, 2009.
[44] Shehroz S Khan and Michael G Madden. A survey of recent trends in one class clas-
sification. In Artificial Intelligence and Cognitive Science, pages 188–197. Springer,
2010.
[45] RI Kilgour, AR Gray, PJ Sallis, and SG MacDonell. A fuzzy logic approach to
computer software source code authorship analysis. In Proceedings of the Fourth In-
ternational Conference on Neural Information Processing, pages 865–868. Springer-
Verlag, 1998.
[46] Sangkyum Kim, Hyungsul Kim, Tim Weninger, Jiawei Han, and Hyun Duk Kim.
Authorship classification: a discriminative syntactic tree mining approach. In Pro-
ceedings of the 34th international ACM SIGIR conference on Research and devel-
opment in Information Retrieval, pages 455–464. ACM, 2011.
[47] Ron Kohavi et al. A study of cross-validation and bootstrap for accuracy estimation
and model selection. In International Joint Conferences on Artificial Intelligence,
volume 14, pages 1137–1145, 1995.
[48] Moshe Koppel, Jonathan Schler, and Shlomo Argamon. Computational methods
in authorship attribution. Journal of the American Society for information Science
and Technology, 60(1):9–26, 2009.
74
Bibliography
[49] Moshe Koppel, Jonathan Schler, and Shlomo Argamon. Authorship attribution in
the wild. Language Resources and Evaluation, 45(1):83–94, 2011.
[50] Moshe Koppel, Jonathan Schler, Shlomo Argamon, and Eran Messeri. Authorship
attribution with thousands of candidate authors. In Proceedings of the 29th annual
international ACM SIGIR conference on Research and development in information
retrieval, pages 659–660. ACM, 2006.
[51] Moshe Koppel and Yaron Winter. Determining if two documents are written by the
same author. Journal of the Association for Information Science and Technology,
65(1):178–187, 2014.
[52] Jay Kothari, Maxim Shevertalov, Edward Stehle, and Spiros Mancoridis. A prob-
abilistic approach to source code authorship identification. In Information Tech-
nology, 2007. ITNG’07. Fourth International Conference on, pages 243–248. IEEE,
2007.
[53] Ivan Krsul and Eugene H Spafford. Authorship analysis: Identifying the author of
a program. Computers & Security, 16(3):233–257, 1997.
[54] Maarten Lambers and Cor J Veenman. Forensic authorship attribution using
compression distances to prototypes. In Computational Forensics, pages 13–24.
Springer, 2009.
[55] Robert Charles Lange and Spiros Mancoridis. Using code metric histograms and
genetic algorithms to perform author identification for software forensics. In Pro-
ceedings of the 9th annual conference on Genetic and evolutionary computation,
pages 2082–2089. ACM, 2007.
[56] Jonas Lindh Morén. The application of closed frequent subtrees to authorship
attribution. 2014.
[57] Stephen G MacDonell and Andrew R Gray. Software forensics applied to the task
of discriminating between program authors. 2001.
[58] Christopher D Manning, Prabhakar Raghavan, and Hinrich Schütze. Introduction
to information retrieval, volume 1. Cambridge university press Cambridge, 2008.
[59] Sean Massung, Chengxiang Zhai, and Julia Hockenmaier. Structural parse tree fea-
tures for text representation. In Semantic Computing (ICSC), 2013 IEEE Seventh
International Conference on, pages 9–16. IEEE, 2013.
[60] Charles E Metz. Basic principles of roc analysis. In Seminars in nuclear medicine,
volume 8, pages 283–298. Elsevier, 1978.
[61] Dunja Mladenić, Janez Brank, Marko Grobelnik, and Natasa Milic-Frayling. Fea-
ture selection using linear classifier weights: interaction with classification models.
In Proceedings of the 27th annual international ACM SIGIR conference on Research
and development in information retrieval, pages 234–241. ACM, 2004.
75
Bibliography
[62] Andrew Mohan and Nicolas Gold. Programming style changes in evolving source
code. In Program Comprehension, 2004. Proceedings. 12th IEEE International
Workshop on, pages 236–240. IEEE, 2004.
[63] Arvind Narayanan, Hristo Paskov, Neil Zhenqiang Gong, John Bethencourt, Emil
Stefanov, Eui Chul Richard Shin, and Dawn Song. On the feasibility of internet-
scale author identification. In Security and Privacy (SP), 2012 IEEE Symposium
on, pages 300–314. IEEE, 2012.
[64] Paul W Oman and Curtis R Cook. Programming style authorship analysis. In
Proceedings of the 17th conference on ACM Annual Computer Science Conference,
pages 320–326. ACM, 1989.
[65] Nektaria Potha and Efstathios Stamatatos. A profile-based method for authorship
verification. In Artificial Intelligence: Methods and Applications, pages 313–326.
Springer, 2014.
[66] David Martin Powers. Evaluation: from precision, recall and f-measure to roc,
informedness, markedness and correlation. 2011.
[67] Nathan Rosenblum, Xiaojin Zhu, and Barton P Miller. Who wrote this code?
identifying the authors of program binaries. In Computer Security–ESORICS 2011,
pages 172–189. Springer, 2011.
[68] Maytal Saar-Tsechansky and Foster Provost. Handling missing values when apply-
ing classification models. 2007.
[69] Madeleine Sabordo, Shong Y Chai, Matthew J Berryman, and Derek Abbott. Who
wrote the letter to the hebrews?: data mining for detection of text authorship. In
Smart Materials, Nano-, and Micro-Smart Systems, pages 513–524. International
Society for Optics and Photonics, 2005.
[70] Maxim Shevertalov, Jay Kothari, Edward Stehle, and Spiros Mancoridis. On the
use of discretized source code metrics for author identification. In Search Based
Software Engineering, 2009 1st International Symposium on, pages 69–78. IEEE,
2009.
[71] Grigori Sidorov, Francisco Velasquez, Efstathios Stamatatos, Alexander Gelbukh,
and Liliana Chanona-Hernández. Syntactic n-grams as machine learning features
for natural language processing. Expert Systems with Applications, 41(3):853–860,
2014.
[72] Eugene H Spafford and Stephen A Weeber. Software forensics: Can we track code
to its authors? Computers & Security, 12(6):585–595, 1993.
[73] Efstathios Stamatatos. A survey of modern authorship attribution methods. Journal
of the American Society for information Science and Technology, 60(3):538–556,
2009.
76
Bibliography
[74] Efstathios Stamatatos. On the robustness of authorship attribution based on char-
acter n-gram features. 2013.
[75] D.M.J. Tax. Ddtools, the data description toolbox for matlab, July 2014. version
2.1.1.
[76] William J Teahan and David J Harper. Using compression-based language models
for text categorization. In Language Modeling for Information Retrieval, pages
141–165. Springer, 2003.
[77] Matthew F Tennyson. On improving authorship attribution of source code. In
Digital Forensics and Cyber Crime, pages 58–65. Springer, 2013.
[78] Patrick D Terry. Compilers and compiler generators, 1996.
[79] S. Theodoridis and K. Koutroumbas. Pattern Recognition. Elsevier Science, 2008.
[80] Michael Tschuggnall and Günther Specht. Enhancing authorship attribution by
utilizing syntax tree profiles. EACL 2014, page 195, 2014.
[81] Guido Wachsmuth. Language. 2011.
[82] Henry C Williams, Joi N Carter, Willie L Campbell, Kaushik Roy, and Gerry V
Dozier. Genetic & evolutionary feature selection for author identification of html
associated with malware. International Journal of Machine Learning & Computing,
4(3), 2014.
[83] Li Z. An exploratory study on authorship verification models for forensic purpose.
2013. Retrieved from http://repository.tudelft.nl.
[84] Rong Zheng, Jiexun Li, Hsinchun Chen, and Zan Huang. A framework for au-
thorship identification of online messages: Writing-style features and classification
techniques. Journal of the American Society for Information Science and Technol-
ogy, 57(3):378–393, 2006.
77

A
Abstract Syntax of JavaScript
A.1 Syntax definition
This section describes JavaScript its abstract syntax used for feature generation. The
abstract grammar follows the abstact syntax made available in the Mozilla SpiderMonkey
parser API 1. In the used notation, the left hand side is the node-type. The right hand
side describes the possible child node types that are related to the node by the node
attributes (in cursive). A list of child nodes are indicated by an asterix (*). Furthermore,
the possible types of child nodes are separated by a vertical bar (|). An optional child-
node is indicated by an empty node, the NullNode.
Program → body: Statement*
BlockStatement → body: Statement*
ExpressionStatement → expression: Expression
IfStatement → test: Expression
consequent: Statement
alternate: Statement | NullNode
LabeledStatement → label: Identifier
body: Statement
BreakStatement → label: Identifier | NullNode
ContinueStatement → label: Identifier | NullNode
WithStatement → object: Expression
body: Statement
SwitchStatement → discriminant: Expression
cases: SwitchCase*
SwitchCase → test: Expression | NullNode
consequent: Statement*
1https://developer.mozilla.org/en/SpiderMonkey/Parser_API
79
A. Abstract Syntax of JavaScript
ReturnStatement → argument: Expression | NullNode
ThrowStatement → argument: Expression
TryStatement → block: BlockStatement
handler: CatchClause | NullNode
finalizer: BlockStatement | NullNode
CatchClause → param: Identifier
body: BlockStatement
WhileStatement → test: Expression
body: Statement
DoWhileStatement → test: Expression
body: Statement
ForStatement → init: VariableDeclaration | Expression | NullNode
test: Expression | NullNode
update: Expression | NullNode
body: Statement
ForInStatement → left: VariableDeclaration | Expression
right: Expression
body: Statement
FunctionDeclaration → id: Identifier
params: Identifier*
body: BlockStatement | Expression
VariableDeclaration → declaration: VariableDeclarator*
kind: "var" | "let" | "const"
VariableDeclarator → id: Identifier
init: Expression | NullNode
ArrayExpression → elements: (Expression | NullNode)*
ObjectExpression → properties: Property*
Property → key: Literal | Identifier
value: Expression | GetSetFunctionExpression
FunctionExpression → id: Identifier | NullNode
params: Identifier*
body: BlockStatement | Expression
ArrowFunctionExpression → params: Identifier*
body: BlockStatement | Expression
SequenceExpression → expressions: Expression*
UnaryExpression → operator: UnaryOperator
prefix: boolean
argument: Expression
AssignmentExpression → operator: AssignmentOperator
left: Expression
right: Expression
80
A.1. Syntax definition
UpdateExpression → operator: UpdateOperator
prefix: true | false
argument: Expression
LogicalExpression → operator: LogicalOperator
left: Expression
right: Expression
ConditionalExpression → test: Expression
alternate: Expression
consequent: Expression
NewExpression → callee: Expression
arguments: Expression*
CallExpression → callee: Expression
arguments: Expression*
MemberExpression → object: Expression
property: Expression
Identifier → name: string
Literal → value: string | boolean | null | number
raw: string
The following right hand side values are abbreviations and are defined as:
Statement = EmptyStatement | BlockStatement | ExpressionStatement | IfState-
ment | LabeledStatement | BreakStatement | ContinueStatement
| WithStatement | SwitchStatement | ReturnStatement | Throw-
Statement | TryStatement | WhileStatement | DoWhileStatement |
ForStatement | ForInStatement | ForOfStatement | LetStatement |
DebuggerStatement | FunctionDeclaration | VariableDeclaration
Expression = ThisExpression | ArrayExpression | ObjectExpression | FunctionExpres-
sion | ArrowExpression | SequenceExpression | UnaryExpression | Bi-
naryExpression | AssignmentExpression | UpdateExpression | Logical-
Expression | ConditionalExpression | NewExpression | CallExpression |
MemberExpression | (YieldExpression) | (ComprehensionExpression) |
(GeneratorExpression) | (GraphExpression) | (GraphIndexExpression)
| (LetExpression) | Identifier | Literal
UnaryOperator = "-" | "+" | "!" | "∼" | "typeof" | "void" | "delete"
BinaryOperator = "==" | "!=" | "===" | "!==" | "<" | "<=" | ">" | ">=" | "<<" |
">>" | ">>>" | "+" | "-" | "*" | "/" | "%" | "|" | ""̂ | "&" | "in" |
"instanceof" | ".."
LogicalOperator = "||" | "&&"
AssignmentOperator = "++" | "- -"
81
A. Abstract Syntax of JavaScript
A.2 Refactorings to the AST
The following list shows the most important refactorings performed on the original
Mozilla parse tree.
• The MemberExpression node has the boolean attribute computed, which repre-
sents a member accessed by an identifier or by an expression. In the latter case
computed is true. We represent computed member expressions by the node type
CMemberExpression, with associated layout slots.
• Nested parenthesis in expressions do not appear in the original parse tree. We
refactored the tree such that an expression in a nested parenthesis is described in
a BracketExpression. We also defined layout slots in this node type.
• Empty attributes are described by the null value. Instead, we represented this by
a node of the type NullNode.
• The operator in an UnaryExpression, BinaryExpression, LogicalExpression, As-
signmentExpression and UpdateExpression is converted from a textual value (e.g.
‘*’) to a node with a type of the operator.
• The variable kind (var, const and let) in a VariableDeclaration is represented by a
node.
• A property in an object initialization can also be a function acting as getter or
setter such as in the following code fragment:
var person = {
firstName:undefined ,
lastName:undefined ,
get fullName (){
...
},
set fullName(name) {
...
}
}
We represented this by by a GetProperty or SetProperty respectively. Further-
more, the function declaration for the getters and setters deviate from normal
function definitions in the source code, so we represented these as as GetSetFunc-
tionExpression.
• Multiple catch clauses in a try statement are SpiderMonkey-specific. Therefore we
refactored it in order to contain only one catch clause. Also, some statements and
expressions are SpiderMonkey-specific, such as the let statement and array compre-
hension expressions. Furthermore, JavaScript 1.7 introduced destructuring assign-
ment and binding forms, which had not been implemented in Esprima at time of
writing of this work. Alltogether, the follow node types in the Mozilla-AST are not
82
A.3. Layout nodes
considered: LetStatement, ArrowExpression, YieldExpression, ComprehensionEx-
pression, GeneratorExpression, GraphExpression, LetExpression, ObjectPattern
and ArrayPattern.
A.3 Layout nodes
The Layout nodes are added as children to the related nodes in the AST. In the following
table the introduced nodes are listed. In this table, the subscripted indices indicate
locations where the developer may have inserted spacing. We call these locations layout
slots. In case a node has a list of child nodes (e.g. between function arguments) with
layout in between, a list of layout nodes is generated. This is indicated by a superscript
asterix.
Node Layout slots
Program body∗
BlockStatement {1body∗2}
IfStatement if1(2test3)4consequent
WithStatement with1(2object3)4body
TryStatement try1block2handler
CatchClause catch1(2param3)4body
WhileStatement while1(2test3)4body
DoWhileStatement do1body2while3(4test5)
ForStatement for1(2init3;4test5;6update7)8body
ForInStatement for1(2left3in4right5)6body
FunctionDeclaration function1id2(3params∗4)5body
VariableDeclaration kind1declarations∗
ObjectExpression {1properties∗2}
Property key1:2value
ArrayExpression [1elements∗2]
FunctionExpression function1id2(3params∗4)5body
UnaryExpression operator1argument
BinaryExpression left1operator2right
AssignmentExpression left1operator2right
LogicalExpression left1operator2right
ConditionalExpression test1?2consequent3:4alternate
CallExpression callee1(2arguments∗3)
MemberExpression object1.2property
CMemberExpression object1[2property3]
SequenceExpression expressions∗
SwitchStatement switch1(2discriminant3)4{5cases∗6}
83
A. Abstract Syntax of JavaScript
BracketExpression (1value2)
ReturnStatement return1argument2
ThrowStatement throw1argument2
UpdateExpression operator1argument
VariableDeclarator id1=2init
SwitchCase case1test2:3consequent
NewExpression new1callee2(3arguments∗4)
84
B
Domain Specific Features
In this appendix the domain specific features obtained from the AST are listed. For
every feature we describe which nodes are considered in this feature. Then we examine
the attribute value of the selected node and increment the number of observations of the
histogram category that applies.
B.1 Layout
By means of the value attribute in the introduced layout nodes, we identified for each
feature the frequency of using layout. More precisely, we track for each feature the
frequency that no layout was used, or that zero, one, two or many spaces, tabs, or
carriage returns were used. We defined the layout features shown in the following table.
The numbers refer to the defined layout nodes (see next).
In addition, the ExpressionStatement, DebuggerStatement, ContinueStatement, Break-
Statement and LabeledStatement are optionally closed by a semicolon. Therefore we
track the use of semicolons and other spacing after these statement types.
Feature Selected nodes
1 Layout between statements Program.*, BlockStatement.*
2 Layout behind opening curly
bracket ({) in block
BlockStatement.1, SwitchStatement.1
3 Layout before opening curly
bracket ({) in block
BlockStatement.2, SwitchStatement.5
4 Layout between keyword/id and
opening parenthesis
IfStatement.1, WithStatement.1, CatchClause.1,
WhileStatement.1, DoWhileStatement.3, ForStatement.1,
ForInStatement.1, SwitchStatement.1, FunctionDeclaration.2,
FunctionExpression.2, CallExpression.1, NewExpression.2
85
B. Domain Specific Features
5 Layout behind opening parenthe-
sis
IfStatement.2, WithStatement.l1, CatchClause.2,
WhileStatement.2, DoWhileStatement.4, ForStatement.2,
ForInStatement.2, SwitchStatement.2, FunctionDeclaration.3,
FunctionExpression.3, CallExpression.2, NewExpression.3,
BracketExpression.4
6 Layout before closing parenthe-
sis
IfStatement.3, WithStatement.3, CatchClause.3,
WhileStatement.3, DoWhileStatement.5, ForStatement.7,
ForInStatement.5, SwitchStatement.3, FunctionDeclaration.4,
FunctionExpression.4, CallExpression.3, NewExpression.4,
BracketExpression.3
7 Layout between closing paren-
thesis and body
IfStatement.4, WithStatement.4, CatchClause.4,
WhileStatement.4, ForStatement.8, ForInStatement.6,
SwitchStatement.4, FunctionDeclaration.5,
FunctionExpression.5
8 Layout before semicolon in for-
statement
ForStatement.3, ForStatement.5
9 Layout behind semicolon in for-
statement
ForStatement.4, ForStatement.6
10 Layout before comma in function
parameters
FunctionDeclaration.*, FunctionExpression.*
11 Layout behind comma in func-
tion parameters
FunctionDeclaration.*, FunctionExpression.*
12 Layout before comma in func-
tion/object arguments
CallExpression.*, NewExpression.*
13 Layout behind comma in func-
tion/object arguments
CallExpression.*, NewExpression.*
14 Layout before comma in variable
declarations
VariableDeclaration.*
15 Layout behind comma in variable
declarations
VariableDeclaration.*
16 Layout before comma in Object
and Array Expressions
ObjectExpression.*, ArrayExpression.*
17 Layout behind comma in Object
and Array Expressions
ObjectExpression.*, ArrayExpression.*
18 Layout before properties c.q. el-
ements in Object and Array Ex-
pressions
ObjectExpression.1, ArrayExpression.1
19 Layout behind properties c.q. el-
ements in Object and Array Ex-
pressions
ObjectExpression.2, ArrayExpression.2
20 Layout before operators BinaryExpression.1, AssignmentExpression.1,
LogicalExpression.1, ConditionalExpression.1,
ConditionalExpression.3, UpdateExpression.1,
VariableDeclarator.1, ForInStatement.3
86
B.2. String patterns
21 Layout behind operators UnaryExpression.1, BinaryExpression.2, AssignmentExpression.2,
LogicalExpression.2, ConditionalExpression.2,
ConditionalExpression.4, UpdateExpression.1,
VariableDeclarator.2, ForInStatement.4
22 Layout before dot in MemberExp MemberExpression.1
23 Layout behind dot in Member-
Exp
MemberExpression.2
24 Layout behind statement ExpressionStatement, DebuggerStatement, ContinueStatement,
BreakStatement, LabeledStatement, ReturnStatement,
ThrowStatement
25 Layout between keyword and ex-
pression
NewExpression.1, FunctionDeclaration.1, FunctionExpression.1,
SwitchCase.1, VariableDeclaration.1, ReturnStatement.1,
ThrowStatement.1
26 Layout around colon SwitchCase.2, SwitchCase.3, Property.1, Property.2
B.2 String patterns
For the following features we track the frequency of matching regular expressions in the
attribute value of the selected nodes.
Feature Selected nodes Attribute Categories
27 First character
in identifiers
related to
function
identifiers
NewExpression.callee, CallExpression.callee,
FunctionExpression.id,
FunctionDeclaration.id / Identifier
name /ˆ[A-Z]/,
/ˆ[a-z]/,
/ˆ[ˆA-Za-z]/
28 First character
in identifiers
related to
objects
WhileStatement.test, IfStatement.test,
ForStatement.init,
ReturnStatement.argument,
VariableDeclarator.id,
FunctionDeclaration.params,
CallExpression.arguments,
NewExpression.arguments,
FunctionExpression.params and every
expression except CallExpression.callee,
NewExpression.callee, FunctionExpression.id
/ Identifier
name /ˆ[A-Z]/,
/ˆ[a-z]/,
/ˆ[ˆA-Za-z]/
29 Number of
capitals in
identifiers
Identifier name /[A-Z]/g
matching
0,1,2,3,4,5,6,8,9,∞
times
30 Number of
non-letter
characters in
identifiers
Identifier name /ˆ[ˆA-Za-z]/g
matching
0,1,2,∞ times
87
B. Domain Specific Features
31 Literal types Literal raw String with
double quotes,
string with
single quotes,
null, number
or regex1
B.3 Comments
Feature Selected nodes Attribute Categories
32 The distribution of
the parent nodes of
block comments
BlockComment parent ArrayExpression, ObjectExpression,
FunctionExpression, CallExpression,
MemberExpression, BlockStatement,
ExpressionStatement, SwitchStatement,
FunctionDeclaration,
VariableDeclaration, SwitchCase,
Program and ‘Otherwise’
33 The distribution of
the parent nodes of
line comments
LineComment parent ArrayExpression, ObjectExpression,
FunctionExpression, BinaryExpression,
LogicalExpression,
ConditionalExpression, NewExpression,
CallExpression, MemberExpression,
BracketExpression, BlockStatement,
ExpressionStatement, IfStatement,
SwitchStatement, ReturnStatement,
ThrowStatement, FunctionDeclaration,
VariableDeclaration, SwitchCase,
Program and ‘Otherwise’.
34 The ratio between
line and block
comments
BlockComment |
LineComment
type BlockComment, LineComment
35 The length of block
comments
BlockComment value 0, 8, 16, 32, 64, 128, 256, 512, 1024, ∞
36 The length of line
comments
LineComment value 0, 8, 16, 32, 64, 128, 256, 512, ∞
B.4 String length
For the following features we track the length of the given attribute.
Feature Selected nodes Attribute Categories
37 Length of
identifiers
Identifier name 0, 1, 2, 3, 5, 8, 13,
21, 34, 55, ∞
1/ˆ’.*’$/, /ˆtrue|false$/, /ˆnull$/, /ˆ".*"$/,/\d+/, /ˆ\/.*\//
88
B.5. List length
38 Length of
identifiers related
to function names
NewExpression.callee,
CallExpression.callee,
FunctionExpression.id,
FunctionDeclaration.id / Identifier
name 0, 1, 2, 3, 5, 8, 13,
21, 34, 55, ∞
39 Length of
identifiers related
to object variables
WhileStatement.test,
IfStatement.test, ForStatement.init,
ReturnStatement.argument,
VariableDeclarator.id and every
expression except CallExpression,
NewExpression, FunctionExpression
/ Identifier
name 0, 1, 2, 3, 5, 8, 13,
21, 34, 55, ∞
40 Length of literals Literal raw 0, 1, 2, 3, 5, 8, 13,
21, 34, 55, ∞
B.5 List length
For the following features we track the length of the list in the attribute value of the
selected nodes.
Description Selected nodes Attribute Categories
41 Number of
parameters in
function
declaration
FunctionDeclaration params 0, 1, 2, 3, 5, 8, ∞
42 Number of
parameters in
function
expression
FunctionExpression params 0, 1, 2, 3, 5, 8, ∞
43 Number of
statements in
function
declaration body
FunctionDeclaration.body /
BlockStatement
body 0, 1, 2, 4, 8, 16,
32, 64, 128, ∞
44 Number of
statements in
function
expression body
FunctionExpression.body /
BlockStatement
body 0, 1, 2, 4, 8, 16,
32, 64, 128, ∞
45 Number of
statements in
body of loops
WhileStatement.body /
BlockStatement |
DoWhileStatement.body /
BlockStatement |
ForStatement.body /
BlockStatement |
ForInStatement.body /
BlockStatement
body 0, 1, 2, 4, 8, 16,
32, 64, 128, ∞
46 Number of
arguments in new
expressions
NewExpression arguments 0, 1, 2, 3, 5, 8, ∞
89
B. Domain Specific Features
47 Number of
arguments in call
expression
CallExpression arguments 0, 1, 2, 3, 5, 8, ∞
48 Number of
elements in array
expressions
ArrayExpression elements 0, 1, 2, 4, 8, 16,
32, 64, 128, ∞
49 Number of
properties in
object expressions
ObjectExpression properties 0, 1, 2, 4, 8, 16,
32, 64, 128, ∞
50 Number of
declarations in
variable
declaration
VariableDeclaration declarations 0, 1, 4, ∞
B.6 Descendant count
For every measurement we use the following distribution: 0, 1, 2, 4, 8, 16, 32, 64, 128,
∞.
Feature Selected nodes Attribute
51 Test in conditional
expressions
IfStatement | WhileStatement | ForStatement
| ConditionalExpression | DoWhileStatement
test
52 Call argument CallExpression arguments
53 Array element ArrayExpression elements
54 Return argument ReturnStatement argument
55 Property in object
expressions
ObjectExpression properties
56 Left hand side
assignment expression
AssignmentExpression left
57 Right hands side
assignment expression
AssignmentExpression right
58 Object of member
expression
MemberExpression object
59 Property of member
expression
MemberExpression property
60 Callee of new
expression
NewExpression callee
61 Initialization value of
variable declarator
VariableDeclarator init
90
C
Dataset Description
In this appendix we describe the datasets used for validating the proposed authorship
authorship analysis techniques.
C.1 Dataset A
Table C.1 describes the size of repositories, the file size within these repositories, and
the size of the developers, before and after cleaning dataset A. Figure C.1 depicts the
empirical cumulative distribution of repository and author sizes in this dataset.
C.2 Dataset B
Dataset B consists of two clusters of developers that collaboratively developed a number
of software projects. Figure C.2 details these networks and the contributions of the
Min. Max. Mean Median Std.
Before cleaning
Author size (no. repositories) 1 208 27.1 21 27.3
Repository size (kB) 23 · 10−3 15.8 · 103 31.7 3.0 299.4
File size (kB) 7 · 10−3 9318.1 12.3 1.8 126.5
After cleaning
Author size (repositories) 5 167 24.0 17.5 22.2
Author size (kB) 10.8 761.5 160.9 138.9 131.4
Author size (LOC) 339 31347 5867 5020 4878.7
File size (kB) 13 · 10−3 47.5 2.9 1.6 4.1
Repository size (kB) 1.0 49.4 6.7 3.8 7.7
Repository size (LOC) 7 2614 244.1 148.0 271.0
Repository size (files) 1 32 2.29 1.00 2.6
Table C.1: The author, repository and file size in dataset A, before and after cleaning the data.
91
C. Dataset Description
0
0.2
0.4
0.6
0.8
1
0 5 10 15 20
F
n
Number of files
Size (kB)
Size (100·LOC)
(a)
0
0.2
0.4
0.6
0.8
1
0 800 1600 2400
F
n
Size (kB)
(b)
0
0.2
0.4
0.6
0.8
1
0 40 80 120
F
n
Size (kLOC)
Repositories
(c)
Figure C.1: Figure (a) shows the cumulative density function of the number of files per repository
and the size in kB of repositories in dataset A. Figure (b) and (c) show the of author sizes. Dashed
lines correspond to the uncleaned dataset.
authors to the projects. The size of the projects are in kB. The edges represent the
contribution of an author (a green node) to a project (a blue node) in kB.
92
C.2. Dataset B
cantodo (86.44)
funcit (69.5)jmvcdoc (50.2)
mxui (228.5)
srchr(19.3)
andykant (80.8)
justinbmeyer (71.7)
moschel (44.5)
arijo (24.5)
retro (36.8)
jeremyckahn (54.5)
76.1
1.2
1.8
22.2
29.7
17.5
2.6
1.8
28.8
4.7
45.7
19.6
22.7
4.0
14.5
5.5
2.8
11.1
(a)
knowuh (233.6)
MySystem-Wise-Integration-WIP (62.1)
scytacki (96.4)
rklancer (294.9)
sfentress (404.0)
biologica.js (52.3)
psndcsrv (134.5)
stepheneb (112.1)
lab (1432.6)
mysystem_sc (296.9)
gigamorph (60.0)
smartgraphs-authoring (72.2)
sparks (344.4)
44.1
19.1
47.8
5.6
7.5
20.7
234.4
41.1
117.1109.3
121.2
44.3
39.6
29.0
11.7
19.7
60.8
12.3
20.9
285.8
2.7
40.3
(b)
0
10
20
30
40
50
60
70
80
90
andykant
justinbmeyer
jeremyckahn
moschel
retro
arijo
0
2
4
6
8
10
Si
ze
(k
B)
Re
po
sit
or
ies
Size
Repositories
(c)
0
50
100
150
200
250
300
350
400
450
sfentress
rklancer
knowuh
psndcsrv
stepheneb
scytacki
gigamorph
0
2
4
6
8
10
Si
ze
(k
B)
Re
po
sit
or
ies
Size
Repositories
(d)
Figure C.2: The size of repositories, authors and contributions in respectively dataset C and D.
93

D
Graphs of Experimental Results
This Appendix contains the results of the performed experiments. Figure D.1, D.2,
D.3 and D.4 show the accuracy of the evaluated authorship identification techniques on
datasets A1 and A2, with a varying number of training examples and number of authors.
Figure D.5 shows the accuracy when the features of the domain specific and n-gram based
approach were combined. In these figures n refers to the number of training examples.
Further, all approaches are compared to the Scap approach, which is indicated with a
dashed line. The ROC-curves of the performed authorship identification tests are shown
in Figure D.6 (dataset A1) and Figure D.7 (dataset A2). In these plots, the Scap method
is tested with either one cumulative outlier profile, and with one profile per author.
0
0.2
0.4
0.6
0.8
1
0 10 20 30 40 50 60 70 80
Ac
cu
ra
cy
Number of authors
n = 1
n = 2
n = 4
n = 7
n = 13
n = 25
(a) Dataset A1
0
0.2
0.4
0.6
0.8
1
0 10 20 30 40 50 60 70 80
Ac
cu
ra
cy
Number of authors
n = 1
n = 2
n = 4
n = 7
n = 13
n = 25
(b) Dataset A2
Figure D.1: Accuracy of the Scap approach on dataset A1 and A2.
95
D. Graphs of Experimental Results
0
0.2
0.4
0.6
0.8
1
0 10 20 30 40 50 60 70 80
Ac
cu
ra
cy
Number of authors
n = 7
n = 13
n = 25
(a) Dataset A1
0
0.2
0.4
0.6
0.8
1
0 10 20 30 40 50 60 70 80
Ac
cu
ra
cy
Number of authors
n = 7
n = 13
n = 25
(b) Dataset A2
Figure D.2: Accuracy of domain specific approach on dataset A1 and A2.
0
0.2
0.4
0.6
0.8
1
0 10 20 30 40 50 60 70 80
Ac
cu
ra
cy
Number of authors
n = 7
n = 13
n = 25
(a) Dataset A1
0
0.2
0.4
0.6
0.8
1
0 10 20 30 40 50 60 70 80
Ac
cu
ra
cy
Number of authors
n = 7
n = 13
n = 25
(b) Dataset A2
Figure D.3: Accuracy of compression based approach on dataset A1 and A2.
96
0
0.2
0.4
0.6
0.8
1
0 10 20 30 40 50 60 70 80
Ac
cu
ra
cy
Number of authors
n = 4
n = 7
n = 13
n = 25
(a) Dataset A1
0
0.2
0.4
0.6
0.8
1
0 10 20 30 40 50 60 70 80
Ac
cu
ra
cy
Number of authors
n = 4
n = 7
n = 13
n = 25
(b) Dataset A2
Figure D.4: Accuracy of the n-gram based approach on dataset A1 and A2.
0
0.2
0.4
0.6
0.8
1
0 10 20 30 40 50 60 70 80
Ac
cu
ra
cy
Number of authors
n = 4
n = 7
n = 13
n = 25
(a) Dataset A1
0
0.2
0.4
0.6
0.8
1
0 10 20 30 40 50 60 70 80
Ac
cu
ra
cy
Number of authors
n = 4
n = 7
n = 13
n = 25
(b) Dataset A2
Figure D.5: Accuracy of combined result of n-grams and domain specific features on dataset A and
B with various number of training examples.
97
D. Graphs of Experimental Results
0
0.2
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1
TP
R
FPR
n = 1
n = 2
n = 4
n = 7
n = 13
n = 25
(a) Scap
0
0.2
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1
TP
R
FPR
n = 1
n = 2
n = 4
n = 7
n = 13
n = 25
(b) Scap with cumulative outlier represen-
tation
0
0.2
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1
TP
R
FPR
n = 1
n = 2
n = 4
n = 7
n = 13
n = 25
(c) Domain specific features
0
0.2
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1
TP
R
FPR
n = 1
n = 2
n = 4
n = 7
n = 13
n = 25
(d) n-grams
0
0.2
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1
TP
R
FPR
n = 1
n = 2
n = 4
n = 7
n = 13
n = 25
(e) Compression distances
0
0.2
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1
TP
R
FPR
n = 1
n = 2
n = 4
n = 7
n = 13
n = 25
(f) Domain specific features and n-grams
Figure D.6: ROC curves dataset A1.
98
0
0.2
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1
TP
R
FPR
n = 1
n = 2
n = 4
n = 7
n = 13
n = 25
(a) Scap
0
0.2
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1
TP
R
FPR
n = 1
n = 2
n = 4
n = 7
n = 13
n = 25
(b) Scap with cumulative outlier represen-
tation
0
0.2
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1
TP
R
FPR
n = 1
n = 2
n = 4
n = 7
n = 13
n = 25
(c) Domain specific features
0
0.2
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1
TP
R
FPR
n = 1
n = 2
n = 4
n = 7
n = 13
n = 25
(d) n-grams
0
0.2
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1
TP
R
FPR
n = 1
n = 2
n = 4
n = 7
n = 13
n = 25
(e) Compression distances
0
0.2
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1
TP
R
FPR
n = 1
n = 2
n = 4
n = 7
n = 13
n = 25
(f) Domain specific features and n-grams
Figure D.7: ROC curves dataset A2.
99
