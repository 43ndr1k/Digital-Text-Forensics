Universiteit Antwerpen 
Faculteit Taal- en Letterkunde 
Departement Germaanse Taal- en Letterkunde 
 
 
 
 
 
 
 
 
 
Syntax-Based Features and Machine 
Learning techniques for Authorship 
Attribution 
 
 
 
 
 
 
 
Eindverhandeling ingediend tot het bekomen van de graad licentiaat door: 
Kim Luyckx 
2e licentie Taalkunde (Computerlinguïstiek) 
 
Promotor: Prof. Walter Daelemans 
Tweede lezer: Dr. Guy De Pauw 
 
Academiejaar 2003-2004 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
!Copyright 2004 – Kim Luyckx (University of Antwerp) 
 
No part of this book may be reproduced in any form, by print, photoprint, microfilm, or 
any other means without written permission from the author. 
 
Acknowledgements 
 
I would like to thank all people who made an important contribution to this thesis. My 
promoter, Professor Walter Daelemans, deserves special appraisal for being so actively 
involved during the last two years and for having confidence in me. I am also grateful to Dr. 
Guy De Pauw and Dr. Erik Tjong Kim Sang for giving me technical advice concerning 
software. 
  4 
Abstract 
In the framework of the domain of Text Mining, and Document Classification in particular, 
an introduction into the field of Authorship Attribution is presented. Especially syntax-
based features and Machine Learning techniques suggest a new path in the identification of 
the author of a text which has not been subject to elaborate research yet. This paper reports 
experiments in Authorship Attribution in which the efficiency of syntax-based features is 
compared with that of lexical and token-level features. Memory Based Shallow Parsing 
(MBSP), scripts in the AWK programming language and the Rainbow program are applied for 
the extraction of style markers. Classification is performed by means of Machine Learning 
algorithms. The syntax-based features achieve an accuracy of 55% after testing on new data, 
while a combination of syntax-based, lexical and token-level features performs seven 
percent better. These results suggest that syntax-based features are good clues but not yet 
able to identify the author of a text independently. Nevertheless, we believe that extensive 
research on syntax-based features will lead to success. 
  5 
1. Theoretical background 
This chapter gives an introduction to the background of Authorship Attribution and the 
features, Computational Linguistics tools and classification techniques used in basic 
research in the field. This classification problem is situated in the domain of Text Mining 
and a short definition is given. We consider the linguistic features that can be used to 
identify the author of a document and take a look at the available Computational Linguistics 
tools to obtain those features. After a thorough examination of the techniques for 
Authorship Attribution (viz. Machine Learning, statistical and compression techniques), we 
discuss the research carried out in Authorship Attribution in the last decade. 
The theoretical background in this chapter forms the necessary basis for the 
experiments with specific features and classification techniques in the next chapter. Some 
sections will often be referred to in the second chapter of this paper. 
  6 
1.1 Authorship Attribution in the Domain of Text Mining 
 
Before getting into any technical details about how to automatically identify the author of a 
text, we should get a clear view of the position of Authorship Attribution in the domain of 
Text Mining. The main goal of most Natural Language Processing (NLP) applications is to 
automatically extract meaning from text (Figure 1). Text Mining is an essential branch of 
this process. It denotes the tasks that try to extract useful information by doing a linguistic 
analysis of large quantities of text and by detecting usage patterns (Sebastiani, 2002: 2). An 
important step in the Text Mining domain is Text Categorization – also called Document 
Classification. 
 
 
 
 
 
 
 
j 
 
 
 
 
Figure 1: Overview of the domain of Text Mining1 
 
Text Categorization (TC) labels documents according to a set of predefined 
categories. A Boolean value is assigned to each pair dj, ci ∈ D x C, with D as the set of 
documents and C as the set of predefined categories. If a value of T (True) is assigned to dj, 
ci, this indicates that dj is labelled under ci. A value of F (False) means that the document 
cannot be classified under ci (Sebastiani, 2002: 2-3). 
Applications of Text Categorization are numerous. The most important ones are 
document indexing and filtering, word sense disambiguation, the hierarchical categorization 
of web pages and web search engines (Sebastiani, 2002: 1-2). Authorship Attribution (AA), 
like Language Identification, is a classification problem. Starting from a text document, the 
                                                 
1 Walter Daelemans (2003) CNTS: Centrum voor Nederlandse Taal en Spraak. 
http://cnts.uia.ac.be/~walter/courses/cs/. 
Text 
 
 
 
 
 
 
 
 
 
 
 
 
 
Meaning 
NLP Applications 
OCR 
Spelling Error Recognition 
Grammar Checking 
Information Retrieval 
 
 
 
 
 
Dialogue Systems 
Machine Translation 
Document Classification 
Information Extraction 
Summarization 
Question Answering 
Ontology Extraction 
  7 
task is to automatically extract information concerning the author – or in case of Language 
Identification the language – of the text. The predefined categories are then the languages or 
authors under research or discussion. Authorship Attribution is one of the first steps in 
recovering or discovering knowledge from a document or a set of documents. Document 
Dating and Language Identification are the next phases1. 
A technique that is used very often for Authorship Attribution is the statistical 
analysis of style or stylometry. Researchers assume that all authors have specific style 
characteristics that are outside their conscious control. On the basis of those linguistic 
characteristics, the author of a document can be identified (Diederich et al., 2000: 1-2). 
Applications are discussions about disputed authorship, the detection of the age and gender 
of the author, and forensic linguistics. 
Section 1.2 gives an overview of the array of linguistic features which are 
characteristic for certain authors. Available Computational Linguistics (CL) tools are 
discussed in Section 1.3 and Section 1.4 deals with the three types of classification 
techniques that have been used with Authorship Attribution problems, viz. Machine 
Learning techniques, Statistical techniques and Data compression algorithms. Section 1.5 
gives an overview of the research carried out and the current state-of-the-art in Authorship 
Attribution. 
  8 
1.2 Features 
 
One of the main concerns in Authorship Attribution is the search for quantifiable features 
that are able to differentiate between authors. The subdivision in four types of features that 
is made in this paper, is taken from a paper on Genre and Author Categorization (Stamatatos 
et al., 2001a). The technical details about the different features are based on a paper on 
Authorship Attribution (Holmes, 1994). 
  9 
1.2.1 Token-Level Features 
 
Most of the research in Authorship Attribution is concerned with features on the token level. 
This means that the input is considered as a set of tokens, e.g. as a set of characters or n-
grams. Tokenization is necessary to analyse the text in terms of tokens, words or sentences 
(cf. Section 1.3.2). It is a very basic approach since most of those features are easily 
quantifiable, which in this case means that they can be counted. 
Studies in the 1950’s already were based on these features because no powerful 
computers were available (Holmes, 1994). But today there are still researchers who swear 
by token-level features because they are so simple but still tend to work well for Authorship 
Attribution. Stamatatos et al. criticise token-level features, although their experiments are 
based on them. 
 
It is not possible for such measures to lead to reliable results. 
Therefore, they can only be used as complement to other, 
more complicated features. (Stamatatos et al., 2001b: 195) 
 
The more complicated features that are mentioned will be discussed further on in 
Sections 1.2.2, 1.2.3 and 1.2.4 of this chapter, when we talk about syntax-based features, 
vocabulary richness and common word frequencies. In the following subsections, a number 
of token-level features is discussed. 
 
 Word Length 
 
Already in 1887, Mendenhall proposed that word length could be a characteristic for 
authors. The famous 1964 study of Mosteller and Wallace on the authorship of the 
Federalist Papers was also based on χ² tests on word lengths (Mosteller & Wallace, 1964). 
In 1998, a study used the number of words of a given length as a linguistic feature for 
Language Identification and found that this method had a good precision (Somers, 1998). 
Diederich et al. did experiments with Support Vector Machines (cf. Section 1.4.1) on 
frequencies of words of different lengths. Words longer than 25 letters were combined in a 
separate category. Based on those words, machine learning algorithms were able to classify 
75% of the text documents correctly (2000). 
 
 
 
 
  10 
 Sentence Length 
 
In 1938 sentence length was used as a feature in a study on the authorship of The Imitation 
of Christ. But Yule concluded that it was not a reliable stylistic variable. He found that 
looking for features that represent the specific style of an author actually means looking for 
things he does unconsciously (Yule, 1938). Clearly, the disadvantage of working with 
sentence lengths is, that they are under the conscious control of the author and depend on 
the punctuation, which in the case of authors like Shakespeare is under a lot of discussion. 
Still, some researchers are convinced that the distribution of sentence lengths – and not the 
average sentence length per text – is useful for Authorship Attribution (Holmes, 1994: 89). 
 
 Syllables 
 
Some Authorship Attribution researchers hypothesise that the average number of syllables 
per word or the distribution and frequency of n-syllabled words in a text are features that are 
able to capture the style of a specific author. But Fucks and Lauter argue in Mathematische 
Analyse des literarischen Stils from 1965 that frequency distributions of syllables per word 
discriminate different languages better than specific authors (Holmes, 1994: 89). The 
syllable approach in Authorship Attribution is under a lot of criticism since it lacks 
generality. The Flesch-Kincaid Readability score2, which will be discussed later (cf. Section 
1.4.2), is based on the average number of syllables per word and on the average number of 
words per sentence. 
 
 N-grams 
 
An n-gram of characters or letters is a sequence of n characters or letters. Section 1.4.3 
discusses first-order Markov models. These represent the probability that the next state 
depends only on the current state in bigrams (Jurafsky & Martin, 2000: 198). N-grams may 
avoid text tokenization (discussed in Section 1.3.2), and this is an advantage for Language 
Identification of e.g. Arabic. Some Authorship Attribution studies try to prove that 
probabilities and frequencies of occurrence of n-grams of letters are useful for 
characterising the style of an author (Soboroff et al., 1998; Khmelev & Tweedie, 2002; 
Kukushkina et al., 2002). 
 
                                                 
2  Rudolf Flesch: How to Write Plain English. 
http://www.mang.canterbury.ac.nz/courseinfo/AcademicWriting/Flesch.htm. 
  11 
 Other Features 
 
Other features on the token level are word count, sentence count, character per word count 
and punctuation marks count (Stamatatos et al., 2001a: 473). As stated above, the Flesch-
Kincaid Readability Formula works with the average number of words per sentence. One of 
the tests of the Claremont Shakespeare Authorship Clinic computed the frequency of 
exclamation points in the texts that are said to be from Shakespeare. Donald W. Foster, who 
has comments on the Clinic’s results, says variants like exclamation point frequencies are 
largely determined by the Shakespeare editors (Foster, 1999: 500). Punctuation mark 
frequencies are useful in Text Genre Detection as well (Stamatatos et al., 2000). 
  12 
1.2.2 Syntax-Based Features 
 
This fairly new type of features needs the cooperation of some kind of Natural Language 
Processing tool, like a Part-of-Speech Tagger or a Shallow Parser (cf. Section 1.3). The 
most recent contributions in Authorship Attribution are based on words and their 
frequencies of occurrence, but the frequencies of occurrence of rewrite rules or POS tags in a 
text seem to be an interesting new path for Authorship Attribution that still has to be 
explored. 
 
 Phrase-Level Features 
 
The Greek researchers Stamatatos, Fakotakis and Kokkinakis selected phrase-level features 
from the output of a Sentence and Chunk Boundaries Detector (SCBD): noun phrase count, 
word included in noun phrases count, prepositional phrase count, word included in 
prepositional phrases count, etc. (Stamatatos et al., 1999: 159). 
 
 Passive Count 
 
Readability formulae assume that the number of passive constructions in a text contribute to 
its readability. The more passive constructions a text contains, the more difficult it will be. 
 
 Distribution of Part-of-Speech (POS) Tags 
 
The different percentages of nouns, verbs, adjectives, adverbs and other POS tags in a text 
can be useful stylistic characteristics if they are defined correctly. Somers suggested the 
following: 
 
A more cultivated intellectual habit of thinking can increase 
the number of substantives used, while a more dynamic 
empathy and active attitude can be habitually expressed by 
means of an increased number of verbs. It is also possible to 
detect a number of idiosyncrasies in the use of prepositions, 
subordinations, conjunctions and articles. (quoted in Holmes, 
1994: 89) 
 
The Claremont Shakespeare Authorship Clinic used this type of features as well. 
One of its many authorship tests counted the relative clauses per 1,000 words. Although 
  13 
Foster is very critical about the results of the Clinic, he suggests further investigation on this 
feature (Foster, 1999: 502-503). An important question researchers have to ask themselves 
is how much information of the POS tags they are going to use in their study. Kukushkina et 
al. claim that: 
 
The frequencies of usage of letter pairs and pairs of 
grammatical classes are stable characteristics of the author. 
(Kukushkina et al., 2002: 172) 
 
They distinguish between two types of grammatical classes – ‘incomplete’ and ‘complete’ 
grammatical classes – and find that: 
 
the ‘complete’ grammatical classes were the most ineffective 
in authorship attribution both in the case of pairs frequencies 
and in the case of individual frequencies. (Kukushkina et al., 
2002: 181) 
 
They conclude that complete syntactic annotation is not needed to distinguish between 
authors (Kukushkina et al., 2002: 184). POS tags are also used for routing documents 
according to style. A combination of trigrams of function words and trigrams of POS tags 
achieves an error rate of 15% (Argamon et al., 1998). 
 
 Frequencies of Occurrence of Rewrite Rules 
 
A study by Baayen et al. makes use of syntax for Authorship Attribution. The researchers 
are convinced that: 
 
not only differences between authors, but also differences in 
register or text type are reflected in the relative frequencies of 
linguistic variables, many of which are syntactic in nature. 
(Baayen et al., 1996: 121) 
 
They discovered that a Principal Components Analysis (PCA) (cf. Section 1.4.2) of 
frequencies of occurrence of rewrite rules distinguishes between authors and text types 
better than word-based methods (Baayen et al. 1996: 129). 
 
 
 
 
  14 
 Other Features 
 
Glover and Hirst report on other syntax-based features that have been used for Authorship 
Attribution. Some of them are listed here: distribution of verb forms (tense, aspect, etc.), 
frequencies of word parallelism and distribution of word-class patterns (e.g. determiner + 
noun + verb). As stated above, these features need the cooperation of a Part-of-Speech 
Tagger. A Shallow Parser is required for features like the distribution of phrase structures 
(Glover & Hirst, 1995: 4). 
  15 
1.2.3 Vocabulary Richness 
 
Authors not only differ with respect to syntactic or structural features, but also in the 
diversity of the vocabulary they use. A lot of research focuses on measures of vocabulary 
richness. Holmes discusses some of these features and writes that: 
 
The basic assumption is that the writer has available a certain 
stock of words, some of which he/she may favour more than 
others. If we sample a text produced by the writer, we might 
expect the extent of his/her vocabulary to be reflected in the 
sample frequency profile. (Holmes, 1994: 91) 
 
The features concerning vocabulary richness have been criticised because they tend 
to be highly text length dependent and unstable for texts shorter than 1,000 words 
(Stamatatos et al., 1999: 162). 
 
 Type-token ratio V/N 
 
The type-token ratio is a measure that is a function of all vocabulary frequencies. It 
adequately characterizes the sample frequency distribution. If V is the size of the vocabulary 
of the sample text, and N is the number of tokens – the total number of running words 
(Jurafsky & Martin, 2000: 195) – of the sample text, then the type-token ratio is defined by 
R = V/N (Holmes, 1994: 92). If we compare the type-token ratios of texts from different 
authors, it should be clear that every author has his own vocabulary richness or diversity. 
 
 Simpson’s Index (D) 
 
D is the chance that the two members of an arbitrarily chosen pair of word tokens will 
belong to the same type. This measure is computed as follows: 
{ )}1(/)1( −−= NNVrrD r
i
 
with  2,1=r  
Vr is the number of types that occur just r times in a sample of text. The problem with this 
measure is, that it is very sensitive to variations on higher frequency words, but pays almost 
no attention to lower frequency words, which nevertheless make a strong contribution to the 
author’s vocabulary richness (Holmes, 1994: 92). 
  16 
 Yule’s Characteristic (K) 
 
This measure is based on the assumption that the occurrence of a given word is based on 
chance and can be regarded as a Poisson distribution (Holmes, 1994: 92).!The latter can be 
defined as a mathematical statement of the probability that exactly k discrete events will 
take place during an interval of length t.3 
 
 Hapax Legomena & Dislegomena 
 
As stated above, not only high-frequency words are able to discriminate between authors, 
but lower-frequency words are too. Hapax Legomena are words that occur only once in a 
text. There are only few words that occur often in a text when compared to the enormous 
amount of words that occur only once in a text. This is known as the Zipf Law (Holmes, 
1994: 97). Hapax Dislegomena are words that occur twice in the text (Holmes, 1994: 98). 
                                                 
3  The Institute for Telecommunication Services: 
http://glossary.its.bldrdoc.gov/fs-1037/dir-028/_4051.htm. 
  17 
1.2.4 Common Word Frequencies 
 
Many studies in Authorship Attribution use counts of frequencies of occurrence of 
individual words in the sample text as a discriminator for authors. These measures can 
easily be calculated, but a lot of effort has been done to select the most appropriate words. 
According to Dunning, a researcher in the Language Identification field, this approach 
works well if enough text is available to be classified. Still, he proposes to use n-grams 
because they have better results and are similar to common words since they tend to be short 
as well (Dunning, 1994: 16). 
 
 Content versus Function Words 
 
This is a basic contrast in Authorship Attribution studies. Authors who write about the same 
topic, tend to use a set of content words for that specific topic. Still, those authors have a 
conscious or unconscious preference for certain other content words. Function words do not 
seem to be characteristic style markers for authors, since they are very frequent and occur in 
every text by every author. The use and frequency of function words – e.g. determiners, 
conjunctions, prepositions, etc. – however, is characteristic for authors. A big advantage of 
function words is, that they are not under the conscious control of the author (Holmes, 1994: 
90-91). 
Hoover investigates the impact of content and function words on the classification of 
texts of different authors. His findings are that a selection of the 500 most frequent words of 
all kinds works better than the 200 most frequent function words (Hoover, 2001). A lot of 
studies worked with function word frequencies, like that of Mosteller and Wallace on the 
Federalist Papers (Mosteller & Wallace, 1964). Researchers like Morton developed 
techniques to study the position and context of function words (Holmes, 1994: 90). Fung 
and Mangasarian present a Support Vector Machine on the basis of three common function 
words: upon, to and would (Fung & Mangasarian, 2003). Trigrams of function words seem 
to work well for routing documents according to style (Argamon et al., 1998). 
 
 N Most Frequent Words 
 
Still, some researchers believe no distinction should be made between function words and 
content words. A study of the authorship of six Jane Austen novels by Burrows in 1987 used 
the thirty most common words and was able to compare and contrast Austen with other 
writers on the basis of these features (Holmes, 1994: 91). Stamatatos et al. compare the 
  18 
frequencies of occurrence of the most frequent words in a training corpus with those of the 
British National Corpus. This technique proves to be a more reliable stylistic discriminator 
(Stamatatos et al., 2000). Baayen et al. compare a word-based method – in this case the 
selection of the fifty most frequent words – with a syntax-based method – the frequency of 
occurrence of rewrite rules – and find that the latter gives an increase in classification 
accuracy (Baayen et al., 1996). 
  19 
1.3 Computational Linguistics (CL) Tools 
 
In order to be able to work with features like Part-of-Speech (POS) tags and rewrite rules, 
researchers need Computational Linguistics (CL) tools which perform a specific linguistic 
analysis of the texts. What follows, is a list of the CL tools – sorted by the degree of 
linguistic analysis required – that are used or adapted or could be used for Text 
Categorization problems like Authorship Attribution. 
 
1.3.1 Optical Character Recognition (OCR) 
 
OCR is the automatic recognition of machine or hand-printed characters. It translates 
optically scanned bitmaps of characters into character codes like ASCII-files. This way, hard-
copy materials can be edited and manipulated on a computer without you having to type the 
text4. Figure 2 shows how the OCR process works: first it is trained on a number of texts and 
on the basis of that training it analyses other data. The problem with OCR is that it has even 
higher error rates than human typists, although it makes other mistakes: it often misreads 
‘D’ as ‘O’ or ‘ri’ as ‘n’ (Jurafsky & Martin, 2000: 143). 
 
 
Figure 2: The many facets of OCR4. 
 
Two researchers in the field of Language Identification, Sibun and Spitz, describe a 
method for converting a document image into character shape codes and word shape codes. 
They want to be able to identify the language of incoming faxes, patent applications and 
                                                 
4  Lais, Sami: Optical Character Recognition: 
http://www.computerworld.com/softwaretopics/software/apps/story/0,10801,73023,00.html. 
  20 
office memos. These are only accessible on paper, and OCR is used to digitise it into 
character-coded text. But Sibun and Spitz have an alternative for OCR to use in Language 
Identification: four horizontal lines that define the boundaries of three significant zones on 
each text line (Sibun & Spitz, 1994: 16): 
 
!
    Figure 3: Text line parameter positions 
 
1.3.2 Tokenization 
 
Tokenization is one of the first stages of text input analysis. It separates punctuation marks 
and brackets from the text – mostly by means of a space. For languages like English this is 
fairly easy because word boundaries often coincide with whitespace or punctuation in the 
input text. But many Asian languages do not use spaces to separate words but in order to 
separate paragraphs5. 
Biskri and Delisle avoid Tokenization for Language Identification because the 
simple notion of tokens defined as strings of characters separated by spaces is an 
oversimplification that is highly inadequate for many situations and languages. The n-gram 
method avoids the Tokenization process because it also uses punctuation marks and 
whitespaces. It has several advantages: it is easily recognisable, language-independent and 
statistically meaningful (Biskri & Delisle, 2002: 110). 
 
1.3.3 Sentence and Chunk Boundaries Detector (SCBD) 
 
Stamatatos et al. use in their experiments for Authorship Attribution the already existing 
Sentence and Chunk Boundaries Detector (SCBD). The first phase is Sentence Boundary 
Detection. Period, exclamation point, question mark, and ellipsis are considered as potential 
sentence boundaries. The Chunk Boundaries Detector uses multiple-pass parsing on a 450-
keyword lexicon and a 300-suffix lexicon. The first passes detect easily recognizable chunks 
and the last passes deal with the more complicated ones. Figure 4 shows the essential 
structure of the CL tool (Stamatatos et al., 2001a: 476-477): 
                                                 
5  Center for Spoken Language Understanding: 
http://cslu.cse.ogi.edu/HLTsurvey/ch5node5.html. 
  21 
 
      Figure 4: SCBD system structure 
 
1.3.4 Part-of-Speech (POS) Tagger 
 
Part-of-Speech (POS) tags can be subdivided into open (new words can be added) and closed 
class words (a fixed set of words). The open class consists of nouns, verbs, adjectives and 
verbs, and the closed class contains prepositions, determiners, pronouns, conjunctions, 
auxiliary verbs, particles and numerals. A POS Tagger automatically assigns a POS tag to 
every word in a text (Jurafsky & Martin, 2000: 298). The output of the Memory-Based 
Tagger6 on the first sentence of this paragraph is: 
 
POS tags can be subdivided into open ( new words can be added ) and closed class words ( a 
fixed set of words ) .  
POS//NNP tags/NNS can/MD be/VB subdivided//VBN into/IN open/JJ (/( new/JJ 
words/NNS can/MD be/VB added/VBN )/) and/CC closed/VBD class/NN words/NNS (/( 
a/DT fixed/VBN set/VBN of/IN words/NNS )/) ./. 
Figure 5: Output of the MBT Tagger 
Legend: see Appendix 1 
 
As is clear from the example, Tokenization is integrated in this POS Tagger since 
brackets and punctuation marks are separated from the input text by whitespaces. 
Tagged text is needed for Authorship Attribution experiments on two types of 
features. The first type is that of features that represent vocabulary richness (cf. Section 
1.2.3), e.g. type-token ratio. The second type consists of syntax-based features (cf. Section 
1.2.2): 
 distribution of parts-of-speech 
 distribution of verb forms 
                                                 
6  MBT: Memory Based Tagging demo: 
http://ilk.uvt.nl/~zavrel/tagtest.html. 
  22 
 frequency of word parallelism 
 distribution of word-class patterns 
 distribution of nominal forms (Glover & Hirst, 1995: 4) 
As said above, a lot of research still has to be carried out with this last type of 
features in Authorship Attribution. 
 
1.3.5 Shallow Parser 
 
A Partial or Shallow Parser does not give a complete parse of the input text. It is to be 
preferred over Full Parsing because a full analysis is slower and often makes a lot of errors 
(Appelt & Israel, 1999: 20). Usually Shallow Parsers use cascades of finite-state automata in 
which the states and transitions represent rewrite rules. As you can derive from the example 
below, the Memory-Based Shallow Parser (MBSP)7 runs through four steps in the analysis: it 
tokenises the input, does a POS analysis of the text, looks for noun phrase (NP), verb phrase 
(VP) and prepositional phrase (PNP) chunks, while the subject and objects of the sentence are 
detected in the last phase7: 
 
Tokenised output 
POS tags can be subdivided into open ( new words can be added ) and closed class words ( a 
fixed set of words ) . 
Tagger output 
POS//NNP tags/NNS can/MD be/VB subdivided//VBN into/IN open/JJ (/( new/JJ 
words/NNS can/MD be/VB added/VBN )/) and/CC closed/VBD class/NN words/NNS (/( 
a/DT fixed/VBN set/VBN of/IN words/NNS )/) ./. 
Chunker output 
[NP POS//NNP tags/NNS NP] [VP can/MD be/VB subdivided//VBN VP] {PNP [Prep 
into/IN Prep] [NP open/JJ NP] PNP} (/( new/JJ [NP words/NNS NP] [VP can/MD be/VB 
added/VBN VP] )/) and/CC [VP closed/VBD VP] [NP class/NN words/NNS NP] (/( [NP 
a/DT fixed/VBN NP] [VP set/VBN VP] {PNP [Prep of/IN Prep] [NP words/NNS NP] 
PNP} )/) ./. 
Subject/Object Detector output 
[NP1Subject POS//NNP tags/NNS NP1Subject] [VP1 can/MD be/VB subdivided//VBN VP1] 
{PNP [P into/IN P] [NP open/JJ NP] PNP} (/( new/JJ [NP2Subject words/NNS NP2Subject] [VP2 
can/MD be/VB added/VBN VP2] )/) and/CC [VP3 closed/VBD VP3] [NP3Object class/NN 
words/NNS NP3Object] (/( [NP4Subject a/DT fixed/VBN NP4Subject] [VP4 set/VBN VP4] {PNP [P 
of/IN P] [NP words/NNS NP] PNP} )/) ./. 
Figure 6: Output of the MBSP Parser 
                                                 
7  MBSP: Memory-Based Shallow Parser demo: 
http://ilk.uvt.nl/cgi-bin/tstchunk/demo.pl. 
  23 
As stated above, Stamatatos et al. not only use the Sentence and Chunk Boundaries 
Detector, but also do multiple pass parsing. This means that the first parsing pass analyses a 
part of the sentence, while the other parts are to be analysed by the subsequent passes. The 
first passes detect easily recognisable chunk boundaries and the other passes detect the more 
difficult ones (Stamatatos et al., 1999: 160). 
According to Glover & Hirst, there is still a lot of research to be done in Authorship 
Attribution with features based on parsed text. A selection of the features they propose is 
listed here: 
 frequency of clause types 
 frequency of syntactic parallelism 
 distribution of genitive forms (of and ‘s) 
 distribution of phrase structures 
 frequency of imperative, interrogative, and declarative sentences 
 frequency of passive voice (Glover & Hirst, 1995: 4). 
  24 
1.4 Classification Techniques 
 
So far, this paper has dealt with the feature types and Computational Linguistics tools that 
can be used for extracting the features. Since Authorship Attribution is essentially a Text 
Categorization or Document Classification problem, we should focus on the classification 
techniques that have been and could be applied in Authorship Attribution. A subdivision in 
three types can be made: Machine Learning techniques (cf. Section 1.4.1), Statistical 
techniques (cf. Section 1.4.2) and Data Compression algorithms (cf. Section 1.4.3). 
  25 
1.4.1 Machine Learning (ML) Techniques 
 
The goal of Machine Learning (ML) is to construct computer programs that automatically 
learn and improve with experience. ML algorithms are able to make generalizations and 
discover rules from examples (Mitchell, 1997: 1). We call this the inductive approach. 
Eager Learning algorithms are efficient and minimalist because they delete the 
original examples and only keep the rules. Lazy Learning algorithms on the other hand 
memorize all the examples and generalize from these examples when this is needed, which 
makes them less efficient and maximalist. They focus on classification rather than on 
learning. The k-Nearest Neighbour algorithm is an example of a Lazy Learning algorithm. 
All other learning algorithms that are discussed in this paper are Eager (Mitchell, 1997: 
244). The most popular Machine Learning techniques for Text Categorization are Naive 
Bayes, Neural Networks, k-Nearest Neighbour classifiers, and Support Vector Machines. 
Most of the state-of-the-art in Text Categorization uses Machine Learning techniques. A 
version of all these Machine Learning algorithms is available in the New-Zealand Waikato 
Environment for Knowledge Analysis (WEKA) software package (Witten & Frank, 1999)8. 
This section is based on the articles Machine Learning in Automated Text 
Categorization by Fabrizio Sebastiani (Sebastiani, 2002) and A Re-Examination of Text 
Categorization Methods by Yang and Liu (Yang & Liu, 1999) and on the Machine Learning 
book by Tom Mitchell (Mitchell, 1997). Again we link several articles to the ML techniques 
discussed in the following subsections. Most of these articles will be discussed in Section 
1.5, when we take a better look at the main focus of this paper, namely Authorship 
Attribution. 
 
 Probabilistic Classifiers: Naive Bayes 
 
Probabilistic classifiers are based on the Bayes’ theorem, which computes the probability 
that a document, represented by a vector j of (binary or weighted) terms, belongs to a 
category ci. P( j) is the probability that a randomly picked document has vector j as its 
representation and P(ci)is the probability that a randomly picked document belongs to ci: 
 (Sebastiani, 2002: 20) 
Naive Bayes classifiers are based on this theorem and use the joint probabilities of 
words and categories to estimate the probabilities of categories given a document. They are 
                                                 
8  WEKA, The University of Waikato: 
http://www.cs.waikato.ac.nz/~ml/index.html. 
  26 
called naïve because they assume that the conditional probability of a word given a category 
is independent from the conditional probabilities of other words given that category. This 
theory has been proven to be not true, but this wrong assumption seems to be working in 
Naive Bayes. The words mentioned here can be interpreted as the features that are selected 
to identify the language or author of a document (Yang & Liu, 1999: 45). Dunning explains 
how Bayesian decision rules can be useful in Language Identification: 
 
Given the task of deciding which of two possible phenomena 
have caused a particular observation, we can minimize our 
probability of error by computing which is most likely to have 
given rise to the observation. (Dunning, 1994: 8) 
 
The language model with the highest probability is the one that is most likely to have 
generated the observed string (Dunning, 1994: 8). 
 
 Decision Tree (DT) Classifiers 
 
Decision Tree (DT) Classifiers are easily interpretable learning algorithms because they 
consist of nested if-then rules. Internal nodes are labelled by terms, branches departing from 
them are labelled by tests on the weight that the term has in the test document, and leafs are 
labelled by categories. In the example given below (Figure 7), colour is the root node. 
Every node represents a test of some attribute of the instance. The branches that depart from 
colour are green or * (non-green). Reject and allow are the leaf nodes which represent the 
categories. 
 
          Figure 7: Example of a Decision Tree9 
 
The goal of a DT learning algorithm is to find the tests that best separate the 
classification problem into the different categories. The one that gives the best separation of 
examples should be the root node or test. If not all the training examples have the same label 
                                                 
9  Generation 5: 
http://www.generation5.org/aisolutions/dtprolog.shtml. 
  27 
(e.g. reject), new tests should be designed until all the training examples are separated in a 
number of categories (Sebastiani, 2002: 22-23). 
Applied to an Authorship Attribution task, discriminatory features are represented by 
the DT nodes or tests. The values on the branches can be thresholds – e.g. if the feature value 
is below 20, the test document is written by Shakespeare and if the feature value is 20 or 
more, it is written by Marlowe – or yes/no values – e.g. if the test document has that feature, 
it must be labelled Shakespeare, and if it does not have the feature, it should be labelled 
Marlowe. There are no Authorship Attribution studies that work with Decision Trees 
because other learning algorithms tend to work better. 
Popular Decision Tree classifiers are ID3 and C4.5, which were developed by Ross 
Quinlan (Quinlan, 1993). Both are part of the WEKA software package mentioned before. 
 
 Decision Rule Classifiers 
 
Decision Rule classifiers are very similar to Decision Tree classifiers but generate more 
compact classifiers. The main difference is, that Decision Rule classifiers try to select the 
best rule from all the rules that correctly classify all the training examples (Sebastiani, 2002: 
23). 
This is interesting for our main classification problem because we are looking for the 
features or rules that best characterize the author of a text. As far as we know, there are no 
Authorship Attribution studies that are based on Decision Rule classifiers. 
 
 Artificial Neural Networks (NNet) 
 
An Artificial Neural Networks classifier can be seen as a network of units. Figure 8 
represents the Simple Perceptron that is on the basis of every NNet. The input units (i1,…, in) 
represent terms. They are weighted (w1,…, wn) and their sum (Σ) is calculated, taking their 
weights into account. If the sum is higher than the threshold (θ), the output unit (o) is fired. 
All categories have their own separate Neural Network because they each assign other 
weights to input units and handle different thresholds (Sebastiani, 2002: 27): 
 
  28 
 
         Figure 8: The Simple Perceptron10 
 
I selected two articles that work with Neural Networks for the identification of 
spoken language (Berkling & Barnard, 1996 and Kwan & Hirose, 1996). GRAMEXCO, the 
Text Categorization software two Canadian researchers have developed, is based on 
Artifical Neural Networks and is used to identify the language of a document (Biskri & 
Delisle, 2002: 112). 
 
 Category Profiles 
 
Some classifiers learn and classify on the basis of an explicit profile for documents or a 
prototypical document for a certain category. The main advantage of this ML technique is 
that it is much more transparent than for example a Neural Network. In the Written 
Language Identification domain, Cavnar and Trenkle wrote an article on n-grams and made 
a system that calculates and compares profiles of n-gram frequencies. Figure 9 represents 
the system structure. 
The first step of the system is to generate a profile for the training set data that 
represents the different categories. Making a profile means reading incoming text and 
counting the occurrences of all n-grams. Then the system computes a profile for the 
incoming document that has to be classified. It computes the distance between the profile of 
the sample profile and the incoming document profile. The system selects the category 
whose profile has the smallest distance to the document’s profile (Cavnar & Trenkle, 1994). 
Since Authorship Attribution is a problem very similar to Language Identification, Category 
Profiles could also be used for identifying the author of a document. 
 
                                                 
10 Language Technology: Machine Learning of Language: 
http://ilk.kub.nl/~antalb/ltuia/nn.html. 
  29 
 
Figure 9: System structure for n-gram based Text Categorization 
 
 k-Nearest Neighbour Classifiers (kNN) 
Classify thy nearest Neighbour as thy self. 
-- Anonymous11 
 
As stated above, kNN classifiers are Lazy Learning algorithms – also called memory-based 
learners – because they memorize all the training examples and only generalise when test 
data have to be classified. kNN has been applied in Text Categorization since the early stages 
of research. The algorithm examines to which category the k training documents that are 
most similar to the test document belong. Figure 10 represents a kNN classifier problem with 
k equalling 1. The vector space on the left is the input of the kNN classifier and the one on 
the right has been classified. The test document q is labelled as its nearest neighbour *: 
 
                                                 
11 Nearest Neighbor Classification: 
http://jeff.cs.mcgill.ca/~godfried/research/nearest.neighbor.html. 
  30 
 
         Figure 10: kNN classifier with k = 1 12 
 
The test document is labelled the same category as its k nearest Neighbours. Every 
Neighbour document gets a similarity score, and that way the category candidates are 
weighted. Figure 11 shows a classification problem handled with a kNN classifier with k 
equalling 4. The test document ? is categorized as its light grey Neighbours: 
 
Figure 11: kNN classifier with k = 4 13 
 
kNN achieves top performance but the algorithm needs a lot of time because it has to 
go through the entire training data set before it is able to classify a test document (Yang & 
Liu, 1999: 44). Although there are no Text Categorization studies that focus on k-Nearest 
Neighbour classifiers, still there is a lot of future in the use of the algorithm for problems 
like Language Identification and Authorship Attribution. 
 Support Vector Machines (SVM) 
 
Support Vector Machines try to find the surface that best separates the positive (+) from the 
negative (o) training examples. On the basis of a small set of training examples, called the 
support vectors, the best decision surface σi is determined. Figure 12 shows that σi is the 
                                                 
12 Nearest Neighbor Search: 
http://www.cs.sunysb.edu/~algorith/files/nearest-neighbor.shtml. 
13 The k-Nearest-Neighbor Algorithm: 
http://mathforum.org/~ken/categorize/007.html. 
  31 
best decision surface or hyperplane (the thicker line) because it is the middle element of the 
parallel decision surfaces (the thin lines). The support vectors are the small boxes 
(Sebastiani, 2002: 30): 
 
 
Figure 12: Learning Support Vector classifiers 
 
A concrete example can be taken from a recent paper on the disputed authorship of 
the Federalist Papers (Fung & Mangasarian, 2003). This article will be discussed in Section 
1.5.3. A study by Diederich et al. uses Support Vector Machines for Authorship Attribution 
or stylometry – as they call it. They say that one of the main advantages of SVMs is, that they 
can process documents of significant length and databases with a large number of texts. The 
method’s training time compares favourably with other methods like Neural Networks. For 
that reason, SVMs are currently the method of choice for authorship attribution and Text 
Categorization as a whole (Diederich et al., 2000: 15). 
  32 
1.4.2 Statistical Techniques 
 
Although there is a lot to say in favour of the use of Machine Learning techniques for Text 
Categorization, most of the articles on Language Identification and Authorship Attribution 
use statistical techniques for identifying the language or author of a text. Yet it would be 
wrong to consider Machine Learning techniques and Statistical techniques as two different 
categories, as most of the ML techniques use Statistical techniques, e.g. Naive Bayes uses 
the Bayes’ theorem. This section gives a short description of most of the statistical 
techniques in TC and refers to articles that apply those techniques. They are also often linked 
to or combined with Computational Linguistics (CL) tools. Holmes describes the importance 
of the statistical analysis in Authorship Attribution as follows: 
 
The statistical analysis of a literary text can be justified by the 
need to apply an objective methodology to works which for a 
long time may have received only impressionistic and 
subjective treatment. (Holmes, 1994: 87) 
 
In the following subsections, some of the most frequent statistical techniques are 
discussed. Most of them have been used in Authorship Attribution. 
 
 Multivariate Statistical Techniques 
 
Holmes mentions four types of multivariate statistical techniques, of which three will be 
discussed in this paper because they occur in Authorship Attribution studies: Discriminant 
Analysis, Cluster Analysis, and Principal Components Analysis (PCA) (Holmes, 1994: 98-
99). These multivariate techniques are often combined with e.g. frequencies of occurrence 
of certain features (e.g. in Baayen et al., 1996). Hoover argues that multivariate statistical 
techniques are very important, especially in Authorship Attribution: 
 
Multivariate analysis, which has long been widely used in the 
sciences and social sciences, has recently been used 
increasingly in authorship and stylistic studies because its 
powerful tools seem appropriate for the very large amounts of 
information represented by texts. (Hoover, 2001: 421) 
 
 
 
 
  33 
o Discriminant Analysis (DA) 
 
Discriminant Analysis (DA) predicts group membership on the basis of a set of predictors. 
The technique tries to maximize the variance between the groups and to minimize the 
variance within the groups. That way, groups are delineated14. 
In an article on Language Identification by Sibun and Spitz, also mentioned in 
Section 1.3.1 when we talked about Optical Character Recognition (OCR), Linear 
Discriminant Analysis (LDA) is used to build a statistical model of the language 
categorizations. They try to automatically identify twenty-three languages and select the 23 
tokens – selected from the most frequent word shape tokens from every language – to  
represent the languages. LDA makes a profile for each language and has a high accuracy on 
the data set (Sibun & Spitz, 1994: 17). Stamatatos et al., 2000 and Baayen et al., 1996 also 
use Discriminant Analysis, but this time for Authorship Attribution. 
 
o Cluster Analysis 
 
This statistical technique from the 1950’s has been applied quite recently in Text 
Categorization studies. It tries to organise information about variables so that the data can be 
gathered in natural homogeneous groups or clusters. Cluster Analysis is quite similar to 
Discriminant Analysis in that it also wants clusters to be internally homogenous and 
externally heterogeneous15. Important decisions have to be made by the users on the 
calculation of clusters, and these decisions have a strong influence on the classification. 
Cluster Analysis is said to be an objective method for Document Classification16. 
 Hoover examines the accuracy and effectiveness of cluster analysis of the 
frequencies of high-frequency words for grouping texts by the same author and 
distinguishing texts by different authors (Hoover, 2001: 422). He finds that cluster analyses 
achieve an accuracy rate of less than 90 percent and concludes that: 
 
These failures suggest general rather than local problems with 
the technique, and cast doubt on the effectiveness of cluster 
analysis for authorship attribution and stylistic study. (Hoover, 
2001: 438) 
                                                 
14 Pacific Forestry Service: 
http://www.pfc.forestry.ca/profiles/wulder/mvstats/discrim_e.html. 
15 Bill Trochim’s Center for Social Research Methods: 
http://trochim.human.cornell.edu/tutorial/flynn/ cluster.htm. 
16 Pacific Forestry Service: 
http://www.pfc.forestry.ca/profiles/wulder/mvstats/cluster_e.html. 
  34 
o Principal Components Analysis (PCA) 
 
This technique is used for Data Compression algorithms (cf. Section 1.4.3) because it 
reduces the dimensionality of the input. It rotates and scales the data set, which is 
represented in the vector space. Its goal is to understand the relationships and correlations in 
a data set. All the variables of the data set should be compared, which means that a vector 
space should be generated for every pair of variables. The axes of the vector space are the 
Principal Components: the components that show the greatest variation in the data set. 
These components are compared to each other and the best Principal Component is 
selected17. Figure 13 shows a simple visualisation of the general idea of Principal 
Components Analysis. The left part shows a representation in a vector space of a data set on 
the basis of three variables. The right part is the same data set but rotated (after PCA): 
 
 
   Figure 13: Visualisation of Principal Components Analysis18 
 
It is clear that a clear pattern can be detected from the right part of Figure 13. 
Baayen et al. use PCA for a comparison of word-based and syntactic features for Authorship 
Attribution (Baayen et al., 1996) and Stamatatos et al. use it to represent their test corpus 
(Stamatatos et al., 1999). 
 
 Latent Semantic Indexing (LSI) 
 
Some Data Compression algorithms use LSI as another way to reduce the dimensionality of 
the input. It visualises authorship in multiple dimensions. For a problem in five dimensions, 
ten separate vector spaces are needed because they can only handle two dimensions at a 
time. Latent Semantic Indexing enables researchers to represent a problem in a five-
dimensional space (Soboroff et al., 1998: 43). 
                                                 
17 Pacific Forestry Service: 
http://www.pfc.forestry.ca/profiles/wulder/mvstats/pca_fa_e.html. 
18 The Aberystwyth Quantitative Biology and Analytical Biotechnology Group, University of Wales: 
http://qbab.aber.ac.uk/auj/depttalk96/pca.active.html. 
  35 
 Frequencies and Probabilities – Principle of Maximal Likelihood 
 
Frequencies are also popular in TC studies because they are very easy to calculate. 
Stamatatos et al. try to detect text genres by comparing the frequencies of occurrence of the 
most frequent words in the test documents with the frequencies of occurrence of the most 
frequent words of the entire written language. They extract these from the British National 
Corpus (BNC). They also compare the frequencies of the most frequent punctuation marks 
(Stamatatos et al., 2000). 
 It is amazing to see how many frequency tests the Claremont Shakespeare 
Authorship Clinic came up with in order to be able to identify the actual authors of the so-
called Shakespeare Canon. A few of them are listed here: o vs. oh, ‘d vs. -ed, the frequency 
of exclamation points, and the Whenas vs. Whereas test (Foster, 1999: 496, 498). 
Frequencies are often used to estimate probabilities. There are few studies that do 
not link frequencies to probabilities. Probabilities are popular statistical techniques in 
Authorship Attribution as well as in Language Identification because they can be calculated 
easily, just like frequencies. Probabilities are usually linked to the Principle of Maximal 
Likelihood: 
 
For each matrix we calculate the probability of the anonymous 
or disputed [own remark] text and we choose the author with 
the maximal corresponding probability and the chose author is 
assumed to be the true author. (Kukushkina et al., 2002: 174) 
 
Authorship Attribution studies often work with probability distributions of Markov 
models or chains. These are random processes in which the probability distribution of the 
next state depends only on the current state (Dunning, 1994: 6). Markov chains of letters are 
actually just probabilities of letter n-grams. Khmelev and Tweedie consider the probabilities 
of the subsequent letter in a letter bigram, or in other words: given that a particular letter is 
an ‘f’ which letters are most likely to follow it? They apply this method on three data sets 
concerning disputed authorship (Khmelev & Tweedie, 2002: 300). The sequel of this study 
came the same year and computes probabilities of transitions from one letter to another for 
each author but also of part-of-speech tags (Kukushkina et al., 2002). 
 Although this technique has been used in Authorship Attribution studies frequently, 
Khmelev and Tweedie formulate drawbacks: letter sequences do not add to the stylistic 
interpretation of the texts and might depend on the subject of the texts (Khmelev & 
Tweedie, 2002: 306). 
  36 
 Multiple Regression 
 
This technique is designed to provide insight into the relationship between several 
independent (i.e., predictor) variables and a dependent (i.e., response, criterion) variable. 
The independent variables can predict the dependent variable, and Multiple Regression can 
be used for the estimation of the significance of the independent variables. It tells us which 
independent variables have the highest prediction value. On Text Genre Detection, the 
identification error of Multiple Regression is a bit higher than that of Discriminant Analysis 
(Stamatatos et al. 1999, 2001a). 
 
 Readability 
 
This last statistical technique has not been applied in Authorship Attribution as such, but 
Readability might be able to distinguish between authors. Readability metrics are mostly 
used in a school context: teachers ought to know whether their students are able to 
understand their books and papers. The Flesch-Kincaid Readability Formula computes 
readability based on the average number of syllables per word and the average number of 
words per sentence, measures which each have been applied to Text Categorization. The 
score indicates a grade-school level. Reading level classifiers are actually combinations of a 
language model and surface linguistic features (Si & Callan, 2000). The English version of 
this Formula is integrated in Microsoft Word, which makes it highly accessible19. Since the 
Readability Formula is based on syllables and words, it seems very plausible that every 
author has a sort of constant level of readability in his writings. 
                                                 
19 University of Utah: Health Sciences: 
http://www.med.utah.edu/pated/authors/readability.html. 
  37 
1.4.3 Data Compression Algorithms 
 
Data compression algorithms are believed to be a promising alternative approach to 
categorization. This belief is based on the general Information Theory, which says that the 
Entropy of a string is a measure for the surprise of the source emitting the character 
sequences – e.g. DNA, protein sequences, and language. The central question in the use of 
data compression algorithms for categorization is: is it possible to obtain from this measure 
the information we were trying to extract from the sequence? Distance is then used as a 
concept of remoteness or similarity between pairs of sequences based on their relative 
informational content. 
 File compressors or zippers try to find duplicated strings in the input and look for the 
longest matching strings. They tend to encode more frequent sequences with few bytes, 
while rare sequences will be encode with more bytes (Benedetto et al., 2002: 1). 
Researcher Joshua Goodman wrote a furious comment on the article Language Trees 
and Zipping by Benedetto et al. He compared data compression algorithms to the Naive 
Bayes algorithm, only to find that: 
 
The zipping algorithm was 17 times slower and produced 
more than 3 times as many errors. It was only marginally 
simpler to implement, requiring 50 lines of perl script, instead 
of 70 lines. (Goodman, 2002: 1) 
 
Still, there are several potential advantages to the use of data compression 
algorithms, according to Frank et al.: 
 they give an overall judgement of the document as a whole; 
 they avoid the problem of defining word ambiguities; 
 they can take account of phrasal effects that span word boundaries instead; 
 they offer a uniform way of dealing with different types of documents; 
 they minimise arbitrary decisions (Frank et al., 2000: 1). 
 
To test whether the algorithms can come up to the expectations, Frank et al. perform 
extensive experiments on newswire stories from the Reuters collection with Prediction by 
Partial Matching (PPM), a data compression algorithm. They find that: 
 
Compared to state-of-the-art machine learning techniques for 
categorizing English text, PPM produces inferior results 
because it is insensitive to subtle differences between articles 
  38 
that belong to a category and those that do not. (Frank et al., 
2000: 10) 
 
Other researchers tend to be very enthusiastic about the use of data compression 
algorithms for categorization. They can have various applications in Text Categorization: 
Cryptology, Language Identification (even the identification of language period for 
historical texts), Authorship Attribution, and Spelling Correction. A short description of the 
procedure for Language Identification and Authorship Attribution is appropriate here. The 
test document is compressed by means of character-based language or author models. Those 
models have been trained on representative training texts in various languages or from 
various authors. Every language or author has its of his own model. The real language or 
author of the test document is the one that has the best compression performance on the 
model (Teahan & Cleary, 1997). In a small appendix ‘Application of Data Compression 
Algorithms in Authorship Attribution’, Kukushkina et al. use lossless data compression 
algorithms. Those algorithms recreate the original file exactly, contrary to lossy 
compression algorithms. A zipper called rarw version 2.00 achieves an accuracy of 86,59%, 
which is more than the 84.15% accuracy Markov Chains achieve (Kukushkina et al., 2002: 
Appendix). It is clear that there is quite some controversy on the use of data compression 
algorithms on categorization, but there is still a lot of research to be done with this type of 
techniques. 
  39 
1.5 Authorship Attribution (AA) 
 
Let us first recapitulate the main focuses of this chapter. A short introduction was given in 
Section 1.1 and Section 1.2 described the types of features used in research in Authorship 
Attribution (AA). The third section of this first chapter explained the different 
Computational Linguistics tools and Section 1.4 considered which techniques for Text 
Categorization problems have been or could be applied. 
It is time to examine the matter a bit further. Section 1.5 will go into the central 
problem of this paper: Authorship Attribution. Section 1.5.1 gives a short task description 
and some background, while Section 1.5.2 discusses a number of applications of Authorship 
Attribution. An overview of the research is given by means of a selected number of articles 
in Section 1.5.3. Finally, Section 1.5.4 considers the current state of the art and some future 
directions in Authorship Attribution. 
 
1.5.1 Task Description 
 
Authorship Attribution (AA) can be described as the search for style characteristics which 
are not under the author’s conscious control. In Section 1.2, we gave a detailed overview of 
the features for AA. Most of the studies are based on tokens or words and their frequencies 
of occurrence. But since a few years, syntax-based features have been suggested as a new 
path for capturing of the specific style characteristics of authors. Until now, not much 
research has been done with that type of features. Diederich et al. formulate difficulties to 
the selection of features for Authorship Attribution: 
 
However, the style of an author may be variable because of 
differences in topic or genre, and the personal development of 
the author over time. It may also be changed by explicit 
imitation of literary styles. Ideally stylometry should identify 
features which are invariant to these effects but are expressive 
enough to discriminate an author from other writers. 
(Diederich et al., 2000: 2) 
 
Although there are a lot of techniques for Authorship Attribution (cf. Section 1.3), 
the majority of the studies applies statistical techniques because they are easy to compute 
and because they are believed to offer an objective method. Holmes did an experiment using 
  40 
the most frequently used features and techniques in Authorship Attribution studies, namely 
token-level and word-level features and statistical techniques. He states the following about 
stylometry (Holmes 1994): 
 
The statistical analysis of a literary text can be justified by the 
need to apply an objective methodology to works which for a 
long time may have received only impressionistic and 
subjective treatment. […] The stylometrist therefore looks for 
a unit of counting which translates accurately the “style” of 
the text, where we may define “style” as a set of measurable 
patterns which may be unique to an author. (Holmes, 1994: 
87) 
 
1.5.2 Applications 
 
One of the domains to which Authorship Attribution can be applied, is forensic linguistics. 
This can be defined as: 
 
The science of analysis and measurement of texts or 
documents which interface with the law, for example a 
statement allegedly made by an accused person, a term paper 
which is suspected of having been plagiarised, written 
research which may have been stolen by other researchers. 
Forensic linguistics uses linguistic and statistical means to 
determine such questions as text alteration, authorship, etc. 20 
 
Several universities have sought the advice of a forensic linguist on the detection of student 
plagiarism, and murderers have been convicted on the basis of texts attributed to them20. 
The Forensic Linguistics Institute (FLI) developed a style checker, which will be discussed 
in Section 1.5.4. Krsul and Spafford discuss features that distinguish between different 
authors for the attribution of authorship of a program using Discriminant Analysis and 
Neural Networks. They work with three types of features that are believed to be able to 
distinguish between authors of programs: programming layout metrics, programming style 
metrics, and programming structure metrics. Classification on Neural Networks achieves an 
                                                 
20 Forensic Linguistics Institute (FLI): 
http://www.thetext.co.uk/info.html. 
  41 
error rate of less than 2% (Krsul & Spafford, 1997). Plagiarism in student essays can be 
detected by means of word trigrams or an idiolect profiling technique, which enables us to 
detect that an essay is not written by the alleged author even though we have not identified 
the actual source (Van Halteren, 2003: Abstract). Internet plagiarism is a regrettable result 
of the rapid expansion of the World Wide Web (WWW) and could be controlled with the 
help of an Authorship Attribution program (Diederich et al., 2000: 109; Stamatatos et al., 
2001a: 471). 
An application closer to the roots of research in Authorship Attribution is disputed 
authorship. A famous case is that of the Federalist Papers, which were written in 1787-
1788 by Alexander Hamilton, John Jay, and James Madison in order to persuade the citizens 
of the State of New York to ratify the U.S. Constitution. There is a consensus on the 
authorship of most of the papers: John Jay wrote 85 papers, Hamilton 51, Madison 14, and 
Madison and Hamilton together 3. The remaining 12 papers are subject to a lot of 
discussion. Mosteller and Wallace were the first researchers to apply statistical inference to 
the problem. They found that the 12 papers were written by Madison (Mosteller & Wallace, 
1964). Another example of research about disputed authorship is the case of the Claremont 
Shakespeare Authorship Clinic, which tried to determine whether Shakespeare’s canon was 
really written by Shakespeare or by the thirty-seven claimants (Foster, 1999: 491). Foster 
heavily criticises the Clinic: 
 
Foster (1996b) details extensive and persistent flaws in the 
Clinic’s work; data were collected haphazardly; canonical and 
comparative text-samples were chronologically mismatched; 
procedural controls for genre, stanzaic structure, and date 
were lacking. (Foster, 1999: 491) 
 
A subject matter related to Authorship Attribution is the detection of the age or 
gender of the author. Koppel et al. use ideas from stylometry and Text Categorization and 
achieve surprising results (Koppel et al., 2003: 42). The gender of an author of an unseen 
formal written document is categorized with an accuracy of approximately 80% when 
function words and POS n-grams are used in a tandem (Koppel et al., 2003: 409). The 
Koppel-Argamon Gender Predictor is available on the internet21. 
 
                                                 
21 Koppel-Argamon Gender Predictor: 
http://www33.brinkster/echoloc8/Default.asp. 
  42 
1.5.3 Research in Authorship Attribution 
 
This section briefly summarizes a selected number of articles in the Authorship Attribution 
domain with special attention for the features, CL tools and techniques that have been 
applied. 
The article ‘Outside the Cave of Shadows: Using Syntactic Annotation to Enhance 
Authorship Attribution’ written by Baayen et al. is very important because it introduces the 
use of frequencies of rewrite rules in Authorship Attribution and compares this type of 
features with word-based measures. Rewrite rules have the advantage that they are less 
variable than the words an author uses. The most frequent rewrite rule is V:VP  MVB:LV, 
which means that the verb phrase (VP) functioning as a verb (V) consists of a lexical verb 
(LV) functioning as a main verb (MVB). The rewrite rules are translated into pseudo words, 
e.g. W0001 for the most frequent rewrite rule. The researchers evaluate the features with a 
Principal Components Analysis (PCA) and Discriminant Analysis (DA). They find that high- 
and low-frequency rewrite rules not only distinguish between authors, but also between 
register and text type in 95% of the texts. One of the problems with rewrite rules is that they 
require substantial time investment because the texts have to be annotated. Nevertheless, 
Baayen et al. conclude by saying that the results of their experiments suggest that: 
 
the lowest frequency ranges might provide a clue to 
authorship that is less contaminated by conscious rhetorical 
manipulation and thematic structuring that probably affect the 
higher-frequency units of analysis. (Baayen et al., 1996: 127) 
 
Another study by Diederich et al. uses Support Vector Machines (SVM) on 
frequencies of word forms, word lengths, tagwords and bigrams of tagwords. Their task is to 
find the surface that best separates the positive from the negative training examples (cf. 
Section 1.4.1). In experiments with word forms, the target author is not classified correctly 
in a number of cases (Diederich et al., 2000: 12). The input vector of the SVM consists of 
three sub-vectors: the frequency of words of different lengths, the frequency of tagwords 
(e.g. in German: vor_PRP_DAT) and the frequency of bigrams of tagwords (Diederich et al., 
2000: 13). Working with SVMs leads to good results, and their advantage is that they can 
process documents of significant length and databases with a large number of texts. For that 
reason, SVMs are currently the method of choice for authorship attribution and Text 
Categorization as a whole (Diederich et al., 2000: 15). 
  43 
In his article ‘Statistical Stylistics and Authorship Attribution, an Empirical 
Investigation, Hoover aims to examine the accuracy and effectiveness of cluster analysis of 
the frequencies of high-frequency words for grouping texts by the same author and 
distinguishing texts by different authors. The first dataset in the study consists of 3000-word 
sections of 50 contemporary novels, which are tested on the most frequent function and 
content words. Cluster Analysis is only moderately effective for contemporary novels. A 
second dataset consists of 50,000-word sections of 46 modern British and American novels. 
The accuracy increases until 87.5% after having selected the 3rd-person narratives only. A 
cluster analysis of the most frequent disambiguated homographic function words (e.g. to: 
preposition or infinitive marker) is inferior to the most frequent function words without 
disambiguation. Contemporary literary criticism forms the third dataset. The overall 
conclusion is, that a Cluster Analysis of the most frequent words is a useful first step in 
determining authorship, but that it should be combined with other features and/or techniques 
(Hoover 2001). 
Khmelev and Tweedie present a technique based on Markov chains of letters, which 
considers the probabilities of the subsequent letter: given that a particular letter is an ‘f’ 
which letters are most likely to follow it? (Khmelev & Tweedie, 2002: 300). They apply this 
technique on three text databases: 
 
1. Authors of texts in English, obtained from the Project Gutenberg archives, 
2. Data from the Baayen et al. (1996) paper: ‘Outside the Cave of Shadows’, 
3. The Federalist Papers (Khmelev & Tweedie, 2002: 302-303). 
 
Since the success rate goes up to 83,72%, we can say that the results are quite remarkable 
for linguistically microscopic data. One of its disadvantages is, that the unit is too small for 
meaningful conclusions to be reached regarding characteristics of the texts by the 
individual authors (Khmelev & Tweedie, 2002: 306). 
A sequel to the study by Khmelev and Tweedie described above is described by 
Kukushkina et al. It investigates Markov chains of letter pairs and pairs of grammatical 
classes. An important question is how much information of the POS tags they are going to 
use in their study. They distinguish between two types of grammatical classes. The first type 
consists of pairs of ‘incomplete’ or more generalized classes, which represent the fourteen 
grammatical classes in Russian and four other categories, of which one is called ‘unclear 
class’ in order to make the system more robust. The second type consists of pairs of 
‘complete’ or less generalized grammatical classes, e.g. animated noun, unanimated noun, 
  44 
qualitative adjective, relative adjective, possessive adjective, etc. According to the results of 
their study, 
the ‘complete’ grammatical classes were the most ineffective 
in authorship attribution both in the case of pairs frequencies 
and in the case of individual frequencies. (Kukushkina et al., 
2002: 181) 
 
Grammatical information is useful, efficient and comparable with letter pairs, but 
generalized or ‘incomplete’ grammatical classes are more effective than only individual or 
‘complete’ grammatical classes (Kukushkina et al., 2002: 175). 
Stamatatos et al. use the Sentence and Chunk Boundaries Detector (SCBD) which we 
discussed in Section 1.3.3 of this chapter. SCBD performs multiple pass parsing in order to 
get a higher accuracy. Three types of measures or style markers are selected on the basis of 
this CL tool: token-level, phrase-level, and analysis-level measures. The training corpus is 
trained by means of Multiple Regression and Principal Components Analysis classifies the 
test documents. This study gives encouraging results for the use of style markers for 
Authorship Attribution (Stamatatos et al. 1999). 
 
1.5.4 State-of-the-art in Authorship Attribution 
 Style-U-Check - Forensic Linguistics Institute (FLI)22 
 
Style-U-Check is developed by the Forensic Linguistics Institute (FLI) mentioned before (cf. 
Section 1.5.2). It is designed to analyse whether a given text has more than one style. The 
FLI uses three reliable indicators of style, namely average values for comma frequency, 
word length, and sentence length. The system measures the values and represents them in 
separate graphs per indicator and combines the results in a combination chart. Figure 14 
shows the combination chart for an analysis of the introduction to the first chapter of this 
paper – without the title (cf. Chapter 1). The bars represent the different sentences in the 
input text. Four of the five sentences have a cluster of values. A cluster is a congregation of 
values at any one point in the text. If a sentence has a higher comma frequency, a higher 
word length average, and more words than the average sentence of the text, we talk about a 
cluster of values. We can conclude for the introduction to the first chapter of this paper that 
all five sentences are written in a similar style. The authors of the program want to stress the 
                                                 
22 Forensic Linguistics Institute (FLI): Style-U-Check: 
http://www.thetext.co.uk/download.html. 
  45 
fact that the presence of multiple styles in a text does not inhibit multiple authorship. The 
combination of the three measures in Style-U-Check is helpful for Authorship Attribution, 
but the program cannot really be seen as an AA-system. 
 
 
 Figure 14: Combination Chart after Style-U-Check analysis 
 
 Textus Lite – Forensic Linguistics Institute (FLI)23 
 
The Style-U-Check program is no longer available on the website of the Forensic 
Linguistics Institute, but has been replaced by a program called Textus Lite. This anti-
plagiarism tool consists of three tests. First of all, there is a ‘Web Test’ which allows you to 
browse the web and search for random phrases in your text. You can test your memory of 
your own text against the actual text in a second test called the ‘Self Test’. The third test 
allows you to compare two texts for common content. The program looks for words and 
phrases the two texts have in common.  
Figure 15 shows the window in which I pasted two texts with similar contents: the 
introduction to chapter 1 (cf. Chapter 1) and the first paragraph of the conclusion of that 
same chapter (cf. Section 1.6). The program selects the common words and reports the 
number of common phrases of 50 or more letters or characters. It found no phrases that 
occur in both texts, but it does not take into account fixed phrases, such as ‘all the best’. If 
                                                 
23 Plagiarism and text software from the Forensic Linguistics Institute: 
http://www.thetext.co.uk/download.html. 
  46 
two texts have more than three common phrases per 1,000 words, this is highly indicative of 
plagiarism, according to the experience of the FLI. 
As far as the words are concerned, a match of 0.2 is significant enough to warrant a 
plagiarism investigation. In case of my two texts, there is a match of 0.54 in lexically rich 
content and the length discrepancy between the two texts is 0.59. This means that there is a 
percentage match of 32% – which is normal for an introduction and conclusion to the same 
chapter. The program advises to compare texts from other authors with that same source text 
so that you can establish a threshold for the degree of similarity. 
 
 
     Figure 15: Plagiarism test 3 
  47 
1.6 Conclusion 
 
In this first chapter we introduced the main issues in Authorship Attribution. Of the four 
types of features, especially the word- and token-based ones are very popular. A new path 
for Authorship Attribution is the use of syntax-based features like Part-of-Speech tags and 
rewrite rules. We discussed the available Computational Linguistics tools that are needed 
for the syntax-based ones. 
The predominant classification techniques for Authorship Attribution are statistical. 
Two new promising methods have appeared recently. There has been some research in 
Authorship Attribution with Machine Learning techniques, and the existing experiments 
suggest that the future may lie in the extensive use of Artificial Neural Networks. The use of 
Data Compression algorithms as an alternative approach to classification of documents is 
subject to a lot of controversy. Some researchers believe it has already been demonstrated 
that those algorithms have a higher error rate than the simplest Machine Learning technique, 
while others believe they are the future of Text Categorization research. After a detailed 
overview of the techniques, we went a bit deeper into the central task of this thesis. 
Authorship Attribution, though a research topic since decades, is actually still in the 
beginning phase of its development. 
In the next chapter, we will describe the circumstances and results of a number of 
experiments with syntax-based, lexical and token-level features using Machine Learning 
techniques and scripts in the AWK programming language. 
 
  48   
2. Experiments 
 
There is still a lot of research to be done concerning the efficiency of syntax-based features 
(cf. Section 1.2.2) and Machine Learning (ML) techniques (cf. Section 1.4.1) for Authorship 
Attribution. The most convincing reason for applying syntax-based features rather than 
word-based features is, that they are connected to the more abstract, largely unconscious 
and hence most revealing habits of our authors. Furthermore, they are subject to intra-
textual variation to a lesser extent than the use of words (Baayen et al., 1996: 128). 
The central hypothesis of this paper is, that Authorship Attribution can be seen as a 
special case of Text Categorization and that using advanced language technology techniques 
for extracting linguistic features and Machine Learning for author identification gives 
acceptable results. The training and test experiments described below support this 
hypothesis. 
Another important hypothesis is, that syntax-based features are stable characteristics 
of any author’s style and therefore able to predict the author of an unknown text. We will 
support this hypothesis by comparing the performance of syntax-based, lexical and token-
level style markers on training and test data. Classification is performed by means of ML 
algorithms. 
In this chapter we will report a number of experiments of which the overall goal is to 
investigate whether those features and techniques can be applied for the identification of the 
author of a text and we will also demonstrate how well they perform. We stated above that 
the main goal of most National Language Processing (NLP) tools – like Authorship 
Attribution – is to extract meaning from text by means of Text Mining and illustrated this by 
means of Figure 1 (cf. Section 1.1). Now we can complete this figure by adding the shallow 
parsing tools we need to extract the syntax-based features so that we get a clear overview of 
the task (Figure 16). 
In the first section of this chapter the most important measures for evaluating 
experiments are introduced (Section 2.1). The following sections focus on the different steps 
in the inductive approach. Our first concern is the extraction of style markers. In Section 
2.2, the selected training and test corpora are introduced. By means of Computational 
Linguistics (CL) tools and scripts in the AWK programming language, document feature 
vectors are constructed on the basis of those corpora (Section 2.3). Section 2.4 discusses the 
training (Section 2.4.1) and testing phases (Section 2.4.2) of document classification. These 
basically have the same structure. First, the training and test corpus used for the experiments 
  49   
are presented, and then the experimental design is discussed. The results of the different 
experiments are presented in graphs and discussed. In Sections 2.4.1.3 and 2.4.2.3, 
conclusions are drawn concerning the use of syntax-based features and ML techniques for 
Authorship Attribution. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 16: The extraction of meaning from text by means of CL tools and NLP Applications24 
 
 
                                                 
24 Walter Daelemans (2003) CNTS: Centrum voor Nederlandse Taal en Spraak. 
http://cnts.uia.ac.be/~walter/courses/cs/. 
NLP Applications 
OCR 
Spelling Error Recognition 
Grammar Checking 
Information Retrieval 
 
 
 
 
 
Dialogue Systems 
Machine Translation 
Document Classification 
Information Extraction 
Summarization 
Question Answering 
Ontology Extraction 
Text 
 
 
 
 
 
 
 
 
 
 
 
 
Meaning 
CL tools 
Lexical / Morphological analysis 
 
 
 
 
 
 
Semantic Analysis 
Reference Resolution 
Discourse Analysis 
Tagging 
Chunking 
Syntactic Analysis 
Word Sense Disambiguation 
Grammatical Relation Finding 
Named Entity Recognition 
  50   
2.1 Evaluation measures 
 
There are several measures for the evaluation of classifiers in the Authorship Attribution 
task. These should be introduced before we use them in the following sections, when we 
discuss the performance of the classifiers (cf. Sections 2.4.1.2 and 2.4.2.2). Since analytical 
evaluation is subjective and therefore nonformalizable (Sebastiani, 2002: 32), the evaluation 
of classifiers will be conducted experimentally. Experimental evaluation can be defined as 
follows: 
 
The experimental evaluation of a classifier usually measures 
its effectiveness (rather than its efficiency), that is, its ability 
to take the right classification decisions. (Sebastiani, 2002: 
32) 
 
Precision, recall, F-score and accuracy are the most important measures for 
experimental evaluation. First we give a short definition of the measure and then we 
illustrate it by means of an example. On the basis of this confusion table, we can explain the 
different measures (Sebastiani, 2002: 33): 
 
Expert judgements 
Category 
Yes No 
Yes True Positives (TPi) False Positives (FPi) 
Classifier judgements 
No False Negatives (FNi) True Negatives (TNi) 
 
Precision (π) is the probability that, if a random document is classified under a 
specific category, this decision is correct. Recall (ρ) can be defined as the probability that, if 
a random document ought to be classified under a specific category, this decision is taken 
(Sebastiani, 2002, 33). The F-score is calculated as the harmonic average of precision and 
recall. Accuracy is the number of correct predictions divided by the number of decisions 
made (Tjong Kim Sang, 2002: 566). The F-score is generally accepted as the more reliable 
evaluation measure. Error reduction computes the reduction in error after an analysis by a 
second classifier. It can be applied to all evaluation measures25. 
                                                 
25 Performance Measures for Machine Learning: 
http://www.cs.cornell.edu/Courses/cs578/2003fa/performance_measures.pdf. 
  51   
ii
i
i FPTP
TP
+
=π  
ii
i
i FNTP
TP
+
=ρ  
ii
iiF
ρπβ
ρπβ
β +∗
∗∗+= 2
2 )1(
 
iiii
ii
TNFNFPTP
TNTP
Accuracy
+++
+=  
 
In a two-class Authorship Attribution problem with 100 text documents for the class 
of texts by William Shakespeare and 100 text documents for the class of Christopher 
Marlowe, the confusion table could be as follows: 
 
Expert judgements 
Category 
Shakespeare Marlowe 
Shakespeare 67 27 
Classifier judgements 
Marlowe 33 73 
 
This means that 67 of Shakespeare’s texts were assigned to Shakespeare by the 
classifier and that 73 of Marlowe’s texts were correctly assigned to Marlowe. 33 of 
Shakespeare’s texts were assigned to Marlowe and 27 of Marlowe’s texts to Shakespeare. 
The Precision of the decisions made by the classifier is 71.28%, while the Recall is 67%. 
The F-score is 69% and the accuracy is 70%. When another classifier assigns 80 of 
Shakespeare’s texts correctly and 90 of Marlowe’s texts correctly, we can say that the 
reduction in error rate is 39.39% for Shakespeare and 67.96% for Marlowe. 
In classification tasks with more than two categories, there are two methods to obtain 
estimates of precision (π) and recall (ρ): micro- and macro-averaging. Micro-averaging 
computes π and ρ by summing over all individual decisions, which means that the different 
contingency tables are reduced to a single contingency table by summing over category-
specific contingency tables. Macro-averaging, on the other hand, first evaluates π and ρ 
‘locally’ for each category, and then ‘globally’ by averaging over the results of the different 
categories. Depending on the application requirements, you can select micro- or macro-
averaging (Sebastiani, 2002: 33). 
  52   
2.2 Corpora 
 
In this section, we discuss the corpora selected for the training and test phases. The style 
markers we will extract by means of scripts in the AWK programming language (cf. Section 
2.3.2), are based on these corpora. The document feature vectors are the basis for the 
experiments we present in Section 2.4. 
 
2.2.1 Training corpus 
 
The corpus used for the training phase consists of three hundred articles downloaded from 
the online archive of the Belgian daily newspaper De Standaard26. The goal of the 
experiments is to differentiate between two authors who both write about national politics: 
Anja Otte and Bart Brinckman. 
It is essential to Authorship Attribution that the authors in the corpus write about the 
same topic because otherwise the vocabulary and style of writing will differ. In order to 
make the system more robust, there is a third class of ‘Other writers’. That way, the system 
will not only be able to identify the article as written by Anja Otte (class A) or Bart 
Brinckman (class B), but also as not being written by the other ten authors from class O or 
by both Anja Otte and Bart Brinckman together. The collaborative articles may be 
interesting for later research on the attribution of authorship to articles written by two 
authors. 
The average article in the De Standaard corpus consists of 582 words. The texts are 
saved in *.txt files so that the Memory-Based Shallow Parser (MBSP) (cf. Section 2.3.1) is 
able to tokenise, tag, chunk, and parse them. Unnecessary headings like De Standaard 
Online, the author’s name, and the date of the article have been removed. 
Figure 17 shows the structure of the training corpus, while Figure 18 describes the 
average training corpus texts per class according to the features we extracted from the MBSP 
output (cf. Section 2.3.1) by means of AWK scripts (cf. Section 2.3.2). Despite the fact that 
the texts belonging to the three classes seem quite similar in respect to the features, subtle 
differences will appear to be able to classify the author of a text in 78% of the cases with 
TOTAL.arff after training-and-validation (cf. Section 2.4.1.2). 
                                                 
26 De Standaard Online: 
www.destandaard.be. 
  53   
CLASS 
# OF 
WORDS 
AUTHOR NAME 
# OF 
ARTICLES SECTION 
A 57,682 Anja Otte 100 National politics 
B 54,479 Bart Brinckman 100 National politics 
O ‘The Others’ 62,531 Anja Otte & Bart Binckman 
Alexandra De Laet 
Bart Dobbelaere 
Mark Eeckhaut 
Guy Fransen 
Inge Ghijs 
Dominique Minten 
Michel Vandersmissen 
Filip Verhoest 
Antoon Wouters 
Tom Ysebaert 
10 
9 
9 
9 
9 
9 
9 
9 
9 
9 
9 
National politics 
Total 174,692  300  
       Figure 17: The De Standaard Training Corpus 
 
 
                     FEATURE 
    CLASS 
 
# 
W
O
R
D
S 
A
D
J 
A
R
T 
N
O
U
N
S 
PR
EP
 
V
ER
BS
 
PV
 E
V
 
PV
 M
V
 
PV
 M
ET
-T
 
IN
F 
V
D
 
O
D
 
R
EA
D
A
BI
LI
TY
 
A 
B 
O 
577 
545 
625 
35 
39 
41 
59 
63 
66 
121 
118 
137 
66 
68 
78 
81 
76 
89 
24 
24 
24 
10 
9 
13 
18 
14 
17 
18 
19 
20 
9 
9 
12 
1 
1 
2 
40.354 
41.4998 
40.5417 
TOTAL 582 38 63 125 71 82 24 11 16 19 10 1 40.7985 
Figure 18: The average training corpus text 
 
 
2.2.1 Test corpus 
 
The corpus used for the testing phase consists of hundred articles downloaded from the 
online archive of the Belgian daily newspaper De Standaard27. The goal of the test 
experiments is to evaluate the model trained on the texts of three author groups in the 
national politics section of the newspaper. 
The average article in the De Standaard corpus consists of 683 words. The texts are 
saved in *.txt files so that the Memory-Based Shallow Parser (MBSP) (cf. Section 2.3.1) is 
able to tokenise, tag, chunk, and parse them. Unnecessary headings like De Standaard 
                                                 
27 De Standaard Online: 
www.destandaard.be. 
  54   
Online, the author’s name, and the date of the article have been removed. Figure 19 shows 
the structure of the training corpus: 
 
CLASS 
# OF 
WORDS 
AUTHOR NAME 
# OF 
ARTICLES SECTION 
A 20739 Anja Otte 34 National politics 
B 25684 Bart Brinckman 34 National politics 
O ‘The Others’ 21871 Anja Otte & Bart Binckman 
Bart Dobbelaere 
Mark Eeckhaut 
Guy Fransen 
Inge Ghijs 
Dominique Minten 
Michel Vandersmissen 
Filip Verhoest 
Antoon Wouters 
Tom Ysebaert 
Alexandra De Laet 
3 
3 
3 
3 
3 
3 
3 
3 
3 
3 
2 
National politics 
Total 68294  100  
       Figure 19: The De Standaard Test Corpus 
 
Figure 20 describes the average test corpus texts per class according to the features we 
extracted from the MBSP output (cf. Section 2.3.1) by means of AWK scripts (cf. Section 
2.3.2). Subtle differences will again appear to be able to classify the author of a text in 62% 
of the cases with TOTALTEST.arff after testing on the trained classifier (cf. Section 2.4.2.2). 
 
 
                     FEATURE 
    CLASS 
 
# 
W
O
R
D
S 
A
D
J 
A
R
T 
N
O
U
N
S 
PR
EP
 
V
ER
BS
 
PV
 E
V
 
PV
 M
V
 
PV
 M
ET
-T
 
IN
F 
V
D
 
O
D
 
R
EA
D
A
BI
LI
TY
 
A 
B 
O 
610 
755 
683 
39 
54 
48 
65 
90 
71 
126 
160 
149 
72 
95 
82 
85 
104 
101 
25 
35 
28 
10 
10 
15 
21 
19 
18 
17 
26 
24 
10 
12 
13 
2 
2 
2 
39,1263 
41,3614 
40,6777 
TOTAL 683 47 75 145 83 96 29 12 19 22 12 2 40,5219 
Figure 20: The average test corpus text 
 
  55   
2.3 Style markers 
 
There are four steps to be distinguished in the inductive or Machine Learning (ML) approach 
to Text Mining. The first step is to make separate files for all texts from the different authors 
(cf. Section 2.2 for a discussion of the corpora). The construction and selection of features is 
probably the most important step in the classification process. Those features have to be 
represented in document feature vectors which are needed for the Machine Learning 
algorithms to be able to classify the texts according to a specific document category (cf. 
Sections 2.4.1.2 and 2.4.2.2 for a discussion of the selection and performance of Machine 
Learning algorithms). 
This section consists of two subsections which represent the two phases in the 
construction of document feature vectors based on syntax-based, lexical and token-level 
features: pre-processing (Section 2.3.1) and the extraction of style markers (Section 2.3.2). 
  56   
2.3.1 Pre-processing: Memory-Based Shallow Parsing (MBSP) 
 
As we stated above (cf. Section 1.2.2), syntax-based features need the intervention of a CL 
tool like a POS-tagger (cf. Section 1.3.3) or a shallow parser (cf. Section 1.3.4). For our 
Authorship Attribution experiments, we used Memory-Based Shallow Parsing (MBSP)28 
based on the spoken Dutch corpus (CGN)29 to analyse the text documents syntactically. 
There are a lot of similarities between the English and Dutch versions of MBSP. Therefore 
we will refer to articles about MBSP for English in our discussion. 
Before analysing the output MBSP generates, we should explain what Memory-Based 
Learning (MBL) means. It is an example of a lazy learner (cf. Section 1.4.1) because it 
memorizes all training data in order to achieve a better performance. Just like kNN-
classifiers, Memory-Based Learning are more accurate than eager methods for many 
language processing tasks. Natural language involves a lot of regularities, subregularities 
and exceptions. Whereas eager learners tend to make abstractions, lazy learners remember 
all exceptions they come across (Daelemans et al., 1999: 53). 
The three memory-based modules are POS tagging, chunking and the identification of 
syntactic relations. The output MBSP for Dutch generates is organised as follows: 
De De LID(bep,stan,rest) B-NP B-SU O 
kandidaten kandidaat N(soort,mv,basis) I-NP I-SU O 
moesten moeten WW(pv,verl,mv) B-VP B-HD O 
alleen alleen BW() O O O 
laten laten WW(inf,vrij,zonder) B-VP O O 
weten weten WW(inf,vrij,zonder) B-VP O O 
in in VZ(init) B-PP O O 
welke welk VNW(vb,det,stan,prenom,met-e,rest) B-NP O O 
gevangenis gevangenis N(soort,ev,basis,zijd,stan) I-NP O O 
ze ze VNW(pers,pron,stan,red,3,mv) B-NP B-SU O 
het het VNW(pers,pron,stan,red,3,ev,onz) B-NP O O 
liefst liefst BW() O O O 
zouden zullen WW(pv,verl,mv) B-VP B-HD O 
werken werken WW(inf,vrij,zonder) B-VP O O 
. . LET() O O O 
 Figure 21: Sample MBSP output 
Legend for POS tags: see Appendix 2 
                                                 
28 MBSP: Memory-Based Shallow Parsing for English: 
http://ilk.uvt.nl/cgi-bin/tstchunk/demo.pl. 
29 Het Corpus Gesproken Nederlands (CGN): 
http://lands.let.kun.nl/cgn/home.htm. 
  57   
As can be seen from the example, the sentences are represented vertically. We can 
therefore say that the output consists of six fields. The first field contains the original word. 
Lemmas – dictionary entries of the words – are represented in field 2. Field 3 consists of the 
POS tag with extra information about the grammatical class and morphology of the word (cf. 
Appendix 2). The fourth field shows the output after the chunking task, which can be 
described as follows: 
 
A text chunker divides sentences in phrases which consist of a 
sequence of consecutive words which are syntactically 
related. (Tjong Kim Sang, 2002: 567) 
 
The chunker adds tags for each word inside a Noun Phrase (NP), Prepositional Phrase (PP) or 
Verb Phrase (VP). A word inside a baseNP is tagged as I_NP, while a word outside a baseNP 
or VP is symbolised by O. B_NP is the tag for a word inside a baseNP, with the preceding 
word being another baseNP (Daelemans et al., 1999: 54). MWU means merged-word-unit and 
is a symbol for complex names and numbers (Hoekstra et al., 2001: 85). CLB indicates 
coordinating and subordinating conjunctions. The results of the Subject and Object 
Detection task are represented in the fifth field (Daelemans et al., 1999: 55). First names and 
surnames are represented by means of I-FIRST and I-SUR in the sixth field. 
There are two remarks to be made concerning the output MBSP based on the Spoken 
Dutch Corpus (CGN) generates. These are added in Appendix 3. 
  58   
2.3.2 Extraction of style markers 
 
As we said above, the Memory-Based Shallow Parsing (MBSP) tool is used for the extraction 
of style markers. The goal of the experiments this paper reports, is to investigate whether 
syntax-based features (cf. Section 1.2.2) can be useful for Authorship Attribution. The style 
markers this study focuses on are: 
 
1. the frequency and distribution of parts-of-speech (POS) 
2. the frequency and distribution of basic verb forms 
3. the frequency and distribution of specific verb forms 
4. the presence or absence of specific Noun Phrase (NP) patterns 
5. the frequency and distribution of specific Noun Phrase (NP) patterns 
6. the presence or absence of specific words based on mutual information 
a. for training purposes 
b. for testing purposes 
7. the readability score 
8. a combination of the measures 
 
The four first style markers are syntax-based features (cf. Section 1.2.2). The choice for 
those specific features is based on suggestions by Glover & Hirst concerning features based 
on tagged text (1995: 4). The other two style markers are on the token level (cf. Section 
1.2.1). This enables us to compare the performance of classifiers on the two types of 
features. 
In order to be able to compute the frequencies and distributions listed above, we 
created a number of scripts in the AWK programming language. The features extracted by 
means of these scripts are represented in document feature vectors. The so-called ARFF 
(Attribute-Relation File Format) files were developed for use with the WEKA machine 
learning software and consist of two distinct sections. The first one is the Header, which 
contains the name of the relation, a list of the attributes and their types. This section is 
followed by the Data information which contains a list of numeric values separated by 
commas. For each attribute in the Header section, there is a value in the Data section. Each 
line is a single instance or text document in this case. The last entry of the document feature 
vector is the class30. An example of a document feature vector for one instance is given 
below: 
                                                 
30 Attribute-Relation File Format (ARFF): 
http://www.cs.waikato.ac.nz/~ml/weka/arff.html. 
  59   
@RELATION pos.arff 
 
@ATTRIBUTE adj REAL 
@ATTRIBUTE adv REAL 
@ATTRIBUTE punc REAL 
@ATTRIBUTE art REAL 
@ATTRIBUTE noun REAL 
@ATTRIBUTE propn REAL 
@ATTRIBUTE interj REAL 
@ATTRIBUTE num REAL 
@ATTRIBUTE conj REAL 
@ATTRIBUTE pron REAL 
@ATTRIBUTE prep REAL 
@ATTRIBUTE verb REAL 
@ATTRIBUTE total REAL 
@ATTRIBUTE adjtot REAL 
@ATTRIBUTE advtot REAL 
@ATTRIBUTE punctot REAL 
@ATTRIBUTE arttot REAL 
@ATTRIBUTE nountot REAL 
@ATTRIBUTE propntot REAL 
@ATTRIBUTE interjtot REAL 
@ATTRIBUTE numtot REAL 
@ATTRIBUTE conjtot REAL 
@ATTRIBUTE prontot REAL 
@ATTRIBUTE preptot REAL 
@ATTRIBUTE verbtot REAL 
@ATTRIBUTE class {A,B,O} 
 
@DATA 
 
25,25,59,57,120,26,0,26,15,22,52,69,496,0.050403,0.050403.0.118952 
,0.114919,0.241935,0.052419,0,0.052419,0.030242,0.044355,0.104839 
,0.139113,A 
Figure 22: Fragment of POS.arff 
 
These ARFF files are the input for Machine Learning (ML) algorithms in the WEKA 
software package (Witten & Frank, 1999). The classifiers in WEKA will hopefully allow us 
to identify the author of the texts in our corpora (cf. Section 2.2). 
In the following subsections we motivate our choice for each specific feature and 
explain the goals and output of the AWK scripts. All scripts are added in appendix. 
For the sixth feature set (viz. the presence or absence of specific words based on 
mutual information), we use a program called Rainbow31. This program performs statistical 
text classification. 
 
 
 
 
 
 
 
                                                 
31 Rainbow: 
http://www-2.cs.cmu.edu/~mccallum/bow/rainbow. 
  60   
 1. Frequency and distribution of parts-of speech (POS) 
 
Because most word-based (cf. Sections 1.2.3 and 1.2.4) features are highly author and 
language dependent, rules inferred by Machine Learning (ML) classifiers cannot be 
generalised to other authors or other languages (Stamatatos et al., 1999: 159). Syntax-based 
features like parts-of-speech do not have this problem because they are not under the 
conscious control of the author. The distribution of parts-of-speech is seen as a possible 
feature for Authorship Attribution (Glover & Hirst, 1995: 4). 
The construction of a document feature vector for the frequency and distribution of 
parts-of-speech (POS) is performed by means of a script in the AWK programming language 
we called pos.awk.txt (cf. Appendix 4). 
As we said above when we discussed Memory-Based Shallow Parsing (MBSP) (cf. 
Section 2.3.1), the output is represented in columns or fields. The AWK programming 
language splits up each line into fields and represents them by means of the symbol $, 
followed by the number of the column. Since the third field ($3) contains the part-of-speech 
of each word, the AWK script that computes the frequency of adjectives (ADJ) is built up as 
follows: 
{ if ($3 ~ /^ADJ/) adjectives++ } 
END{ 
   if (adjectives == 0) 
      printf(0",") 
   else printf(adjectives",") 
} 
The following figure represents all POS tags and their mean frequency over the three author 
classes (cf. Section 2.2): 
POS TAG EXPLANATION MEAN FREQUENCY 
ADJ adjectives 38 
BW adverbs 33 
LET punctuation 72 
LID articles 63 
N nouns 125 
SPEC proper nouns 22 
TSW interjections 0.19 
TW numerals 10 
VG conjunctions 21 
VNW pronouns 45 
VZ prepositions 71 
WW verbs 82 
   Figure 23: List of POS tags and their mean frequency 
  61   
We also computed the distribution of parts-of-speech so that the length of the text 
document would not influence classification. The number of adjectives is divided by the 
number of POS in the entire document. 
{ if ($3 =! /^ /) total++ } 
END{ printf(adjectives/total",") } 
The AWK interpreter reads the script and evaluates its syntax. The input file is read line after 
line. It checks whether each line satisfies the conditions formulated in the script. If that is 
indeed the case, the actions are performed. The output for one instance is as follows (the 
input file was 1A.txt.mbsp): 
25,25,59,57,120,26,0,26,15,22,52,69,496,0.050403,0.050403.0.11
8952,0.114919,0.241935,0.052419,0,0.052419,0.030242,0.044355,0
.104839,0.139113,A 
All document feature vectors from the training corpus are saved in POS.arff, those from the 
test corpus in POSTEST.arff. We will use this file for our machine learning experiments (cf. 
Section 2.4). 
 
 2. Frequency and distribution of basic verb forms 
 
According to Glover and Hirst, verb forms are also plausible syntax-based style markers for 
Authorship Attribution (Glover & Hirst, 1995: 4). In order to be able to investigate how 
much grammatical information is needed, we decided to construct separate document 
feature vectors for the basic verb forms and for specific verb forms. In the following 
subsection we will discuss the AWK script for the second group. 
The basic verb forms Memory-Based Shallow Parsing (MBSP) uses, are based on the 
Spoken Dutch Corpus (CGN), which distinguishes six basic verb forms (cf. Appendix 2). 
The AWK script verbbasic.awk.txt is built up in the same way as pos.awk.txt (cf. Appendix 
5). 
If the third field – the one that represents the POS tags – contains pv,tgw,ev 
pv,verl,ev or pv,conj,ev, these are counted to form the number of singular inflected verb 
forms. 
{ 
if ($3 ~ /pv,tgw,ev/ || $3 ~ /pv,verl,ev/ || $3 ~ /pv,conj,ev/) 
   pvev++ 
} 
The same is done for plural inflected verb forms, inflected verb forms ending in -t (e.g. third 
person singular zoekt), infinitives, past and present participles. The distribution of basic verb 
  62   
forms is computed by dividing the number of singular inflected verb forms with the total 
number of verb forms in the text document. 
{ if ($3 ~ /WW/) WW++ } 
END{ 
   printf(pvev/WW",") 
} 
The following figure represents all basic verb forms and their mean frequency over the three 
author classes (cf. Section 2.2): 
VERB FORMS MEAN FREQUENCY 
pv ev 24 
pv mv 11 
pv met-t 16 
inf 19 
vd 10 
od 1 
Figure 24: List of POS tags and their mean frequency 
 
After invoking the script on all MBSP files, the output is saved in VERBBASIC.arff (training) 
and VERBBASICTEST.arff (test). It is organised as follows (the input file was 1A.txt.mbsp): 
19,14,14,11,10,1,69,0.275362,0.202899,0.202899,0.15942,0.144928,0.01 
4493,A 
 
 3. Frequency and distribution of specific verb forms 
 
The specific verb forms that are meant here, are the ones we found in the MBSP input files. 
There are seventeen specific verb forms: 
1 
2 
WW(pv,tgw,ev) 
WW(pv,tgw,met-t) 
 10 
11 
WW(vd,nom,met-e,zonder-n) 
WW(vd,nom,met-e,mv-n) 
3 WW(pv,tgw,mv)  12 WW(od,vrij,zonder) 
4 WW(pv,verl,ev)  13 WW(od,prenom,zonder) 
5 WW(pv,verl,mv)  14 WW(od,prenom,met-e) 
6 WW(pv,conj,ev)  15 WW(inf,vrij,zonder) 
7 WW(vd,vrij,zonder)  16 WW(inf,prenom,zonder) 
8 WW(vd,prenom,zonder)  17 WW(inf,nom,zonder,zonder-n) 
9 WW(vd,prenom,met-e)   
       Figure 25: Specific verb forms 
 
Verb.awk.txt, the AWK script that constructs document feature vectors for the 
frequency and distribution of specific verb forms, is designed in the same way as the 
previous scripts we discussed in the above subsections (cf. Appendix 5). 
  63   
In VERB.arff and VERBTEST.arff, the output is saved. Below is an example of the 
output, based on one text document analysed by MBSP (1A.txt.mbsp): 
4,14,4,15,10,0,10,0,0,0,0,0,0,1,11,0,0,69,0.057971,0.202899,0.057971
,0.217391,0.144928,0,0.144928,0,0,0,0,0,0,0.014493,0.15942,0,0,A 
 
 4. Presence or absence of specific Noun Phrase (NP) patterns 
 
Word-class patterns are other syntax-based features that could be able to help us identify the 
author of a text (Glover & Hirst, 1995: 4). A first step in investigating whether that is true, is 
to indicate which specific NP patterns are present in the text document. The second step (cf. 
next subsection) is constructing a document feature vector for the frequency and distribution 
of those patterns. 
First np.awk.txt was developed, an AWK script that lists all pos tags per sentence, 
separated by a whitespace (cf. Appendix 7). The output is saved in *rij.txt. Below are the 
first five sentences of the output (the input file was 1A.txt.mbsp): 
TW VZ TW ADJ N BW VZ N N WW N N SPEC BW WW LID TW VZ LID TW ADJ N VNW N LET 
BW WW LID N VNW N VG LID N VZ N VZ LID N VZ LID ADJ N LET 
VNW N WW LID N VZ LID N VZ N N LET 
VZ VNW N WW LID N VZ ADJ ADJ N VZ LID N LET 
TW N WW LID ADJ N LID N VZ N VZ LID N BW VNW LID N WW WW LET 
The *rij.txt files will be used for the construction of document feature vectors for the 
frequency and distribution of specific NP patterns as well (cf. next subsection). 
The second step is to determine which NP patterns occur in Dutch texts. Most 
patterns are combinations of other patterns with prepositions (VZ) or conjunctions (VG). A 
complex noun phrase like het sluitstuk van het cipiersakkoord van eind mei can be analysed 
as LID N VZ LID N VZ N N. It actually consists of a combination of pattern 3, a preposition 
(vz), pattern 3, a preposition and two times pattern 1. We therefore distinguished twelve NP 
patterns: 
1 
2 
N, VNW OR SPEC 
ADJ N 
 7 
8 
LID ADJ N 
N ADJ N 
3 LID N  9 TW ADJ N 
4 N SPEC  10 LID TW N 
5 VNW N  11 N TW N 
6 TW N  12 LID TW ADJ N 
     Figure 26: Our list of NP patterns 
 
Patternstep1.awk.txt substitutes the different patterns by the word pattern and the number 
code of the pattern, while patternstep2.awk.txt makes one long list of the output (cf. 
  64   
Appendix 8). It should be mentioned that the program begins with looking for pattern12, the 
most complex NP pattern, and then pattern11, pattern10, etc. The reason for this reverse 
order is simple. Since pattern12 contains pattern9, pattern3 and pattern1, there would be no 
instances of pattern12 if the program had been in the ‘right’ order because N would have 
been substituted in the first phase of the program. The third sentence of 1Arij.txt is presented 
as follows: 
1ARIJ.TXT after PATTERNSTEP[1-2].AWK.TXT 
VNW N pattern5 
WW WW 
LID N pattern3 
VZ VZ 
LID N pattern3 
VZ VZ 
N pattern1 
N pattern1 
LET LET 
    Figure 27: Output after patternstep[1-2].awk.txt 
 
Patternstep3.awk.txt constructs a document feature vector for the presence or 
absence of the NP patterns we selected. The output is structured as follows: 
1,1,1,1,1,1,1,1,1,1,1,1,A 
The output is saved in PATTERNBIN.arff and PATTERNBINTEST.arff so that we can use the 
document feature vectors for classification by means of Machine Learning algorithms (cf. 
Section 2.4). 
 
 5. Frequency and distribution of specific Noun Phrase (NP) patterns 
 
Glover and Hirst stated that the distribution of word-class patterns would be a good syntax-
based discriminator for Authorship Attribution (Glover & Hirst, 1995: 4). We computed the 
frequency and distribution of the NP patterns we selected in the previous subsection by 
means of patternstep4.awk.txt (cf. Appendix 8). This script is invoked on the *rij.txt files in 
the command line. 
All document feature vectors have been saved in PATTERNNUM.arff and 
PATTERNNUMTEST.arff. They are organised as follows: 
71,4,41,4,9,6,6,1,2,5,2,1,152,0.467105,0.026316,0.269737,0.026316,0.
059211,0.039474,0.039474,0.006579,0.013158,0.032895,0.013158,0.00657
9,A 
 
  65   
 6. Presence or absence of specific words based on mutual information 
 
o For training purposes 
Mutual information (MI) is a measure for dimensionality reduction, which has as a goal to 
reduce the size of the vector space from |T| to |T’| ‹‹ |T|. |T’| is the reduced term set 
(Sebastiani, 2002: 13). We can define mutual information as a means to attempt to select, 
from the original set T, the set T’ of terms (with |T’| ‹‹ |T|) that, when used for document 
indexing, yields the highest effectiveness (Sebastiani, 2002: 14). We use mutual information 
to determine which ‘information’ is shared by the three author classes and is able to 
distinguish between them (cf. Section 2.2). In our case this ‘information’ will be words. 
The program we used for the extraction of document feature vectors for the presence 
(value 1) or absence (value 0) of the selected words based on mutual information is called 
Rainbow. This program performs statistical text classification. First, a model of the three 
classes has to be built. The --prune-vocab-by-infogain=N (-T) command removes all but the 
top N words by selecting words with the highest information gain. One of Rainbow’s other 
options, --print-word-probabilities, prints a list of word probabilities. The --print-
matrix=abe option makes document feature vectors that represent binary presence/absence 
information on the basis of the model32. The command is invoked as follows: 
rainbow -d ~/model -T 20 --print-word-probabilities=Anja --print-
matrix=abe 
The output (for 1A.txt) looks like this: 
 0  1  0  1  1  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0 
These document feature vectors are saved in MUTUALINFO.arff. The twenty words with 
highest mutual information Rainbow selected are: 
1 
2 
partij 
spa 
 11 
12 
vld 
beide 
3 blijkt  13 mr 
4 zegt  14 gewest 
5 wie  15 tegelijk 
6 echter  16 steeds 
7 altijd  17 erg 
8 aldus  18 afgelopen 
9 evenwel  19 momenteel 
10 blok  20 wilde 
Figure 28: Twenty words with highest mutual information 
                                                 
32 Rainbow: 
http://www-2.cs.cmu.edu/~mccallum/bow/rainbow/. 
  66   
This selection is based on the pruned vocabularies of the three author classes in this study 
(cf. Section 2.2). This feature allows us to compare the results of ML experiments on lexical, 
token-level and syntax-based features (cf. Sections 2.4.1.2). 
 
o For testing purposes 
In order to be compatible, train and test feature sets need the same structure. Our AWK 
script, mutualinfotest.awk.txt, investigates whether the specific words based on the mutual 
information are present in the texts part of the test corpus (cf. Section 2.2.2) (cf. Appendix 
9). 
 
 7. Readability score 
 
In Section 1.4.2, we discussed the readability score as a statistical technique that computes 
readability based on the average number of syllables per word and the average number of 
words per sentence. This is also called the Flesch-Kincaid Readability Formula. We also 
stated that it is plausible that every author has a more or less constant level of readability. 
Readability.awk.txt is a script that computes readability for all texts and represents 
them in a document feature vector (cf. Appendix 10). The basic formula is: 
( ) ( )ASWASL ×−×− 6.84015.1835.206  
ASL (Average Sentence Length) is the mean sentence length or the number of words divided 
by the number of sentences. ASW (Average number of Syllables per Word) is the mean 
number of syllables per word. We took 1.779667 as the average ASW over all documents. 
The output is a number between zero and hundred and it is saved in READABILITY.arff and 
READABILITYTEST.arff. The scores can be understood as follows: 
 
SCORE EXPLANATION 
90-100 very simple 
80-90 simple 
70-80 fairly simple 
60-70 standard difficulty 
50-60 fairly difficult 
30-50 difficult 
0-30 very difficult 
Figure 29: Readability scores 
 
 
 
 
  67   
 8. Combination 
 
A combination of most of the features mentioned in the above subsections (1-7) forms a sort 
of ‘super’-classifier. We selected the features and vectors from POS.arff or POSTEST.arff, 
VERBBASIC.arff or VERBBASICTEST.arff, VERB.arff or VERBTEST.arff, PATTERNBIN.arff or 
PATTERNBINTEST.arff, PATTERNNUM.arff or PATTERNNUMTEST.arff, MUTUALINFO.arff or 
MUTUALINFOTEST.arff and READABILITY.arff or READABILITYTEST.arff to be saved in 
respectively TOTAL.arff and TOTALTEST.arff. 
In TOTALAB.arff and TOTALTESTAB.arff, we saved the document feature vectors for 
two classes. This allows us to compare the results of the style markers we selected on all 
three author classes with their results on the two homogeneous classes (A and B). 
 
  68   
2.4 Classification 
 
The classification of the document vectors for syntax-based, lexical and token-level features 
(cf. Section 2.3) is performed using Machine Learning algorithms. There are two phases in 
classifying documents as belonging to specific author classes: a training (cf. Section 2.4.1) 
and a testing phase (cf. Section 2.4.2). The training set is used to form the learned 
hypothesis, while the validation set (cf. Section 2.4.2) is used to evaluate the accuracy of 
this hypothesis (Mitchell, 1997: 69). In the testing phase, new unseen text documents are 
classified by means of the trained model. 
Both sections have the same structure. First, the design of the experiments is 
discussed. Then the results of the training-and-validation and test experiments are presented 
and discussed. 
  69   
2.4.1 Training and validation 
 
The training data train and validate the machine learner (Sebastiani, 2002: 9). In Section 
2.2.1, a training corpus was selected for the training phase. The next step is to explain the 
experimental design (cf. Section 2.4.1.1). We discuss the Machine Learning algorithms 
selected for the training experiments and the feature weighting and selection techniques 
which are used for increasing performance. The results of the experiments are presented and 
discussed (cf. Section 2.3.1.2) and finally conclusions are drawn (cf. Section 2.3.1.3). 
 
 
  70   
2.4.1.1 Experimental design 
 
In this section we discuss and motivate the Machine Learning algorithms we selected to 
classify the training corpus and the feature weighting and selection techniques we used for 
improving the classifier’s performance. 
We apply the WEKA software package for Authorship Attribution experiments using 
Machine Learning algorithms, voting schemes and feature weighting and selection. WEKA, 
short for the Waikato Environment for Knowledge Analysis, developed by researchers at the 
University of Waikato in New Zealand, consists of a number of Machine Learning 
algorithms to solve Text Mining problems like Authorship Attribution. 
 
 Machine Learning (ML) algorithms 
 
We discuss a number of ML algorithms and voting schemes we used for our experiments. 
The first ML algorithm we selected is OneR, a weak classifier that selects one feature and 
derives simple feature rules per feature values. 
The Naive Bayes classifier is a strong, very simple but efficient classifier. It selects 
the most probable class given the input feature values and converts the feature-class table to 
distribution tables per class. To fill in these tables, there is a standard procedure: 
Given each new instance, 
- for each feature value, get its class distribution 
- for each class, take the product of its found distributional 
probabilities 
- the class with the highest probability is the predicted class. 
The probabilities are easy to compute but Naive Bayes does not deal with exceptions and 
loses information about the examples. Nevertheless it works a great deal better than OneR. 
Decision Trees (DT) (cf. Section 1.4.1) are present in WEKA as j48.J48, also known as 
C4.5. This DT learner was developed by J.R. Quinlan (Quinlan, 1993). Decision Tree (DT) 
Classifiers consist of a number of nodes with branches departing from them which are 
labelled by tests, while the leafs are labelled by categories. The most important parameters 
in j48.J48 are -c and -m. The Confidence Factor or -c parameter is kept at 100% or 1.0 
during the experiment. C4.5 sticks to the Minimum Description Length Principle which says 
that the best model is the one that minimizes the sum of 
 
a- the number of bytes for the rules and the model and 
b- the number of bytes to store exceptions 
  71   
For example, if the value of MinNumObj (hence -m) is two, C4.5 only makes a new branch 
if its subset covers two or more instances. This parameter is used to avoid noise. When we 
discuss the performance of the syntax-based, lexical and token-level features (cf. Section 
2.4.1.2), we will talk about the influence of the -m parameter. It’s important not to give too 
high values to -m for language data, according to Zipf’s law, which claims that: 
 
The probability of occurrence of words or other items starts 
high and tapers off. Thus, a few occur very often while many 
others occur rarely.33 
 
Especially in language data, there are a lot of exceptions that only appear a few times. If the 
-m value is too high, the Decision Tree will not cover these exceptions. The advantage of DT 
classifiers in the WEKA package is, that they show the rules they deduct from the input 
instances. Below an example of a J48 pruned tree (Figure 30): 
 
pattern6 = 1 
|   pattern12 = 1 
|   |   pattern8 = 1: O (16.0/4.0) 
|   |   pattern8 = 0: A (14.0/6.0) 
|   pattern12 = 0 
|   |   pattern10 = 1 
|   |   |   pattern8 = 1 
|   |   |   |   pattern11 = 1: O (12.0/5.0) 
|   |   |   |   pattern11 = 0 
|   |   |   |   |   pattern9 = 1: O (12.0/6.0) 
|   |   |   |   |   pattern9 = 0: B (33.0/18.0) 
|   |   |   pattern8 = 0 
|   |   |   |   pattern11 = 1 
|   |   |   |   |   pattern9 = 1: O (4.0/1.0) 
|   |   |   |   |   pattern9 = 0: B (18.0/10.0) 
|   |   |   |   pattern11 = 0 
|   |   |   |   |   pattern9 = 1: A (16.0/9.0) 
|   |   |   |   |   pattern9 = 0: B (44.0/25.0) 
|   |   pattern10 = 0 
|   |   |   pattern4 = 1 
|   |   |   |   pattern9 = 1 
|   |   |   |   |   pattern11 = 1: A (10.0/5.0) 
|   |   |   |   |   pattern11 = 0: O (15.0/6.0) 
|   |   |   |   pattern9 = 0: A (81.0/46.0) 
|   |   |   pattern4 = 0: B (2.0/1.0) 
pattern6 = 0 
|   pattern9 = 1 
|   |   pattern12 = 1 
|   |   |   pattern8 = 1: B (1.0) 
|   |   |   pattern8 = 0: A (1.0) 
|   |   pattern12 = 0: B (3.0) 
|   pattern9 = 0: B (18.0/9.0) 
 Figure 30: J48 pruned tree (on PATTERNBIN.arff) 
 
                                                 
33 National Institute of Standards and Technology: http://www.nist.gov/dads/HTML/zipfslaw.html. 
  72   
ID3 is another decision tree classifier that can only be used with binary features like 
the ones part of PATTERNBIN.arff and MUTUALINFO.arff. This DT classifier has no editable 
properties. 
A fifth ML algorithm is IBk, a k-Nearest Neigbour classifier (cf. Section 1.4.1). k-
Nearest Neighbour can be defined as 
 
an instance-based algorithm for approximating real-valued or 
discrete-valued target functions, assuming instances 
correspond to points in an n-dimensional Euclidian space. The 
target function value for a new query is estimated from the 
known values of the k nearest training examples. (Mitchell, 
1997: 246) 
 
The k-value is a parameter with influence on the performance of the classifier on the 
training data (cf. Section 2.4.1.2). 
Artificial Neural Networks, the last ML algorithm in our study, consist of a network 
of units. The input (terms i1,…, in) is weighted (w1,…, wn) and the sum (Σ) of the terms is 
calculated, taking into account the weights. If the sum is higher than a predefined threshold 
(θ), the output unit (o) is fired. Figure 31 represents the basic Neural Network: 
 
 
Figure 31: The Simple Perceptron34 
 
The training data are split up in a training and a validation set. This is done by means 
of k-fold cross validation. K-fold cross validation allows us to give an indication of how 
well the learner will do when it is asked to make new predictions for data it has not already 
seen35. The data set is divided into k subsets. K times one of the subsets is used as test set 
and the other subsets as training set. In our Authorship Attribution experiments, we use ten-
fold cross validation. 
                                                 
34 Induction of Linguistic Knowledge: 
http://ilk.kub.nl/~antalb/ltuia/nn.html. 
35 Cross Validation: 
http://www-2.cs.cmu.edu/~schneide/tut5/node42.html. 
  73   
 Bagging and Boosting voting schemes 
 
In order to optimise the ML learners’ performance, we use the Bagging and Boosting voting 
schemes: 
Breiman’s bagging and Freund and Shapire’s boosting are 
recent methods for improving the predictive power of 
classifier learning systems. (Quinlan, 1996: 725) 
 
Bagging and Boosting are two voting schemes that both train on resamples of the original 
data sets in order to reduce the error rate. The Bagging voting scheme works with weak 
classifiers that have a high variance and a small bias. 
 
Unstable classifiers such as trees characteristically have high 
variance and low bias. Stable classifiers like linear 
discriminant analysis have low variance, but can have high 
bias.’ (Breiman, 1996) 
 
Later in the same article, he writes that reducing variance is the main effect of bagging. Bias 
is to be understood as the expected value and variance as a measure for variability (Twomey 
& Smith, 1998). Bagging randomly resamples the training material and trains the weak 
classifier on each resample. The result is a series of different rules or decision trees – 
depending on the classifier – which have been found by coincidence as it were. Weighted 
voting decides which rules or decision trees score best. 
In the WEKA package, the Boosting algorithm is present in the ADABOOSTM1 
version (Freund & Schapire, 1996). Boosting works with both weak and strong classifiers, 
but it is of course interesting to see how the voting scheme works for weak classifiers. 
Below the Boosting procedure is given: 
1- train weak classifier 
2- repeat until system does not get better: 
a- gather errors made, through cross-validating the complete 
system so far (on training) 
b- create new training set with resampling: higher probability 
for erroneously classified instances 
c- train weak classifier on resampled training set 
d- add trained weak classifier to voting ensemble of 
classifiers 
e- loop 
 
If the output of the first training contains 25% errors, a new training set is 
constructed which takes twice the training data that gave the errors. If the classifiers are 
trained on these training data and if the training data are resampled a few times, you get a 
  74   
better output. Different from Bagging, Boosting not only reduces variance, but also reduces 
bias. Freund & Shapire formulate the effect of Boosting: 
 
Boosting combines two effects. It reduces the bias of the weak 
learner by forcing the weak learner to concentrate on different 
parts of the instance space, and it also reduces the variance of 
the weak learning by averaging several hypotheses that were 
generated from different subsamples of the training set. 
(Freund & Shapire, 1996: 154) 
 Increasing performance by feature weighting and selection 
 
Another way of increasing the performance of the ML algorithms is feature weighting. The 
problem is, that some features will be more informative for the prediction of the class than 
others. Those other features might distract the classifier and will not be selected after feature 
weighting. This can be solved by computing the features’ Gain Ratio. The features with the 
highest information value are selected for extra experiments. The other experiments are 
based on the entire feature set. 
In WEKA, feature weighting is performed by means of the ‘Select Attributes’ option. 
The Attribute Evaluator we selected is called WrapperSubsetEval, which evaluates attribute 
sets by using a learning scheme, in our case the Naive Bayes algorithm. The wrapper 
approach works as follows: 
 
When a new term set is generated, a classifier based on it is 
built and then tested on a validation set. The term set that 
results in the best effectiveness is chosen (Sebastiani, 2002: 14) 
 
Our Search Method is RankSearch, which uses an attribute/subset evaluator to rank all 
attributes. The Attribute Evaluator we selected is Gain Ratio. Gain Ratio is a measure that 
computes the information present in a specific attribute. The output after feature weighting 
is a list of the attributes that best separates the instances according to the classes. Below an 
example: 
Selected attributes: 6,8,14,16,19,21,23,24 : 8 
                     propn 
                     num 
                     adjtot 
                     punctot 
                     propntot 
                     numtot 
                     prontot 
                     preptot 
       Figure 32: Feature selection (on POS.arff) 
  75 
  
2.4.1.2 Performance and discussion 
 
For every group of style markers (cf. Section 2.3.2), the selected ML algorithms and voting 
schemes (cf. Section 2.4.1.1) are used to classify the training corpus (cf. Section 2.2.1) by 
means of ten-fold cross validation. After feature weighting and selection (cf. Section 2.4.1.1), 
the ML algorithms and voting schemes classify the training corpus on the basis of the style 
markers with the highest information value. We then also investigate the influence of the -m 
parameter for Decision Tree learners (cf. Section 1.4.1) and of the k value for k-Nearest 
Neighbour classifiers (cf. Section 1.4.1) on performance. 
In this section we investigate the performance of the ML classifiers trained on the 
different style markers we extracted on the basis of AWK scripts (cf. Section 2.3.2). The 
evaluation measures used are (generalisation) accuracy and F-score (cf. Section 2.1). Per style 
marker, we select the features with highest Gain Ratio (cf. Section 2.4.1.1) and perform 
experiments on those features. The influence of -m and k parameters is discussed as well. All 
results are represented in graphs. A discussion of the results is planned in Section 2.3.1.3. 
 
 1. Frequency and distribution of parts-of speech (POS.arff) 
 
In our first experiments with parts-of-speech (POS) the entire feature set is used. Later we will 
perform feature selection. We extracted the features by means of AWK scripts (cf. Section 
2.3.2). The list of features in POS.arff is shown below (Figure 33): 
1 ADJ  14 ADJTOT 
2 ADV  15 ADVTOT 
3 PUNC  16 PUNCTOT 
4 ART  17 ARTTOT 
5 NOUN  18 NOUNTOT 
6 PROPN  19 PROPNTOT 
7 INTERJ  20 INTERJTOT 
8 NUM  21 NUMTOT 
9 CONJ  22 CONJTOT 
10 PRON  23 PRONTOT 
11 PREP  24 PREPTOT 
12 VERB  25 VERBTOT 
13 TOTAL  
        Figure 33: Feature set (POS.arff) 
  76 
  
o Entire feature set 
 
Graph 1 represents the accuracy of ML algorithms and the Bagging and Boosting voting 
schemes trained on the entire feature set of POS.arff. Ten-fold cross validation allows us to test 
the classifiers trained on the training corpus. A performance of 58% accuracy is obtained by 
means of the Bagging voting scheme with a Decision Tree classifier – in this case j48.J48. The 
best performances are achieved with Naive Bayes, Decision Trees and Neural Networks, which 
all reach over 50% accuracy. OneR, a basic and simple classifier, only gets to 45% accuracy 
when Boosting is applied. kNN does not seem to be working well, but the k value has a big 
influence on accuracy, as we will see when we discuss Graph 8 (after feature selection). 
It should be mentioned that there is only little difference in accuracy between the 
Bagging and Boosting voting schemes. Both serve to decrease the error rate and thus increase 
the accuracy of the classifiers. Graph 2 throws another light on this aspect by using the Error 
Reduction (ER) measure. The use of Bagging reduces the error rate of j48.J48 with 9.35%. This 
means an improvement in accuracy from 53.67% to 58% (cf. Appendix 12: Graph 2). 
The mean F-score of all ML algorithms and voting schemes is parallel to their accuracy. 
Bagging j48.J48 achieves a mean F-score of 57.97% (cf. Appendix 12: Graph 3). The F-score 
is the harmonic mean of precision and recall. WEKA computes precision and recall by applying 
macro-averaging. First, precision and recall are evaluated ‘locally’ for each category, and then 
‘globally’ by averaging over the results of the different categories (Sebastiani, 2002: 33). 
Graph 1. Generalisation accuracy (POS.arff)
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
On
eR
Ba
g O
ne
R
Bo
os
t O
ne
R
Na
ive
 B
ay
es
Ba
g N
aiv
e B
Bo
os
t N
aiv
e B
j48
.J4
8 (
m1
)
Ba
g j
48
.J4
8
Bo
os
t j4
8.J
48
kN
N 
(k1
)
Ba
g k
NN
Bo
os
t k
NN
Ne
ura
l N
etw
ork
s
ML algorithms
A
cc
ur
ac
y 
(in
 %
)
 
Graph 3bis represents the F-score per class. The algorithms are able to predict the A 
class in 50.50% of the cases, the B class in 50% of the cases and the O class in 46.75% of the 
instances. This means that the features based on the parts-of-speech contain more evidence for 
  77 
  
the A and B classes than for the O class. This is probably due to the fact that the O class is 
heterogeneous since it consists of texts by eleven authors (cf. Section 2.2.1). 
Graph 3bis. F-score per class (POS.arff)
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
On
eR
Ba
g O
ne
R
Bo
os
t O
ne
R
Na
ive
 B
ay
es
Ba
g N
aiv
e B
Bo
os
t N
aiv
e B
j48
.J4
8 (
m1
)
Ba
g j
48
.J4
8
Bo
os
t j4
8.J
48
kN
N 
(k1
)
Ba
g k
NN
Bo
os
t k
NN
Ne
ura
l N
etw
ork
s
ML algorithms
A
cc
ur
ac
y 
(in
 %
)
class A
class B
class O
 
o After feature selection 
 
In order to select the features that best separate between the three author classes, feature 
weighting is performed in the WEKA packet (cf. Section 2.4.1.1). These are 6 (PROPN), 8 (NUM), 
14 (ADJTOT), 16 (PUNCTOT), 19 (PROPNTOT), 21 (NUMTOT), 23 (PRONTOT) and 24 (PREPTOT). 
On this set of selected features, we did some more experiments with ML algorithms and 
voting schemes. The highest accuracy – 60.33% – is achieved by means of the Naive Bayes 
algorithm. Boosting performed via Naive Bayes obtains the same accuracy. 
The influence of feature selection is apparent in this case. Before feature selection, 
Naive Bayes achieved 52.33% accuracy, and 60.27% after feature selection (Graph 4). 
Boosting OneR leads to an Error Reduction of 13.14% (cf. Appendix 12: Graph 5). The 
mean F-score of all ML algorithms and voting schemes is parallel to their accuracy. The only 
difference is, that accuracy takes into account the True Negatives (TN) (cf. Section 2.1). After 
feature selection, Naive Bayes achieves an F-score of 60.27% (cf. Appendix 12: Graph 6). 
  78 
  
Graph 4. Generalisation accuracy after feature selection 
(POS.arff)
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
On
eR
Ba
g O
ne
R
Bo
os
t O
ne
R
Na
ive
 B
ay
es
Ba
g N
aiv
e B
Bo
os
t N
aiv
e B
j48
.J4
8 (
m1
)
Ba
g j
48
.J4
8
Bo
os
t j4
8.J
48
kN
N 
(k1
)
Ba
g k
NN
Bo
os
t k
NN
Ne
ura
l N
etw
...
ML algorithms
A
cc
ur
ac
y 
(in
 %
)
 
Graph 6bis represents the F-score per class. The algorithms are able to predict the A 
class in 54.35% of the cases, the B class in 52.46% of the cases and the O class in 49.85% of the 
instances. This difference in F-score is caused by the heterogeneity of the O class and is present 
whether feature selection is performed or not. 
Graph 6bis. F-score per class after feature
selection (POS.arff)
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
On
eR
Ba
g O
ne
R
Bo
os
t O
ne
R
Na
ive
 B
ay
es
Ba
g N
aiv
e B
Bo
os
t N
aiv
e B
j48
.J4
8 (
m1
)
Ba
g j
48
.J4
8
Bo
os
t j4
8.J
48
kN
N 
(k1
)
Ba
g k
NN
Bo
os
t k
NN
Ne
ura
l N
etw
...
ML algorithms
A
cc
ur
ac
y 
(in
 %
)
class A
class B
class O
 
As we said above (cf. Section 2.4.1.1), two important parameters are the -m value in 
j48.J48 and the k value in kNN. Both have influence on the accuracy of the classifiers trained on 
our data set. Graph 7 represents the influence of the -m value on Bagging j48.J48. We selected 
Bagging j48.J48 because it was the best setting for j48.J48. The Decision Tree classifier 
distinguishes between the classes best when a new branch is formed only when it covers 32 of 
the 300 instances. There is a clear decrease in accuracy when -m equals 1, 2, 128 or 256 (cf. 
Appendix 12: Graph 7). 
 
  79 
  
Graph 8 shows the results of varied k in Boosting kNN, which was the best performing 
setting for kNN. The 8 nearest neighbours help the ML algorithm to classify the instances 
according to the author classes in more than 63% of the cases on the basis of the most 
informative POS features. An accuracy of 45.67% was achieved by kNN with a k value of 1 
before feature selection. Varying the k values clearly improves accuracy for POS.arff. K values 
that also achieve over 60% are the 6 and 16. The accuracy drops if the k value is lower than 4 or 
higher than 16. We can say that there is quite some variability between the texts as far as POS is 
concerned (cf. Appendix 12: Graph 8). 
 
 2. Frequency and distribution of basic verb forms (VERBBASIC.arff) 
 
The full set of features that indicate the frequency and distribution of basic verb forms consists 
of 13 features (Figure 34). Again the entire feature set is used in the first experiments with 
VERBBASIC.arff. Later, feature selection is performed. 
 
1 PVEV  8 PVEVTOT 
2 PVMV  9 PVMVTOT 
3 PVMETT  10 PVMETTTOT 
4 INF  11 INFTOT 
5 VD  12 VDTOT 
6 OD  13 ODTOT 
7 TOTAL  
      Figure 34: Feature set (VERBBASIC.arff) 
 
o Entire feature set 
 
The full feature set is divided in a training set and a test set which validates the model built by 
the ML algorithms and the Bagging and Boosting voting schemes. Ten times, another part of the 
data set is selected as the test set (cf. Section 2.4.1.1). 
As you can see on Graph 9, a performance of 51.67% accuracy is obtained by means of 
a Naive Bayes classifier. Boosting with Naive Bayes achieves the same accuracy. POS.arff also 
had best results with Naive Bayes, but that was after feature selection. The mean F-score of 
Naive Bayes and Boosting Naive Bayes is 51.17% (cf. Appendix 12: Graph 11). Other 
classifiers that work in 50% or more of the instances are Bagging with Naive Bayes and Neural 
Networks. The other classifiers barely reach 40% accuracy. It has to be remarked that kNN’s 
accuracy is influenced by the k value, as we will see when we discuss Graph 16. 
  80 
  
Again there is not much difference between the performance of Bagging and Boosting. 
As a matter of fact, Boosting does not influence the error rate of Naive Bayes trained on 
VERBBASIC.arff, while Bagging only leads to a lower accuracy. Graph 10 represents and 
compares the Error Reduction of Bagging and Boosting. The highest ER is obtained by means 
of Bagging OneR (cf. Appendix 12: Graph 10). 
Graph 9. Generalisation accuracy (VERBBASIC.arff)
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
On
eR
Ba
g O
ne
R
Bo
os
t O
ne
R
Na
ive
 B
ay
es
Ba
g N
aiv
e B
Bo
os
t N
aiv
e B
j48
.J4
8 (
m1
)
Ba
g j
48
.J4
8
Bo
os
t j4
8.J
48
kN
N 
(k1
)
Ba
g k
NN
Bo
os
t k
NN
Ne
ura
l N
etw
ork
s
ML algorithms
A
cc
ur
ac
y 
(in
 %
)
 
 
Graph 11bis represents the F-score per class. Similar to the features based on parts-of-
speech, the features based on basic verb forms contain more evidence for the A and B classes 
than for the O class. The B class seems to be the easiest one to predict by means of the 
frequency and distribution of basic verb forms (Figure 35). 
CLASS MEAN F-SCORE OVER 
ALL ML ALGORITHMS 
A 42.55% 
B 43.36% 
O 40.37% 
Figure 35: Mean F-score per class (VERBBASIC.arff) 
  81 
  
Graph 11bis. F-score per class (VERBBASIC.arff)
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
On
eR
Ba
g O
ne
R
Bo
os
t O
ne
R
Na
ive
 B
ay
es
Ba
g N
aiv
e B
Bo
os
t N
aiv
e B
j48
.J4
8 (
m1
)
Ba
g j
48
.J4
8
Bo
os
t j4
8.J
48
kN
N 
(k1
)
Ba
g k
NN
Bo
os
t k
NN
Ne
ura
l N
etw
ork
s
ML algorithms
A
cc
ur
ac
y 
(in
 %
)
class A
class B
class O
 
o After feature selection 
 
Feature weighting gives as output the features with the highest informative value for the 
classification problem. These are 2 (PVMV), 5 (VD) and 10 (PVMETTTOT). On this set of selected 
features, we did some more experiments with ML algorithms and voting schemes. 
From Graph 12 we can derive that the highest accuracy – 52% – is achieved by means 
of the Naive Bayes algorithm. Boosting performed via Naive Bayes obtains the same accuracy. 
This means that Boosting has no influence on the performance of Naive Bayes. Naive Bayes 
and Boosting Naive Bayes have an F-score of 51.87% (cf. Appendix 12: Graph 14). 
Feature selection has a small positive influence on the performance of Naive Bayes. It 
achieved 51.67% accuracy before feature selection and 52% after. The voting scheme that best 
reduces the error rate – viz. with 15.46% – is Bagging with OneR, a weak classifier. 
Nevertheless, Bagging OneR only reaches an accuracy of 45.33% (cf. Appendix 12: Graph 13). 
Bagging and Boosting have little or no influence on the other classifiers used to train the data 
set by means of selected features for the frequency and distribution of the basic verb forms. 
  82 
  
Graph 12. Generalisation accuracy after feature selection 
(VERBBASIC.arff)
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
On
eR
Ba
g O
ne
R
Bo
os
t O
ne
R
Na
ive
 B
ay
es
Ba
g N
aiv
e B
Bo
os
t N
aiv
e B
j48
.J4
8 (
m1
)
Ba
g j
48
.J4
8
Bo
os
t j4
8.J
48
kN
N 
(k7
)
Ba
g k
NN
Bo
os
t k
NN
Ne
ura
l N
etw
ork
s
ML algorithms
A
cc
ur
ac
y 
(in
 %
)
 
Figure 36 represents the mean F-score over all ML algorithms per author class. This 
time, there is a clear difference between the accuracy of the algorithms for the A and B classes 
and the O class since it comes to almost 10%. Most classifiers have less difficulties classifying 
texts belonging to the A class than the ones part of the B class (Graph 14bis). 
CLASS MEAN F-SCORE OVER 
ALL ML ALGORITHMS 
A 47.31% 
B 46.19% 
O 37.80% 
Figure 36: Mean F-score per class after feature selection (VERBBASIC.arff) 
Graph 14bis. F-score per class after feature
selection (VERBBASIC.arff)
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
On
eR
Ba
g O
ne
R
Bo
os
t O
ne
R
Na
ive
 B
ay
es
Ba
g N
aiv
e B
Bo
os
t N
aiv
e B
j48
.J4
8 (
m1
)
Ba
g j
48
.J4
8
Bo
os
t j4
8.J
48
kN
N 
(k1
)
Ba
g k
NN
Bo
os
t k
NN
Ne
ura
l N
etw
...
ML algorithms
A
cc
ur
ac
y 
(in
 %
)
class A
class B
class O
 
Graph 15 represents the influence of the -m value on Boosting j48.J48, which seemed to 
perform better than Bagging j48.J48 or j48.J48 alone. The Decision Tree classifier is able to 
classify 46.33% of the instances correctly when a new branch is formed only when it covers 1 
  83 
  
of the 300 instances. This means that all texts have different characteristics as far as basic verb 
forms are concerned (cf. Appendix 12: Graph 15). 
Graph 16 shows the influence of varying the k value in Bagging k-Nearest Neighbour. 
The 64 or 128 nearest neighbours help the ML algorithm to classify the instances according to 
the author classes in 52.33% of the cases. With 1 as k value and before feature selection, the 
classifier was only able to do that in 39% of the cases (cf. Appendix 12: Graph 16). 
 
 3. Frequency and distribution of specific verb forms (VERB.arff) 
 
In our first experiments with the frequency and distribution of specific verb forms, the entire 
feature set is used. Later we will perform feature selection. VERB.arff consists of 35 features 
representing the different specific verb forms we came across in the texts (cf. Appendix 9). 
 
o Entire feature set 
 
Graph 17 represents the accuracy of ML algorithms and the Bagging and Boosting voting 
schemes trained on the entire feature set of VERB.arff. A performance of 50% accuracy is 
obtained by means of Bagging with Naive Bayes – a strong classifier. Bagging Naive Bayes 
obtains an F-score over all classes of 49.07% and an accuracy of 50% (cf. Appendix 12: Graph 
19). For Naive Bayes, Bagging leads to a reduction in error rate of 6.25%. The best 
performances are achieved with Naive Bayes, Decision Trees and Neural Networks, which all 
reach over 45% accuracy. OneR, a basic and simple classifier, only gets to 35.67% accuracy 
maximum. Bagging and Boosting only increase its error rate. kNN does not seem to be working 
well either, but the k value has quite some influence on the accuracy, as we will see when we 
discuss Graph 24 (after feature selection). j48.J48 will be influenced by the minimal number of 
objects (or -m value) selected for classification (cf. discussion of Graph 23). 
As said above, Bagging and Boosting only increase the error rate of OneR. Graph 18 
represents the Error Reduction (ER) of Bagging and Boosting. The use of Bagging reduces the 
error rate of j48.J48 with 12.97%, which is quite a lot (cf. Appendix 12: Graph 18). 
  84 
  
Graph 17. Generalisation accuracy (VERB.arff)
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
On
eR
Ba
g O
ne
R
Bo
os
t O
ne
R
Na
ive
 B
ay
es
Ba
g N
aiv
e B
Bo
os
t N
aiv
e B
j48
.J4
8 (
m1
)
Ba
g j
48
.J4
8
Bo
os
t j4
8.J
48
kN
N 
(k1
)
Ba
g k
NN
Bo
os
t k
NN
Ne
ura
l N
etw
...
ML algorithms
A
cc
ur
ac
y 
(in
 %
)
 
Graph 19bis represents the F-score per class. The algorithms are able to predict the A 
class in 42.41% of the cases, the B class in 45.18% of the cases and the O class in only 36.46% 
of the instances. This means that the features based on the specific verb forms contain more 
evidence for the A and B classes than for the O class. Again, the B class is easier to predict than 
the A class. 
Graph 19bis. F-score per class (VERB.arff)
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
On
eR
Ba
g O
ne
R
Bo
os
t O
ne
R
Na
ive
 B
ay
es
Ba
g N
aiv
e B
Bo
os
t N
aiv
e B
j48
.J4
8 (
m1
)
Ba
g j
48
.J4
8
Bo
os
t j4
8.J
48
kN
N 
(k1
)
Ba
g k
NN
Bo
os
t k
NN
Ne
ura
l N
etw
...
ML algorithms
A
cc
ur
ac
y 
(in
 %
)
class A
class B
class O
 
 
 
o After feature selection 
 
In order to select the features that best separate between the three author classes, feature 
weighting is performed in the WEKA packet (cf. Section 2.4.1.1). The five most informative 
features are 3 (PVTGWMV – Dutch inflected verb form, present, plural), 5 (PVVERLMV – Du. 
inflected verb form, past, plural), 7 (VDVRIJZONDER – Du. past participle without suffix), 9 
  85 
  
(VDPRENOMMETE – Du. past participle, in prenominal use, suffix -e), and 20 (PVTGWMETTTOT – 
distribution of Du. inflected verb form, present, 3rd person singular ending in -t). 
On this set of selected features, we did some more experiments with ML algorithms and 
voting schemes. Graph 20 represents the accuracy of the different algorithms on the feature set. 
The highest accuracy – 51% – is achieved by means of the Naive Bayes algorithm. Boosting 
performed via Naive Bayes obtains the same accuracy. Before feature selection, Boosting 
Naive Bayes only obtained 46.67% accuracy. We can therefore say that it has a positive 
influence on performance for VERB.arff. 
The voting scheme that best reduces the error rate – viz. with almost 12% – is Bagging 
with OneR, a weak classifier. Naive Bayes, a stronger classifier, again performs much better, 
even without voting schemes to reduce the error rate (cf. Appendix 12: Graph 21). The mean F-
score of all ML algorithms and voting schemes is parallel to the accuracy. Naive Bayes and 
Boosting Naive Bayes have a mean F-score of 50.33% and an accuracy of 51% (cf. Appendix 
12: Graph 22). 
Graph 20. Generalisation accuracy after feature selection 
(VERB.arff)
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
On
eR
Ba
g O
ne
R
Bo
os
t O
ne
R
Na
ive
 B
ay
es
Ba
g N
aiv
e B
Bo
os
t N
aiv
e B
j48
.J4
8 (
m1
)
Ba
g j
48
.J4
8
Bo
os
t j4
8.J
48
kN
N 
(k1
)
Ba
g k
NN
Bo
os
t k
NN
Ne
ura
l N
etw
...
ML algorithms
A
cc
ur
ac
y 
(in
 %
)
 
 
Graph 22bis represents the F-score per class. The algorithms are able to predict the A 
class in 45.54% of the cases, the B class in 46.52% of the cases and the O class in 37.28% of the 
instances. There is a difference of almost 10% in the predictability of the A and B classes and 
the O class. The results after feature selection seem to confirm the idea that the B class is easiest 
to predict by means of specific verb forms. 
  86 
  
Graph 22bis. F-score per class after feature
selection (VERB.arff)
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
On
eR
Ba
g O
ne
R
Bo
os
t O
ne
R
Na
ive
 B
ay
es
Ba
g N
aiv
e B
Bo
os
t N
aiv
e B
j48
.J4
8 (
m1
)
Ba
g j
48
.J4
8
Bo
os
t j4
8.J
48
kN
N 
(k1
)
Ba
g k
NN
Bo
os
t k
NN
Ne
ura
l N
etw
...
ML algorithms
A
cc
ur
ac
y 
(in
 %
)
class A
class B
class O
 
The -m value in j48.J48 and the k value in kNN have influence on the accuracy of the 
classifiers trained on our data set. We found that Bagging j48.J48 performed better than j48.J48 
alone or with the Boosting voting scheme and that the accuracy was higher before than after 
feature selection. Graph 23 represents the influence of the -m value on Bagging j48.J48 before 
feature selection. It indicates that the Decision Tree classifier distinguishes between the classes 
best when a new branch in the tree is formed only when it covers 2 of the 300 instances. There 
is a clear decrease in accuracy when -m equals 128 or 256. An -m value of 1 only allowed us to 
classify 46.33% of the training instances correctly, so varying the value improves the accuracy. 
We can say that most texts in the training corpus have different characteristics concerning 
specific verb forms (cf. Appendix 12: Graph 23). 
Graph 24 shows the results of experiments with varied k in k-Nearest Neighbour 
classifiers after feature selection. The 16 nearest neighbours help the ML algorithm to classify 
the instances according to the author classes in almost 50% of the cases. K values that also 
achieve about 50% are 32, 64 and 128. The other k values perform much worse. The classifier 
was only able to predict the author class of 39.67% with a k value of 1 and before feature 
selection. We can say that there is relative variability between the texts as far as the frequency 
and distribution of specific verb forms is concerned. The classifier does not need a lot of 
information to be able to make good decisions (cf. Appendix 12: Graph 24). 
 
 
 
 
 
 
  87 
  
 4. Presence or absence of specific NP patterns (PATTERNBIN.arff) 
 
We selected twelve noun phrase (NP) patterns that occur in the texts from the training corpus 
(cf. Section 2.3.2). The first feature set contains information about the presence or absence of 
those patterns in the texts, while a second feature set focuses on the frequency and distribution 
of the patterns (cf. next subsection). The entire feature set is used in the first experiments with 
PATTERNBIN.arff. Later, feature weighting is performed in order to select the most informative 
patterns. The twelve patterns are: 
 
1 
2 
N, VNW OR SPEC 
ADJ N 
 7 
8 
LID ADJ N 
N ADJ N 
3 LID N  9 TW ADJ N 
4 N SPEC  10 LID TW N 
5 VNW N  11 N TW N 
6 TW N  12 LID TW ADJ N 
        Figure 37: Our list of NP patterns 
 
o Entire feature set 
 
Naive Bayes and Boosting with Naive Bayes once more prove to be the most reliable 
classifiers, as we can derive from Graph 25. They both have an accuracy of 40.67%. The other 
classifiers obtain lower accuracies, but once more we have to stress the importance of the -m 
value for Decision Tree classifiers like Bagging j48.J48 and of the k value for Bagging kNN. We 
also introduce a new classifier here. PATTERNBIN.arff is a binary feature set. ID3 is another 
Decision Tree classifier that cannot be applied to numeric features. ID3 performs slightly better 
than j48.J48. The mean F-score of Naive Bayes and Boosting Naive Bayes is 40.50% while 
their accuracy is 40.67% (cf. Appendix 12: Graph 27). 
Bagging and Boosting do not have much influence on performance. As a matter of fact, 
Boosting does not influence the error rate of Naive Bayes trained on VERBBASIC.arff, while 
Bagging only leads to a lower accuracy. Graph 26 represents and compares the Error Reduction 
of Bagging and Boosting. The highest ER – 6.79% – is obtained by means of Bagging OneR (cf. 
Appendix 12: Graph 26). 
  88 
  
Graph 25. Generalisation accuracy (PATTERNBIN.arff)
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
On
eR
Ba
g O
ne
R
Bo
os
t O
ne
R
Na
ive
 B
ay
es
Ba
g N
aiv
e B
Bo
os
t N
aiv
e B
j48
.J4
8 (
m1
)
Ba
g j
48
.J4
8
Bo
os
t j4
8.J
48
kN
N 
(k1
)
Ba
g k
NN
Bo
os
t k
NN id3
Ba
g i
d3
Bo
os
t id
3
Ne
ura
l N
etw
ork
s
ML algorithms
A
cc
ur
ac
y 
(in
 %
)
 
Graph 27bis represents the F-score per class. Contrary to the previously discussed 
feature sets, features based on the presence or absence of specific NP patterns contain less 
evidence for the A class than for the B and O classes. Again we see that the algorithms have 
least difficulties with classifying texts belonging to the B class (Figure 38). 
Graph 27bis. F-score per class (PATTERNBIN.arff)
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
On
eR
On
eR
Ba
g O
ne
R
Bo
os
t O
ne
R
Na
ive
 B
ay
es
Ba
g N
aiv
e B
Bo
os
t N
aiv
e B
j48
.J4
8 (
m1
)
Ba
g j
48
.J4
8
Bo
os
t j4
8.J
48
kN
N 
(k1
)
Ba
g k
NN
Bo
os
t k
NN id3
Ba
g i
d3
Bo
os
t id
3
ML algorithms
A
cc
ur
ac
y 
(in
 %
)
class A
class B
class O
 
CLASS MEAN F-SCORE OVER 
ALL ML ALGORITHMS 
A 34.74% 
B 38.85% 
O 35.06% 
Figure 38: Mean F-score per class (PATTERNBIN.arff) 
 
 
 
 
  89 
  
o After feature selection 
 
Feature weighting gives as output the features with the highest informative value for the 
classification problem. These are patterns 4, 6, 8, 9, 10, 11 and 12. On this set of selected 
features, we did some more experiments with ML algorithms and voting schemes. 
The highest accuracy – 40.67% – is achieved by means of the Naive Bayes algorithm. 
Bagging and Boosting performed via Naive Bayes obtain the same accuracy (Graph 28). 
Bagging Naive Bayes has an F-score of 40.57% and an accuracy of 40.67% (cf. Appendix 12: 
Graph 30). We can say that feature selection has no influence in the case of PATTERNBIN.arff 
since all ML algorithms achieve the same accuracies as before feature selection. 
The voting scheme that best reduces the error rate – viz. with 6.8% – is Bagging with 
OneR, a weak classifier. Nevertheless, Bagging OneR only reaches an accuracy of 36.00%, 
while the Naive Bayes classifier reached 40.67%. Bagging and Boosting again have little or no 
influence on the other classifiers used to train the data set by means of selected features for the 
frequency and distribution of the basic verb forms (cf. Appendix 12: Graph 29). 
Graph 28. Generalisation accuracy after feature selection 
(PATTERNBIN.arff)
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
On
eR
Ba
g O
ne
R
Bo
os
t O
ne
R
Na
ive
 B
ay
es
Ba
g N
aiv
e B
Bo
os
t N
aiv
e B
j48
.J4
8 (
m1
)
Ba
g j
48
.J4
8
Bo
os
t j4
8.J
48
kN
N 
(k1
)
Ba
g k
NN
Bo
os
t k
NN id3
Ba
g i
d3
Bo
os
t id
3
Ne
ura
l N
etw
ork
s
ML algorithms
A
cc
ur
ac
y 
(in
 %
)
 
 
Figure 39 and Graph 30bis represent the F-score per author class and per ML algorithm. 
Contrary to the previously discussed feature sets, the features based on the presence or absence 
of specific NP patterns with the highest Gain Ratio contain less evidence for the A class than for 
the B and O classes. 
 
 
 
Figure 39: Mean F-score per class after feature selection (PATTERNBIN.arff) 
CLASS MEAN F-SCORE OVER ALL ML ALGORITHMS 
A 34.53% 
B 38.46% 
O 35.54% 
  90 
  
Graph 30bis. F-score per class after feature
selection (PATTERNBIN.arff)
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
On
eR
Ba
gg
ing
 O
ne
R
Bo
os
tin
g O
ne
R
Na
ive
 B
ay
es
Ba
gg
ing
 N
ai.
.
Bo
os
tin
g N
ai.
.
j48
.J4
8 (
m1
)
Ba
gg
ing
 j4
8.J
48
Bo
os
tin
g j
48
...
kN
N 
(k1
)
Ba
gg
ing
 kN
N
Bo
os
tin
g k
NN id3
Ba
gg
ing
 id
3
Bo
os
tin
g i
d3
Ne
ura
l N
etw
...
ML algorithms
A
cc
ur
ac
y 
(in
 %
)
class A
class B
class O
  
 
Graph 31 represents the influence of the -m value on generalisation accuracy after 
feature selection. It signals that Bagging j48.J48 – the best setting for the Decision Tree 
classifier – distinguishes between the classes in 41% of the training instances when a new 
branch is formed when it covers 32 of the 300 instances (cf. Appendix 12: Graph 31). 
Graph 32 shows the results of varied k in Bagging kNN – the best parameter setting for 
kNN. The 128 nearest neighbours help the ML algorithm to classify the instances according to 
the author classes in 43.67% of the cases. With 1 as k value and before feature selection, kNN 
was only able to correctly classify 36% of the training instances (cf. Appendix 12: Graph 32). 
 
 5. Frequency and distribution of specific NP patterns (PATTERNNUM.arff) 
 
This feature set is based on the frequency and distribution of twelve noun phrase (NP) patterns 
we also used in the previous subsection (PATTERNBIN.arff). First, the entire feature set is used, 
and later the most informative features are selected. 
 
o Entire feature set 
 
As can be derived from Graph 33, most ML classifiers achieve a similar accuracy on 
PATTERNNUM.arff, but Naive Bayes and Neural Networks achieve 46% or more. Bagging Naive 
Bayes reaches an accuracy of 49.33% and an F-score of 49.05% (cf. Appendix 12: Graph 35). 
Bagging with Naive Bayes is the voting scheme that best reduces the error rate, namely 
with 2.55%. The Error Reduction of Bagging and Boosting for PATTERNNUM.arff is however 
small to nonexistent (cf. Appendix 12: Graph 34). 
  91 
  
Graph 33. Generalisation accuracy (PATTERNNUM.arff)
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
On
eR
Ba
g O
ne
R
Bo
os
t O
ne
R
Na
ive
 B
ay
es
Ba
g N
aiv
e B
Bo
os
t N
aiv
e B
j48
.J4
8 (
m1
)
Ba
g j
48
.J4
8
Bo
os
t j4
8.J
48
kN
N 
(k1
)
Ba
g k
NN
Bo
os
t k
NN
Ne
ura
l N
etw
...
ML algorithms
A
cc
ur
ac
y 
(in
 %
)
  
Graph 35bis represents the F-score per class. The algorithms are able to predict the A 
class in 40.98% of the cases, the B class in 45.22% of the cases and the O class in 40.93% of the 
instances. This means that the features based on the frequency and distribution of specific NP 
patterns contain more evidence for the B class than for the A and O classes. Apparently, the B 
class is more stable as far as NP patterns are concerned. 
Graph 35bis. F-score per class (PATTERNNUM.arff)
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
On
eR
Ba
g O
ne
R
Bo
os
t O
ne
R
Na
ive
 B
ay
es
Ba
g N
aiv
e B
Bo
os
t N
aiv
e B
j48
.J4
8 (
m1
)
Ba
g j
48
.J4
8
Bo
os
t j4
8.J
48
kN
N 
(k1
)
Ba
g k
NN
Bo
os
t k
NN
Ne
ura
l N
etw
...
ML algorithms
A
cc
ur
ac
y 
(in
 %
)
class A
class B
class O
 
o After feature selection 
 
In order to select the features that best separate between the three author classes, feature 
weighting is performed in the WEKA packet (cf. Section 2.4.1.1). The five most informative 
features are patterns 6 and 8 and the distribution of patterns 5, 7 and 8. After feature weighting 
on PATTERNBIN.arff, binary information about patterns 4, 6, 8, 9, 10, 11 and 12 was selected as 
most informative. Especially NP patterns 6 and 8 seem to be stable characteristics of the authors. 
  92 
  
On the set of selected features, we did some extra experiments with ML algorithms and 
voting schemes. The highest accuracy – 54% – is achieved by means of the Neural Networks 
algorithm, as can be seen on Graph 36. Naive Bayes and Boosting Naive Bayes also reach the 
50% accuracy. Neural Networks have an F-score of 53.53%, while Naive Bayes achieves 
50.43% (cf. Appendix 12: Graph 38). 
The influence of feature selection is clear: before feature selection, Neural Networks 
only achieved 43.70% accuracy. After feature selection, it performs much better and even gets 
to 54% accuracy. 
The Error Reduction caused by the Bagging is highest with the weak classifier OneR, 
viz. 6.87%. Boosting does not lead to any Error Reduction and even increases the error rate in 
some cases (cf. Appendix 12: Graph 37). 
Graph 36. Generalisation accuracy after feature selection 
(PATTERNNUM.arff)
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
On
eR
Ba
g O
ne
R
Bo
os
t O
ne
R
Na
ive
 B
ay
es
Ba
g N
aiv
e B
Bo
os
t N
aiv
e B
j48
.J4
8 (
m1
)
Ba
g j
48
.J4
8
Bo
os
t j4
8.J
48
kN
N 
(k1
)
Ba
g k
NN
Bo
os
t k
NN
Ne
ura
l N
etw
...
ML algorithms
A
cc
ur
ac
y 
(in
 %
)
 
Graph 38bis represents the F-score per class. The algorithms are able to predict the A 
class in 40.62% of the cases, the B class in 47.45% of the cases and the O class in 44.62% of the 
instances. The A class is apparently more difficult to predict than the B and O classes. The b 
class is the easiest to predict by means of the frequency and distribution of specific NP patterns. 
  93 
  
Graph 38bis. F-score per class after feature
selection (PATTERNNUM.arff)
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
On
eR
Ba
g O
ne
R
Bo
os
t O
ne
R
Na
ive
 B
ay
es
Ba
g N
aiv
e B
Bo
os
t N
aiv
e B
j48
.J4
8 (
m1
)
Ba
g j
48
.J4
8
Bo
os
t j4
8.J
48
kN
N 
(k1
)
Ba
g k
NN
Bo
os
t k
NN
Ne
ura
l N
etw
ork
s
ML algorithms
A
cc
ur
ac
y 
(in
 %
)
class A
class B
class O
  
Two important parameters are the -m value in j48.J48 and the k value in kNN. Both have 
influence on the accuracy of the classifiers trained on our data set. Graph 39 represents the 
influence of the -m value on the performance of Bagging j48.J48 – the best performing 
Decision Tree classifier. After feature selection, the classifier distinguishes between the classes 
in 52.33% of the training instances when a new branch is formed only when it covers 64 of the 
300 instances. There is a clear decrease in accuracy when -m equals 256 (cf. Appendix 12: 
Graph 39). 
Graph 40 shows the effect of varying k in k-Nearest Neighbour classifiers when applied 
to the entire feature set. The 32 nearest neighbours help the ML algorithm to classify the 
instances according to the author classes in 50% of the cases. K values that achieve over 48% 
are 16, 64 and 128. There is relative variability between the texts as far as the frequency and 
distribution of NP patterns is concerned (cf. Appendix 12: Graph 40). 
 6. Presence or absence of specific words based on mutual information 
(MUTUALINFO.arff) 
 
MUTUALINFO.arff is a binary feature set that is based on the mutual information measure. For 
each word in the training corpus, the mutual information for the three classes is computed. The 
twenty words with highest mutual information are the features MUTUALINFO.arff consists of: 
 
1 PARTIJ  11 VLD 
2 SPA  12 BEIDE 
3 BLIJKT  13 MR 
4 ZEGT  14 GEWEST 
  94 
  
5 WIE  15 TEGELIJK 
6 ECHTER  16 STEEDS 
7 ALTIJD  17 ERG 
8 ALDUS  18 AFGELOPEN 
9 EVENWEL  19 MOMENTEEL 
10 BLOK  20 WILDE 
Figure 40: List of features (MUTUALINFO.arff) 
 
o Entire feature set 
 
Graph 41 represents the accuracy of ML algorithms and the Bagging and Boosting voting 
schemes trained on the entire feature set of MUTUALINFO.arff. A performance of 77% accuracy 
is obtained by means of the Bagging voting scheme with Naïve Bayes. The worst performance 
– viz. 51.33% – is achieved with OneR, a weak, basic classifier. kNN does not seem to be 
working well, but the k value has clear influence on accuracy, as we will see when we discuss 
Graph 44 (after feature selection). MUTUALINFO.arff is a binary feature set. Id3 is another 
Decision Tree classifier that cannot be applied to numeric features. Id3 performs better than 
j48.J48. The mean F-score of Bagging Naive Bayes is 79.50% (cf. Appendix 12: Graph 43). 
The Bagging and Boosting voting schemes serve to decrease the error rate and thus 
increase the accuracy of the classifiers. Graph 42 tells us that the use of Bagging reduces the 
error rate of j48.J48 with 27.27% (cf. Appendix 12: Graph 42). 
Graph 41. Generalisation accuracy (MUTUALINFO.arff)
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
On
eR
Ba
g O
ne
R
Bo
os
t O
ne
R
Na
ive
 B
ay
es
Ba
g N
aiv
e B
Bo
os
t N
aiv
e B
j48
.J4
8 (
m1
)
Ba
g j
48
.J4
8
Bo
os
t j4
8.J
48
kN
N 
(k1
)
Ba
g k
NN
Bo
os
t k
NN id3
Ba
g i
d3
Bo
os
t id
3
Ne
ura
l N
etw
...
ML algorithms
A
cc
ur
ac
y 
(in
 %
)
 
Graph 43bis represents the F-score per class. There are big differences between the 
predictability of classes A, B and O. The algorithms are able to predict the A class in 65.37% of 
the cases, the B class in 75.04% of the cases and the O class in only 50.11% of the instances. 
  95 
  
This means that the lexical features based on the mutual information measure contain most 
evidence for the B class and that the O class is much harder to predict. This can be explained by 
the heterogeneity of the O class. 
Graph 43bis. F-score per class (MUTUALINFO.arff)
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
On
eR
Ba
g O
ne
R
Bo
os
t O
ne
R
Na
ive
 B
ay
es
Ba
g N
aiv
e B
Bo
os
t N
aiv
e B
j48
.J4
8 (
m1
)
Ba
g j
48
.J4
8
Bo
os
t j4
8.J
48
kN
N 
(k1
)
Ba
g k
NN
Bo
os
t k
NN id3
Ba
g i
d3
Bo
os
t id
3
Ne
ura
l N
etw
...
ML algorithms
A
cc
ur
ac
y 
(in
 %
)
class A
class B
class O
 
 
o After feature selection 
 
In order to select the features that best separate between the three author classes, feature 
weighting is performed in the WEKA packet (cf. Section 2.4.1.1). All features were informative, 
except for features 1 and 10. 
The highest accuracy – 78.33% – is achieved by means of Naive Bayes. Bagging 
performed via Naive Bayes obtains the same accuracy, as we can derive from Graph 44. OneR 
does not achieve an accuracy of more than 51.33%. Naive Bayes and Boosting Naive Bayes 
have a mean F-score of 78.37% and an accuracy of 78.33% (cf. Appendix 12: Graph 46). 
Feature selection has a positive influence on performance since it improves the results of 
training with the Naive Bayes algorithm – viz. 76.67% before and 78.33% after feature 
selection. 
The voting scheme that best reduces the error rate – viz. with 17.10% – is Bagging with 
j48.J48. Naive Bayes, a stronger classifier, nevertheless performs much better, even without 
voting schemes to reduce the error rate (cf. Appendix 12: Graph 45). 
  96 
  
Graph 44. Generalisation accuracy after feature selection 
(MUTUALINFO.arff)
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
On
eR
Ba
g O
ne
R
Bo
os
t O
ne
R
Na
ive
 B
ay
es
Ba
g N
aiv
e B
Bo
os
t N
aiv
e B
j48
.J4
8 (
m1
)
Ba
g j
48
.J4
8
Bo
os
t j4
8.J
48
kN
N 
(k1
)
Ba
g k
NN
Bo
os
t k
NN id3
Ba
g i
d3
Bo
os
t id
3
Ne
ura
l N
etw
...
ML algorithms
A
cc
ur
ac
y 
(in
 %
)
 
 
Graph 46bis represents the F-score per class. The algorithms are able to predict the A 
class in 66.61% of the cases, the B class in 76.60% of the cases and the O class in 49.66% of the 
instances. There is a difference of more than 25% in the predictability of the B and O classes and 
of 10% in the predictability of the B and A classes. 
Graph 46bis. F-score per class after feature
selection (MUTUALINFO.arff)
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
On
eR
Ba
gg
ing
 O
ne
R
Bo
os
tin
g O
ne
R
Na
ive
 B
ay
es
Ba
gg
ing
 N
ai.
.
Bo
os
tin
g N
ai.
.
j48
.J4
8 (
m1
)
Ba
gg
ing
 j4
8.J
48
Bo
os
tin
g j
48
...
kN
N 
(k1
)
Ba
gg
ing
 kN
N
Bo
os
tin
g k
NN id3
Ba
gg
ing
 id
3
Bo
os
tin
g i
d3
Ne
ura
l N
etw
...
ML algorithms
A
cc
ur
ac
y 
(in
 %
)
class A
class B
class O
 
The -m value in j48.J48 and the k value in kNN have influence on the accuracy of the 
classifiers trained on our data set. Graph 47 represents the influence of the -m value on Bagging 
j48.J48 when applied to the entire feature set. The Decision Tree classifier distinguishes 
between the classes in 71.33% of the instances when a new branch is formed only when it 
covers 8 of the 300 instances. This means that there is great variability between the texts as far 
as the presence or absence of specific words is concerned. We see a clear decrease in accuracy 
when the -m value exceeds 8. With -m = 1, j48.J48 allowed us to predict the author in 59.67% 
  97 
  
of the training texts. We can conclude that varying the -m value has a positive influence on 
performance (cf. Appendix 12: Graph 47). 
Graph 48 shows the influence of varying the k value in k-Nearest Neighbour classifiers. 
After feature selection, the 16 nearest neighbours help the ML algorithm to classify the instances 
according to the author classes in 72.33% of the cases. Most other k values also achieve about 
70%. This means there is relative variability between the texts. With k equalling 1 and before 
feature selection, the kNN algorithm was only able to classify 64% of the training data (cf. 
Appendix 12: Graph 48). 
 
 7. Readability score (READABILITY.arff) 
 
The experiments are performed on the entire feature set. Feature weighting is not applied since 
READABILITY.arff consists of one feature, the readability score. The metric indicates the level of 
difficulty of a text and involves the Average Sentence Length (ASL) and the Average number of 
Syllables per Word (ASW) – two token-level features.  
The feature set is divided in a training set and a test set for validation purposes. The 
generalisation accuracy of the best performing ML algorithm – Bagging OneR – is 42.33%. 
Naive Bayes – with and without Bagging or Boosting – and Bagging j48.J48 also achieve a 
40% accuracy. We will also discuss the influence of the -m value on j48.J48 (cf. discussion of 
Graph 52) and the influence of the k value on the performance of kNN classifiers (cf. discussion 
of Graph 53). 
The mean F-score of Bagging OneR is 41.53%. The other classifiers that achieved an 
accuracy of more than 40%, have a mean F-score of less than 40% (cf. Appendix 12: Graph 
51). Again the influence of Bagging and Boosting on accuracy is small to nonexistent. 
Applying Bagging to OneR leads to a reduction in error rate of 5.46%, while Bagging and 
Boosting with other ML classifiers often lead to an increase in error rate (cf. Appendix 12: 
Graph 50). 
  98 
  
Graph 49. Generalisation accuracy (READABILITY.arff)
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
On
eR
Ba
g O
ne
R
Bo
os
t O
ne
R
Na
ive
 B
ay
es
Ba
g N
aiv
e B
Bo
os
t N
aiv
e B
j48
.J4
8 (
m1
)
Ba
g j
48
.J4
8
Bo
os
t j4
8.J
48
kN
N 
(k1
)
Ba
g k
NN
Bo
os
t k
NN
Ne
ura
l N
etw
ork
s
ML algorithms
A
cc
ur
ac
y 
(in
 %
)
 The differences between the F-scores per class are enormous. As you can see on Figure 
41, there is hardly any evidence for the O class in readability scores. Especially Naive Bayes, 
Decision Trees (j48.J48) and Neural Networks – classifiers that give the best results with other 
feature sets – are not able to classify more than 7.30% of the instances belonging to the O class 
(Graph 51bis). 
CLASS MEAN F-SCORE OVER 
ALL ML ALGORITHMS 
A 43.45% 
B 44.41% 
O 19.55% 
Figure 41: Mean F-score per class (READABILITY.arff) 
Graph 51bis. F-score per class (READABILITY.arff)
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
On
eR
Ba
g O
ne
R
Bo
os
t O
ne
R
Na
ive
 B
ay
es
Ba
g N
aiv
e B
Bo
os
t N
aiv
e B
j48
.J4
8 (
m1
)
Ba
g j
48
.J4
8
Bo
os
t j4
8.J
48
kN
N 
(k1
)
Ba
g k
NN
Bo
os
t k
NN
Ne
ura
l N
etw
...
ML algorithms
A
cc
ur
ac
y 
(in
 %
)
class A
class B
class O
 
The influence of the -m value on Bagging j48.J48 – the best performing setting for 
j48.J48 – is not big when the readability feature is involved. Best performance – viz. 42.33% – 
  99 
  
is achieved when the Decision Tree classifier makes a new branch when it covers 6 of the 300 
instances. This means the 300 training texts are not very similar as far as readability is 
concerned (cf. Appendix 12: Graph 52). 
Graph 53 shows the results of experiments with varied k in k-Nearest Neighbour 
classifiers. There is a clear peak at -m=16. This indicates that the 16 nearest neighbours help 
the ML algorithm to classify the instances according to the author classes in more than 46% of 
the cases (cf. Appendix 12: Graph 53). 
 
 8. Combination (TOTAL.arff) 
 
TOTAL.arff contains all seven previously discussed feature sets, which means it consists of 131 
features. First, we use the entire feature set, but later feature weighting and selection are 
performed. In order to examine the performance of a combination of syntax-based features 
only, we do another feature weighting and selection in the end. 
 
o Entire feature set 
 
Graph 54 represents the accuracy of ML algorithms and the Bagging and Boosting voting 
schemes trained on the entire feature set of TOTAL.arff. Ten-fold cross validation again allows 
us to already test the classifiers trained on the training corpus in order to improve the results. A 
performance of 76% accuracy is obtained by means of Neural Networks. The Bagging voting 
scheme with  Decision Tree classifier j48.J48 achieves 74% accuracy and is second-best. Naive 
Bayes, kNN and Neural Networks all reach over 60% accuracy. After feature selection, we will 
vary the -m value in j48.J48 and the k value for kNN in order to improve performance. 
Bagging j48.J48 reaches a 73.40% F-score, while weak classifier OneR only achieves a 
41.67% F-score (cf. Appendix 12: Graph 56). 
  100 
  
Graph 54. Generalisation accuracy (TOTAL.arff)
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
On
eR
Ba
g O
ne
R
Bo
os
t O
ne
R
Na
ive
 B
ay
es
Ba
g N
aiv
e B
Bo
os
t N
aiv
e B
j48
.J4
8 (
m1
)
Ba
g j
48
.J4
8
Bo
os
t j4
8.J
48
kN
N 
(k1
)
Ba
g k
NN
Bo
os
t k
NN
Ne
ura
l N
etw
...
ML algorithms
A
cc
ur
ac
y 
(in
 %
)
 
This time Bagging and Boosting lead to better accuracy. Boosting brings Naive Bayes to 
24.79% Error Reduction and Bagging reduces the error rate of j48.J48 with 27.78%. With other 
classifiers, Bagging and Boosting have little or no effect (Graph 55). 
Graph 55. Error Reduction through Bagging and Boosting 
(TOTAL.arff)
-100,00%
-50,00%
0,00%
50,00%
100,00%
B
ag
gi
ng
(O
ne
R
)
B
oo
st
in
g
(O
ne
R
)
B
ag
gi
ng
(N
ai
ve
 B
)
B
oo
st
in
g
(N
ai
ve
 B
)
B
ag
gi
ng
(j4
8.
J4
8)
B
oo
st
in
g
(j4
8.
J4
8)
B
ag
gi
ng
(k
N
N
)
B
oo
st
in
g
(k
N
N
)
 
The F-score per class is presented in Graph 56bis. The ML algorithms are able to predict 
the A class in 60.58% of the cases, the B class in 65.68% of the cases and the O class in 56.85% 
of the instances. The heterogeneous O class is the most difficult class to predict and the b class 
seems to be the most stable one as far as our combination of syntax-based, lexical and token-
level features is concerned. 
  101 
  
Graph 56bis. F-score per class (TOTAL.arff)
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
On
eR
Ba
g O
ne
R
Bo
os
t O
ne
R
Na
ive
 B
ay
es
Ba
g N
aiv
e B
Bo
os
t N
aiv
e B
j48
.J4
8 (
m1
)
Ba
g j
48
.J4
8
Bo
os
t j4
8.J
48
kN
N 
(k1
)
Ba
g k
NN
Bo
os
t k
NN
Ne
ura
l N
etw
ork
s
class A
class B
class O
 
o After feature selection on all features 
 
In order to select the features that best separate between the three author classes, feature 
weighting is performed in the WEKA packet (cf. Section 2.4.1.1). These are 8 (NUM), 16 
(PUNCTOT), 19 (PROPNTOT), 23 (PRONTOT), 42 (PVVERLEV), 112, 113, 114, 116, 118, 119, 122, 
123, 124, 125, 126, 128, 129 and 130. The features beginning from number 112 are all lexical 
and token-level features and will be omitted in the next section, when we perform another 
feature selection by selecting only the syntax-based features. 
Graph 57 tells us that 78% accuracy is achieved by means of the Naive Bayes 
algorithm. Bagging and Boosting performed via Naive Bayes obtain the same accuracy. The 
other classifiers also reach over 60% accuracy, except for OneR, which barely gets to 45%. 
After feature selection, Naive Bayes and Boosting Naive Bayes lead to an F-score of 77.80% 
(cf. Appendix 12: Graph 59). 
Feature selection clearly has a positive effect on performance. Before feature selection, 
the Naive Bayes algorithm allowed us to classify 61% of the training instances correctly. This 
percentage goes to 78% after feature selection. Nevertheless, Neural Networks achieve 76% 
before and only 72% accuracy after feature selection. 
  102 
  
Graph 57. Generalisation accuracy after feature selection 
(TOTAL.arff)
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
On
eR
Ba
g O
ne
R
Bo
os
t O
ne
R
Na
ive
 B
ay
es
Ba
g N
aiv
e B
Bo
os
t N
aiv
e B
j48
.J4
8 (
m1
)
Ba
g j
48
.J4
8
Bo
os
t j4
8.J
48
kN
N 
(k1
)
Ba
g k
NN
Bo
os
t k
NN
Ne
ura
l N
etw
...
ML algorithms
A
cc
ur
ac
y 
(in
 %
)
 
 
Thanks to Bagging, the error rate in j48.J48 is reduced with 17.77%. Boosting causes an 
Error Reduction of 10.29% for j48.J48 (Graph 58). 
Graph 58. Error Reduction through Bagging and Boosting 
after feature selection (TOTAL.arff)
-100,00%
-50,00%
0,00%
50,00%
100,00%
B
ag
gi
ng
(O
ne
R
)
B
oo
st
in
g
(O
ne
R
)
B
ag
gi
ng
(N
ai
ve
 B
)
B
oo
st
in
g
(N
ai
ve
 B
)
B
ag
gi
ng
(j4
8.
J4
8)
B
oo
st
in
g
(j4
8.
J4
8)
B
ag
gi
ng
(k
N
N
)
B
oo
st
in
g
(k
N
N
)
 
Graph 59bis represents the F-score per class. The algorithms are able to predict the A 
class in 67.18% of the cases, the B class in 70.51% of the cases and the O class in 58.72% of the 
instances. This difference in F-score is caused by the heterogeneity of the O class and is present 
whether feature selection is performed or not. Apparently, the B class is more stable than the 
two other classes as far as the most informative features in TOTAL.arff are concerned. 
  103 
  
Graph 59bis. F-score per class after feature
selection (TOTAL.arff)
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
On
eR
Ba
g O
ne
R
Bo
os
t O
ne
R
Na
ive
 B
ay
es
Ba
g N
aiv
e B
Bo
os
t N
aiv
e B
j48
.J4
8 (
m1
)
Ba
g j
48
.J4
8
Bo
os
t j4
8.J
48
kN
N 
(k1
)
Ba
g k
NN
Bo
os
t k
NN
Ne
ura
l N
etw
...
ML algorithms
A
cc
ur
ac
y 
(in
 %
)
class A
class B
class O
 
As we said in Section 2.4.1.1, two important parameters are the -m value in j48.J48 and 
the k value in kNN. Both have influence on the accuracy of the classifiers trained on our data 
set. Graph 60 represents the influence of the -m value on Bagging j48.J48 before feature 
selection. It indicates that the Decision Tree classifier distinguishes between the classes in 
74.33% of the cases when a new branch is formed when it covers 2 of the 300 instances. There 
is a clear decrease in accuracy when -m equals 16 or more. This means that only few training 
texts have similar – syntax-based, lexical and token-level – characteristics. 
Graph 60. Influence of the -m value in Bagging j48.J48 on 
generalisation accuracy (TOTAL.arff)
0,00%
10,00%
20,00%
30,00%
40,00%
50,00%
60,00%
70,00%
80,00%
90,00%
100,00%
1 2 4 6 8 16 32 64 128 256
-m value
A
cc
ur
ac
y 
(in
 %
)
 
Graph 61 shows the results of varied k in Bagging kNN after feature selection. The 4 
nearest neighbours help the ML algorithm to classify the instances according to the author 
classes in 74.33% of the cases. Again we can say that there is a big variability between the 
training instances. 
  104 
  
Graph 61. Influence of the k value in Bagging kNN on 
generalisation accuracy after feature selection (TOTAL.arff)
0,00%
10,00%
20,00%
30,00%
40,00%
50,00%
60,00%
70,00%
80,00%
90,00%
100,00%
1 2 4 6 8 16 32 64 128 256
k value
A
cc
ur
ac
y 
(in
 %
)
 
o All syntax-based features 
 
A second feature selection allows us to work on syntax-based features only. Later, we will 
select the most informative syntax-based features. 
From Graph 62, it can be derived that an accuracy of 63.67% is achieved by means of a 
Neural Networks classifier. The mean F-score of Neural Networks on a feature set consisting of 
syntax-based features only is 63.57% (cf. Appendix 12: Graph 64). 
Graph 62. Generalisation accuracy of syntax-based features 
(TOTAL.arff)
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
On
eR
Ba
g O
ne
R
Bo
os
t O
ne
R
Na
ive
 B
ay
es
Ba
g N
aiv
e B
Bo
os
t N
aiv
e B
j48
.J4
8 (
m1
)
Ba
g j
48
.J4
8
Bo
os
t j4
8.J
48
kN
N 
(k1
)
Ba
g k
NN
Bo
os
t k
NN
Ne
ura
l N
etw
...
ML algorithms
A
cc
ur
ac
y 
(in
 %
)
 
As you can see on Graph 63, only Bagging has a meaningful influence on the ML 
classifiers trained on all syntax-based features. It reduces the error rate of j48.J48 with 13.04%. 
Nevertheless, Naive Bayes performs better, even without Bagging or Boosting. 
  105 
  
Graph 63. Error Reduction through Bagging and Boosting with 
syntax-based features (TOTAL.arff)
-100,00%
-50,00%
0,00%
50,00%
100,00%
B
ag
gi
ng
(O
ne
R
)
B
oo
st
in
g
(O
ne
R
)
B
ag
gi
ng
(N
ai
ve
 B
)
B
oo
st
in
g
(N
ai
ve
 B
)
B
ag
gi
ng
(j4
8.
J4
8)
B
oo
st
in
g
(j4
8.
J4
8)
B
ag
gi
ng
(k
N
N
)
B
oo
st
in
g
(k
N
N
)
 
Figure 64bis represents the mean F-score over all ML algorithms per author class. The B 
class – which proved to be the most stable one on the combination of syntax-based, lexical and 
token-level features as well – seems to be the most stable one as far as syntax-based features 
alone are concerned (50.58%), while the A class is not as easily predictable (49.28%). The 
heterogeneous O class is the most difficult class to predict (46.75%). 
Graph 64bis. F-score per class of syntax-based features
(TOTAL.arff)
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
On
eR
Ba
g O
ne
R
Bo
os
t O
ne
R
Na
ive
 B
ay
es
Ba
g N
aiv
e B
Bo
os
t N
aiv
e B
j48
.J4
8 (
m1
)
Ba
g j
48
.J4
8
Bo
os
t j4
8.J
48
kN
N 
(k1
)
Ba
g k
NN
Bo
os
t k
NN
Ne
ura
l N
etw
ork
s
class A
class B
class O
 
o Feature selection on syntax-based features 
 
The most informative syntax-based features are 6 (PROPN), 8 (NUM), 14 (ADJTOT), 16 (PUNCTOT), 
19 (PROPNTOT), 21 (NUMTOT), 23 (PRONTOT), 24 (PREPTOT), 27 (PVMV), 35 (PVMETTTOT), 41 
(PVTGWMV), 43 (PVVERLMV), 45 (VDVRIJZONDER), 58 (PVTGWMETTTOT), 77 (PATTERN4BIN), 79 
(PATTERN6BIN), 91 (PATTERN6), 102 (PATTERN4TOT), 104 (PATTERN6TOT) and 105 
(PATTERN7TOT). The most stable ones are part of POS.arff (8 features), VERBBASIC.arff (2 
  106 
  
features), VERB.arff (4 features), PATTERNBIN.arff (2 features) and PATTERNNUM.arff (4 
features). It is remarkable to see how well information about parts-of-speech (POS) is able to 
predict the class of a specific text. 
Bagging Naive Bayes classifies 61.67% of the training instances correctly, but Naive 
Bayes and Boosting Naive Bayes perform almost equally well (viz. 61% accuracy). The weak 
classifier OneR barely reaches 45% accuracy. Bagging Naive Bayes has an F-score of 61.67% 
(cf. Appendix 12: Graph 67). 
The effect of feature selection for syntax-based features is clear: before feature 
selection, Bagging Naive Bayes achieved 53% accuracy and 61.67% after feature selection.  
Graph 65. Generalisation accuracy after feature selection on 
syntax-based features (TOTAL.arff)
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
On
eR
Ba
g O
ne
R
Bo
os
t O
ne
R
Na
ive
 B
ay
es
Ba
g N
aiv
e B
Bo
os
t N
aiv
e B
j48
.J4
8 (
m1
)
Ba
g j
48
.J4
8
Bo
os
t j4
8.J
48
kN
N 
(k1
)
Ba
g k
NN
Bo
os
t k
NN
Ne
ura
l N
etw
...
ML algorithms
A
cc
ur
ac
y 
(in
 %
)
 
The voting scheme that best reduces the error rate – viz. with 17.79% – is Bagging with 
j48.J48, a Decision Tree classifier. Boosting j48.J48 achieves an Error Reduction of 15.94% 
(Graph 66). Bagging and Boosting have little influence on the other classifiers used to train the 
data set by means of selected syntax-based features. 
  107 
  
Graph 66. Error Reduction through Bagging and Boosting 
after feature selection on syntax-based features (TOTAL.arff)
-100,00%
-50,00%
0,00%
50,00%
100,00%
B
ag
gi
ng
(O
ne
R
)
B
oo
st
in
g
(O
ne
R
)
B
ag
gi
ng
(N
ai
ve
 B
)
B
oo
st
in
g
(N
ai
ve
 B
)
B
ag
gi
ng
(j4
8.
J4
8)
B
oo
st
in
g
(j4
8.
J4
8)
B
ag
gi
ng
(k
N
N
)
B
oo
st
in
g
(k
N
N
)
 
 
Figure 42 represents the mean F-score over all ML algorithms per author class. There is 
not a big difference between the accuracy of the algorithms for the A and B classes and the 
heterogeneous O class (Graph 67bis). 
CLASS MEAN F-SCORE OVER 
ALL ML ALGORITHMS 
A 52.19% 
B 54.18% 
O 51.18% 
Figure 42: Mean F-score per class after feature selection (TOTAL.arff) 
Graph 67bis. F-score per class after feature
selection on syntax-based features (TOTAL.arff)
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
On
eR
Ba
g O
ne
R
Bo
os
t O
ne
R
Na
ive
 B
ay
es
Ba
g N
aiv
e B
Bo
os
t N
aiv
e B
j48
.J4
8 (
m1
)
Ba
g j
48
.J4
8
Bo
os
t j4
8.J
48
kN
N 
(k1
)
Ba
g k
NN
Bo
os
t k
NN
Ne
ura
l N
etw
...
ML algorithms
A
cc
ur
ac
y 
(in
 %
)
class A
class B
class O
 
Graph 68 represents the influence of the -m value on Bagging j48.J48 – the best 
performing setting for j48.J48. It indicates that the Decision Tree classifier distinguishes 
between the classes in 60.33% of the cases when a new branch is formed only when it covers 
32 of the 300 instances. This means that most texts have different characteristics as far as a 
  108 
  
selection of syntax-based features is concerned. With -m = 1 and before feature selection, 
j48.J48 was able to classify 46.33% of the training instances correctly. Varying the -m value 
allows us to raise the accuracy with 14 percent. 
Graph 68. Influence of the -m value in Bagging j48.J48 on 
generalisation accuracy after feature selection on syntax-
based features (TOTAL.arff)
0,00%
10,00%
20,00%
30,00%
40,00%
50,00%
60,00%
70,00%
80,00%
90,00%
100,00%
1 2 4 6 8 16 32 64 128 256
-m value
A
cc
ur
ac
y 
(in
 %
)
 
Graph 69 shows the influence of varying the k value in Bagging kNN. The 4 nearest 
neighbours help the ML algorithm to classify the instances according to the author classes in 
60.33% of the cases. A k value of 1 (before feature selection) allowed us to classify 45.67% of 
the instances correctly. Here we see a big increase in accuracy due to a different k value. 
Graph 69. Influence of the k value in Bagging kNN on 
generalisation accuracy after feature selection on syntax-
based features (TOTAL.arff)
0,00%
10,00%
20,00%
30,00%
40,00%
50,00%
60,00%
70,00%
80,00%
90,00%
100,00%
1 2 4 6 8 16 32 64 128 256
k value
A
cc
ur
ac
y 
(in
 %
)
 
  109 
  
2.4.1.3 Conclusions 
 
After the training-and-validation experiments, a number of conclusions should be drawn 
concerning the use of certain machine learning algorithms and voting schemes. We should also 
consider the classification per class and draw conclusions about the nature and characteristics of 
the author classes. Another important aspect is the efficiency of the feature sets we selected (cf. 
Section 2.3.2) for Authorship Attribution. 
In most of the feature sets, the k-Nearest Neighbour classifier – with or without Bagging 
or Boosting – is the best classifier. Naive Bayes, Neural Networks and j48.J48 are always 
among the best classifiers. OneR, a very weak and basic classifier, performs worst. This leads 
to the fact that there is most Error Reduction (ER) with Bagging OneR since the other classifiers 
trained during the experiments already performed well from the beginning. We can account for 
this by saying that Bagging tends to work well for weak classifiers, while Boosting works with 
both weak and stronger classifiers (cf. Section 2.4.1.1). 
The influence of the MinNumObj (or -m) parameter in j48.J48 on accuracy is certain. 
The -m value indicates the number of instances – texts in this study – a new branch in the 
Decision Tree (DT) learner should cover. As we indicated in the discussions of the results 
above, the efficiency of a specific -m value is related to the characteristics of the texts part of 
the training corpus. Abstracting from the results per feature set, we can say that a number of 
feature sets performed best with high -m values, while other feature sets preferred low -m 
values. The first group of feature sets consists of the feature set for the frequency and 
distribution of parts-of-speech (POS.arff), the one for the absence or presence of specific NP 
patterns (PATTERNBIN.arff), the one for the frequency and distribution of those patterns 
(PATTERNNUM.arff) and finally the combination feature set containing only syntax-based 
features (TOTAL.arff). They all perform best with -m equalling 32 or more. We can conclude 
from this that these features are stable characteristics of an author’s style since most instances 
can be put in groups of 32. The second group of feature sets contains the feature set for the 
frequency and distribution of basic verb forms (VERBBASIC.arff), the one for the frequency and 
distribution of specific verb forms (VERB.arff), the binary feature set based on the words with 
highest mutual information (MUTUALINFO.arff), the readability feature (READABILITY.arff) and 
the feature set combining all previously mentioned feature sets (TOTAL.arff). They achieve a 
good performance with -m equalling 16 or less. These features are less stable characteristics of 
  110 
  
the training texts because they cannot be put in groups of 32. The classifier has to be very 
specific about the types of instances to perform well. 
We also investigated the influence of the k value in k-Nearest Neighbour classifiers. The 
k value indicates how many neighbours in the vector space the classifier has to take into 
account for classification. If the best performing k value is high, this means that a lot of texts 
are close to each other in the vector space and influence classification. If the k value needs to be 
low to achieve good results, few texts are close to each other in the vector space but at the same 
time there are clearly delimited groups of instances. The groups of feature sets are not the same 
as with the varied -m value since the k value involves the grouping of texts in the vector space. 
It indicates how far the texts from different authors are away from each other. A first group of 
feature sets – the ones that work best with high k value – consists of the feature set for the 
frequency and distribution of basic verb forms (VERBBASIC.arff), the one for the absence or 
presence of specific NP patterns (PATTERNBIN.arff), and also the feature set for the frequency 
and distribution of those patterns (PATTERNNUM.arff). These perform best with k equalling 32 or 
more. These features are able to discern the different author classes to a certain extent, but the 
classifier needs a lot of those features to make a clear distinction. The second group of feature 
sets contains the feature set for the frequency and distribution of parts-of-speech (POS.arff), the 
one for the frequency and distribution of specific verb forms (VERB.arff), the binary feature set 
based on the words with highest mutual information (MUTUALINFO.arff), the readability feature 
(READABILITY.arff) and finally the combination feature set (TOTAL.arff). They achieve a good 
performance with k equalling 16 or less. These features clearly delimit the different groups of 
articles. The classifier does not need a lot of information to be able to make a good decision. 
In the training-and-validation experiments discussed above, we found that in most cases, 
the B class is easiest to discern. Apparently, the B class in general contains more stable 
characteristics than the A and O classes. The O class however causes a lot of problems. This can 
be brought back to the fact that the O class is heterogeneous since it consists of texts by eleven 
authors. It contains a lot of different styles and most classifiers have some difficulties with 
distinguishing between the classes because of the nature of the O class. 
Although we have not tested the features to unseen test data yet, we can already draw 
some conclusions about the efficiency of the specific feature sets by means of their 
performance after training and validation. POS.arff, the feature set for the frequency and 
distribution of parts-of-speech, is one of the most basic but also the best performing syntax-
  111 
  
based feature set. It achieves an accuracy and mean F-score of about 63.67% by means of 
Boosting kNN (k 8) after feature weighting and selection. The most informative features part of 
VERBBASIC.arff perform better than the most informative ones part of VERB.arff, which 
nevertheless contains more specific information about the verb forms. VERBBASIC.arff allows us 
to classify 52.33% of the training instances correctly by means of Bagging kNN (k 64), while 
VERB.arff only achieves 51% accuracy (Boosting Naive Bayes). Apparently, specific 
information tends to confuse the classifier. Kukushkina et al. came to a similar conclusion after 
experiments with ‘incomplete’ and ‘complete’ grammatical classes (Kukushkina et al., 2002: 
181). PATTERNNUM.arff is much more reliable for Authorship Attribution than PATTERNBIN.arff, 
even after feature weighting and selection PATTERNBIN.arff only achieves 43.67% accuracy 
with Bagging kNN (k 128) while PATTERNNUM.arff achieves 54% with Neural Networks. This 
was to be expected since most patterns could be found back in all texts, while their frequency 
and distribution is of course different in every text. The binary feature set based on the words 
with highest mutual information, MUTUALINFO.arff, reached an accuracy of 78.53% after feature 
selection. The fact that it is a lexical feature will probably lead to problems in the testing phase, 
when unseen texts have to be classified. The list of twenty words with highest mutual 
information on the training corpus will probably not coalesce with the list you could make on 
the basis of the unseen texts. READABILITY.arff classifies only 46% of the training instances 
correctly. TOTAL.arff, the combination feature set, reaches a 78% accuracy after feature 
selection. If we leave out all token-level and lexical features – i.e. only use syntax-based 
features – we still achieve a 63.67% accuracy, which is equally well as the best performing 
syntax-based feature set, POS.arff. After test experiments, we will see that there is a difference 
in performance between POS.arff and TOTAL.arff with only syntax-based features. 
For a lot of feature sets, feature weighting and selection lead to better results. Figure 43 
shows the results before and after feature selection per feature set. As you can see, feature 
selection has a positive effect on the performance of most ML algorithms and most feature sets. 
Only Neural Networks experience negative effects through feature selection with five of the 
eight feature sets to which feature selection can be applied. 
 
 
 
 
 
 
  112 
  
FEATURE SETS ML ALGORITHMS PERFORMANCE 
  ENTIRE 
FEATURE SET 
AFTER FEATURE 
SELECTION 
pos.arff Boosting OneR 45% 49.33% 
 Boosting Naive Bayes 52.67% 60.33% 
 Neural Networks 51.33% 54.33% 
verbbasic.arff Bagging OneR 37.33% 45.33% 
 Boosting Naive Bayes 
Neural Networks 
51.67% 
50.67% 
52% 
50.33% 
verb.arff Bagging OneR 34% 43.67% 
 Boosting Naive Bayes 
Neural Networks 
47% 
47.67% 
51% 
47.67% 
patternbin.arff Bagging OneR 
Bagging Naive Bayes 
Neural Networks 
36% 
40.67% 
38.33% 
36% 
40.67% 
37.67% 
patternnum.arff Bagging OneR 36% 41.33% 
 Boosting Naive Bayes 
Neural Networks 
48% 
46% 
50.33% 
54% 
mutualinfo.arff Bagging OneR 51.33% 51.33% 
 Boosting Naive Bayes 
Neural Networks 
76.67% 
74.67% 
78.33% 
72% 
total.arff    
o all features Boosting OneR 40.67% 46.67% 
 Boosting Naive Bayes 70.67% 78% 
 Neural Networks 76% 72% 
o syntax-based features Bagging OneR 43.33% 45.33% 
 Bagging Naive Bayes 53% 61.67% 
 Neural Networks 63.67% 59.33% 
         Figure 43: The effect on training performance of feature selection  
 
 
  113 
2.4.2 Testing 
 
The test data not involved in the training phase (cf. Section 2.4.1) allow us to test the model 
trained on the training data. The test corpus was discussed in Section 2.2.2. The next step is 
to explain the experimental design. For the test experiments, we use the classifiers that 
performed best on the training experiments (cf. Section 2.4.2.1). We introduce the Machine 
Learning algorithms and the feature weighting and selection techniques we selected for the 
experiments. The results of the testing experiments are presented and discussed (cf. Section 
2.4.2.2) and finally conclusions are drawn (cf. Section 2.4.2.3). 
 
  114 
2.4.2.1 Experimental design 
 
For testing the trained model on the unseen test data, we selected the ML algorithms that best 
classified the training instances in the experiments described above (cf. Section 2.1.4.2). Per 
feature set, the best conditions for OneR, Naive Bayes, j48.J48, kNN, id3 and Neural 
Networks are applied to the testing phase. These conditions concern 
 
1. the selection of Bagging or Boosting in order to optimise the results 
2. setting the best value for -m (j48.J48) and k (kNN) 
3. the effect and efficiency of feature weighting and selection 
 
We train the different feature sets (e.g. TOTAL.arff) and use a supplied test set (e.g. 
TOTALTEST.arff). This allows us to test the trained classifier on new, unseen texts. Below a 
list of the ML algorithms selected per feature set (Figure 44). In general we can say that 
Bagging OneR, Bagging Naive Bayes, Bagging j48.J48, Bagging kNN and Neural Networks 
perform best for most feature sets. This is due to the positive influence of Bagging for the 
accuracy of the ML algorithms.  
 
ALGORITHMS NUMBER OF FEATURE SETS 
Bagging OneR 7 
Boosting OneR 2 (POSTEST.arff & TOTALTEST.arff (all features)) 
Bagging Naive Bayes 6 
Naive Bayes 
Boosting Naive Bayes 
1 (READABILITY.arff) 
2 (PATTERNBINTEST.arff & TOTALTEST.arff (syntax-based 
features)) 
Bagging j48.J48 8 
Boosting j48.J48 1 (VERBBASICTEST.arff) 
kNN 3 (VERBTEST.arff, MUTUALINFOTEST.arff & 
READABILITYTEST.arff) 
Bagging kNN 5 
Boosting kNN 1 (POSTEST.arff) 
Bagging ID3 1 (PATTERNBINTEST.arff) 
Boosting ID3 1 (MUTUALINFOTEST.arff) 
Neural Networks 9 
       Figure 44: The algorithms and voting schemes selected per feature set 
 
  115 
2.4.2.2 Performance and discussion 
 
The trained classifier per feature set (cf. 2.4.1) is tested on a new test corpus (cf. 2.2.1) by 
means of the ML algorithms and voting schemes we selected (cf. Section 2.4.2.1). In this 
section we investigate the performance of the trained model on the new unseen data. This 
allows us to evaluate the Authorship Attribution classifier presented in this study. The 
evaluation measures used are (generalisation) accuracy and F-score (cf. Section 2.1). All 
results are represented in graphs. The most important ones are in the body of this section, 
the others are in appendix. Conclusions are drawn in Section 2.4.2.3. 
 
  116 
  1. Frequency and distribution of parts-of-speech (POSTEST.arff) 
 
Graph 70 presents the percentage of test instances correctly classified by the trained model. 
All machine learners were trained on the most informative features part of POSTEST.arff. 
These features are 6 (PROPN), 8 (NUM), 14 (ADJTOT), 16 (PUNCTOT), 19 (PROPNTOT), 21 
(NUMTOT), 23 (PRONTOT) and 24 (PREPTOT) (cf. Section 2.4.1.2, subsection 1). Neural 
Networks have the best performance since they make 51% correct decisions. After training, 
the best performing classifier was Boosting kNN with 8 as k value. The mean F-score of 
Neural Networks is 49.90% (cf. Appendix 13: Graph 71). 
Graph 70. Generalisation accuracy of the best performing 
algorithms (POSTEST.arff)
45,00% 49,00% 47,00% 46,00% 51,00%
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
Bo
os
t O
ne
R
Bo
os
t N
aiv
e B
Ba
g J
48
 (-
m 
32
)
Bo
os
t k
NN
 (k
 8)
Ne
ura
l N
etw
...
ML algorithms
A
cc
ur
ac
y 
(in
 %
)
 
If we consider the F-score per author class on Graph 71bis, we see that there is a 10 
percent gap between the B and O class. Four of the five classifiers achieve better results with 
the B class (52.28%) than with the A class (47.48%), which means that the texts by Bart 
Brinckman are more stable as far as the frequency and distribution of POS is concerned. The 
O class performs worst with 41.44%, but this can be explained by its heterogeneity. 
Boosting OneR is only able to classify 30.50% of the test instances part of the O class. 
Graph 71bis. F-score per class of the best
performing algorithms (POSTEST.arff)
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
Bo
os
t O
ne
R
Bo
os
t N
aiv
e B
Ba
g J
48
 (-
m 
32
)
Ba
g k
NN
 (k
 8)
Ne
ura
l N
etw
...
ML algorithms
A
cc
ur
ac
y 
(in
 %
)
class A
class B
class O
 
  117 
  2. Frequency and distribution of basic verb forms (VERBBASICTEST.arff) 
 
It should be remarked that the Neural Networks classifier performed best when trained on 
the entire feature set. The four other algorithms are positively influenced by feature 
weighting and selection which indicate 2 (PVMV), 5 (VD) and 10 (PVMETTTOT) as the most 
informative features. Bagging kNN allows us to correctly classify 55% of the test instances, 
as can be concluded from Graph 72. Its mean F-score over all classes is 54.83% (cf. 
Appendix 13: Graph 73). This time, Neural Networks perform only third-best. There is a 
clear gap between the performance of Bagging kNN and Boosting Naive Bayes and of the 
other three classifiers. The Bagging voting scheme and varying the k value apparently have 
a positive influence on the accuracy of the k-Nearest Neighbour classifier. 
Graph 72. Generalisation accuracy of the best performing 
algorithms (VERBBASICTEST.arff)
42,00% 52,00% 43,00%
55,00% 44,00%
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
Ba
g O
ne
R
Bo
os
t N
aiv
e B
Ba
g J
48
 (-
m 
1)
Ba
g k
NN
 (k
 64
)
Ne
ura
l N
etw
...
ML algorithms
A
cc
ur
ac
y 
(in
 %
)
feature selection
no feature selection
 
Graph 73bis represents the F-score per class. It indicates that the O class often 
achieves the best results. The features based on basic verb forms contain more evidence for 
the A class (49.92%) than for the B (40.24%) and O (49.46%) classes. Texts written by Anja 
Otte seem to be more stable as far as the frequency and distribution of basic verb forms is 
concerned. Especially test instances belonging to the B class are difficult to classify. 
Graph 73bis. F-score per class of the best
performing algorithms (VERBBASICTEST.arff)
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
Ba
g O
ne
R
Bo
os
t N
aiv
e B
Ba
g J
48
 (-
m 
1)
Ba
g k
NN
 (k
 64
)
Ne
ura
l N
etw
...
ML algorithms
A
cc
ur
ac
y 
(in
 %
)
class A
class B
class O
 
  118 
  3. Frequency and distribution of specific verb forms (VERBTEST.arff) 
 
Boosting Naive Bayes obtains an accuracy of 55% after testing the trained model on unseen 
texts, as can be concluded from Graph 74. Just like with POSTEST.arff and 
VERBBASICTEST.arff, Bagging OneR is clearly the worst performing machine learner. The 
mean F-score of Boosting Naive Bayes is 54.67% (cf. Appendix 13: Graph 75). kNN is the 
second-best classifier, although it is not supported by Bagging or Boosting. Even without 
feature weighting and selection, Bagging j48.J48 is able to identify the author of 51% of the 
test data. Varying the k and -m values does the trick here. 
Graph 74. Generalisation accuracy of the best performing 
algorithms (VERBTEST.arff)
44,00% 55,00% 54,00% 52,00%51%
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
Ba
g O
ne
R
Bo
os
t N
aiv
e B
Ba
g J
48
 (-
m 
2)
kN
N 
(k 
16
)
Ne
ura
l N
etw
...
ML algorithms
A
cc
ur
ac
y 
(in
 %
)
feature selection
no feature selection
 
Graph 75bis represents the F-score per class. Three classifiers find the A class easiest 
to identify, the two other classifiers the O class. The algorithms are able to predict the A 
class in 53.18% of the cases, the B class in 46.68% of the cases and the O class in 51.42% of 
the instances. This means that the features based on the specific verb forms contain more 
evidence for unseen texts belonging to the A class than for the B and O classes. Specific verb 
forms are more stable characteristics of the O class than of the B class. 
Graph 75bis. F-score per class of the best
performing algorithms (VERBTEST.arff)
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
Ba
g O
ne
R
Bo
os
t N
aiv
e B
Ba
g J
48
 (-
m 
2)
kN
N 
(k 
16
)
Ne
ura
l N
etw
...
ML algorithms
A
cc
ur
ac
y 
(in
 %
)
class A
class B
class O
 
 
  119 
  4. Presence or absence of specific NP patterns (PATTERNBINTEST.arff) 
 
This time, Bagging OneR clearly sticks out as the best classifier, as can be seen on Graph 
76. It correctly classifies 46% of the test data, while the other classifiers only reach a 
maximum of 36%. The mean F-score of Bagging OneR is only 41.03%, since it only obtains 
an F-score of 27.30% for the B class (cf. Appendix 13: Graph 77). The worst performing 
classifier is Neural Networks on a selected feature set. The most informative features are NP 
patterns 4, 6, 8, 9, 10, 11 and 12. 
Graph 76. Generalisation accuracy of the best performing 
algorithms (PATTERNBINTEST.arff)
46,00% 35,00% 36,00% 36,00% 36,00% 33,00%
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
Ba
g O
ne
R
Ba
g N
aiv
e B
Ba
g J
48
 (-
m 
32
)
Ba
g k
NN
 (k
 12
8)
Ba
g I
D3
Ne
ura
l N
etw
ork
s
ML algorithms
A
cc
ur
ac
y 
(in
 %
)
feature selection
no feature selection
 
Graph 77bis represents the F-score per class. Bagging OneR has a mean F-score of 
58.80% on the A class. All other classifiers achieve best on the B class (36.10%). Most 
evidence can be found for the A class (37.20%), and least for the O class (34.02%). This 
means that the A class is more stable for features based on the presence or absence of 
specific NP patterns than the other two author classes. 
Graph 77bis. F-score per class of the best
performing algorithms (PATTERNBINTEST.arff)
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
Ba
g O
ne
R
Ba
g N
aiv
e B
Ba
g J
48
 (-
m 
32
)
Ba
g k
NN
 (k
 12
8)
Ba
g I
D3
Ne
ura
l N
etw
ork
s
ML algorithms
A
cc
ur
ac
y 
(in
 %
)
class A
class B
class O
 
 
 
 
  120 
  5. Frequency and distribution of specific NP patterns 
(PATTERNNUMTEST.arff) 
 
Most ML classifiers achieve a similar accuracy on PATTERNNUMTEST.arff (viz. a maximum 
of 46%), but Neural Networks achieve 48%, as you can see on Graph 78. Neural Networks 
have a mean F-score over all classes of 47.73% (cf. Appendix 13: Graph 79). The second-
best classifier is Bagging kNN, which enables us to identify the author of 46% of the test 
instances when trained on the entire feature set. Again, Bagging OneR is the worst 
performing machine learner. 
Graph 78. Generalisation accuracy of the best performing 
algorithms (PATTERNNUMTEST.arff)
40,00% 45,00% 43% 48,00%46,00%
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
Ba
g O
ne
R
Bo
os
t N
aiv
e B
Ba
g J
48
 (-
m 
64
)
Ba
g k
NN
 (k
 32
)
Ne
ura
l N
etw
...
ML algorithms
A
cc
ur
ac
y 
(in
 %
)
feature selection
no feature selection
 
Graph 79bis presents the F-score per class. All machine learners have least difficulty 
with the B class. The trained model is able to predict the A class in 39.28% of the test cases, 
the B class in 50.00% of the cases and the O class in 42.22% of the instances. The frequency 
and distribution of NP patterns seem to be more stable characteristics of the B class than of 
the A and O classes. The O class is easier to predict than the A class by means of numeric 
information about NP patterns. 
Graph 79bis. F-score per class of the best
performing algorithms (PATTERNNUMTEST.arff)
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
Ba
g O
ne
R
Bo
os
t N
aiv
e B
Ba
g J
48
 (-
m 
64
)
Ba
g k
NN
 (k
 32
)
Ne
ura
l N
etw
...
ML algorithms
A
cc
ur
ac
y 
(in
 %
)
class A
class B
class O
 
  121 
 6. Presence or absence of specific words based on mutual information 
(MUTUALINFOTEST.arff) 
 
Graph 80 represents the accuracy of trained ML algorithms tested on the 
MUTUALINFOTEST.arff feature set. The two best performing algorithms are tested on the 
entire feature sets, while the others are tested on a selected set of lexical features. A 
performance of 64% accuracy and 63.63% F-score is obtained by means of Neural 
Networks (cf. Appendix 13: Graph 81). Bagging j48.J48 reaches the 59% accuracy when 
applied to the entire feature set. Boosting ID3 performs even worse than Bagging OneR, 
which was the algorithm that performed best with most previously discussed feature sets. 
Graph 80. Generalisation accuracy of the best performing 
algorithms (MUTUALINFOTEST.arff)
51,00% 57,00% 54% 43%
59% 64,00%
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
Ba
g O
ne
R
Bo
os
t N
aiv
e B
Ba
g J
48
 (-
m 
8)
kN
N 
(k 
16
)
Bo
os
t ID
3
Ne
ura
l N
etw
...
ML algorithms
A
cc
ur
ac
y 
(in
 %
)
feature selection
no feature selection
 
Graph 81bis represents the F-score per class and per algorithm. There are big 
differences between the predictability of classes A, B and O. The algorithms are able to 
predict the A class in only 48.67% of the test cases, the B class in 64.28% of the cases and 
the O class in 44.78% of the instances. Bagging OneR is unable to classify any of the test 
instances belonging to the O class. All algorithms have least difficulty identifying the B 
class. This means that the lexical features based on the mutual information measure contain 
most evidence for the B class and that the A class is much harder to predict. 
  122 
Graph 81bis. F-score per class of the best
performing algorithms (MUTUALINFOTEST.arff)
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
Ba
g O
ne
R
Bo
os
t N
aiv
e B
Ba
g J
48
 (-
m 
8)
kN
N 
(k 
16
)
Bo
os
t ID
3
Ne
ura
l N
etw
...
ML algorithms
A
cc
ur
ac
y 
(in
 %
)
class A
class B
class O
 
  7. Readability score (READABILITYTEST.arff) 
 
The generalisation accuracy of the best performing ML algorithm – Naive Bayes – is 52%, 
as you can see on Graph 82. Neural Networks also achieve more than 50% accuracy. The 
worst classifier is Bagging OneR, just like with a number of previously discussed feature 
sets. The mean F-score of the Neural Networks classifier is a bit lower (48.53%) because it 
is only able to classify 29.60% of the training instances belonging to the O class (cf. 
Appendix 13: Graph 83). 
Graph 82. Generalisation accuracy of the best performing 
algorithms (READABILITYTEST.arff)
42,00% 52,00% 45% 44% 51,00%
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
Ba
g O
ne
R
Na
ive
 B
ay
es
Ba
g J
48
 (-
m 
6)
Ba
g k
NN
 (k
 16
)
Ne
ura
l N
etw
ork
s
 
As can be derived from Graph 83, the differences between the F-scores per class are 
big. There is not much evidence for the O class in readability scores. Naive Bayes is unable 
to classify any of the test instances belonging to the O class. The model is able to predict 
53.42% of the instances belonging to the A class, 53.04% of the B class instances and only 
25.84% of the O class instances. Three classifiers find the A class easiest to predict, and the 
two other classifiers have least difficulty with the B class. 
  123 
Graph 83bis. F-score per class of the best
performing algorithms (READABILITYTEST.arff)
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
Ba
g O
ne
R
Na
ive
 B
ay
es
Ba
g J
48
 (-
m 
6)
Ba
g k
NN
 (k
 16
)
Ne
ura
l N
etw
...
ML algorithms
A
cc
ur
ac
y 
(in
 %
)
class A
class B
class O
 
  8. Combination (TOTALTEST.arff) 
 
TOTALTEST.arff contains all seven previously discussed feature sets, which means it consists 
of 131 features. First, we use the entire feature set, but later only syntax-based features are 
selected. 
 
o  Entire feature set 
 
Graph 84 represents the accuracy of ML algorithms tested on the model trained on the entire 
feature set of TOTALTEST.arff. A performance of 62% accuracy is obtained by means of 
Naive Bayes. The F-score of Boosting Naive Bayes is 60.13% (cf. Appendix 13: Graph 85). 
Once more, Boosting OneR proves to be the worst performing classifier of the selected ML 
algorithms. The algorithms that were applied on the entire feature set do not perform as well 
as the Naive Bayes classifier. 
Graph 84. Generalisation accuracy of the best performing 
algorithms on all features (TOTALTEST.arff)
40,00%
62,00% 58%55% 58,00%
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
Bo
os
t O
ne
R
Na
ive
 B
ay
es
Ba
g J
48
 (-
m 
2)
Ba
g k
NN
 (k
 4)
Ne
ura
l N
etw
...
ML algorithms
A
cc
ur
ac
y 
(in
 %
)
feature selection
no feature selection
 
The F-score per class and per algorithm is presented in Graph 85bis. The trained 
classifiers are able to predict the A class in 51.06% of the test cases, the B class in 57.90% of 
  124 
the cases and the O class in 53.36% of the instances. Apparently the B class is more stable as 
far as the combination of syntax-based, lexical and token-level features is concerned. 
Graph 85bis. F-score per class of the best
performing algorithms on all features
(TOTALTEST.arff)
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
Bo
os
t O
ne
R
Na
ive
 B
ay
es
Ba
g J
48
 (-
...
Ba
g k
NN
 (k
 4)
Ne
ura
l N
e..
.
ML algorithms
A
cc
ur
ac
y 
(in
 %
)
class A
class B
class O
 
o  All syntax-based features 
 
Selecting only the syntax-based features allows us to classify 55% of the test instances 
correctly (Graph 86). This is only seven percent less than with a combination of syntax-
based, lexical and token-level features. We can say that syntax-based features are able to 
achieve acceptable results for Authorship Attribution. The 55% accuracy is obtained by 
means of Neural Networks, a classifier that also reaches a 54.87% F-score (cf. Appendix 13: 
Graph 87). Bagging OneR is only able to identify the author of 34% of the test instances 
because it classifies 20.40% of the O class instances correctly. 
Graph 86. Generalisation accuracy of the best performing 
algorithms on syntax-based features (TOTALTEST.arff)
34,00%
51,00% 51,00% 49,00% 55,00%
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
Ba
g O
ne
R
Ba
g N
aiv
e B
Ba
g J
48
 (-
m 
32
)
Ba
g k
NN
 (k
 4)
Ne
ura
l N
etw
...
ML algorithms
A
cc
ur
ac
y 
(in
 %
)
feature selection
no feature selection
 
Graph 87bis represents the mean F-score over all ML algorithms per author class. 
The A class (51.82%) seems to be the most stable one as far as syntax-based features are 
concerned, while the B class – which proved to be the most stable one on the combination of 
  125 
features –  is not as easily predictable (46.68%). The heterogeneous O class is the most 
difficult class to predict (44.96%). 
Graph 87bis. F-score per class of the best
performing algorithms on syntax-based
features (TOTALTEST.arff)
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
Ba
g O
ne
R
Ba
g N
aiv
e B
Ba
g J
48
 (-
...
Ba
g k
NN
 (k
 4)
Ne
ura
l N
et.
..
ML algorithms
A
cc
ur
ac
y 
(in
 %
)
class A
class B
class O
 
 
  126 
2.4.2.3 Conclusions 
 
After having tested the trained model’s performance on unseen data, we can draw some 
important conclusions concerning the use of syntax-based, lexical and token-level features 
for Authorship Attribution. We should first consider the efficiency of the different machine 
learning algorithms and voting schemes we used in our test experiments. The F-score per 
class allows us to draw some conclusions about the nature and characteristics of the author 
classes. These conclusions help us to formulate an answer to the following question: Do 
advanced language technology techniques for extracting linguistic features and machine 
learning lead to acceptable results for Authorship Attribution? (cf. Section 2.5). 
Testing the trained models on the different feature sets – containing the 
characteristics of the new, unseen texts by the three author groups (A, B and O) – taught us 
that Neural Networks are the best option for four of the nine feature sets, viz. POSTEST.arff, 
PATTERNNUMTEST.arff, MUTUALINFOTEST.arff, TOTALTEST.arff (with syntax-based features). 
Three other feature sets obtain highest results with Naive Bayes or Boosting Naive Bayes. 
Bagging kNN is the best option for VERBBASICTEST.arff. A real exceptional case is that of 
PATTERNBINTEST.arff, which works best with the Bagging OneR machine learner. For all 
other feature sets, Bagging OneR was the worst option. After training, we already saw that 
PATTERNBIN.arff was not influenced by feature weighting and selection. Neural Networks 
even performed better before feature selection. The feature set consists of binary 
information about NP patterns. The problem with these features is, that almost all patterns 
are present in the training and test data. Therefore, the model trained on these characteristics 
does not seem to be able to identify the author of a trained text in more than 43.67% of the 
cases. Fortunately, the results are better when applied to unseen texts: Bagging OneR allows 
us to classify 46% of the test instances. 
Abstracting from the different feature sets, we can say that the A class is in most 
cases the easiest to discern. Five of the nine feature sets (viz. VERBBASICTEST.arff, 
VERBTEST.arff, PATTERNBINTEST.arff, READABILITYTEST.arff and the syntax-based features 
part TOTALTEST.arff) allow us to identify more instances belonging to the A class than to the 
other classes. The four other feature sets have least difficulty with the B class. Apparently, 
the A class in general contains more stable – syntax-based, lexical and token-level – 
characteristics than the B and far more than the O class. The A class seems to be the more 
stable one as far as syntax-based features are concerned. A lot of problems in classification 
are caused by the O class. This can be brought back to the fact that the O class is 
heterogeneous since it consists of texts by eleven authors (cf. Section 2.2.2). It contains a lot 
  127 
of different styles and most classifiers have some difficulties with distinguishing between 
the classes because of the nature of the O class. Especially in READABILITYTEST.arff this 
becomes apparent, where the classifiers are only able to classify 25.84% of the test instances 
belonging to the O class, and more than 53% of the instances belonging to the A and B 
classes. 
The most important conclusions that have to be drawn concern the efficiency of the 
specific feature sets by means of their performance after testing the trained classifiers. 
POSTEST.arff, the feature set for the frequency and distribution of parts-of-speech, is one of 
the most basic but also one of the best performing sets. It achieves an accuracy and mean F-
score of about 51% by means of Neural Networks after selecting the most informative 
features. VERBBASICTEST.arff and VERBTEST.arff perform equally well on the unseen test 
data. After training, features belonging to VERBBASIC.arff seemed to be more stable 
characteristics, but now we see that VERBBASICTEST.arff achieves 55% by means of Bagging 
kNN (k value 64) and VERBTEST.arff by means of Boosting Naive Bayes. As we had already 
concluded after training, PATTERNNUMTEST.arff is more reliable for Authorship Attribution 
than PATTERNBINTEST.arff. The gap is less big, however. The binary feature set for NP 
patterns only achieves 46% accuracy with Bagging OneR, while the numeric feature set 
achieves 48% with Neural Networks. This was to be expected since most patterns could be 
found back in all texts, while their frequency and distribution is of course different in every 
text. The binary feature set based on the words with highest mutual information, 
MUTUALINFOTEST.arff, reached an accuracy of 64% after testing. This is 14 percent less than 
after training. We had expected a decrease in accuracy because a list of twenty words with 
highest mutual information on the training corpus does probably not coalesce with the list 
you could make on the basis of the unseen texts. READABILITY.arff was only able to classify 
46% of the training instances correctly. When applied to unseen texts, readability scores 
appeared to perform much better: 52% with Naive Bayes. This leads to the conclusion that 
readability scores – a token-level feature – are rather good predictors. Testing on the most 
informative features part of TOTALTEST.arff, the combination feature set, leads to a 62% 
accuracy. A combination of all previously discussed syntax-based features 
(TOTALTEST.arff), allows us to classify 55% of the test instances correctly. Figure 45 
compares the training and performance results per feature set. 
Lexical and token-level features have been used since the beginning of research in 
Authorship Attribution. Nevertheless, a combination of syntax-based, lexical and token-
level features appears to perform only seven percent better than syntax-based features alone. 
  128 
This opens up new perspectives for research on the use of syntax-based features for 
Authorship Attribution. 
 
FEATURE SETS TRAINING PERFORMANCE TEST PERFORMANCE 
1. POS: the frequency and 
distribution of parts-of-
speech 
63.67% 
(Boost kNN, k 8) 
51% 
(Neural Networks) 
2. VERBBASIC: the 
frequency and distribution 
of basic verb forms 
52.33% 
(Bag kNN, k 64) 
55% 
(Bag kNN, k 64) 
3. VERB: the frequency and 
distribution of specific verb 
forms 
51% 
(Boost Naive Bayes) 
55% 
(Boost Naive Bayes) 
4. PATTERNBIN: the 
absence or presence of 
specific NP patterns 
43.67% 
(Bag kNN, k 128) 
46% 
(Bag OneR) 
5. PATTERNNUM: the 
frequency and distribution 
of specific NP patterns 
54% 
(Neural Networks) 
48% 
(Neural Networks) 
6. MUTUALINFO: the 
presence or absence of 
specific words based on 
mutual information 
78.33% 
(Boost Naive Bayes) 
64% 
(Neural Networks) 
7. READABILITY: the 
readability scores 
46% 
(kNN, k 16)) 
52% 
(Naive Bayes) 
8. TOTAL  
o all features 78% 
(Boost Naive Bayes) 
 
62% 
(Naive Bayes) 
o syntax-based features 63.67% 
(Neural Networks) 
55% 
(Neural Networks) 
 Figure 45: Highest training and test performance per feature set 
  129 
2.5 Conclusions 
 
The goal of the experiments was to allow us to support the central hypothesis of this paper: 
Authorship Attribution can be seen as a special case of Text Categorization and using 
advanced language technology techniques for extracting linguistic features and Machine 
Learning for Authorship Attribution gives acceptable results. Another important hypothesis 
is, that syntax-based features are stable characteristics of any author’s style and therefore 
able to predict the author of an unknown text almost equally well as lexical and token-level 
features. 
On the basis of training and test experiments on an Authorship Attribution task 
concerning newspaper articles from the national politics section, we found enough evidence 
to support both hypotheses. Although there does not seem to be much difference between 
texts by different authors included in the training and test corpus, linguistic features 
extracted by means of scripts in the AWK programming language appear to lead to 
acceptable results on the Authorship Attribution task. 
The experiments are evaluated in terms of accuracy, F-score and Error Reduction 
(ER). By means of macro-averaging, the mean F-score is computed (Sebastiani, 2002: 33) 
(cf. Section 2.1). We selected a training and a test corpus. This last one is not involved in 
the training experiments so that we could test the trained model on unseen data (cf. Section 
2.2). The construction and selection of features is the most important step in preparing for 
classification. Those features have to be represented in document feature vectors which 
enable the Machine Learning algorithms to classify the texts according to a specific 
document category. After Memory-Based Shallow Parsing (MBSP), features are extracted by 
means of AWK scripts and the Rainbow program for statistical text categorization (cf. 
Section 2.3). 
Classification by means of the extracted syntax-based, lexical and token-level 
features is performed using Machine Learning algorithms. The training set is used to build a 
model for Authorship Attribution, while the validation set (cf. Section 2.4.2) is used to 
evaluate the accuracy of the trained model at the same time (Mitchell, 1997: 69). In the 
testing phase, new unseen text documents are classified by means of the trained model. 
In the training experiments, we investigated the influence of: 
1. the Machine Learning algorithm selected for classification 
2. the Bagging and Boosting voting schemes 
3. feature weighting and selection 
  130 
4. the authors’ styles 
5. varying the number of instances a new branch in the Decision Tree 
classifier should cover 
6. varying the number of instances the k-Nearest Neighbour classifier should 
take into account for classification 
For the test experiments, we selected the best performing classifiers after training-and-
validation. 
As far as the classification of the training data is concerned, we found the k-Nearest 
Neighbour machine learner – with or without Bagging or Boosting – to be the most reliable 
classifier for most feature sets. Naive Bayes, Neural Networks and j48.J48 (a Decision Tree 
classifier) are often among the best classifiers. Selecting OneR or j48.J48 often means a low 
percentage of correctly classified training instances. Nevertheless, Bagging reduces the error 
rate of those algorithms that perform weak on the Authorship Attribution task. Often, 
Bagging OneR performs better than OneR because the voting scheme reduces the error rate 
of the classifier. 
By comparing the performance of the different classifiers before and after feature 
selection, we can say that feature weighting and selection on training-and-validation have 
positive influence on four of the eight feature sets. Three other feature sets experience an 
increase in error rate through feature selection. The feature set consisting of binary 
information about NP patterns (PATTERNBIN.arff) is not influenced by feature selection. 
The best performing classifiers were selected for identifying the author of the unseen 
test data. Neural Networks and Naive Bayes once more prove to be the best machine 
learners for these Authorship Attribution experiments. Bagging OneR was in all cases the 
worst option. Most feature sets found the A class easiest to discern, while the training 
experiments had indicated that the B class was the most stable class. The differences in 
performance between the A and B class are however small. Despite the fact that the texts 
belonging to both classes look very much alike, we are able to achieve acceptable results on 
the Authorship Attribution task. 
A combination of all syntax-based features from other feature sets enables us to 
classify 63.67% of the training instances and 55% of the test instances correctly. Testing the 
model trained on a combination of all syntax-based, lexical and token-level features 
achieves an accuracy of 62%. We can conclude that syntax-based features achieve 
acceptable results when applied to a three-class Authorship Attribution problem. 
If we compare our results with those of other researchers in Authorship Attribution, 
we see that frequencies of rewrite rules are able to distinguish between authors, register and 
  131 
text type in 95% of the texts. However, Baayen et al. point out that their method is too 
extensive to be used in actual Authorship Attribution practice: 
 
With the general lack of syntactically annotated text material, 
it is unlikely that the works in question are available in such 
an annotated form. (Baayen et al., 1996 : 129) 
 
Experiments on frequencies of word forms, word lengths, tagwords and bigrams of 
tagwords reported on by Diederich et al. indicate that results lie between 60 and 80 percent 
of the cases. Recall values range between 55 and 100 percent for lexical features and 
between 15 and 40 percent for a combination of token-level and syntax-based features. This 
shows that our own test results are within line and even considerably better as far as syntax-
based features are concerned (Diederich et al., 2000). Cluster analysis, a statistical 
technique, can also be applied to Authorship Attribution. On third-person narratives only, 
frequencies of high frequency words reach a 87.5% accuracy (Hoover, 2001: 428). 
MUTUALINFO.arff, our feature set based on the mutual information between the classes, only 
achieves 64% accuracy after testing and by means of Machine Learning techniques. This 
means there is room for improvement for our lexical features. The success rate of Markov 
chains goes up to 83.72%, but these token-level features are too small for meaningful 
conclusions regarding characteristics of the texts by the individual authors (Khmelev & 
Tweedie, 2002: 306). A sequel of this study indicates that Markov chains (73% and 62%) 
achieve better results than grammatical classes (61% and even 4%). Their accuracies for 
grammatical features are in line with our results, although they believe that linguistically 
microscopic data like Markov chains are the best approach for a Authorship Attribution 
problem (Kukushkina et al., 2002). Stamatatos et al. extracted token-level, phrase-level and 
analysis-level features by means of the Sentence and Chunk Boundaries Detector (SCBD) 
and found that they have an average error rate of 31% over all authors – and thus a success 
rate of 69% (Stamatatos et al., 1999: 162). Our combination of lexical, token-level and 
syntax-based features achieves 62% accuracy, which comes close to the above mentioned 
investigations. 
In a two-class problem – with the two classes being A and B, our set of selected 
syntax-based, lexical and token-level features classifies 77.94% of the test instances 
correctly. Selected syntax-based features achieve an accuracy of 69.12% (Appendix 14). 
We can conclude from all these results that the syntax-based, lexical and token-level 
features we extracted by means of scripts in the AWK programming language and the 
Rainbow program for statistical text categorization are able to successfully tackle two- and 
  132 
three-class Authorship Attribution problems. As a matter of fact, syntax-based features 
perform almost equally well as lexical and token-level features, which were believed to be 
the most reliable discriminators for authors. 
  133 
3. Conclusion and further research 
 
The current advances and improvements in language technology and shallow parsing allow 
us to use insights from these fields in Authorship Attribution. Starting from a document, the 
task is to automatically extract information concerning the author of the text. 
Although a research topic for decades, Authorship Attribution has almost exclusively 
dealt with token-level and lexical features. Only ten years ago, a number of researchers 
suggested the use of syntax-based features for Authorship Attribution (Baayen et al., 1996; 
Diederich et al., 2000; Holmes, 1994; Kukushkina et al., 2002; Stamatatos et al., 1999, 
2000, 2001a, 2001b). 
In this study, linguistic analysis in the form of Memory-Based Shallow Parsing 
(MBSP) is used for pre-processing the training and test corpora. Style markers are extracted 
by means of AWK scripts and the Rainbow program. Machine Learning (ML) algorithms are 
applied in order to identify the author of an unknown text. These classifiers are first trained 
on a set of training instances and later tested on unseen data. Especially Artificial Neural 
Networks are believed to be the future for Authorship Attribution (Holmes, 1994: 104). 
There has not been much research on the efficiency of syntax-based features yet. 
Though the results are promising, a lot of researchers try to avoid this type of features 
because they are painstaking: 
 
Current NLP tools are not able to provide accurate calculation 
results for many of the previously proposed style markers. In 
the study of register variation conducted by Biber (1995), a 
subset of the measures (i.e., the simplest ones) was calculated 
by computational tools and the remaining were counted 
manually. Additionally, the automatically acquired measures 
were counterchecked manually (Stamatatos et al., 2001a: 474) 
 
The central hypothesis of this study was that using advanced language technology 
techniques for extracting linguistic features and Machine Learning for Authorship 
Attribution gives acceptable results. We automatically extracted seven feature sets and 
combined these in an eighth feature set. Classification is performed by means of ML 
algorithms. After training the model on the training data, the best performing machine 
learners are selected for the test phase. 
  134 
The best machine learners for identifying the author of unknown texts appeared to be 
the k-Nearest Neighbour classifier, Naive Bayes and Neural Networks. Feature weighting 
selection has a positive influence on the classification process. We found that syntax-based 
features alone perform almost equally well as a combination of syntax-based, lexical and 
token-level features for a three-class Authorship Attribution task. This means our 
experiments on syntax-based features support the central hypothesis of this study. Whereas 
researchers believed syntax-based features could be a help for other types of features, we 
have reason to believe that they might be able to identify the author of an unknown text 
independent of lexical or token-level features. 
This clearly opens up new perspectives for further research on syntax-based features 
and Machine Learning algorithms for Authorship Attribution. First of all, more research 
should be done on syntax-based features based on parsed text, e.g. the frequency of clause 
types, the frequency of syntactic parallelism, the ratio of main to subordinate clauses, etc. 
(Glover & Hirst, 1995: 4). Secondly, there should be research on the efficiency of specific 
combinations of types of features for specific Authorship Attribution problems. This 
integrated approach is also closely related to the field of Plagiarism Detection. Someone 
who commits plagiarism, will probably use the same words or synonyms of the words 
appearing in the source text. Both texts may be lexically similar because the words are on 
the surface level of the text and can be manipulated. Nevertheless, the two texts will be 
syntactically different because every author has his own syntactic style which is not subject 
to conscious manipulation because it is a process of years or even decades. Another research 
subject related to Authorship Attribution is the use of cue phrases like in spite of, 
nevertheless, whereas for English, or ondanks het feit dat, desalniettemin, terwijl for Dutch. 
According to Rhetorical Structure Theory (RST), coherence relations between parts of a text 
can be defined (Mann & Thompson, 1988). According to Knott & Dale, cue phrases are 
closely related to coherence relations (Knott & Dale, 1994). Whereas sentence structure is 
based on strict syntactic principles, text structure is based on the logical coherence between 
sentences and parts of sentences. These differ from author to author and could therefore be 
used for Authorship Attribution when the coherence relations are automatically linked to 
cue phrases. 
  135   
References 
 
Appelt, D. & D. Israel (1999) Introduction to Information Extraction Technology. A tutorial 
prepared for IJCAI 99, pp. 1-41. 
Argamon, S., M. Koppel & G. Avneri (1998) Routing Documents According to Style. First 
International Workshop on Innovative Information Systems. 
Baayen, H., H. Van Halteren & F. Tweedie (1996) Outside the Cave of Shadows: Using 
Syntactic Annotation to Enhance Authorship Attribution. Literary and Linguistic 
Computing, 11(3), pp. 121-131. 
Benedetto, D., E. Caglioti & V. Loreto (2002) Language Trees and Zipping. Physical 
Review Letters, 88(4). 
Biskri, I. & S. Delisle (2002) Text Classification and Multilinguism: Getting at Words via 
N-Grams of Characters. Proceedings of the 6th World Multiconference on Systemics, 
Cybernetics and Informatics (SCI-2002), vol. 5, pp. 110-115. 
Breiman, L. (1996) Bias, Variance, and Arcing Classifiers. Technical Report 460. Statistics 
Department. Berkeley: University of California, 22 pages. 
Cavnar, W. & J. Trenkle (1994) N-gram-based text categorization. Proceedings of SDAIR-
94, 3rd Annual Symposium on Document Analysis and Information Retrieval, pp. 161-
175. 
Daelemans, W., S. Buchholz & J. Veenstra (1999) Memory-Based Shallow Parsing. 
Proceedings of CoNLL-99, Bergen, Norway. pp. 53-60. 
Diederich, J., J. Kindermann, E. Leopold & G. Paass (2000) Authorship Attribution with 
Support Vector Machines. Applied Intelligence, 19(1-2), pp. 109-123. 
Dunning, T. (1994) Statistical Identification of Language. Technical Report MCCS 94-273. 
New Mexico: State University. 
Foster, D. (1999) The Claremont Shakespeare Authorship Clinic: How Severe Are the 
Problems? Computers and the Humanities, 32(6), pp. 491-510. 
Frank, E., C. Chui & I. Witten (2000) Text Categorization Using Compression Models. 
Proceedings of the Data Compression Conference. 
Freund, Y. & R. Schapire (1996) Experiments with a New Boosting Algorithm. Proc 
International Conference on Machine Learning, pp. 148-156. 
Fung, G. & O. Mangasarian (2003) The Disputed Federalist Papers: SVM Feature Selection 
via Concave Minimization. Draft of the talk presented at The 2002 Annual Meeting of the 
Classification Society of North America. 
  136   
Glover, A., and G. Hirst (1995) Detecting Stylistic Inconsistencies in Collaborative Writing. 
Writers at Work: Professional Writing in the Computerized Environment. London: 
Springer-Verlag. 
Goodman, J. (2002) Extended Comment on Language Trees and Zipping. cf. Joshua 
Goodman’s homepage at Microsoft Research: http://research.microsoft.com/~joshuago/. 
Hoekstra, H., M. Moortgat, I. Schuurman & T. van der Wouden (2001) Syntactic 
Annotation for the Spoken Dutch Corpus Project. CLIN Proceedings, pp. 73-87: 
http://www.let.uu.nl/~Paola.Monachesi/personal/papers/cgnshorten.pdf. 
Holmes, D. (1994) Authorship Attribution. Computers and the Humanities, 28(2), pp. 87-
106. 
Hoover, D. (2001) Statistical Stylistics and Authorship Attribution: An Empirical 
Investigation. Literary and Linguistic Computing, 6(4), pp. 421-444. 
Jurafsky, D. & J. Martin (2000) Speech and Language Processing. New Jersey: Prentice 
Hall. 
Khmelev, D. & F. Tweedie (2001) Using Markov Chains for Identification of Writers. 
Literary and Linguistic Computing, 16(4), pp. 299-307. 
Knott, A. & R. Dale  (1994) Using Linguistic Phenomena to Motivate a Set of Coherence 
Relations. Discourse Processes, 18, pp. 35-62. 
Koppel, M., S. Argamon & A. Shimoni (2003) Automatically categorizing written texts by 
author gender. Literary and Linguistic Computing, 17(4), pp. 401-412. 
Krsul, I. & E. Spafford (1997) Authorship Analysis: Identifying the Author of a Program. 
Computers and Security, 16(3), pp. 233-248. 
Kukushkina O., A. Polikarpov & D. Khmelev (2002) Using Literal and Grammatical 
Statistics for Authorship Attribution, Appendix: Application of Data Compression 
Algorithms in Authorship Attribution. Problemy Peredachi Informatsii, 37(2), pp. 96-108, 
2001. Translated in Problems of Information Transmission, pp. 172-184. 
Mann, W. & S. Thompson (1988) Rhetorical Structure Theory: Toward a Functional Theory 
of Text Organisation. Text, 8, pp. 243-281. 
Mitchell T. (1997) Machine Learning. New York [a.o.]: The McGraw-Hill Companies, Inc. 
Mosteller, F. & D. Wallace (1964) Inference and Disputed Authorship: The Federalist. 
Series in Behavioral Science: Quantitative Methods Edition. Reading: Addison-Wesley. 
Quinlan, J. (1993) C4.5: Programs for Machine Learning. San Mateo: Morgan Kaufmann. 
Quinlan, J. (1996) Bagging, Boosting, and C4.5. Proceedings of the Thirteenth National 
Conference on Artificial Intelligence, pp. 725-730. 
  137   
Sebastiani, F. (2002) Machine Learning in Automated Text Categorization. ACM 
Computing Surveys, 34(1), pp. 1-47. 
Si, L. & J. Callan (2001) A Statistical Model for Scientific Readability. Proceedings of the 
10th International Conference on Information and Knowledge Management (CIKM), pp. 
574-576. 
Sibun, P. & A. Spitz (1994) Language Determination: Natural Language Processing from 
Scanned Document Images. Proceedings of the Fourth Conference on Applied Natural 
Language Processing (ANLP), pp. 15-21. 
Soboroff, I., C. Nicholas, J. Kukla & D. Ebert (1998) Visualizing Document Authorship 
Using N-grams and Latent Semantic Indexing. Proceedings of the Workshop on New 
Paradigms in Information Visualization and Manipulation (NPIVM ’97), pp. 43-48. 
Somers, H. (1998) An Attempt to Use Weighted Cusums to Identify Sublanguages. 
NeMLaP3/CoNLL98: New Methods in Language Processing and Computational Natural 
Language Learning, pp. 131-139. 
Stamatatos, E., N. Fakotakis & G. Kokkinakis (1999) Automatic Authorship Attribution. 
Proceedings of EACL '99. 
Stamatatos, E., N. Fakotakis & G. Kokkinakis (2000) Text Genre Detection Using Common 
Word Frequencies. COLING 2000, 18th International Conference on Computational 
Linguistics, vol. 2, pp. 808-814. 
Stamatatos, E., N. Fakotakis & G. Kokkinakis (2001a). Automatic Text Categorization in 
Terms of Genre and Author. Computational Linguistics, 26(4), pp. 471-495. 
Stamatatos, E., N. Fakotakis & G. Kokkinakis (2001b) Computer-Based Authorship 
Attribution without Lexical Measures. Computers and the Humanities, 35(2), pp. 193-214. 
Teahan, W., & J. Cleary (1997) Applying Compression to Natural Language Processing. 
Proceedings of ANLP ’97. 
Tjong Kim Sang, E. (2002) Memory-Based Shallow Parsing. Journal of Machine Learning 
Research, 2, pp. 559-594. 
Twomey, J. & A. Smith (1998) Bias and variance of validation methods for function 
approximation neural networks under conditions of sparse data. IEEE Transactions on 
Systems, Man, and Cybernetics, Part C, vol. 28, pp. 417-430. 
Witten, I. & E. Frank (1999) Data Mining: Practical Machine Learning Tools with Java 
Implementations. San Fransisco: Morgan Kaufmann. 
Yang, Y. & X. Liu (1999) A Re-Examination of Text Categorization Methods. 22nd Annual 
International ACM SIGIR Conference on Research and Development in Information 
Retrieval (SIGIR'99), pp. 42-49. 
  138   
Yule, G. (1938) On Sentence-Length as a Statistical Characteristic of Style in Prose, with 
Application to Two Cases of Disputed Authorship. Biometrika, 30, pp. 363-390. 
 
  139   
Appendix 1: Penn Treebank of POS Tags1 
 
TAG DESCRIPTION 
CC Coordinating conjunction 
CD Cardinal number 
DT Determiner 
EX Existential there 
FW Foreign word 
IN Preposition or subordinating conjunction 
JJ Adjective 
JJR Adjective, comparative 
JJS Adjective, superlative 
LS List item marker 
MD Modal 
NN Noun, singular or mass 
NNS Noun, plural 
NNP Proper noun, singular 
NNPS Proper noun, plural 
PDT Predeterminer 
POS Possessive ending 
PRP Personal pronoun 
PRP$ Possessive pronoun 
RB Adverb 
RBR Adverb, comparative 
RBS Adverb, superlative 
RP Particle 
SYM Symbol 
TO To 
UH Interjection 
VB Verb, base form 
VBD Verb, past tense 
VBG Verb, gerund or present participle 
                                                 
1 Penn Treebank P.O.S. Tags: 
http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html. 
  140   
VBN Verb, pas participle 
VBP Verb, non-3rd person singular present 
VBZ Verb, 3rd person singular present 
WDT Wh-determiner 
WP Wh-pronoun 
WP$ Possessive wh-pronoun 
WRB Wh-adverb 
 
  141   
Appendix 2: POS tags for the Spoken Dutch Corpus 
(CGN) 
 
Four types of nouns: 
 SYMBOL EXPLANATION 
1. N(soort,ev) common noun, singular 
2. N(soort,mv) common noun, plural 
3. N(eigen,ev) proper noun, singular 
4. N(eigen,mv) proper noun, plural 
 
Twelve types of adjectives: 
 SYMBOL EXPLANATION 
1. ADJ(prenom,basis) prenominal adjective, base form 
2. ADJ(prenom,comp) prenominal adjective, comparative form 
3. ADJ(prenom,sup) prenominal adjective, superlative form 
4. ADJ(nom,basis) nominalized adjective, base form 
5. ADJ(nom,comp) nominalized adjective, comparative 
6. ADJ(nom,sup) nominalized adjective, superlative 
7. ADJ(postnom,basis) postnominal adjective, base form 
8. ADJ(postnom,comp) postnominal adjective, comparative form 
9. ADJ(vrij,basis) adjective used predicatively, base form 
10. ADJ(vrij,comp)  
11. ADJ(vrij,sup)  
12. ADJ(vrij,dim) adjective, diminutive form 
 
Six (morphologically defined) verb forms: 
 SYMBOL EXPLANATION 
1. WW(pv,ev) inflected verb form, singular 
2. WW(pv,mv) inflected verb form, plural 
3. WW(pv,met-t) inflected verb form with –t 
4. WW(inf) infinitive 
5. WW(vd) past participle 
6. WW(od) present participle 
 
  142   
Ordinals and cardinals: 
 SYMBOL EXPLANATION 
1. TW(hoofd) ordinal number 
2. TW(rang) cardinal number 
 
Types of (morphologically defined) pronouns: 
 SYMBOL EXPLANATION 
1. VNW(pers,pron) personal pronoun 
2. VNW(pr,pron)  
3. VNW(refl,pron) reflexive pronoun 
4. VNW(recip,pron) reciprocal pronoun 
5. VNW(bez,det) possessive pronoun 
6. VNW(vrag,pron) question word 
7. VNW(betr,pron) relative pronoun 
8. VNW(vb,pron)  
9. VNW(vb,adv-pron)  
10. VNW(excl,pron) exclamative pronoun 
11. VNW(vb,det)  
12. VNW(excl,det)  
13. VNW(aanw,pron) demonstrative pronoun 
14. VNW(aanw,det)  
15. VNW(aanw,adv-pron)  
16. VNW(aanw,det)  
17. VNW(onbep,pron) indefinite pronoun 
18. VNW(onbep,adv-pron)  
19. VNW(onbep,det)  
20. VNW(onbep,grad)  
 
Other POS labels: 
 SYMBOL EXPLANATION 
1. LID determiner 
2. VZ preposition 
3. VG(neven) coordinating element 
4. VG(onder) subordinating element 
  143   
5. BW adverb 
6. TSW interjection 
7. SPEC rest category 
8. LET interpunction 
 
(Hoekstra et al., 2001: 84-85) 
  144   
Appendix 3: Some remarks about the MBSP output 
 
There are two remarks to be made concerning the output Memory-Based Shallow Parsing 
(MBSP) based on the Spoken Dutch Corpus (CGN) generates. First of all, most proper names 
in the Dutch input files are tagged as SPEC(deeleigen) instead of N(eigen,ev). Therefore, 
there will be much more SPECs in the analyses than there would have been in the other case. 
A second problem is the analysis of a word like SP.A-ministers (ministers of the SP.A party). 
The tokenizer embedded in MBSP tries to guess whether a whitespaces have been deleted 
and analyses SP as the last word of the sentence and A-ministers as the beginning of another. 
Because the tokenizer is one of the modules part of the MBSP, you need access to the source 
codes to correct this mistake. That is why I changed the word SP.A-ministers to SPA-ministers 
in the original texts and did the same with other compounds with SP.A. 
 
  145   
Appendix 4: pos.awk.txt 
 
# pos.awk.txt 
# 1. Document feature vector for the frequency and 
# distribution of POS tags 
 
{ 
   if ($3 ~ /^ADJ/) 
      adjectives++ 
   if ($3 ~ /^BW/) 
      adverbs++ 
   if ($3 ~ /^LET/) 
      punctuation++ 
   if ($3 ~ /^LID/) 
      articles++ 
   if ($3 ~ /^N/) 
      nouns++ 
   if ($3 ~ /^SPEC/) 
      propernouns++ 
   if ($3 ~ /^TSW/) 
      interjections++ 
   if ($3 ~ /^TW/) 
      numerals++ 
   if ($3 ~ /^VG/) 
      conjunctions++ 
   if ($3 ~ /^VNW/) 
      pronouns++ 
   if ($3 ~ /^VZ/) 
      prepositions++ 
   if ($3 ~ /^WW/) 
      verbs++ 
   if ($3 =! /^ /) 
      total++ 
} 
END{ 
   if (adjectives == 0) 
      printf(0",") 
   else printf(adjectives",") 
   if (adverbs == 0) 
      printf(0",") 
   else printf(adverbs",") 
   if (punctuation == 0) 
      printf(0",") 
   else printf(punctuation",") 
   if (articles == 0) 
      printf(0",") 
   else printf(articles",") 
   if (nouns == 0) 
      printf(0",") 
   else printf(nouns",") 
   if (propernouns == 0) 
  146   
      printf(0",") 
   else printf(propernouns",") 
   if (interjections == 0) 
      printf(0",") 
   else printf(interjections",") 
   if (numerals == 0) 
      printf(0",") 
   else printf(numerals",") 
   if (conjunctions == 0) 
      printf(0",") 
   else printf(conjunctions",") 
   if (pronouns == 0) 
      printf(0",") 
   else printf(pronouns",") 
   if (prepositions == 0) 
      printf(0",") 
   else printf(prepositions",") 
   if (verbs == 0) 
      printf(0",") 
   else printf(verbs",") 
   printf(total",") 
   printf(adjectives/total",") 
   printf(adverbs/total",") 
   printf(punctuation/total",") 
   printf(articles/total",") 
   printf(nouns/total",") 
   printf(propernouns/total",") 
   printf(interjections/total",") 
   printf(numerals/total",") 
   printf(conjunctions/total",") 
   printf(pronouns/total",") 
   printf(prepositions/total",") 
   printf(verbs/total",") 
   print "A" 
#   print "B" 
#   print "O" 
} 
  147   
Appendix 5: verbbasic.awk.txt 
 
# verbbasic.awk.txt 
# 2. Document feature vector for the frequency and 
# distribution of basic verb forms 
 
{ 
   if ($3 ~ /pv,tgw,ev/ || $3 ~ /pv,verl,ev/ || $3 ~ 
/pv,conj,ev/) 
      pvev++ 
   if ($3 ~ /pv,tgw,mv/ || $3 ~ /pv,verl,mv/) 
      pvmv++ 
   if ($3 ~ /pv,tgw,met-t/) 
      pvmett++ 
   if ($3 ~ /inf/) 
      inf++ 
   if ($3 ~ /vd/) 
      vd++ 
   if ($3 ~ /od/) 
      od++ 
   if ($3 ~ /WW/) 
      WW++ 
} 
END { 
   if (pvev == 0) 
      printf(0",") 
   else printf(pvev",") 
   if (pvmv == 0) 
      printf(0",") 
   else printf(pvmv",") 
   if (pvmett == 0) 
      printf(0",") 
   else printf(pvmett",") 
   if (inf == 0) 
      printf(0",") 
   else printf(inf",") 
   if (vd == 0) 
      printf(0",") 
   else printf(vd",") 
   if (od == 0) 
      printf(0",") 
   else printf(od",") 
   printf(WW",") 
   printf(pvev/WW",") 
   printf(pvmv/WW",") 
   printf(pvmett/WW",") 
   printf(inf/WW",") 
   printf(vd/WW",") 
   printf(od/WW",") 
   print "A" # print "B" or print "O" for the other classes 
} 
  148   
Appendix 6: verb.awk.txt 
 
# verb.awk.txt 
# 3. Document feature vector for the frequency and 
# distribution of specific verb forms 
 
{ 
   if ($3 ~ /pv,tgw,ev/) 
      pvtgwev++ 
   if ($3 ~ /pv,tgw,met-t/) 
      pvtgwmett++ 
   if ($3 ~ /pv,tgw,mv/) 
      pvtgwmv++ 
   if ($3 ~ /pv,verl,ev/) 
      pvverlev++ 
   if ($3 ~ /pv,verl,mv/) 
      pvverlmv++ 
   if ($3 ~ /pv,conj,ev/) 
      pvconjev++ 
   if ($3 ~ /vd,vrij,zonder/) 
      vdvrijzonder++ 
   if ($3 ~ /vd,prenom,zonder/) 
      vdprenomzonder++ 
   if ($3 ~ /vd,prenom,met-e/) 
      vdprenommete++ 
   if ($3 ~ /vd,nom,met-e,zonder-n/) 
      vdnommetezondern++ 
   if ($3 ~ /vd,nom,met-e,mv-n/) 
      vdnommetemvn++ 
   if ($3 ~ /od,vrij,zonder/) 
      odvrijzonder++ 
   if ($3 ~ /od,prenom,zonder/) 
      odprenomzonder++ 
   if ($3 ~ /od,prenom,met-e/) 
      odprenommete++ 
   if ($3 ~ /inf,vrij,zonder/) 
      infvrijzonder++ 
   if ($3 ~ /inf,prenom,zonder/) 
      infprenomzonder++ 
   if ($3 ~ /inf,nom,zonder,zonder-n/) 
      infnomzonderzondern++ 
   if ($3 ~ /WW/) 
      WW++ 
} 
END{ 
   if (pvtgwev == 0) 
      printf(0",") 
   else printf(pvtgwev",") 
   if (pvtgwmett == 0) 
      printf(0",") 
   else printf(pvtgwmett",") 
  149   
   if (pvtgwmv == 0) 
      printf(0",") 
   else printf(pvtgwmv",") 
   if (pvverlev == 0) 
      printf(0",") 
   else printf(pvverlev",") 
   if (pvverlmv == 0) 
      printf(0",") 
   else printf(pvverlmv",") 
   if (pvconjev == 0) 
      printf(0",") 
   else printf(pvconjev",") 
   if (vdvrijzonder == 0) 
      printf(0",") 
   else printf(vdvrijzonder",") 
   if (vdprenomzonder == 0) 
      printf(0",") 
   else printf(vdprenomzonder",") 
   if (vdprenommete == 0) 
      printf(0",") 
   else printf(vdprenommete",") 
   if (vdnommetezondern == 0) 
      printf(0",") 
   else printf(vdnommetezondern",") 
   if (vdnommetemvn == 0) 
      printf(0",") 
   else printf(vdnommetemvn",") 
   if (odvrijzonder == 0) 
      printf(0",") 
   else printf(odvrijzonder",") 
   if (odprenomzonder == 0) 
      printf(0",") 
   else printf(odprenomzonder",") 
   if (odprenommete == 0) 
      printf(0",") 
   else printf(odprenommete",") 
   if (infvrijzonder == 0) 
      printf(0",") 
   else printf(infvrijzonder",") 
   if (infprenomzonder == 0) 
      printf(0",") 
   else printf(infprenomzonder",") 
   if (infnomzonderzondern == 0) 
      printf(0",") 
   else printf(infnomzonderzondern",") 
   printf(WW",") 
   printf(pvtgwev/WW",") 
   printf(pvtgwmett/WW",") 
   printf(pvtgwmv/WW",") 
   printf(pvverlev/WW",") 
   printf(pvverlmv/WW",") 
   printf(pvconjev/WW",") 
   printf(vdvrijzonder/WW",") 
  150   
   printf(vdprenomzonder/WW",") 
   printf(vdprenommete/WW",") 
   printf(vdnommetezondern/WW",") 
   printf(vdnommetemvn/WW",") 
   printf(odvrijzonder/WW",") 
   printf(odprenomzonder/WW",") 
   printf(odprenommete/WW",") 
   printf(infvrijzonder/WW",") 
   printf(infprenomzonder/WW",") 
   printf(infnomzonderzondern/WW",") 
   print "A" # print "B" or print "O" for the other classes 
} 
  151   
Appendix 7: np.awk.txt 
 
# np.awk.txt 
# Lists the POS tags of each sentence on one line 
 
BEGIN{ 
   ORS = " " 
} 
{ 
   if ($3 ~ /^ADJ/) 
      print "ADJ" 
   if ($3 ~ /^BW/) 
      print "BW" 
   if ($3 ~ /^LET/) 
      print "LET" 
   if ($3 ~ /^LID/) 
      print "LID" 
   if ($3 ~ /^N/) 
      print "N" 
   if ($3 ~ /^SPEC/) 
      print "SPEC" 
   if ($3 ~ /^TSW/) 
      print "TSW" 
   if ($3 ~ /^TW/) 
      print "TW" 
   if ($3 ~ /^VG/) 
      print "VG" 
   if ($3 ~ /^VNW/) 
      print "VNW" 
   if ($3 ~ /^VZ/) 
      print "VZ" 
   if ($3 ~ /^WW/) 
      print "WW" 
   if ($0 ~ /^ /) 
      print "\n" 
} 
  152   
Appendix 8: patternstep[1-4].awk.txt 
 
The AWK scripts described below are invoked on the command line as follows: 
gawk –f patternstep1.awk.txt 1Arij.txt | 
gawk –f patternstep2.awk.txt | 
gawk –f patternstep3.awk.txt >> patternbin.arff 
The AWK scripts described below are invoked on the command line as follows: 
gawk –f patternstep1.awk.txt 1Arij.txt | 
gawk –f patternstep2.awk.txt | 
gawk –f patternstep4.awk.txt >> patternnum.arff 
 
# patternstep1.awk.txt 
# 4A. & 5A. Substitutes the specific pattern by the word 
'pattern' and the number code of the pattern 
 
{ 
   gsub("LID TW ADJ N", "pattern12") 
   gsub("N TW N", "pattern11") 
   gsub("LID TW N", "pattern10") 
   gsub("TW ADJ N", "pattern9") 
   gsub("N ADJ N", "pattern8") 
   gsub("LID ADJ N", "pattern7") 
   gsub("TW N", "pattern6") 
   gsub("VNW N", "pattern5") 
   gsub("N SPEC", "pattern4") 
   gsub("LID N", "pattern3") 
   gsub("ADJ N", "pattern2") 
   gsub("SPEC", "pattern1") 
   gsub("VNW", "pattern1") 
   gsub("N", "pattern1") 
   print $0 
} 
 
# patternstep2.awk.txt 
# 4B. & 5B. Makes one long list of the output 
 
{ 
   gsub(" ", "\n") 
   print $0 
} 
 
# patternstep3.awk.txt 
# 4C. Document Feature Vector for the presence or absence of 
# specific NP-patterns 
 
{ 
   if ($1 ~ /pattern1$/) 
      pattern1++ 
  153   
   if ($1 ~ /pattern2/) 
      pattern2++ 
   if ($1 ~ /pattern3/) 
      pattern3++ 
   if ($1 ~ /pattern4/) 
      pattern4++ 
   if ($1 ~ /pattern5/) 
      pattern5++ 
   if ($1 ~ /pattern6/) 
      pattern6++ 
   if ($1 ~ /pattern7/) 
      pattern7++ 
   if ($1 ~ /pattern8/) 
      pattern8++ 
   if ($1 ~ /pattern9/) 
      pattern9++ 
   if ($1 ~ /pattern10/) 
      pattern10++ 
   if ($1 ~ /pattern11/) 
      pattern11++ 
   if ($1 ~ /pattern12/) 
      pattern12++ 
} 
END { 
   if (pattern1 > 0) 
      printf(1",") 
   else printf(0",") 
   if (pattern2 > 0) 
      printf(1",") 
   else printf(0",") 
   if (pattern3 > 0) 
      printf(1",") 
   else printf(0",") 
   if (pattern4 > 0) 
      printf(1",") 
   else printf(0",") 
   if (pattern5 > 0) 
      printf(1",") 
   else printf(0",") 
   if (pattern6 > 0) 
      printf(1",") 
   else printf(0",") 
   if (pattern7 > 0) 
      printf(1",") 
   else printf(0",") 
   if (pattern8 > 0) 
      printf(1",") 
   else printf(0",") 
   if (pattern9 > 0) 
      printf(1",") 
   else printf(0",") 
   if (pattern10 > 0) 
      printf(1",") 
  154   
   else printf(0",") 
   if (pattern11 > 0) 
      printf(1",") 
   else printf(0",") 
   if (pattern12 > 0) 
      printf(1",") 
   else printf(0",") 
   printf("A\n") 
#   printf("B\n") 
#   printf("O\n") 
} 
 
# patternstep4.awk.txt 
# 5C. Document feature vector for the frequency and 
# distribution of specific NP-patterns 
 
{ 
   if ($1 ~ /pattern1$/) 
      pattern1++ 
   if ($1 ~ /pattern2/) 
      pattern2++ 
   if ($1 ~ /pattern3/) 
      pattern3++ 
   if ($1 ~ /pattern4/) 
      pattern4++ 
   if ($1 ~ /pattern5/) 
      pattern5++ 
   if ($1 ~ /pattern6/) 
      pattern6++ 
   if ($1 ~ /pattern7/) 
      pattern7++ 
   if ($1 ~ /pattern8/) 
      pattern8++ 
   if ($1 ~ /pattern9/) 
      pattern9++ 
   if ($1 ~ /pattern10/) 
      pattern10++ 
   if ($1 ~ /pattern11/) 
      pattern11++ 
   if ($1 ~ /pattern12/) 
      pattern12++ 
   total = pattern1 + pattern2 + pattern3 + pattern4 + 
pattern5 + pattern6 + pattern7 + pattern8 + pattern9 + 
pattern10 + pattern11 + pattern12 
} 
END { 
   if (pattern1 == 0) 
      printf(0",") 
   else printf(pattern1",") 
   if (pattern2 == 0) 
      printf(0",") 
   else printf(pattern2",") 
   if (pattern3 == 0) 
  155   
      printf(0",") 
   else printf(pattern3",") 
   if (pattern4 == 0) 
      printf(0",") 
   else printf(pattern4",") 
   if (pattern5 == 0) 
      printf(0",") 
   else printf(pattern5",") 
   if (pattern6 == 0) 
      printf(0",") 
   else printf(pattern6",") 
   if (pattern7 == 0) 
      printf(0",") 
   else printf(pattern7",") 
   if (pattern8 == 0) 
      printf(0",") 
   else printf(pattern8",") 
   if (pattern9 == 0) 
      printf(0",") 
   else printf(pattern9",") 
   if (pattern10 == 0) 
      printf(0",") 
   else printf(pattern10",") 
   if (pattern11 == 0) 
      printf(0",") 
   else printf(pattern11",") 
   if (pattern12 == 0) 
      printf(0",") 
   else printf(pattern12",") 
   printf(total",") 
   printf(pattern1/total",") 
   printf(pattern2/total",") 
   printf(pattern3/total",") 
   printf(pattern4/total",") 
   printf(pattern5/total",") 
   printf(pattern6/total",") 
   printf(pattern7/total",") 
   printf(pattern8/total",") 
   printf(pattern9/total",") 
   printf(pattern10/total",") 
   printf(pattern11/total",") 
   printf(pattern12/total",") 
   printf("A\n") 
#   printf("B\n") 
#   printf("O\n") 
} 
  156   
Appendix 9: mutualinfotest.awk.txt 
 
# mutualinfotest.awk.txt 
# 6. Document Feature Vector for the presence or absence of 
# specific words based on mutual information 
 
{ 
   if ($0 ~ /partij/) 
      partij++ 
   if ($0 ~ /spa/) 
      spa++ 
   if ($0 ~ /blijkt/) 
      blijkt++ 
   if ($0 ~ /zegt/) 
      zegt++ 
   if ($0 ~ /wie/) 
      wie++ 
   if ($0 ~ /echter/) 
      echter++ 
   if ($0 ~ /altijd/) 
      altijd++ 
   if ($0 ~ /aldus/) 
      aldus++ 
   if ($0 ~ /evenwel/) 
      evenwel++ 
   if ($0 ~ /blok/) 
      blok++ 
   if ($0 ~ /vld/) 
      vld++ 
   if ($0 ~ /beide/) 
      beide++ 
   if ($0 ~ /mr/) 
      mr++ 
   if ($0 ~ /gewest/) 
      gewest++ 
   if ($0 ~ /tegelijk/) 
      tegelijk++ 
   if ($0 ~ /steeds/) 
      steeds++ 
   if ($0 ~ /erg/) 
      erg++ 
   if ($0 ~ /afgelopen/) 
      afgelopen++ 
   if ($0 ~ /momenteel/) 
      momenteel++ 
   if ($0 ~ /wilde/) 
      wilde++ 
} 
END{ 
   if (partij == 0) 
      printf(0",") 
   else printf(1",") 
  157   
   if (spa == 0) 
      printf(0",") 
   else printf(1",") 
   if (blijkt == 0) 
      printf(0",") 
   else printf(1",") 
   if (zegt == 0) 
      printf(0",") 
   else printf(1",") 
   if (wie == 0) 
      printf(0",") 
   else printf(1",") 
   if (echter == 0) 
      printf(0",") 
   else printf(1",") 
   if (altijd == 0) 
      printf(0",") 
   else printf(1",") 
   if (aldus == 0) 
      printf(0",") 
   else printf(1",") 
   if (evenwel == 0) 
      printf(0",") 
   else printf(1",") 
   if (blok == 0) 
      printf(0",") 
   else printf(1",") 
   if (vld == 0) 
      printf(0",") 
   else printf(1",") 
   if (beide == 0) 
      printf(0",") 
   else printf(1",") 
   if (mr == 0) 
      printf(0",") 
   else printf(1",") 
   if (gewest == 0) 
      printf(0",") 
   else printf(1",") 
   if (tegelijk == 0) 
      printf(0",") 
   else printf(1",") 
   if (steeds == 0) 
      printf(0",") 
   else printf(1",") 
   if (erg == 0) 
      printf(0",") 
   else printf(1",") 
   if (afgelopen == 0) 
      printf(0",") 
   else printf(1",") 
   if (momenteel == 0) 
      printf(0",") 
  158   
   else printf(1",") 
   if (wilde == 0) 
      printf(0",") 
   else printf(1",") 
#   print "A" 
#   print "B" 
   print "O" 
} 
  159   
Appendix 10: readability.awk.txt 
 
# readability.awk.txt 
# 7. Document feature vector for the readability score 
 
{ 
   if ($1 == ".") 
      sentences++ 
   if ($1 ~ ".") 
      words++ 
   if ($3 ~ /^LET/) 
      punctuation++ 
   asw = 1.779667 
} 
END{ 
   print 206.835 - (1.015 * asl) - (84.6 * asw) ",A" 
# ",B" or ",O" for the other classes 
} 
  160   
Appendix 11: List of features in VERB.arff 
 
1 pvtgwev  19 pvtgwevtot 
2 pvtgwmett  20 pvtgwmetttot 
3 pvtgwmv  21 pvtgwmvtot 
4 pvverlev  22 pvverlevtot 
5 pvverlmv  23 pvverlmvtot 
6 pvconjev  24 pvconjevtot 
7 vdvrijzonder  25 vdvrijzondertot 
8 vdprenomzonder  26 vdprenomzondertot 
9 vdprenommete  27 vdprenommetetot 
10 vdnommetezondern  28 vdnommetezonderntot 
11 vdnommetemvn  29 vdnommetemvntot 
12 odvrijzonder  30 odvrijzondertot 
13 odprenomzonder  31 odprenomzondertot 
14 odprenommete  32 odprenommetetot 
15 infvrijzonder  33 infvrijzondertot 
16 infprenomzonder  34 infprenomzondertot 
17 infnomzonderzondern  35 infnomzonderzonderntot 
18 ww    
 
  161   
Appendix 12: Graphs after training 
 1. Frequency and distribution of parts-of-speech (POS.arff) 
Graph 2. Error Reduction through Bagging and Boosting 
(POS.arff)
-100,00%
-50,00%
0,00%
50,00%
100,00%
B
ag
gi
ng
(O
ne
R
)
B
oo
st
in
g
(O
ne
R
)
B
ag
gi
ng
(N
ai
ve
 B
)
B
oo
st
in
g
(N
ai
ve
 B
)
B
ag
gi
ng
(j4
8.
J4
8)
B
oo
st
in
g
(j4
8.
J4
8)
B
ag
gi
ng
(k
N
N
)
B
oo
st
in
g
(k
N
N
)
Voting schemes and ML algorithms
E
rr
or
 R
ed
uc
tio
n 
(in
 %
)
Graph 3. Mean F-score (POS.arff)
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
On
eR
Ba
gg
ing
 O
ne
R
Bo
os
tin
g O
ne
R
Na
ive
 B
ay
es
Ba
gg
ing
 N
ai.
.
Bo
os
tin
g N
ai.
.
j48
.J4
8 (
m1
)
Ba
gg
ing
 j4
8.J
48
Bo
os
tin
g j
48
...
kN
N 
(k1
)
Ba
gg
ing
 kN
N
Bo
os
tin
g k
NN
Ne
ura
l N
etw
ork
s
ML algorithms
M
ea
n 
F-
sc
or
e 
(in
 %
)
Graph 5. Error Reduction through Bagging and Boosting 
after feature selection (POS.arff)
-100,00%
-50,00%
0,00%
50,00%
100,00%
B
ag
gi
ng
(O
ne
R
)
B
oo
st
in
g
(O
ne
R
)
B
ag
gi
ng
(N
ai
ve
 B
)
B
oo
st
in
g
(N
ai
ve
 B
)
B
ag
gi
ng
(j4
8.
J4
8)
B
oo
st
in
g
(j4
8.
J4
8)
B
ag
gi
ng
(k
N
N
)
B
oo
st
in
g
(k
N
N
)
Voting schemes and ML algorithms
E
rr
or
 R
ed
uc
tio
n 
(in
 %
)
 
  162   
Graph 6. Mean F-score after feature selection (POS.arff)
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
On
eR
Ba
g O
ne
R
Bo
os
t O
ne
R
Na
ive
 B
ay
es
Ba
g N
aiv
e B
Bo
os
t N
aiv
e B
j48
.J4
8 (
m1
)
Ba
g j
48
.J4
8
Bo
os
t j4
8.J
48
kN
N 
(k1
)
Ba
g k
NN
Bo
os
t k
NN
Ne
ura
l N
etw
ork
s
ML algorithms
M
ea
n 
F-
sc
or
e 
(in
 %
)
Graph 7. Influence of the -m value in Bagging j48.J48 on 
generalisation accuracy after feature selection (POS.arff)
0,00%
10,00%
20,00%
30,00%
40,00%
50,00%
60,00%
70,00%
80,00%
90,00%
100,00%
1 2 4 6 8 16 32 64 128 256
-m value
A
cc
ur
ac
y 
(in
 %
)
Graph 8. Influence of the k value in Boosting kNN on 
generalisation accuracy after feature selection (POS.arff)
0,00%
10,00%
20,00%
30,00%
40,00%
50,00%
60,00%
70,00%
80,00%
90,00%
100,00%
1 2 4 6 8 16 32 64 128 256
-m value
A
cc
ur
ac
y 
(in
 %
)
 
 
 
 
 
 
 
 
 
  163   
 2. Frequency and distribution of basic verb forms (VERBBASIC.arff) 
Graph 10. Error Reduction through Bagging and Boosting 
(VERBBASIC.arff)
-100,00%
-50,00%
0,00%
50,00%
100,00%
B
ag
gi
ng
(O
ne
R
)
B
oo
st
in
g
(O
ne
R
)
B
ag
gi
ng
(N
ai
ve
 B
)
B
oo
st
in
g
(N
ai
ve
 B
)
B
ag
gi
ng
(j4
8.
J4
8)
B
oo
st
in
g
(j4
8.
J4
8)
B
ag
gi
ng
(k
N
N
)
B
oo
st
in
g
(k
N
N
)
Voting schemes and ML algorithms
E
rr
or
 R
ed
uc
tio
n 
(in
 %
)
Graph 11. Mean F-score (VERBBASIC.arff)
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
On
eR
Ba
g O
ne
R
Bo
os
t O
ne
R
Na
ive
 B
ay
es
Ba
g N
aiv
e B
Bo
os
t N
aiv
e B
j48
.J4
8 (
m1
)
Ba
g j
48
.J4
8
Bo
os
t j4
8.J
48
kN
N 
(k1
)
Ba
g k
NN
Bo
os
t k
NN
Ne
ura
l N
etw
ork
s
ML algorithms
M
ea
n 
F-
sc
or
e 
(in
 %
)
Graph 13. Error Reduction through Bagging and Boosting 
after feature selection (VERBBASIC.arff)
-1
-0,5
0
0,5
1
B
ag
gi
ng
(O
ne
R
)
B
oo
st
in
g
(O
ne
R
)
B
ag
gi
ng
(N
ai
ve
 B
)
B
oo
st
in
g
(N
ai
ve
 B
)
B
ag
gi
ng
(j4
8.
J4
8)
B
oo
st
in
g
(j4
8.
J4
8)
B
ag
gi
ng
(k
N
N
)
B
oo
st
in
g
(k
N
N
)
Voting schemes and ML algorithms
E
rr
or
 R
ed
uc
tio
n 
(in
 %
)
  164   
Graph 14. Mean F-score after feature selection (VERBBASIC.arff)
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
On
eR
Ba
g O
ne
R
Bo
os
t O
ne
R
Na
ive
 B
ay
es
Ba
g N
aiv
e B
Bo
os
t N
aiv
e B
j48
.J4
8 (
m1
)
Ba
g j
48
.J4
8
Bo
os
t j4
8.J
48
kN
N(
k7
)
Ba
g k
NN
Bo
os
t k
NN
Ne
ura
l N
etw
...
ML algorithms
M
ea
n 
F-
sc
or
e 
(in
 %
)
Graph 15. Influence of the -m value in Boosting j48.J48 on 
generalisation accuracy after feature selection 
(VERBBASIC.arff)
0,00%
10,00%
20,00%
30,00%
40,00%
50,00%
60,00%
70,00%
80,00%
90,00%
100,00%
1 2 4 6 8 16 32 64 128 256
-m value
A
cc
ur
ac
y 
(in
 %
)
Graph 16. Influence of the k value in Bagging kNN on 
generalisation accuracy after feature selection 
(VERBBASIC.arff)
0,00%
10,00%
20,00%
30,00%
40,00%
50,00%
60,00%
70,00%
80,00%
90,00%
100,00%
1 2 4 6 8 16 32 64 128 256
-m value
A
cc
ur
ac
y 
(in
 %
)
 
 
 
 
 
 
 
 
 
 
  165   
 3. Frequency and distribution of specific verb forms (VERB.arff) 
Graph 18. Error Reduction through Bagging and Boosting 
(VERB.arff)
-100,00%
-50,00%
0,00%
50,00%
100,00%
B
ag
gi
ng
(O
ne
R
)
B
oo
st
in
g
(O
ne
R
)
B
ag
gi
ng
(N
ai
ve
 B
)
B
oo
st
in
g
(N
ai
ve
 B
)
B
ag
gi
ng
(j4
8.
J4
8)
B
oo
st
in
g
(j4
8.
J4
8)
B
ag
gi
ng
(k
N
N
)
B
oo
st
in
g
(k
N
N
)
Voting schemes and ML algorithms
E
rr
or
 R
ed
uc
tio
n 
(in
 %
)
Graph 19. Mean F-score (VERB.arff)
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
On
eR
Ba
g O
ne
R
Bo
os
t O
ne
R
Na
ive
 B
ay
es
Ba
g N
aiv
e B
Bo
os
t N
aiv
e B
j48
.J4
8 (
m1
)
Ba
g j
48
.J4
8
Bo
os
t j4
8.J
48
kN
N 
(k1
)
Ba
g k
NN
Bo
os
t k
NN
Ne
ura
l N
etw
...
ML algorithms
M
ea
n 
F-
sc
or
e 
(in
 %
)
Graph 21. Error Reduction through Bagging and Boosting 
after feature selection (VERB.arff)
-100,00%
-50,00%
0,00%
50,00%
100,00%
B
ag
gi
ng
(O
ne
R
)
B
oo
st
in
g
(O
ne
R
)
B
ag
gi
ng
(N
ai
ve
 B
)
B
oo
st
in
g
(N
ai
ve
 B
)
B
ag
gi
ng
(j4
8.
J4
8)
B
oo
st
in
g
(j4
8.
J4
8)
B
ag
gi
ng
(k
N
N
)
B
oo
st
in
g
(k
N
N
)
 
  166   
Graph 22. Mean F-score after feature selection (VERB.arff)
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
On
eR
Ba
g O
ne
R
Bo
os
t O
ne
R
Na
ive
 B
ay
es
Ba
g N
aiv
e B
Bo
os
t N
aiv
e B
j48
.J4
8 (
m1
)
Ba
g j
48
.J4
8
Bo
os
t j4
8.J
48
kN
N 
(k1
)
Ba
g k
NN
Bo
os
t k
NN
Ne
ura
l N
etw
...
ML algorithms
M
ea
n 
F-
sc
or
e 
(in
 %
)
Graph 23. Influence of the -m value in Bagging j48.J48 on 
generalisation accuracy (VERB.arff)
0,00%
10,00%
20,00%
30,00%
40,00%
50,00%
60,00%
70,00%
80,00%
90,00%
100,00%
1 2 4 6 8 16 32 64 128 256
-m value
A
cc
ur
ac
y 
(in
 %
)
Graph 24. Influence of the k value in kNN on generalisation 
accuracy after feature selection (VERB.arff)
0,00%
10,00%
20,00%
30,00%
40,00%
50,00%
60,00%
70,00%
80,00%
90,00%
100,00%
1 2 4 6 8 16 32 64 128 256
k value
A
cc
ur
ac
y 
(in
 %
)
 
 
 
 
 
 
 
 
 
 
  167   
 4. Presence or absence of specific NP patterns (PATTERNBIN.arff) 
Graph 26. Error Reduction through Bagging and Boosting 
(PATTERNBIN.arff)
-100,00%
-50,00%
0,00%
50,00%
100,00%
B
ag
gi
ng
(O
ne
R
)
B
oo
st
in
g
(O
ne
R
)
B
ag
gi
ng
(N
ai
ve
 B
)
B
oo
st
in
g
(N
ai
ve
 B
)
B
ag
gi
ng
(j4
8.
J4
8)
B
oo
st
in
g
(j4
8.
J4
8)
B
ag
gi
ng
(k
N
N
)
B
oo
st
in
g
(k
N
N
)
B
ag
gi
ng
(id
3)
B
oo
st
in
g
(id
3)
Voting schemes and ML algorithms
E
rr
or
 R
ed
uc
tio
n 
(in
 %
)
Graph 27. Mean F-score (PATTERNBIN.arff)
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
On
eR
Ba
g O
ne
R
Bo
os
t O
ne
R
Na
ive
 B
ay
es
Ba
g N
aiv
e B
Bo
os
t N
aiv
e B
j48
.J4
8 (
m1
)
Ba
g j
48
.J4
8
Bo
os
t j4
8.J
48
kN
N 
(k1
)
Ba
g k
NN
Bo
os
t k
NN id3
Ba
g i
d3
Bo
os
t id
3
Ne
ura
l N
etw
ork
s
ML algorithms
M
ea
n 
F-
sc
or
e 
(in
 %
)
Graph 29. Error Reduction through Bagging and Boosting 
after feature selection (PATTERNBIN.arff)
-100,00%
-50,00%
0,00%
50,00%
100,00%
B
ag
gi
ng
(O
ne
R
)
B
oo
st
in
g
(O
ne
R
)
B
ag
gi
ng
(N
ai
ve
 B
)
B
oo
st
in
g
(N
ai
ve
 B
)
B
ag
gi
ng
(j4
8.
J4
8)
B
oo
st
in
g
(j4
8.
J4
8)
B
ag
gi
ng
(k
N
N
)
B
oo
st
in
g
(k
N
N
)
B
ag
gi
ng
(id
3)
B
oo
st
in
g
(id
3)
 
  168   
Graph 30. Mean F-score after feature selection 
(PATTERNBIN.arff)
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
On
eR
Ba
g O
ne
R
Bo
os
t O
ne
R
Na
ive
 B
ay
es
Ba
g N
aiv
e B
Bo
os
t N
aiv
e B
j48
.J4
8 (
m1
)
Ba
g j
48
.J4
8
Bo
os
t j4
8.J
48
kN
N 
(k1
)
Ba
g k
NN
Bo
os
t k
NN id3
Ba
g i
d3
Bo
os
t id
3
Ne
ura
l N
etw
...
ML algorithms
M
ea
n 
F-
sc
or
e 
(in
 %
)
Graph 31. Influence of the -m value in Bagging j48.J48 on 
generalisation accuracy after feature selection 
(PATTERNBIN.arff)
0,00%
10,00%
20,00%
30,00%
40,00%
50,00%
60,00%
70,00%
80,00%
90,00%
100,00%
1 2 4 6 8 16 32 64 128 256
-m value
A
cc
ur
ac
y 
(in
 %
)
Graph 32. Influence of the k value in Bagging kNN on 
generalisation accuracy after feature selection 
(PATTERNBIN.arff)
0,00%
10,00%
20,00%
30,00%
40,00%
50,00%
60,00%
70,00%
80,00%
90,00%
100,00%
1 2 4 6 8 16 32 64 128 256
k value
A
cc
ur
ac
y 
(in
 %
)
 
 
 
 
 
 
 
 
 
  169   
 5. Frequency and distribution of specific NP patterns (PATTERNNUM.arff) 
Graph 34. Error Reduction through Bagging and Boosting 
(PATTERNNUM.arff)
-100,00%
-50,00%
0,00%
50,00%
100,00%
B
ag
gi
ng
(O
ne
R
)
B
oo
st
in
g
(O
ne
R
)
B
ag
gi
ng
(N
ai
ve
 B
)
B
oo
st
in
g
(N
ai
ve
 B
)
B
ag
gi
ng
(j4
8.
J4
8)
B
oo
st
in
g
(j4
8.
J4
8)
B
ag
gi
ng
(k
N
N
)
B
oo
st
in
g
(k
N
N
)
Voting schemes and ML algorithms
E
rr
or
 R
ed
uc
tio
n 
(in
 %
)
Graph 35. Mean F-score (PATTERNNUM.arff)
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
On
eR
Ba
g O
ne
R
Bo
os
t O
ne
R
Na
ive
 B
ay
es
Ba
g N
aiv
e B
Bo
os
t N
aiv
e B
j48
.J4
8 (
m1
)
Ba
g j
48
.J4
8
Bo
os
t j4
8.J
48
kN
N 
(k1
)
Ba
g k
NN
Bo
os
t k
NN
Ne
ura
l N
etw
...
ML algorithms
M
ea
n 
F-
sc
or
e 
(in
 %
)
Graph 37. Error Reduction through Bagging and Boosting 
after feature selection (PATTERNNUM.arff)
-100,00%
-50,00%
0,00%
50,00%
100,00%
B
ag
gi
ng
(O
ne
R
)
B
oo
st
in
g
(O
ne
R
)
B
ag
gi
ng
(N
ai
ve
 B
)
B
oo
st
in
g
(N
ai
ve
 B
)
B
ag
gi
ng
(j4
8.
J4
8)
B
oo
st
in
g
(j4
8.
J4
8)
B
ag
gi
ng
(k
N
N
)
B
oo
st
in
g
(k
N
N
)
 
  170   
Graph 38. Mean F-score after feature selection 
(PATTERNNUM.arff)
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
On
eR
Ba
g O
ne
R
Bo
os
t O
ne
R
Na
ive
 B
ay
es
Ba
g N
aiv
e B
Bo
os
t N
aiv
e B
j48
.J4
8 (
m1
)
Ba
g j
48
.J4
8
Bo
os
t j4
8.J
48
kN
N 
(k1
)
Ba
g k
NN
Bo
os
t k
NN
Ne
ura
l N
etw
...
ML algorithms
M
ea
n 
F-
sc
or
e 
(in
 %
)
Graph 39. Influence of the -m value in Bagging j48.J48 on 
generalisation accuracy after feature selection 
(PATTERNNUM.arff)
0,00%
10,00%
20,00%
30,00%
40,00%
50,00%
60,00%
70,00%
80,00%
90,00%
100,00%
1 2 4 6 8 16 32 64 128 256
-m value
A
cc
ur
ac
y 
(in
 %
)
Graph 40. Influence of the k value in kNN on generalisation 
accuracy (PATTERNNUM.arff)
0,00%
10,00%
20,00%
30,00%
40,00%
50,00%
60,00%
70,00%
80,00%
90,00%
100,00%
1 2 4 6 8 16 32 64 128 256
k value
A
cc
ur
ac
y 
(in
 %
)
 
 
 
 
 
 
 
 
 
  171   
 6. Presence or absence of specific words based on mutual information 
(MUTUALINFO.arff) 
Graph 42. Error Reduction through Bagging and Boosting 
(MUTUALINFO.arff)
-100,00%
-50,00%
0,00%
50,00%
100,00%
B
ag
gi
ng
(O
ne
R
)
B
oo
st
in
g
(O
ne
R
)
B
ag
gi
ng
(N
ai
ve
 B
)
B
oo
st
in
g
(N
ai
ve
 B
)
B
ag
gi
ng
(j4
8.
J4
8)
B
oo
st
in
g
(j4
8.
J4
8)
B
ag
gi
ng
(k
N
N
)
B
oo
st
in
g
(k
N
N
)
B
ag
gi
ng
(id
3)
B
oo
st
in
g
(id
3)
Voting schemes and ML algorithms
E
rr
or
 R
ed
uc
tio
n 
(in
 %
)
Graph 43. Mean F-score (MUTUALINFO.arff)
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
On
eR
Ba
g O
ne
R
Bo
os
t O
ne
R
Na
ive
 B
ay
es
Ba
g N
aiv
e B
Bo
os
t N
aiv
e B
j48
.J4
8 (
m1
)
Ba
g j
48
.J4
8
Bo
os
t j4
8.J
48
kN
N 
(k1
)
Ba
g k
NN
Bo
os
t k
NN id
3
Ba
g i
d3
Bo
os
t id
3
Ne
ura
l N
etw
...
ML algorithms
M
ea
n 
F-
sc
or
e 
(in
 %
)
Graph 45. Error Reduction through Bagging and Boosting 
after feature selection (MUTUALINFO.arff)
-100,00%
-50,00%
0,00%
50,00%
100,00%
B
ag
gi
ng
(O
ne
R
)
B
oo
st
in
g
(O
ne
R
)
B
ag
gi
ng
(N
ai
ve
 B
)
B
oo
st
in
g
(N
ai
ve
 B
)
B
ag
gi
ng
(j4
8.
J4
8)
B
oo
st
in
g
(j4
8.
J4
8)
B
ag
gi
ng
(k
N
N
)
B
oo
st
in
g
(k
N
N
)
B
ag
gi
ng
(id
3)
B
oo
st
in
g
(id
3)
 
  172   
Graph 46. Mean F-score after feature selection 
(MUTUALINFO.arff)
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
On
eR
Ba
g O
ne
R
Bo
os
t O
ne
R
Na
ive
 B
ay
es
Ba
g N
aiv
e B
Bo
os
t N
aiv
e B
j48
.J4
8 (
m1
)
Ba
g j
48
.J4
8
Bo
os
t j4
8.J
48
kN
N 
(k1
)
Ba
g k
NN
Bo
os
t k
NN id3
Ba
g i
d3
Bo
os
t id
3
Ne
ura
l N
etw
...
ML algorithms
M
ea
n 
F-
sc
or
e 
(in
 %
)
Graph 47. Influence of the -m value in Bagging j48.J48 on 
generalisation accuracy (MUTUALINFO.arff)
0,00%
10,00%
20,00%
30,00%
40,00%
50,00%
60,00%
70,00%
80,00%
90,00%
100,00%
1 2 4 6 8 16 32 64 128 256
-m value
A
cc
ur
ac
y 
(in
 %
)
Graph 48. Influence of the k value in kNN on generalisation 
accuracy after feature selection (MUTUALINFO.arff)
0,00%
10,00%
20,00%
30,00%
40,00%
50,00%
60,00%
70,00%
80,00%
90,00%
100,00%
1 2 4 6 8 16 32 64 128 256
k value
A
cc
ur
ac
y 
(in
 %
)
 
 
 
 
 
 
 
 
 
  173   
 7. Readability score (READABILITY.arff) 
Graph 50. Error Reduction through Bagging and Boosting 
(READABILITY.arff)
-100,00%
-50,00%
0,00%
50,00%
100,00%
B
ag
gi
ng
(O
ne
R
)
B
oo
st
in
g
(O
ne
R
)
B
ag
gi
ng
(N
ai
ve
 B
)
B
oo
st
in
g
(N
ai
ve
 B
)
B
ag
gi
ng
(j4
8.
J4
8)
B
oo
st
in
g
(j4
8.
J4
8)
B
ag
gi
ng
(k
N
N
)
B
oo
st
in
g
(k
N
N
)
Graph 51. Mean F-score (READABILITY.arff)
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
On
eR
Ba
g O
ne
R
Bo
os
t O
ne
R
Na
ive
 B
ay
es
Ba
g N
aiv
e B
Bo
os
t N
aiv
e B
j48
.J4
8 (
m1
)
Ba
g j
48
.J4
8
Bo
os
t j4
8.J
48
kN
N 
(k1
)
Ba
g k
NN
Bo
os
t k
NN
Ne
ura
l N
etw
...
ML algorithms
M
ea
n 
F-
sc
or
e 
(in
 %
)
Graph 52. Influence of the -m value in Bagging j48.J48 on 
generalisation accuracy (READABILITY.arff)
0,00%
10,00%
20,00%
30,00%
40,00%
50,00%
60,00%
70,00%
80,00%
90,00%
100,00%
1 2 4 6 8 16 32 64 128 256
-m value
A
cc
ur
ac
y 
(in
 %
)
 
  174   
Graph 53. Influence of the k value in kNN on generalisation 
accuracy after feature selection (READABILITY.arff)
0,00%
10,00%
20,00%
30,00%
40,00%
50,00%
60,00%
70,00%
80,00%
90,00%
100,00%
1 2 4 6 8 16 32 64 128 256
k value
A
cc
ur
ac
y 
(in
 %
)
 
 8. Combination (TOTAL.arff) 
Graph 56. Mean F-score (TOTAL.arff)
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
On
eR
Ba
g O
ne
R
Bo
os
t O
ne
R
Na
ive
 B
ay
es
Ba
g N
aiv
e B
Bo
os
t N
aiv
e B
j48
.J4
8 (
m1
)
Ba
g j
48
.J4
8
Bo
os
t j4
8.J
48
kN
N 
(k1
)
Ba
g k
NN
Bo
os
t k
NN
Ne
ura
l N
etw
ork
s
ML algorithms
M
ea
n 
F-
sc
or
e 
(in
 %
)
Graph 59. Mean F-score after feature selection (TOTAL.arff)
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
On
eR
Ba
g O
ne
R
Bo
os
t O
ne
R
Na
ive
 B
ay
es
Ba
g N
aiv
e B
Bo
os
t N
aiv
e B
j48
.J4
8 (
m1
)
Ba
g j
48
.J4
8
Bo
os
t j4
8.J
48
kN
N 
(k1
)
Ba
g k
NN
Bo
os
t k
NN
Ne
ura
l N
etw
ork
s
ML algorithms
M
ea
n 
F-
sc
or
e 
(in
 %
)
 
  175   
Graph 64. Mean F-score of syntax-based features (TOTAL.arff)
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
On
eR
Ba
g O
ne
R
Bo
os
t O
ne
R
Na
ive
 B
ay
es
Ba
g N
aiv
e B
Bo
os
t N
aiv
e B
j48
.J4
8 (
m1
)
Ba
g j
48
.J4
8
Bo
os
t j4
8.J
48
kN
N 
(k1
)
Ba
g k
NN
Bo
os
t k
NN
Ne
ura
l N
etw
ork
s
ML algorithms
M
ea
n 
F-
sc
or
e 
(in
 %
)
 
Graph 67. Mean F-score after feature selection on syntax-
based features (TOTAL.arff)
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
On
eR
Ba
g O
ne
R
Bo
os
t O
ne
R
Na
ive
 B
ay
es
Ba
g N
aiv
e B
Bo
os
t N
aiv
e B
j48
.J4
8 (
m1
)
Ba
g j
48
.J4
8
Bo
os
t j4
8.J
48
kN
N 
(k1
)
Ba
g k
NN
Bo
os
t k
NN
Ne
ura
l N
etw
...
ML algorithms
M
ea
n 
F-
sc
or
e 
(in
 %
)
 
  176   
Appendix 13: Graphs after testing 
 
 1. Frequency and distribution of parts-of-speech (POSTEST.arff) 
 
Graph 71. Mean F-score of the best performing algorithms 
(POSTEST.arff)
43,77% 49,30% 46,50% 45,87% 49,90%
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
Bo
os
t O
ne
R
Bo
os
t N
aiv
e B
Ba
g J
48
 (-
m 
32
)
Bo
os
t k
NN
 (k
...
Ne
ura
l N
etw
ork
s
ML algorithms
A
cc
ur
ac
y 
(in
 %
)
 
 
 2. Frequency and distribution of basic verb forms (VERBBASICTEST.arff) 
 
Graph 73. Mean F-score of the best performing algorithms 
(VERBBASICTEST.arff)
41,20% 52,07% 42,30%
54,83%
42,30%
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
Ba
g O
ne
R
Bo
os
t N
aiv
e B
Ba
g J
48
 (-
m 
1)
Ba
g k
NN
 (k
 64
)
Ne
ura
l N
etw
...
ML algorithms
A
cc
ur
ac
y 
(in
 %
)
feature selection
no feature selection
 
 
 
 
 
 
 
 
 
 
 
 
 
  177   
 3. Frequency and distribution of specific verb forms (VERBTEST.arff) 
 
Graph 75. Mean F-score of the best performing algorithms 
(VERBTEST.arff)
43,47% 54,67% 54,10% 51,87%51,37%
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
Ba
g O
ne
R
Bo
os
t N
aiv
e B
Ba
g J
48
 (-
m 
2)
kN
N 
(k 
16
)
Ne
ura
l N
etw
...
ML algorithms
A
cc
ur
ac
y 
(in
 %
)
feature selection
no feature selection
 
 
 4. Presence or absence of specific NP patterns (PATTERNBINTEST.arff) 
 
Graph 77. Mean F-score of the best performing algorithms 
(PATTERNBINTEST.arff)
41,03% 34,17% 35,97% 36,07% 35,67% 31,73%
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
Ba
g O
ne
R
Ba
g N
aiv
e B
Ba
g J
48
 (-
m 
32
)
Ba
g k
NN
 (k
 ...
Ba
g I
D3
Ne
ura
l N
etw
...
ML algorithms
A
cc
ur
ac
y 
(in
 %
)
feature selection
no feature selection
 
 
 
 
 
 
 
 
 
 
 
  178   
 5. Frequency and distribution of specific NP patterns 
(PATTERNNUMTEST.arff) 
 
Graph 79. Mean F-score of the best performing algorithms 
(PATTERNNUMTEST.arff)
39,13% 44,67% 42,47% 47,73%45,17%
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
Ba
g O
ne
R
Bo
os
t N
aiv
e B
Ba
g J
48
 (-
m 
64
)
Ba
g k
NN
 (k
 32
)
Ne
ura
l N
etw
...
ML algorithms
A
cc
ur
ac
y 
(in
 %
)
feature selection
no feature selection
 
 6. Presence or absence of specific words based on mutual information 
(MUTUALINFOTEST.arff) 
 
Graph 81. Mean F-score of the best performing algorithms 
(MUTUALINFOTEST.arff)
40,50%
55,77% 53,20% 43,60%
63,63%58,77%
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
Ba
g O
ne
R
Bo
os
t N
aiv
e B
Ba
g J
48
 (-
m 
8)
kN
N 
(k 
16
)
Bo
os
t ID
3
Ne
ura
l N
etw
...
ML algorithms
A
cc
ur
ac
y 
(in
 %
)
feature selection
no feature selection
 
 
 
 
 
 
 
 
 
 
 
  179   
 7. Readability score (READABILITYTEST.arff) 
 
Graph 83. Mean F-score of the best performing algorithms 
(READABILITYTEST.arff)
41,70% 41,33% 44,93% 44,00% 48,53%
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
Ba
g O
ne
R
Na
ive
 B
ay
es
Ba
g J
48
 (-
m 
6)
Ba
g k
NN
 (k
 16
)
Ne
ura
l N
etw
...
ML algorithms
A
cc
ur
ac
y 
(in
 %
)
 
 
 8. Combination (TOTALTEST.arff) 
 
Graph 85. Mean F-score of the best performing algorithms 
on all features (TOTALTEST.arff)
39,83%
61,60% 56,60%55,10% 57,40%
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
Bo
os
t O
ne
R
Na
ive
 B
ay
es
Ba
g J
48
 (-
m 
2)
Ba
g k
NN
 (k
 4)
Ne
ura
l N
etw
...
ML algorithms
A
cc
ur
ac
y 
(in
 %
)
feature selection
no feature selection
Graph 87. Mean F-score of the best performing algorithms 
on syntax-based features (TOTALTEST.arff)
32,07%
51,47% 51,37% 49,33% 54,87%
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
Ba
g O
ne
R
Ba
g N
aiv
e B
Ba
g J
48
 (-
m 
32
)
Ba
g k
NN
 (k
 4)
Ne
ura
l N
etw
...
ML algorithms
A
cc
ur
ac
y 
(in
 %
)
feature selection
no feature selection
 
  180   
Appendix 14: A two-class Authorship Attribution 
task 
 
In this appendix, we investigate the efficiency of the combination feature set on a two-class 
Authorship Attribution problem. First, the ML algorithms that performed best during training 
are tested on all features, later they are tested on the syntax-based features part of 
TOTALTEST.arff. 
 
o Feature selection on all features 
 
As Graphs 88 and 89 show, Bagging j48.J48 achieves an accuracy of 77.94% and an F-
score of 77.90% on a selected set of syntax-based, lexical and token-level features. Naive 
Bayes is the second-best classifier, while Neural Networks perform worst. Different from 
tests with three-class problems, OneR is able to classify 75% of the test instances, while 
Neural Networks only achieve 66.18% accuracy. 
Graph 88. Generalisation accuracy of the best performing 
algorithms on all features (TOTALTEST.arff)
75,00% 76,47% 77,94% 75,00% 66,18%
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
On
eR
Na
ive
 B
Ba
g J
48
 (-
m 
2)
Ba
g k
NN
 (k
 12
8)
Ne
ura
l N
etw
ork
s
ML algorithms
A
cc
ur
ac
y 
(in
 %
)
Graph 89. Mean F-score of the best performing algorithms 
on all features (TOTALTEST.arff)
75,00% 76,30% 77,90% 74,70% 65,25%
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
On
eR
Na
ive
 B
Ba
g J
48
 (-
m 
2)
Ba
g k
NN
 (k
 ...
Ne
ura
l N
etw
...
ML algorithms
A
cc
ur
ac
y 
(in
 %
)
 
  181   
Graph 89bis represents the F-score per class. The B class is easiest to discern for all 
Machine Learning algorithms. The classifiers are able to identify the author of 71.48% of 
the test instances belonging to the A class and of 76.18% of the ones belonging to the B 
class. 
Graph 89bis. F-score per class of the best
performing algorithms on all features
(TOTALTEST.arff)
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
On
eR
Na
ive
 B
Ba
g J
48
 (..
Ba
g k
NN
 (..
.
Ne
ura
l N
e..
.
class A
class B
 
o Feature selection on syntax-based features 
 
If we select the most informative syntax-based features part of TOTALTEST.arff, we find that 
these allow us to classify 69.12% of the test instances. This is almost nine percent less than 
the most informative syntax-based, lexical and token-level features. Still, we can say that 
syntax-based features perform well on the two-class Authorship Attribution task. Graphs 90 
and 91 tell us that Bagging j48.J48 achieves the highest accuracy and F-score of 77.90% on 
a selected set of syntax-based, lexical and token-level features. Again, Naive Bayes is the 
second-best classifier, while OneR – the simplest classifier – performs worst. 
Graph 90. Generalisation accuracy of the best performing 
algorithms on syntax-based features on a two-class problem 
(TOTALTEST.arff)
61,77% 67,65% 69,12% 67,65% 64,71%
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
On
eR
Na
ive
 B
Ba
g J
48
 (..
Ba
g k
NN
 (..
.
Ne
ura
l N
e..
.
  
  182   
Graph 91. Mean F-score of the best performing algorithms 
on syntax-based features on a two-class problem 
(TOTALTEST.arff)
61,75% 67,40% 68,30% 66,25% 64,45%
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
On
eR
Na
ive
 B
Ba
g J
48
 (..
Ba
g k
NN
 (..
.
Ne
ura
l N
e..
.
ML algorithms
A
cc
ur
ac
y 
(in
 %
)
 
 
The F-score per class is presented in Graph 91bis. The B class is for all selected 
classifiers the easiest to discern, with 69.48% of the test instances. The machine learners are 
able to identify the author of 61.78% of the test instances belonging to the A class. This 
means that the B class is more stable as far as syntax-based characteristics are concerned. 
Graph 91bis. F-score per class of the best
performing algorithms on syntax-based
features on a two-class problem (TOTALTEST.arff)
0,00%
20,00%
40,00%
60,00%
80,00%
100,00%
On
eR
Na
ive
 B
Ba
g J
48
 (..
Ba
g k
NN
 (..
.
Ne
ura
l N
e..
.
ML algorithms
A
cc
ur
ac
y 
(in
 %
)
class A
class B
 
