Authorship Attribution of E-Mail:
Comparing Classifiers Over a New Corpus for Evaluation
Ben Allison & Louise Guthrie
Department of Computer Science, University of Sheffield
Sheffield, UK, S1 4DP,
b.allison@dcs.shef.ac.uk, l.guthrie@dcs.shef.ac.uk
Abstract
Until very recently, the email collections that have been available for research have been rather artificially created and consist of emails
that contributors have chosen to make available. These collections serve very well for certain applications, but are certainly not repre-
sentative of a person’s email habits; thus, they have not been realistic resources for testing automatic techniques for detecting the author
of an email. In contrast, the majority of collections available for detection of author make extensive use of out–of–copyright literary
texts freely available on the Internet, and conclusions drawn from experimentation on these corpora are not necessarily applicable to
email. The release of the Enron corpus provided a unique resource because it is largely unfiltered, and therefore presents a relatively
complete collection of emails for a reasonably large number of correspondents. This paper describes a newly created sub-corpus of the
Enron emails, which we suggest can be used to test techniques for authorship attribution, and further shows the application of three
different classification methods to this task to present baseline results. Two of the classifiers used are are standard, and have been shown
to perform well in the literature, and one of the classifiers is novel and based on concurrent work that proposes a Bayesian hierarchical
distribution for word counts in documents. For each of the classifiers, we present results using six text representations, including use of
linguistic structures derived from a parser as well as lexical information. The corpus is available at www.dcs.shef.ac.uk/˜ben.
1. Introduction
Authorship attribution is an area of study which has at-
tracted great interest, and for many different reasons: in
some cases, scholars have sought identification of historic
texts such as Shakespeare’s plays (Merriam, 1998), the
works of John Milton (Tweedie et al., 1998), and the Fed-
eralist papers (Mosteller and Wallace, 1964). In other
cases, efforts have concentrated on identifying the authors
of modern media, such as forum postings (Abbasi and
Chen, 2005), newspaper articles (Diederich et al., 2000)
and newsgroup postings.
Because of email’s ubiquity as a communication tool, attri-
bution of the authorship of email messages is also a critical
problem. However, substantial collections of email have
been extremely difficult to come by, as people are extremely
protective of their personal communications; the release of
the Enron collection of emails after the company’s inves-
tigation by the FERC thus provided an invaluable tool for
research.
The purpose of this paper is two–fold: firstly, we present a
corpus of emails derived from this collection, split by au-
thor, for the purposes of evaluation of methods of author-
ship attribution. Secondly, we provide baseline results for
various representations of the text using several different
classifiers. Two of these classifiers are relatively standard:
an SVM, which has been shown to be highly effective as
a tool for authorship attribution and document classifica-
tion more generally (Dumais et al., 1998; Abbasi and Chen,
2005), and a multinomial probabilistic classifier similar to
those used in (McCallum and Nigam, 1998; Guthrie et al.,
1994). The third classifier is based upon ongoing work to
refine the multinomial classifier by hypothesising an alter-
nate (and more expressive) distribution of word counts in
documents (Allison, 2008), in a similar vein to (Madsen et
al., 2005).
The rest of this paper is organised as follows: Section 2.
describes the original corpus and the manner in which we
create our corpus for email attribution; Section 3. presents
the representations of the text with which we experiment;
Section 4. describes the classifiers in some detail; Section 5.
presents results of the experiments, and finally Section 6.
ends with some brief concluding remarks.
2. The Corpus
The corpus we create for the purposes of this work is de-
rived from the Enron corpus. The Enron email collec-
tion was seized during the FERC investigation into En-
ron’s business practices, and contains email from approx-
imately 160 employees. The data were originally released
by the FERC, and this was followed by substantial work
correcting integrity issues in the data, described in detail
in (Klimt and Yang, 2004). The full dataset is available at
http://www.cs.cmu.edu/˜enron/.
From this raw dataset, we create a corpus using nine em-
ployees’ mail, and select those nine on the basis of the
largest outgoing mailboxes (that is, the “Sent Items” fold-
ers, since there is considerable duplication between several
folders with “Sent” in the title, some of which are machine-
generated). Box 1 gives an example of the format of mes-
sages in the corpus before preprocessing.
The resulting corpus is a nine–way authorship attribution
problem, with 9688 messages in total. We remove message
headers and signatures, and also discount all emails with
fewer than twenty words (the result is that only the body
text in the example would be kept). Preprocessing consists
of selecting contiguous alpha-numeric strings as “words”,
normalising case, but no stemming or stoplisting (except
in the experiments which explicitly use stems). The re-
sulting corpus has 4071 emails, with between 174 and 706
emails per author. The corpus is available for download
Email Example 1 An Example of Email Before Preprocessing
Message-ID: <33403699.1075852498628.JavaMail.evans@thyme>
Date: Wed, 29 Aug 2001 15:00:11 -0700 (PDT)
From: d..steffes@enron.com
To: angela.schwarz@enron.com
Subject: RE: Direct Access
Mime-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-From: Steffes, James D. </O=ENRON/OU=NA/CN=RECIPIENTS/CN=JSTEFFE>
X-To: Schwarz, Angela
X-cc:
X-bcc:
X-Folder: \JSTEFFE (Non-Privileged)\Steffes, James D.\Sent Items
X-Origin: Steffes-J
X-FileName: JSTEFFE (Non-Privileged).pst
Thanks for the info. While there may be some differences in presentation,
there don’t appear to be any basic differences in the analysis. All of
the discussions of the legislation need to be aware that the passage is
highly uncertain.
I agree with the analysis that if you have never been DA and sign up today
there is a very large possibility that you will pay a surcharge
(assuming the legislation passes).
Let me know if there is anything else.
Jim
from www.dcs.shef.ac.uk/˜ben.
After preprocessing, the average length of messages in the
corpus is approximately 75 words, although we note that
the distribution of message length is far from normal – the
majority of messages are short, with the median message
length being 44 words, and the mode length 22. The total
size of the corpus is approximately 305,000 words.
3. The Text Representations
Many authors have noted that the best representation of an
author’s style is not the full bag–of–words that is so often
used for other text classification problems. Generally, it
is considered that an author’s use of his full vocabulary is
highly topic–dependent, and the stylistic signature derived
from a full word set is not appropriate for identifying the
author across topics.
To explore this issue with the corpus described above, we
use several representations of text: the first is the bag–of–
words representation most commonly used. Because we
believe that longer–length n-grams have the potential to
capture more style than single words, we also consider a
bag–of–bigrams and a bag–of–trigrams. We also consider
a bag of stemmed words, where the stemming is done using
an implementation of the popular Porter stemmer (Porter,
1980). From a more stylistically enlightened perspective,
and following on from the work initial work of (Burrows,
1987) and many subsequent studies, we also consider using
purely closed–class words to represent a text, since the use
of these words is believed to be far more invariant across
topic.
Finally, we consider an ambitious stylistic representation
derived from the output of RASP (Briscoe and Watson,
2006). One possible output of the parser, given a docu-
ment, is the list of grammar rules from which the text was
derived. For example, the parse of the sentence “John hit
the ball” is shown in Figure 1, which corresponds to the
following rules:
S → NP V P
NP → N
NP → DetN
V P → V NP
Rules of the form above clearly indicate sentence structure,
and can be viewed in the same way as words (and thus as
“features” for a classifier) by representing a text with counts
of the number of times each rule is used in the generation
of the text. Such a scheme is proposed in (Baayen et al.,
1996), but in conjunction with a corpus of hand-parsed text;
in contrast, in this work all parsing is performed automati-
cally, using the latest release of the RASP parser.
4. Classifiers
This work examines the effectiveness of three different
classifiers on this resource, each of which uses all of the
Figure 1: A parse tree for the sentence “John hit the ball”
six representations discussed in the previous section. Two
of the three classifiers are probabilistic, that is they derive
an explicit estimate for the probability that a new document
belongs to each of the possible classes, and the third is a
linear SVM.
The problem is presented as a mutually exclusive multi-
class problem; that is, the task is to determine which of a
possible nine authors wrote a new email. For the probabilis-
tic classifiers, the extension to cases where there are more
than two possible classes is trivial: for the SVM, the exten-
sion is a little more involved, but we use the most standard
method for this purpose.
4.1. Multinomial Probabilistic Classifier
The multinomial probabilistic classifier has been widely
used, and in many cases has been shown to perform ro-
bustly (see (Lewis, 1998) for an overview of early proba-
bilistic classifiers, and (Guthrie et al., 1994; McCallum and
Nigam, 1998; McCallum et al., 1998) for examples of the
multinomial classifier applied to real problems).
In terms of notation, we use c̃ to represent a random vari-
able and c to represent an outcome. We use roman letters
for observed or observable quantities and Greek letters for
unobservables (i.e. parameters). We write c̃ ∼ ϕ(c) to
mean that c̃ has probability density (discrete or continuous)
ϕ(c), and write p(c) as shorthand for p(c̃ = c). Finally, we
make no explicit distinction in notation between univariate
and multivariate quantities; however, we use θj to refer to
the j-th component of the vector θ.
We consider documents to be represented as vectors of
count–valued random variables such that d = {d1...dv}.
As with most other work, we further assume that words in
a document are exchangeable and thus a document can be
represented simply by the number of times each word oc-
curs.
In classification, interest centres on the conditional distri-
bution of the class variable, given a document. Where doc-
uments are to be assigned to one class only (as in the case
of this paper), this class is judged to be the most proba-
ble class. Classifiers such as the probabilistic classifiers
considered here model the posterior distribution of interest
from the joint distribution of class and document; thus if
c̃ is a variable representing class and d̃ is a vector of word
counts, then:
p(c|d) ∝ p(c) · p(d|c) (1)
For the purposes of this work we also assume a uniform
prior on c̃, meaning the ultimate decision is on the basis of
the document alone.
Note that the classification rule depends upon an estimate
p(d|c) for each possible value of c̃, which can be treated
independently of one another. A natural way to model
the distribution of word counts (rather than the presence
or absence of words) for a fixed class is to let p(d|c) be
distributed multinomially; the multinomial model assumes
that documents are the result of repeated trials, where on
each trial a word is selected at random, and the probability
of selecting the j-th word is θj .
Under multinomial sampling, the term p(d|c) has distribu-
tion:
pmultinomial(d|θ) =
(∑
j dj
)
!∏
j (dj !)
∏
j
θ
dj
j (2)
As is usual, given training data we estimate the vector θ as
its posterior mean assuming a uniform Dirichlet prior. Thus
if θ ∼ Dirichlet(1...1) a priori, the posterior mean for the
j-th component is:
θ̂j = E[θj |D] =
1 + nj
v +
∑
j nj
(3)
where the nj are the sufficient statistics from training doc-
uments, that is the total count of the j-th word in all docu-
ments from the class in question, and there are v words in
the vocabulary.
4.2. Hierarchical Probabilistic Classifier
Following the recent successes of (Madsen et al., 2005), we
also provide results using a hierarchical probabilistic clas-
sifier, but make several notable modifications to the method
in that work—these modifications constitute ongoing work,
and a more complete description can be found in (Allison,
2008). The hierarchical model we propose still assumes
that all documents are samples from some multinomial, but
rather than all being samples from the same multinomial
Words Bigrams Trigrams Stems Closed-Class Rules
Multinomial 74.31 75.83 61.51 78.46 43.06 44.19
Hierarchical 83.62 80.13 71.28 87.05 46.75 43.60
SVM 80.37 79.05 66.42 86.74 45.39 49.84
Table 1: Results of the three classifiers using different representations. Columns denote different representations, and rows
different classifiers. All figures are percentage accuracy over 10 folds. The highest score is highlighted in bold.
(as above), we hypothesise that each is sampled from a dif-
ferent multinomial, and that each of the multinomials are
themselves sampled from some other distribution.
(Madsen et al., 2005) suggest modelling the multinomi-
als as having been sampled from a Dirichlet distribution,
but such a modelling assumption has several drawbacks:
chiefly amongst those is that under the Dirichlet, the ex-
pected value of θj and the variance of θj is functionally
linked. Thus it is impossible to model the differences be-
tween words which are on average used the same amount,
but where in some cases the use of the word varies sig-
nificantly between documents, and in others it varies very
little. This is a particularly important distinction for author-
ship attribution, where topical words have large variance in
their underlying parameter (and thus are not so character-
istic of an author) while truly stylistic words should have
low variance in their parameter and be reliable indicators
of authorship.
To overcome these problems, and following on from (Jan-
sche, 2003; Lowe, 1999) who use similar models for in-
dividual words, we use a classifier which decomposes the
term p(d|c) into a sequence of independent terms of the
form p(dj |c), and hypothesises that conditional on known
class (i.e. c) d̃j ∼ Binomial(θj , n). However, unlike be-
fore, we also assume that θ̃j ∼ Beta(αj , βj), that is θ̃j
is allowed to vary between documents subject only to the
restriction that θ̃j ∼ Beta(αj , βj). Integrating over the un-
known θj in the new document gives the distribution of dj
as:
pbb(dj |αj , βj) =
n!
dj !(n− dj)!
×B(dj + αj , n− dj + βj)
B(αj , βj)
(4)
Where B(•, •) is the beta function:
B(x, y) =
Γ(x)Γ(y)
Γ(x+ y)
(5)
and Γ(•) is the Gamma function.
Then, the whole term p(d|c) has distribution:
pbeta−binomial(d|α, β) =
∏
j
p(dj |αj , βj) (6)
Given training documents from fixed class, we look to es-
timate the parameters of interest, in this case the (αj , βj)
since the θj are integrated out, using a method–of–moments
estimate similar to that in (Jansche, 2003).
4.3. Support Vector Machine Classifier
Our final classifier is a linear Support Vector Machine,
shown in several comparative studies to be the best per-
forming classifier for document categorization (Dumais et
al., 1998; Yang and Liu, 1999).
Briefly, the support vector machine seeks the hyperplane
which maximises the separation between two classes while
minimising the magnitude of errors committed by this hy-
perplane. The preceding goal is posed as an optimisation
problem, evaluated purely in terms of dot products between
the vectors representing individual instances. The flexi-
bility of the machine arises from the possibility to use a
whole range of kernel functions, φ(x1, x2) which is the dot
product between instance vectors x1 and x2 in some trans-
formed space.
Despite the apparent flexibility, the majority of NLP work
uses the linear kernel such that φ(x1, x2) = x1 · x2. Nev-
ertheless, the linear SVM has been shown to perform ex-
tremely well, and so we present results using the the lin-
ear kernel from the SVM light toolkit (Joachims, 1999) (we
note that experimentation with non–linear kernels made lit-
tle difference, with no consistent trends in performance).
We use default parameter values for the SVM, and the most
typical method for transforming the SVM into a multi-class
classifier, the One-Vs-All method, shown to perform ex-
tremely competitively (Rennie and Rifkin, 2001). All vec-
tors are also normed to unit length.
5. Experiments
We prepare the corpus as for experiments as follows: we
define contiguous alpha-numeric strings to be words and
case is normalised. For all representations, a document is
represented as a vector of count–valued features, which for
the case of the SVM is then normalised unit length. In all
cases, we use ten–fold cross validation, where folds are
assigned at random but the distribution of classes is kept.
We report the most simple performance measure, accuracy,
which is simply the total number of correct decisions di-
vided by the corpus size (since all documents are test doc-
uments using ten–fold cross–validation).
The results of the experiments for each of the representa-
tions are shown in Table 1. The table shows that the hierar-
chical classifier generally outperforms the other two meth-
ods, and the SVM generally outperforms the multinomial
classifier.
Perhaps more surprisingly still, the least stylistically en-
lightened approaches have far more discriminatory power
than those which have been suggested as more suitable
for capturing style. There are several possible reasons for
this, and the overall effect is likely a combination of all
of these: firstly, because the emails are short compared to
many document classification scenarios, reliable statistics
are difficult to obtain, and the fewer base units there are to
model, the more accurately this can be achieved. Secondly,
the closed–class words and rewrite rules provide surpris-
ing amounts of information beyond the use of vocabulary,
but clearly not sufficient in isolation. Finally, in order to
achieve a relatively substantial corpus, it has not been pos-
sible to pick a collection which is entirely topic–invariant;
thus there may be some element of topical association in-
fluencing the results.
6. Conclusion
Throughout this paper, we have described a new corpus for
evaluating authorship attribution methods, and have also
described baseline experiments to determine how well this
task can be achieved. Despite the brevity of the messages
(even when discounting extremely short emails), for certain
representations the task can be performed with considerable
accuracy.
The results of the preliminary experiments demonstrate
some general trends: firstly, unigram representations ap-
pear to be generally superior to bigrams, and always to
trigrams: thus while certain stylistic aspects are doubtless
captured by the longer n–grams (i.e. use of phrases, an ele-
mentary representation of sentence structure) the extra and
in many cases nonsensical grouping of words into phrases
generally damages performance. Even more surprisingly,
using word stems allows even better performance than sin-
gle words: this goes somewhat against intuition, since one
would expect that tendencies for different verb tenses, plu-
ralization and so on would set authors apart. Finally, more
complex linguistic features do not allow for such success-
ful discrimination; we suggest that this is perhaps due to the
extremely short and often informal nature of the communi-
cation, and suggest that perhaps methods designed particu-
larly for short messages, and using resources customised to
work with this particular form of writing, will allow even
greater success.
7. References
Ahmed Abbasi and Hsinchun Chen. 2005. Applying au-
thorship analysis to extremist-group web forum mes-
sages. IEEE Intelligent Systems, 20(5):67–75.
Ben Allison. 2008. A Bayesian Model of Word Counts
for Document Classification (in preparation). Ph.D. the-
sis, Department of Computer Science, University of
Sheffield.
H Baayen, H van Halteren, and F Tweedie. 1996. Out-
side the cave of shadows: using syntactic annotation to
enhance authorship attribution. Lit Linguist Computing,
11(3):121–132.
J. Carroll Briscoe, E. and R. Watson. 2006. The second
release of the RASP system. In Proceedings of the COL-
ING/ACL 2006 Interactive Presentation Sessions, Syd-
ney, Australia.
J. Burrows. 1987. Word-patterns and story-shapes: The
statistical analysis of narrative style. Literary and Lin-
guistic Computing, 2(2):61–70.
Joachim Diederich, Jörg Kindermann, Edda Leopold,
and Gerhard Paass. 2000. Authorship attribution with
support vector machines. Applied Intelligence, 19(1-
2):109–123.
Susan Dumais, John Platt, David Heckerman, and Mehran
Sahami. 1998. Inductive learning algorithms and repre-
sentations for text categorization. In CIKM ’98, pages
148–155.
Louise Guthrie, Elbert Walker, and Joe Guthrie. 1994.
Document classification by machine: theory and prac-
tice. In Proceedings COLING ’94, pages 1059–1063.
Martin Jansche. 2003. Parametric models of linguistic
count data. In ACL ’03, pages 288–295.
Thorsten Joachims. 1999. Making large-scale svm learn-
ing practical. Advances in Kernel Methods - Support
Vector Learning.
Bryan Klimt and Yiming Yang. 2004. The enron corpus:
A new dataset for email classification research. In Pro-
ceedings of ECML 2004, pages 217–226.
David D. Lewis. 1998. Naı̈ve (Bayes) at forty: The in-
dependence assumption in information retrieval. In Pro-
ceedings of ECML-98, pages 4–15.
S. Lowe. 1999. The beta-binomial mixture model and its
application to tdt tracking and detection. In Proceedings
of the DARPA Broadcast News Workshop.
Rasmus E. Madsen, David Kauchak, and Charles Elkan.
2005. Modeling word burstiness using the Dirichlet dis-
tribution. In ICML ’05, pages 545–552.
A. McCallum and K. Nigam. 1998. A comparison of event
models for naı̈ve bayes text classification. In Proceed-
ings AAAI-98 Workshop on Learning for Text Catego-
rization.
Andrew K. McCallum, Ronald Rosenfeld, Tom M.
Mitchell, and Andrew Y. Ng. 1998. Improving text
classification by shrinkage in a hierarchy of classes. In
ICML-98, pages 359–367.
Thomas Merriam. 1998. Heterogeneous Authorship in
Early Shakespeare and the Problem of Henry V. Lit Lin-
guist Computing, 13(1):15–28.
F. Mosteller and D. L. Wallace. 1964. Inference and
Disputed Authorship: The Federalist Papers. Addison-
Wesley.
Martin Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130–137.
Jason D. M. Rennie and Ryan Rifkin. 2001. Improv-
ing multiclass text classification with the Support Vec-
tor Machine. Technical report, Massachusetts Insititute
of Technology, Artificial Intelligence Laboratory.
Fiona J. Tweedie, David I. Holmes, and Thomas N. Corns.
1998. The Provenance of De Doctrina Christiana, at-
tributed to John Milton: A Statistical Investigation. Lit
Linguist Computing, 13(2):77–87.
Y. Yang and X. Liu. 1999. A re-examination of text catego-
rization methods. In 22nd Annual International SIGIR,
pages 42–49, Berkley, August.
