Exploiting Multimodal Affect and Semantics to
Identify Politically Persuasive Web Videos
Behjat Siddiquie
behjat.siddiquie@sri.com
SRI International
Dave Chisholm
davec@cs.columbia.edu
Columbia University
Ajay Divakaran
ajay.divakaran@sri.com
SRI International
ABSTRACT
We introduce the task of automatically classifying politically
persuasive web videos and propose a highly effective multi-
modal approach for this task. We extract audio, visual, and
textual features that attempt to capture affect and seman-
tics in the audio-visual content and sentiment in the viewers’
comments. We demonstrate that each of the feature modal-
ities can be used to classify politically persuasive content,
and that fusing them leads to the best performance. We
also perform experiments to examine human accuracy and
inter-coder reliability for this task and show that our best
automatic classifier slightly outperforms average human per-
formance. Finally we show that politically persuasive videos
generate more strongly negative viewer comments than non-
persuasive videos and analyze how affective content can be
used to predict viewer reactions.
Categories and Subject Descriptors
H.3.3 [[Information Search and Retrieval]: Search pro-
cess; I.2.10 [Vision and Scene Understanding]: Video
analysis
General Terms
Algorithms, Design, Experimentation
Keywords
Video Classification, Affect Recognition, Audio Concepts,
Video Concepts, Sentiment Analysis, Multimodal Fusion
1. INTRODUCTION
In the last few years social media has rapidly emerged as
an effective means to disseminate information to a large and
geographically diverse audience. Its low barrier of entry al-
lows not just well funded organizations but also individuals
to share and propagate their opinions and viewpoints. Mul-
timedia content in particular can both express and evoke
strong emotional responses, and there is reason to believe
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
Copyright 20XX ACM X-XXXXX-XX-X/XX/XX ...$15.00.
that audio-visual content can affect viewers more strongly
than text based content [15]. Viewer reactions are also visi-
ble in the form of comments, and so in this way social media
allows content creators and uploaders to spread a strong,
compelling message and observe its impact.
For these reasons, multimedia content on social networks
is particularly suitable to propagate political views, and it
is used for public influence, political persuasion and even
radicalization [10]. It is thus a potent tool to influence and
attract new followers, and there is great interest in many
sectors in detecting and assessing politically charged or oth-
erwise persuasive social media content.
In this paper, we propose a solution to detect such po-
litically persuasive videos posted on social media and also
develop methods to predict and analyze the sentiment of
comment responses. In order to analyze the audio-visual
content present in the videos we focus on robust extraction
of the semantic and affective information. For the audio do-
main we detect several grades of speech arousal and related
semantic categories such as crowd reaction and music. For
the visual domain, we detect visual sentiment and semantic
content. Finally we analyze the sentiment of comments asso-
ciated with a video to determine viewer reaction. Our clas-
sification results indicate that politically persuasive videos
posted online can be reliably detected based on extracted
affective and semantic information. Furthermore, we show
that politically persuasive videos often generate more neg-
ative reactions among viewers and the overall sentiment of
reactions can be predicted with a reasonable degree of suc-
cess solely from audio-visual features. Figure 1 shows an
overview of our approach.
There are two key contributions of our work:
• We demonstrate that affective and semantic informa-
tion extracted from the audio, visual and textual con-
tent associated with a video can be used to reliably
predict whether the video is of a politically persuasive
nature.
• We further show that politically persuasive videos gen-
erate more negative reactions in comments, that af-
fective content can indicate how viewers may react,
and that viewer reaction to semantically similar videos
varies with respect to the videos’ affective content.
We would like to emphasize that the goal of our work is to
demonstrate the utility of multi-modal affective features to
reliably identify politically persuasive media content, rather
than to develop sophisticated or novel content understand-
ing techniques. In particular, we show that an approach
leveraging these affective and semantic features can gener-
ally classify such videos as well or better than humans. We
Figure 1: Audio and visual concept scores are computed from the audio-visual data of a web video. These
scores are used to predict whether the video is politically persuasive and whether the comments in response
will be highly polarized. In addition, the actual comments are also used to predict whether a video is
politically persuasive, and all three modalities are fused for the best prediction results.
believe that our work opens up an exciting new area of re-
search with a number of interesting applications.
2. RELATED WORK
There has been some recent work on detecting persuasion.
Park et al. collected a corpus of movie review videos and
annotated them for their persuasiveness in [31]. From this
data, they demonstrated that the verbal and non-verbal be-
havior of the presenter is predictive of how persuasive they
are. Our work differs from this in two key ways. First, we
rely not only on the affective characteristics of a speaker,
but also on the affective characteristics of viewers’ reactions
as well as semantic content unrelated to a main speaker (e.g.
crowd reaction, background music). Second, we analyze a
corpus that is collected from a variety of unscripted situa-
tions recorded both indoors and outdoors and whose audio-
visual quality ranges from good to poor, whereas the corpus
in [31] consists of movie review videos captured in controlled
environments.
Detection of radical online content is a growing research
area due to the interest shown in it by law enforcement agen-
cies; Correa presented a survey of such work in [10]. This
detection is typically done in two ways: by analysis of the
network structure or by analysis of the content itself. Reid
et al. presented an network based approach in [32] that cap-
italized on the notion that websites that promote the same
ideology tend to be interconnected by hyperlinks. Most ap-
proaches that analyze content, such as the one presented
by Abbasi in [1], tend to focus on the textual and linguis-
tic content of websites rather than audio-visual content of
videos. To the best of our knowledge, Fu et al. presented
the only published work on detecting extremist videos in
[13], and their approach used the meta-data associated with
the video rather than the actual audio-visual content. In
contrast to these works, our approach is independent of any
network structure and exploits the audio-visual content it-
self as well as comments in reaction to it. Moreover, our
target class of politically persuasive videos is broader than
Fu et al.’s class of extremist videos.
There has been some work on sentiment analysis of vi-
sual content. This includes analysis of image aesthetics [19],
analysis of image interestingness and memorability [16] and
analysis of image affect and emotion [28]. There has also
been work on detecting visual sentiment expressed through
human behavior - e.g. facial expressions [37] - as well as
work on detecting image style [21]. We rely directly on two
prior works for visual content analysis. First, we use Jia et
al.’s CAFFE implementation from [17] which was trained
on concepts presented as part of ImageNet in [11]. Second,
we also use the Visual Sentiment Ontology presented in [5],
which consists of a large-scale ontology of semantic concepts
correlated with strong sentiments.
Detecting affect from human speech has grown in impor-
tance because of its applications in enabling realistic human
interactions [27, 33]. However, most of the work in this area
has been demonstrated on datasets collected in controlled
environments with little noise. Similarly detecting semantic
concepts in audio has long been an important problem in
the audio processing community due to its applications in
video retrieval [26, 6, 7]. Our audio concept detection, based
on [9] combines the best of both these approaches.
Sentiment analysis and opinion mining from text have
long been active research topics with varied applications
such as analyzing product reviews, improving question an-
swering systems, and mining political opinions. This area
is well covered by Pang and Lee’s survey in [30]. Recent
work, such as Socher et al.’s approach in [35], has improved
the accuracy of text based sentiment analysis significantly.
There has also been recent work focusing exclusively on text
in social media settings, such as Mohammad et al.’s senti-
ment analysis of Twitter data in [29]. We employ both of
these recent approaches to analyze and gauge user reactions
and opinions to videos in the form of comments.
There has also been work on combining information from
multiple modalities to better detect affect [20, 3]. The key
advantage of multimodal affect detection is that different
modalities contain complementary information and there-
fore fusing the information from them can lead to perfor-
mance improvements in detecting affect. We also adopt a
multimodal fusion approach to detect persuasive videos, and
fuse not only modalities based on audio-visual content (e.g.
speech characteristics, crowd response, visual semantics) but
also on viewer response (e.g. sentiment of user comments).
3. DATASET
For our experiments we use the Rallying a Crowd (RAC)
dataset, introduced in [9] (Figure 2). The RAC dataset con-
sists of 231 videos from YouTube comprising over 27 hours of
content. The dataset contains 132 positive and 99 negative
examples of politically persuasive videos. Positive examples
are often events such as speeches, rallies or protests, whereas
negative examples contain semantically similar content but
lack the highly affected, politically persuasive nature of the
positive examples. The videos were recorded under a wide
variety of conditions (e.g. outdoor vs. indoor, near vs.
far) with various levels of post-production (e.g. professional
quality clips vs. unedited amateur footage). Speakers in
these videos are from all over the world and so the dataset
includes samples with multiple languages and settings. Some
speakers are even bilingual or trilingual and switch languages
during the course of a speech. In this way, RAC covers the
diversity of such politically persuasive content worldwide.
To mitigate the moderate size of the dataset and to avoid
over-fitting to particular speech patterns, gender, or non af-
fective cues, care was taken to include certain speakers in
both the positive and negative set, to ensure video traits
like speaker language and gender or setting (e.g. indoor vs.
outdoor, etc) are equally distributed across both sets, and
rigorously annotate the affective audio contents present in
the videos. Therefore the RAC dataset, due to its focus on
politically persuasive multimedia as an identifiable genre, is
ideal for our purpose.
3.1 Human Accuracy in Classifying Politically
Persuasive Videos
To quantify the difficulty of classifying persuasive content
as well as validate it as a legitimate identifiable class we had
multiple human annotators perform binary labeling of posi-
tive and negative RAC videos. These experiments also allow
us to compare the performance of our automatic approach
to human performance. We had four annotators label the
test portion of the RAC dataset consisting of 80 positive
and 58 negative videos. The annotators were instructed to
label “politically persuasive” content and were first allowed
to view the training set consisting of 52 positive and 40 nega-
tive videos to form an opinion of what constituted politically
persuasive content. The average annotation accuracy of the
four human annotators with respect to the ground truth
was 80.3%, with individual values of 70.6%, 75.0%, 75.7%,
and 100.0%. We attribute the perfect accuracy of the the
fourth annotator to the extra diligence and time this an-
notator took compared to the others, and feel it supports
the validity of the ground truth. Note that RAC ground
truth for positive vs. negative was established by using par-
ticular search terms and carefully discussing each selected
video among all the authors to establish a consensus. An
inter-coder agreement coefficent of 0.524, indicating “mod-
erate agreement”, was measured among all annotators using
Fleiss’s kappa [12, 24]. Inter-coder agreement between all
possible pairs of annotators was also measured using Scott’s
pi and varied from 0.396 (“fair agreement”) to 0.658 (“sub-
stantial agreement”) [34, 24]. We feel the overall classifi-
cation success and moderate agreement between annotators
validate the class of “politically persuasive” videos as coher-
ent and classifiable.
4. APPROACH
4.1 Audio Analysis
Figure 2: Still frames from selected public domain
videos within the RAC dataset.
We first investigated the use of the audio content in the
videos to distinguish between persuasive and non-persuasive
videos. Politically persuasive videos are often characterized
by animated speakers, charged content, crowd response and
occasional music. Hence, detecting these audio categories
would enable us to identify persuasive videos. In order to
do so, we use the audio concept detection approach of [9],
which we briefly describe below.
4.1.1 Audio Concept Detection and Segmentation
We use the approach of [9] to segment the videos based
on their audio content and extract audio concepts. The ap-
proach involves extracting Mel-Frequency Cepstral Coeffi-
cients (MFCCs), Spectrogram and Prosody features from
the audio stream and encoding them using the bag-of-words
representation. A non-linear SVM is trained on each of these
features for concept classification. Additionally, they use a
Multiple Kernel Learning approach [4] to combine these fea-
tures leading to improved results.
Since a single concept label will very rarely apply to an
entire audio track, a multi-scale unsupervised audio segmen-
tation approach was proposed in [9]. The approach is an
adaptation of the Simple Linear Iterative Clustering (SLIC)
algorithm for image segmentation [2], for audio data, and it
temporally segments the audio track into a set of homoge-
neous segments likely to contain a single concept. The ap-
proach has been shown to work well for semantic concepts
on the CCV dataset [18] as well as for affective concepts
on the RAC dataset. Following the work of [9], we detect
the following audio concepts: Crowd, Music + Crowd, Mu-
sic, Music + Speech, Crowd + Speech, Calm Speech, Slightly
Agitated Speech, Agitated Speech, Very Agitated Speech. We
believe that these concepts are well suited to detecting per-
suasive content on the RAC dataset.
4.1.2 SVM Classifier Setup
Given a set of n videos V = {V1, V2, V3, . . . , Vn} and their
corresponding binary labels {y1, y2, y3, . . . , yn} that indicate
whether the video contains persuasive content, we trained a
politically persuasive versus non-persuasive video classifier
as follows. For each video Vi, we segment the audio and
then compute the audio concept scores as described above.
Four concept scores are computed at each scale, using the
MFCC, Spectrogram, Prosody and All Feature kernels noted
in section 4.1.1. We denote the audio concept scores for
video Vi as Oisf , where i denotes the video index, s refers to
the segmentation scale which is {fine, medium, coarse} and f
indicates the low level feature used which could be {Prosody,
MFCC, Spectrogram, All Features}. The dimensionality of
an audio concept score Oisf is T ×C, where T is the number
of segments (dependant on the scale of the segmentation and
length of the video) and C is the number of audio concepts.
We quantize Oisf by linearly resizing it to Tfixed×C, where
Tfixed = 100 to obtain Ōisf . Now corresponding to each
video Vi with scale s, we have a fixed dimensional feature
Ōisf . We train an RBF SVM on this data for classification.
4.1.3 Experimental Setup and Results
We use the training set of RAC, consisting of 52 positive
videos and 40 negative videos, to learn the audio concept
detectors. The test set consists of 80 positive and 59 neg-
ative videos. In order to ensure that the results were not
influenced by the larger size of the positive class, we ran-
domly sample 59 positive videos and perform 10-fold cross-
validation on an equal number of positive and negative in-
stances. The final results represent the mean and standard
deviation of the accuracy over 25 such randomized runs.
This methodology ensures that a random classifier will have
a classification accuracy of 50%. We follow this experimen-
tal protocol in all subsequent experiments. The results are
shown in Table 1. The results show that using audio features
one can perform reasonably well at detecting persuasive con-
tent. We can also see that the best results are obtained when
the audio concept scores are computed using spectrogram
features. The results shown in the table are for segmenta-
tions at the medium scale. The classification accuracy was
not significantly different at other scales.
Audio Concept Features Classification Accuracy
Prosody 77.82 ± 12.79
MFCC 73.64 ± 12.17
Spectrogram 81.03 ± 11.93
All Features (MKL) 78.65 ± 11.76
Table 1: SVM classification performance using the
audio concept detection features. The margins rep-
resent the standard deviations across 25 randomized
runs. Note ”All Features” refers to a combination of
the three low level audio features to produce a single
set of concept scores, not a combination of multiple
scores from individual features.
We further explored classification of videos based on their
audio concept scores using Conditional Random Fields (CRFs)
[23]. The main advantage of CRFs is that they model the
temporal dynamics of the features and unlike the SVMs do
not require artificially quantizing the feature vector. The
results in Table 2 show that CRFs result in a marginal im-
provement in the classification performance over SVMs and
we attribute this to their ability to handle the temporal dy-
namics.
Audio Concept Features Classification Accuracy
Prosody 77.81 ± 12.93
MFCC 76.01 ± 12.56
Spectrogram 82.52 ± 10.43
All Features (MKL) 78.66 ± 12.81
Table 2: CRF classification performance using the
audio concept detection features.
4.1.4 Analysis of Results
Class Id
Fr
am
es
1 2 3 4 5 6 7 8 9
Class Id
Fr
am
es
1 2 3 4 5 6 7 8 9
P
e
rs
u
a
s
iv
e
 
V
id
e
o
s
 
N
o
n
-P
e
rs
u
a
s
iv
e
 
V
id
e
o
s
 
F
ra
m
e
s
 
F
ra
m
e
s
 
Figure 3: Mean concept detection scores for posi-
tive and negative videos. Notice that positive videos
tend to have a higher scores for crowd and agitated
speech, while negative videos have higher scores for
calm speech and slightly agitated speech. (Best viewed
in color - red/yellow indicate higher scores and blue
indicates lower scores.)
In order to better understand what the classifiers were
learning for distinguishing between the persuasive and non-
persuasive videos, we looked at the mean features Ōisf from
the persuasive and non-persuasive videos (Figure 3). The
figures show that persuasive videos contain a stronger re-
sponse for crowd and agitated speech. On the other hand,
non-persuasive videos contain a stronger response for calm
speech and slightly agitated speech. Considering the charged
content and oratory of many of the persuasive videos, this
matched our intuition of how the content would be dis-
tributed. Finally, we believe that the concept scores com-
puted from the spectrogram features lead to the best results
because they detect the relevant concepts such as crowd and
agitated speech more accurately compared to other features
leading to superior results on the classification task.
4.2 Video Analysis
In this subsection, we investigate the use of concepts based
on visual content that could be used to differentiate between
persuasive and non-persuasive videos. Persuasive videos
from the RAC dataset often contain rousing visuals ranging
from cheering crowds and heroic figures to images of graphic
violence. Overall, the visuals are more striking and extreme
than the non-persuasive content, and hence visual features
may be useful to identify persuasive content. We primarily
focus on deep learning based features that identify seman-
tic concepts and sentiment. Recently deep learning models
such as Convolutional Neural Networks (CNNs) have be-
come extremely popular for learning image representations
[22]. CNNs, loosely inspired by human vision, are variants of
multilayer perceptrons consisting of multiple convolutional
and pooling layers followed by fully connected layers [25]. In
our work, we implemented a standard network [22] using the
popular open source framework CAFFE [17] to extract vi-
sual features. We trained two different networks using differ-
ent datasets. The first network was trained on the ImageNet
dataset [11] and the second was trained on the Visual Sen-
timent Ontology dataset [5]. We next describe each of these
processes along with the obtained experimental results.
4.2.1 ImageNet Concepts
In this case, we want to evaluate whether the presence or
absence of certain semantic concepts in the video indicates
the presence of persuasive content. In order to estimate
the presence or absence of concepts in an image we use the
CNN trained on the ILSVRC-2012 dataset which is a subset
of ImageNet, consisting of around 1.2 million labeled data
with 1000 different classes ranging from elephant to space
shuttle to stethoscope. The network was trained to maxi-
mize the multinomial logistic regression objective for these
classes over the training data. We use this 1000 dimensional
output as a feature indicating the presence or absence of
each class. We also use the outputs of intermediate network
layers - which represent more abstract visual features than
the final concept outputs and can often provide high classi-
fication performance - as features. These three features are
referred to as“prob”, “fc7”and“fc8”respectively. We extract
each of these features from every 30th frame (1 sec.) of the
video. As in case of the audio data we now have a vector
of dimension F x D (where F is the number of frames sam-
pled) and D is the dimensionality (1000 in case of the prob
and 4096 in case of “fc7” and “fc8”). Since F varies based on
the length of the video, we linearly resize our feature vector
to Ffixed ×D, where Ffixed = 100, much like was done for
audio in section 4.1.2. Each of the three features were pro-
vided individually to an RBF SVM based classifier, and all
features were also combined by concatenation. We follow the
same experimental protocol as in the earlier experiments de-
scribed in section 4.1.3. The classification results are shown
in Table 3.
Features Classification Accuracy
prob 68.52 ± 3.62
fc8 67.61 ± 3.97
fc7 69.62 ± 4.48
All Features 71.81 ± 5.37
Table 3: Classification performance using different
features extracted from the CNN network trained
on ILSVRC-2012 dataset [11] for recognizing object
categories. “prob” refers to the final concept out-
put scores, whereas “fc7” and “fc8” are scores for
features from intermediate layers of the CNN.
Classification using each of the features significantly out-
performs chance, indicating that visual information is indeed
useful for detecting persuasive content. We can also see that
the features from the “fc7” layer - which contain more ab-
stract, learned features - outperform the features from the
“fc8” and “prob” layers indicating the utility of these over
the more specific final concepts. Finally, we can also see
that combining the features from all of the layers results in
a modest improvement in the performance.
4.2.2 Visual Sentiment Ontology Concepts
In this case, we want to evaluate whether the presence or
absence of certain visual sentiment concepts in a video can
provide information on whether the video contains persua-
sive content. In order to evaluate this we use the Visual
Sentiment Ontology dataset [5] which consists of approxi-
mately 930k images. This dataset was collected by searching
Flickr for Adjective-Noun-Pairs (ANPs) such as “beautiful
flower” or “disgusting food”. The advantage of using these
ANPs is that they relate particular images of sentiment neu-
tral nouns (e.g “flower”) to a strong sentiment by adding an
adjective (e.g. “beautiful flower”). Thus the concepts cap-
ture both semantic and sentiment information. We used the
latest version of Visual Sentiment Ontology DeepSentiBank
[8] which consists of CNN based concept detectors for 2089
ANPs. These concept detectors are trained using the same
deep learning network [22] described above. As in the earlier
case, we use the final “prob” outputs as well as the interme-
diate layer outputs “fc7” and “fc8” as inputs to our classifier.
The results are shown in table 4. Here again we can see
that the intermediate layers “fc7” and “fc8” perform better
than the final outputs (“prob”). The performance of the
SentiBank concepts is slightly better than the performance
of the ImageNet concepts, which could be due to use of sen-
timent in the concepts, or just due to the higher number of
concepts present in the SentiBank dataset. Finally, we also
looked at combining the features from the ImageNet con-
cepts and the SentiBank concepts, but this did not lead to
any meaningful increase in performance.
Features Classification Accuracy
prob 68.46 ± 3.61
fc8 72.40 ± 2.76
fc7 73.95 ± 2.95
All Features 73.13 ± 3.01
Table 4: Classification performance using different
features extracted from the CNN network trained
on the Visual Sentiment Ontology dataset [5] for
recognizing visual sentiment.
4.2.3 Analysis of Results
Ideally, highly scored concepts in positive videos could be
used to explain why a video was classified as politically per-
suasive. In practice we noted a mix of accurate and relevant
concepts (e.g. “bad guy” in a violent video) along with con-
cepts that seemed irrelevant. Moreover, we noted that the
classification was more accurate using intermediate features
than the high level concepts. Thus while the visual concepts
are effective for classification, in order to explain classifica-
tion, a more specific set of visual concepts would need to be
used, or detection would need to be improved.
4.3 Text Analysis
We also investigated the use of text associated with the
videos, in particular viewer comments. Videos uploaded to
YouTube and other video-sharing sites often generate a large
number of comments posted by viewers across the world,
and many of these comments contain reactions of people
to the videos. Intuitively comments generated in response
to politically persuasive videos should be of a more polar-
ized nature while other videos generate comments of a more
neutral or positive nature. Therefore, exploiting the sen-
timents contained within these contents should provide us
with additional information as to whether the video contains
politically persuasive content.
Given a YouTube video we extract all of the associated
comments using the YouTube API. The number of com-
0 1−10 11−100 101−1K 1K−10K >10K
0
10
20
30
40
50
60
Number of Comments
N
um
be
ro
fV
id
eo
s
Figure 4: The distribution of the number of com-
ments across the videos in the dataset.
ments for to each video varies greatly (Figure 4). Since
our videos comprise a geographically diverse range of top-
ics and speakers, a significant fraction of the comments to
some of these videos are in languages other than English.
As a pre-processing step we automatically filter out non-
English text. In order to do so we use a simple approach of
counting the proportion of the words in each comment that
are present in a dictionary learnt from a standard English
text corpus [36]. Despite its simplicity, we empirically found
that this approach worked well for filtering out non-English
text in YouTube comments, regardless of the character set
of the comments or any spelling or compositional errors.
To then detect the sentiment of each comment, we experi-
mented with two different approaches as described below.
SATSVM: We refer to the approach in [29] as Sentiment
Analysis of Tweets using SVMs (SATSVM). SATSVM has
been specifically developed for social media data such as
tweets and so it is appropriate for our data. The approach
relies on extracting a number of features from each comment
and training an SVM to classify the comment as having a
positive or negative sentiment. We implement a simplified
version of this approach and instead of using the binary SVM
outputs, we use the SVM decision scores which roughly indi-
cate the degree of positivity or negativity in the sentiment.
DeepCompositionalModel: We refer to the sentiment
detection approach presented in [35] as the DeepComposi-
tionalModel. It uses a Recursive Neural Tensor Network
to build a representation of sentences based on their struc-
ture and computes sentiment by accounting for how the con-
stituent words compose with each other. Unlike SATSVM,
the DeepCompositionalModel splits each comment into its
sentences and assigns a separate sentiment score to each sen-
tence. Its output is a 5 dimensional probability vector indi-
cating the probability of the sentence being Strongly Nega-
tive, Negative, Neutral, Positive or Strongly Positive.
4.3.1 Detecting Persuasion based on Sentiment
Given a video Vi and the set of associated comments Ci
consisting of N individual comments {ci1, ci2, ci3, . . . ciN},
we run SATSVM on each element of Ci to get a set of
N scores {xi1, xi2, xi3, . . . xiN} normalized within the range
[−1, 1]. We then quantize these scores by binning them
into a histogram consisting of eleven equally spaced bins.
Using this technique, each video Vi can be represented by
a fixed dimensional histogram Hi. We train a RBF SVM
using these histogram features for classifiying videos into
persuasive versus non-persuasive. The classification results
−1.0 −0.8 −0.6 −0.4 −0.2 0.0 0.2 0.4 0.6 0.8 1.0
0
0.1
0.2
0.3
0.4
Sentiment Polarity
Pr
op
or
tio
n
of
C
om
m
en
ts Persuasive
non−Persuasive
Figure 5: Mean sentiment histograms for the po-
litically persuasive and non-persuasive videos based
on SATSVM [29]. Persuasive videos tend to have a
higher proportion of negative comments (bin ‘-0.2’).
are shown in Table 5.
Similarly, when using the DeepCompositionalModel, we
extract the sentiment for each comment {ci1, ci2, ci3, . . . ciN}
obtaining Xi = {xi1,xi2,xi3, . . .xiM}, where M(> N) is the
total number of sentences. (Each comment is split into one
or more sentences.) Each xij is a 5 dimensional probabil-
ity vector as described above. Each video Vi is now repre-
sented by a set of these features Xi. We train an SVM using
a pyramid match kernel [14], which has been shown to be
very effective for learning with sets of features, for classify-
ing these videos. The results are shown in Table 5. From
the results we can see that both SATSVM and DeepCompo-
sitionalModel perform similarly and are significantly better
than random.
Approach Classification Accuracy
SATSVM 69.52 ± 4.31
DeepCompositionalModel 69.94 ± 3.98
Table 5: Classification performance using sentiment
features extracted from SATSVM [29] and Deep-
CompositionalModel [35].
4.3.2 Analysis of Results
We now try and analyze some of the results in an attempt
to understand how sentiment analysis helps in detecting per-
suasive videos. To analyze the results from SATSVM, we
looked at the mean normalized histogram of the positive as
well as the negative videos. The histograms in Fig. 5 show
that politically persuasive videos contain a higher propor-
tion of negative comments than the non-persuasive videos.
We plot similar histograms for the sentiment detection
results obtained from DeepCompositionalModel (Fig. 6).
Here we can see that persuasive videos have a larger pro-
portion of negative comments compared to non-persuasive
videos. Therefore we can see that while the two sentiment
extraction approaches result in different distributions, they
tend to support the hypothesis that persuasive videos lead
to more negative and fewer positive comments when com-
pared to non-persuasive videos and we can use this to help
automatically distinguish between politically persuasive and
non-persuasive videos.
4.4 Multimodal Fusion
V. Neg. Neg. Neutral Pos. V. Pos.
0
0.1
0.2
0.3
0.4
0.5
Sentiment Polarity
Pr
op
or
tio
n
of
C
om
m
en
ts Persuasive
non−Persuasive
Figure 6: Mean sentiment histograms for the po-
litically persuasive and non-persuasive videos based
on the DeepCompositionalModel [35]. Persuasive
videos tend to have a lower proportion of positive
comments (bin ‘Pos.’).
We next looked at fusing the information from the audio,
visual and text modalities. We believe that these modali-
ties contain complementary information and therefore fus-
ing them should boost the overall classification performance.
For fusion we considered three different fusion strategies -
Early Fusion, Simple Late Fusion and Learning based Late
Fusion. For the purpose of fusion, we use the spectrogram
features for audio (Table 1), the“fc7”features from the senti-
ment ontology for video (Table 4) and the SATSVM features
for text (Table 5). In case of Early Fusion we simply con-
catenate the features from all of the modalities and train
a RBF SVM for classification. In Simple Late Fusion, we
add up the decision scores obtained from each modality to
arrive at a composite decision score to perform classifica-
tion. For Learning based Late Fusion, we train a logistic
regression based fusion that combines the decision scores
from each modality in a weighted manner. The results are
shown in Table 6 and they demonstrate that fusion tends to
improve classification results over the individual modalities
and late fusion is more effective than early fusion. Further-
more, Learning based Late Fusion leads to further perfor-
mance gains over Simple Late Fusion.
4.4.1 Comparison to Human Performance
Notably, our best results using Learning based Late Fu-
sion outperform the average performance (80.3%) and three
of four individual performances (70.6%, 75.0%, 75.7%) of
our human annotators. Only one human annotator outper-
formed this automatic classification approach (100.0%).
Modality Classification Accuracy
Audio 81.03 ± 11.93
Video 73.13 ± 3.01
Text 69.52 ± 2.31
Early Fusion 81.37 ± 1.47
Simple Late Fusion 83.32 ± 3.40
Learning based Late Fusion 85.09 ± 1.83
Table 6: Classification performance on the RAC
dataset using different fusion techniques.
4.5 Predicting Viewer Response
Finally, we investigated the prediction of viewer response
to a video. Our goal here is to see whether given a video’s
audio-visual content, can we predict the sentiment polarity
of the comments posted in response to it. In order to do so,
we first clustered the test videos based on their sentiment
histograms Hi (subsection 4.3), computed using SATSVM
[29], in an unsupervised manner. We set the number of clus-
ters to two, partitioning the set of test videos into two clus-
ters that roughly correspond to videos that generated a posi-
tive response and videos that generated a negative response.
Also note that while these clusters roughly map to the per-
suasive and non-persuasive classes, the correspondence is not
exact. We treat this as a supervised classification problem,
using the cluster indices as the class labels, which correspond
to videos generating a positive and negative response. As
features, we use the spectrogram features for audio (Table
1) and the “fc7” features from the sentiment ontology for
video (Table 4). We train non-linear SVMs for classification
based on unimodal features and a logistic-regression based
late-fusion for multimodal fusion. The results are shown in
Table 7. The results show that we can predict the viewer re-
sponse in advance based on just the extracted audio-visual
content with a reasonable degree of accuracy (random ac-
curacy is 50%). Furthermore, fusing the audio and visual
modalities leads to an increase in performance.
Modality Classification Accuracy
Audio 61.97 ± 7.26
Video 61.67 ± 8.83
Learning based Late Fusion 64.69 ± 4.63
Table 7: Classification performance for predicting
the viewer response on the RAC dataset.
4.6 Qualitative Examples
In Fig. 1 we show the output from a test set video, Mar-
tin Luther King’s I Have a Dream speech 1. Our system
correctly predicts that the video is a politically persuasive
video and also correctly infers that the video is likely to have
polarized comments based on its audio-visual content.
In another example, we considered two videos from our
test set, consisting of a rallying speech2 and an interview
3, both involving the same speaker (Harsimrat Kaur Badal,
an Indian Union Cabinet Minister.) Despite the two videos
having the same speaker, our system was able to correctly
predict their labels (persuasive and non-persuasive respec-
tively) based on their audio affect.
5. CONCLUSION
We have demonstrated that affective and semantic audio
and visual concepts as well as sentiment measures on viewer
comments are effective at predicting whether a video con-
tains politically persuasive content. Notably, the best auto-
matic classification approach generally outperforms human
annotators. Individually, audio concepts are the best pre-
dictors, while a fusion of these modalities produces the best
results, indicating that each contains some complementary
information to the others. Both visual and audio features
are also predictive of negatively polarized comments, allow-
ing us to potentially predict the viewer response to a video
before comments are left. There are several possible areas
1https://www.youtube.com/watch?v=smEqnnklfYs
2https://www.youtube.com/watch?v=gHSCiB2jb1g
3https://www.youtube.com/watch?v=7Edk8Ybb5GM
for future work. For instance, an attempt to try to iden-
tify new videos with politically charged content as they are
posted could be based on this approach, or a similarly tai-
lored approach could be applied to other specific types of
content besides politically persuasive videos.4
6. REFERENCES
[1] A. Abbasi. Affect intensity analysis of dark web
forums. Intelligence and Security Informatics, IEEE,
2007.
[2] R. Achanta, A. Shaji, K. Smith, A. Lucchi, P. Fua,
and S. Susstrunk. Slic superpixels compared to
state-of-the-art superpixel methods. In IEEE PAMI,
2012.
[3] M. R. Amer, B. Siddiquie, S. Khan, A. Divakaran, and
H. Sawhney. Multimodal fusion using dynamic hybrid
models. WACV, 2014.
[4] F. R. Bach, G. R. G. Lanckriet, and M. I. Jordan.
Multiple kernel learning, conic duality, and the smo
algorithm. ICML, 2004.
[5] D. Borth, R. Ji, T. Chen, T. Breuel, and S.-F. Chang.
Large-scale visual sentiment ontology and detectors
using adjective noun pairs. In ACM MM, 2013.
[6] S.-F. Chang, D. Ellis, W. Jiang, K. Lee, A. Yanagawa,
A. C. Loui, and J. Luo. Large-scale multimodal
semantic concept detection for consumer video. ACM
MIR, 2007.
[7] S. Chaudhuri, M. Harvilla, and B. Raj. Unsupervised
learning of acoustic unit descriptors for audio content
representation and classification. In INTERSPEECH,
2011.
[8] T. Chen, D. Borth, T. Darrell, and S. Chang.
Deepsentibank: Visual sentiment concept classification
with deep convolutional neural networks. arXiv, 2014.
[9] D. Chisholm, B. Siddiquie, A. Divakaran, and
E. Shriberg. Audio-based affect detection in web
videos. In ICME, 2015.
[10] D. Correa. Solutions to detect and analyze online
radicalization.
[11] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and
L. Fei-Fei. Imagenet: A large-scale hierarchical image
database. In CVPR, 2009.
[12] J. Fleiss. Measuring nominal scale agreement among
many raters. Psychological Bulletin, 76, pages
378–382, 1971.
[13] T. Fu, C.-N. Huang, and H. Chen. Identification of
extremist videos in online video sharing sites.
Intelligence and Security Informatics, IEEE, 2009.
[14] K. Grauman and T. Darrell. The pyramid match
kernel: Efficient learning with sets of features. JMLR,
2007.
[15] J. Hinojosa, L. Carretie, M. Valcarel,
C. Mendez-Bertolo, and M. Pozo. Electrophysiological
differences in the processing of affective information in
4This material is based upon work sponsored by the Defense
Advanced Projects Agency under the U.S. Army Research
Office Contract Number W911NF-12-C-0028, through IBM
Corporation subcontract 4914004308. Any opinions, find-
ings and conclusions or recommendations expressed in this
material are those of the author(s) and do not necessarily
reflect the position or policy of the U.S. Army Research Of-
fice, DARPA, DoD and IBM Corporation and no official
endorsement should be inferred.
words and pictures. Cognitive, Affective, and
Behavioral Neuroscience, 9 2009.
[16] P. Isola, J. Xiao, A. Torralba, and A. Oliva. What
makes an image memorable? CVPR, 2011.
[17] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev,
J. Long, R. Girshick, S. Guadarrama, and T. Darrell.
Caffe: Convolutional architecture for fast feature
embedding. arXiv, 2014.
[18] Y.-G. Jiang, G. Ye, S.-F. Chang, D. Ellis, and A. C.
Loui. Consumer video understanding: A benchmark
database and an evaluation of human and machine
performance. ICMR, 2011.
[19] D. Joshi, R. Datta, E. Fedorovskaya, Q. Luong,
J. Wang, J. Li, and J. Luo. Aesthetics and emotions in
images. Signal Processing Magazine, 2011.
[20] A. Kapoor and R. W. Picard. Multimodal affect
recognition in learning environments. MULTIMEDIA,
2005.
[21] S. Karayev and et al. Recognizing image style. In
BMVC, 2014.
[22] A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet
classification with deep convolutional neural networks.
In NIPS, 2012.
[23] J. Lafferty, A. McCallum, and F. Pereira. Conditional
random fields: Probabilistic models for segmenting
and labeling sequence data. ICML, 2001.
[24] J. Landis and G. Koch. The measurement of observer
agreement for categorical data. Biometrics. 33, 1,
pages 159–174, 1977.
[25] Y. LeCun and Y. Bengio. Convolutional networks for
images, speech, and time series. In The handbook of
brain theory and neural networks, 1995.
[26] K. Lee and D. P. W. Ellis. Audio-based semantic
concept classification for consumer video. IEEE
TASLP, 2010.
[27] D. Littman and K. Forbes. Recognizing emotions from
student speech in tutoring dialogues. ASRU, 2003.
[28] J. Machajdik and A. Hanbury. Affective image
classification using features inspired by psychology
and art theory. ACMMM, 2010.
[29] S. Mohammad, S. Kiritchenko, and X. Zhu.
Nrc-canada: Building the state-of-the-art in sentiment
analysis of tweets. SemEval, 2013.
[30] B. Pang and L. Lee. Opinion mining and sentiment
analysis. Foundations and trends in information
retrieval, 2008.
[31] S. Park, H. S. Shim, M. Chatterjee, K. Sagae, and
L.-P. Morency. Computational analysis of
persuasiveness in social multimedia: A novel dataset
and multimodal prediction approach. ICMI, 2014.
[32] E. Reid, J. Qin, Y. Zhou, G. Lai, M. Sageman,
G. Weimann, and H. Chen. Collecting and analyzing
the presence of terrorists on the web: A case study of
jihad websites. Intelligence and Security Informatics,
2005.
[33] B. Schuller and et. al. Avec 2011 -the first
international audio visual emotion challenge. ACII,
2011.
[34] W. Scott. Reliability of content analysis: The case of
nominal scale coding. Public Opinion Quarterly, 19,
pages 321–325, 1955.
[35] R. Socher, A. Perelygin, J. Y. Wu, J. Chuang, C. D.
Manning, A. Y. Ng, and C. Potts. Recursive deep
models for semantic compositionality over a sentiment
treebank. EMNLP, 2013.
[36] E. Stamatatos, N. Fakotakis, and G. Kokkinakis. Text
genre detection using common word frequencies. In
18th Conference on Computational Linguistics, 2000.
[37] Y.-L. Tian, T. Kanade, and J. F. Cohn. Facial
expression analysis. Handbook of face recognition,
2005.
