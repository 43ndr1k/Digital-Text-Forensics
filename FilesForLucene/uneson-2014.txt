50
When Errors Become the Rule: Twenty Years with
Transformation-Based Learning
MARCUS UNESON, Lund University
Transformation-based learning (TBL) is a machine learning method for, in particular, sequential classifi-
cation, invented by Eric Brill [Brill 1993b, 1995a]. It is widely used within computational linguistics and
natural language processing, but surprisingly little in other areas.
TBL is a simple yet flexible paradigm, which achieves competitive or even state-of-the-art performance
in several areas and does not overtrain easily. It is especially successful at catching local, fixed-distance
dependencies and seamlessly exploits information from heterogeneous discrete feature types. The learned
representation—an ordered list of transformation rules—is compact and efficient, with clear semantics.
Individual rules are interpretable and often meaningful to humans.
The present article offers a survey of the most important theoretical work on TBL, addressing a perceived
gap in the literature. Because the method should be useful also outside the world of computational linguis-
tics and natural language processing, a chief aim is to provide an informal but relatively comprehensive
introduction, readable also by people coming from other specialities.
Categories and Subject Descriptors: I.2.7 [Artificial Intelligence]: Natural Language Processing; I.2.4
[Artificial Intelligence]: Knowledge Representation Formalisms and Methods; I.5.4 [Pattern recogni-
tion]: Applications; J.5 [Arts and Humanities]: Linguistics
General Terms: Algorithms, Performance
Additional Key Words and Phrases: Transformation-based learning, error-driven rule learning, sequential
classification, computational linguistics, natural language processing, supervised learning, brill tagging
ACM Reference Format:
Marcus Uneson. 2014. When errors become the rule: Twenty years with transformation-based learning.
ACM Comput. Surv. 46, 4, Article 50 (March 2014), 51 pages.
DOI: http://dx.doi.org/10.1145/2534189
1. INTRODUCTION
1.1. Four Aspects of a Restaurant Conversation
Consider the following hypothetical fragment of a dialogue (Example 1), perhaps be-
tween a head waiter and a nervous, newly employed colleague in an overworked restau-
rant kitchen.
(1) —Replace the fork on table four.
—OK. Should I apologize for the wait?
—For now it’s enough to light the candle on the table.
It is a perfectly ordinary piece of language, English, in this particular case—indeed,
it may be ordinary enough to be uninteresting to most people. But let us assume that
Author’s address: Marcus Uneson, Centre for Languages and Literature, Lund University, Box 201, 221 00
Lund, Sweden; email: marcus.uneson@ling.lu.se.
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted
without fee provided that copies are not made or distributed for profit or commercial advantage and that
copies show this notice on the first page or initial screen of a display along with the full citation. Copyrights for
components of this work owned by others than ACM must be honored. Abstracting with credit is permitted.
To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this
work in other works requires prior specific permission and/or a fee. Permissions may be requested from
Publications Dept., ACM, Inc., 2 Penn Plaza, Suite 701, New York, NY 10121-0701 USA, fax +1 (212)
869-0481, or permissions@acm.org.
c© 2014 ACM 0360-0300/2014/03-ART50 $15.00
DOI: http://dx.doi.org/10.1145/2534189
ACM Computing Surveys, Vol. 46, No. 4, Article 50, Publication date: March 2014.
50:2 M. Uneson
we have some valid reason to study this sample—maybe we are engineers and want
to build some technical application that might receive it as input, or maybe we are
linguists and would like to investigate the language phenomena it exemplifies.
In any case, we probably have many more samples like it. We would like to describe
them all in some abstracted way, which highlights their similarities and differences
with regard to the aspect we currently happen to be interested in. For instance, in
Example 2, our domain of interest is the sequence of words, and to each element of
this sequence we wish to assign a part-of-speech, or POS—classes such as verb, noun,
preposition, and the like. We will use the notation w1/POS1 w2/POS2 . . . to indicate such
a classification (and generalize as needed).1
(2) —Replace/VB the/DT fork/NN on/IN table/NN four/CD ./.
—OK/JJ ./. Should/MD I/PN apologize/VBP for/IN the/DT wait/NN ?/.
—For/IN now/RB it/PP ’s/VBZ enough/JJ to/TO light/VB the/DET candle/NN
on/IN the/DT table/NN ./.
In another scenario (Example 3), we are more interested in the turns of the dialogue
itself than in the exact wordings of the utterances. In dialogue act tagging, we try
to label entire utterances by an abstracted representation of the speaker’s intentions:
GREET, INFORM, REQUEST, SUGGEST, REJECT, APOLOGIZE . . .
(3) —Replace the fork on table four./ REQUEST
—OK./ACCEPT
Should I apologize for the wait?/ YES-NO-QUESTION
—For now it’s enough/REJECT
to light the candle on the table./ REQUEST
Going from larger elements to very small ones, in Example 4, we instead want to
study how letters correspond to speech sounds (or, to use a posh term, their grapho-
phonemic relationships). Here, the data consist of an alignment of each letter to its
corresponding pronunciation (bottom row, in IPA2), with placeholders or groupings
when one-to-one-alignment is inappropriate. Similar subtasks often appear in speech
processing systems, but may also be useful for things like spelling correction or nor-
malization of names in search queries.3
1The parts-of-speech in this example are taken from the Penn Treebank tagset. So are their sometimes
inscrutable abbreviations – VB for verb, NN for noun, IN for preposition, etc. For the purposes of this article,
it is enough to think of such names as arbitrary, atomic labels; see http://www.cis.upenn.edu/∼treebank/ for
external definitions.
Admittedly, these labels bear little similarity to what one might have learned about word classes in fifth
grade. To remove any possible misunderstandings, parts-of-speech are not given by nature (this becomes
very clear when unrelated languages are compared). Instead, for practical purposes the set of allowable class
labels, the tagset, needs to be specified stipulatively. The Penn Treebank tagset (along with a few others) is
commonly used for English, but needs extensive modification to be useful even for closely related languages.
2IPA is the International Phonetic Alphabet, http://www.langsci.ucl.ac.uk/ipa/ipachart.html.
3This problem formulation, which employs single-letter correspondences, is chosen for illustration rather
than efficiency. It works well for many languages, but due to its very irregular orthography, English is not
one of them. Some unnaturalness in the mapping is unavoidable; here, we (rather arbitrarily) introduced
grouped phonemes as well as “empty” ones (denoted by _). However, the point of this section is to provide
a few introductory examples of sequential classification rather than solve graphophonemic representation
problems, so we will gloss over such details here.
ACM Computing Surveys, Vol. 46, No. 4, Article 50, Publication date: March 2014.
When Errors Become the Rule: Twenty Years with Transformation-Based Learning 50:3
(4)
In yet another setting (Example 5), we are interested in finding exactly what part of
a sentence is modified by some prepositional phrase (PP). For instance, in the example,
we would like to decide whether “on the table” says something about the activity of
lighting or about the candle that is being lighted. This is the problem of PP attachment:
in this case, attachment to the verb (light), or to its associated object noun phrase (the
candle). We index these with α and β, respectively, and indicate the corresponding
association by co-indexing the PP itself as appropriate. In the example, the latter
choice turns out to be the correct one; whereas, for instance, “for now it’s enough to
dance the rumba on the table,” it would have been the former.4
(5) —[Replace]α [the fork]β [on table four]PPβ .
—OK. Should I apologize for the wait?
—For now it’s enough to [light]α [the candle]β [on the table]PPβ .
1.2. Classification of Elements and Sequences
All of the tasks described are common, often needed (as preprocessing steps) in real-
world applications. They all involve classification: given a set of observations, each one
describable by some predefined characteristics (its features), the job is to assign each
observation to one out of a likewise predefined set of discrete classes. A more demanding
variant is probabilistic classification, where we need to return a probability distribution
over the entire set of classes (or, less ambitiously, a ranked list of the k most probable
ones).
What kind of knowledge sources do we have at our disposal, to inform such a classi-
fication? Well, if the observations are taken from a predefined set, then we might have
some a priori knowledge, irrespective of the dataset at hand. We might know what the
most common class for each element is, or we might even have a probability distribu-
tion over all possibilities. If there is no such predefined domain (or if there is one, but
it is not closed and thus not guaranteed to contain all new data), then we will sooner
or later encounter elements that we have never seen before. However, we might still
make an educated guess from a dynamic analysis of the features, which thus constitute
a second knowledge source.
Actually, in Examples 2–4, what we are given is not a set of observations, but a set
of sequences of observations. The classification of each element depends on its local
context: its neighbors (within some not-too-wide window) and their classifications.
Sequential classification tasks often appear when we deal with symbols ordered in time
4Most such ambiguities pass unnoticed by humans – we disambiguate on semantic grounds, usually without
even noticing that we did. But it is not difficult to come up with examples where also humans will be hesitant.
Consider the attachments of the preposition phrase in the following examples:
—I [tripped]α [the man]β [with my umbrella]PPα (?)
—I [tripped]α [the man]β [with the black umbrella]PPβ (?)
—I [tripped]α [the man]β [with the umbrella]PPα/PPβ (??)
ACM Computing Surveys, Vol. 46, No. 4, Article 50, Publication date: March 2014.
50:4 M. Uneson
or space, such as those present in human language. In such tasks, an additional third
knowledge source—by definition—is the sequential context: which are the neighbors of
the sample we are trying to classify, what are their features, and what is our (current)
idea of their classification?5
The example applications illustrate the varying importance of these knowledge
sources:
—In part-of-speech tagging, the domain is semi-closed: most words are likely to be
known beforehand, and we might well have them specified in a lexicon. Still, previ-
ously unseen words are certain to occur now and then in any real-world application,
and we are much helped by being able to make intelligent guesses from dynamic
feature analysis—for instance, guessing that staycation is a noun and defriend is a
verb.6 Generally, ambiguous words cannot be resolved without sequential context.
—In dialogue act tagging, the domain is truly infinite, and only seldom will we listen
to utterances that we have heard in their entirety before (when it does happen, it
is usually short phrases: single words, or word-like groups of words: yes, what’s up,
I don’t know). Thus, appropriate feature extraction is crucial. Sequential context is
clearly important—the answer to a SUGGEST is much more likely to be an instance of
ACCEPT or REJECT than GREET, no matter the phrasing.
—In finding letter-to-sound correspondences, we are unlikely to encounter any previ-
ously unseen letters. Thus, feature extraction is pointless—whatever we might wish
to use features for would better have been included elsewhere, as a priori knowl-
edge. The background knowledge specifies default correspondences, and sequential
context can (crucially, for many languages) be used to emend these.
—In PP attachment, the domain is again infinite, but, in contrast to the other examples,
sequential context has no influence: the fact that a PP was attached to the verb in the
previous sentence tells us nothing about the current one. Thus, intelligent feature
extraction is the single source of information.
Another interesting dimension along which these examples vary is the well-definedness
of the classifier range. In PP attachment, we generally have two answers to choose
from, and if we look at a wide enough context, exactly one of them is correct. In finding
letter-to-sound correspondences, we may argue about the best alignment, but there is
5In pattern recognition, individual samples to be classified are often assumed to be independent and iden-
tically distributed (iid). As we mention, this is not a good fit in all cases; for instance, in many tasks where
time is a dimension (explicit or implicit), elements will then often depend on their context – their closest
neighbors in a sequence.
Rephrasing the element-wise iid assumption for sequential classification can be done along at least two
different axes. One redefines the task itself, so that classification applies to entire sequences, iid among
themselves. The dependencies of individual elements on their context are then hardcoded and implicit in
the representation. Another approach retains the view of classification as something done to individual ele-
ments, and explicitly encodes in the representation any necessary information on context (say, as pointers
to neighboring elements).
The first choice is simple and good enough for most practical linguistic tasks (if we accept the idea that
some sequences may hold only a single element, as in Example 5, and that some sequences are just arbitrary,
imposed serializations of hierarchical structures, as in Figure 13).
In this paper, however, we try not to make silent assumptions on the data, and we thus prefer the second
view on neighborhood. That is, we wish TBL to classify elements which come in isolation, with no neighbors;
or in sequences, with neighbors to the left and/or to the right. But, even if we do not have any specific real-life
examples in mind, we would also like to be able to use TBL on elements which come in n-dimensional grids,
with 2*n neighbors; or indeed in arbitrary graphs, where these can be considered as fixed and given by the
problem specification.
We will still stick to the term “sequential classification”, since it is so commonly used and well describes
(almost) all of our examples. However, for covering the more general cases, if and when the day comes,
something like “structured classification” or “structured prediction” might arguably fit the bill better.
6New entries in the Oxford English Dictionary 2010.
ACM Computing Surveys, Vol. 46, No. 4, Article 50, Publication date: March 2014.
When Errors Become the Rule: Twenty Years with Transformation-Based Learning 50:5
usually reasonable agreement on the lexical pronunciation (at least if we consider
some reference variety of the target language). POS tagging is trickier: it is only
meaningful with respect to some stipulatively defined tagset, specific to a language
and sometimes also to a certain dataset.7 Dialogue act tagging, finally, is less studied
and understood; thus, it has the characteristics of POS tagging to an even higher
extent, with tagsets depending also on domain or setting. As can be expected, human
interannotator agreement for the four tasks decreases in the order given.
1.3. Today’s Meal: Classification by Transformation
The sequences from the restaurant conversation presented earlier were all short. On
the other hand, we may have many of them—thousands, millions, or billions—and we
certainly want a computer to help us with the classification. One way to automate
the chore is to implement a classifier as a set of manually specified rules. For some
combinations of task and data, this is actually the best solution. Restricting the data
type to natural language for the sake of discussion, it is easy enough to write a letter-
to-sound converter for Finnish, Spanish, or Turkish by enumerating the few necessary
rules in a page or two. Much more substantial human effort was invested into the
thousands of rules of the EngCG POS tagger [Karlsson et al. 1995], for a long time one
of the best part-of-speech taggers for English.8
For most instances of sequential classification, including those illustrated in
Section 1.1, this approach is simply infeasible: there are too many and too weak de-
pendencies, and it is far too laborious to try to specify them by hand. Instead, we may
choose among many reasonable machine learning approaches: decision trees (DTs), hid-
den Markov models, neural networks, maximum entropy models, and memory-based
learning, to mention just a few. These are all well-known techniques in the machine
learning community and certainly good choices in many situations. Their main draw-
back for the tasks we are interested in is the opacity of the learned representation. For
the mentioned techniques, learning amounts to filling a black, inscrutable box with
estimated parameters. The main exception is DTs, which do slightly better: they give
us a somewhat interpretable tree with if. . .else questions at every node. Nevertheless,
the questions tend to be overwhelmingly many for real-world tasks.
The focus of the present survey is on yet another machine learning method:
transformation-based learning (TBL). It was invented 20 years ago by Eric Brill [1993b,
1995a] and has been refined by him and many others since. In terms of the techniques
mentioned, TBL is a hybrid: its representation involves rules, or transformations, but
these are learned automatically from the training data. Rules are iteratively created
and evaluated based on how well they deal with the current set of errors in the data;
hence, the approach is often termed error-driven.
TBL is typically used as a supervised machine learning technique for classification
of sequences, where each element is represented as a symbolic feature vector and as-
signed a single symbolic value in the classification. Interpretability of representation
is but one out of several properties that make the method appetizing for applications
involving natural language; some others are the natural ease with which it handles
local (especially fixed-width) dependencies, its resistance to overtraining, and its gen-
eral flexibility and adaptability to different tasks. In the next section, we substantiate
these claims further.
The TBL method itself, however, does not (and good implementations should not)
make any assumptions that are valid only for natural language classification tasks. To
7Of course, such tagsets are not created in a vacuum; they build on each other and, for a given language,
differences between them partly reflect the number of subdivisions made. Thus, a larger set can often be
converted to a smaller with relative ease.
8EngCG was later augmented by automatically derived rules. See also Section 4.2.2.
ACM Computing Surveys, Vol. 46, No. 4, Article 50, Publication date: March 2014.
50:6 M. Uneson
be sure, natural language abounds with ambiguities to pit machine learning methods
against, TBL or others; and it also abounds with weak, mostly local, and incompletely
understood dependencies that these methods may exploit. But life offers many exam-
ples of symbols ordered in time or space, depending on their neighbors to a greater
or lesser degree—just to mention a few, DNA and protein sequences, musical notes,
and suns and clouds in weather forecasts. Furthermore, the problem characteristics
described are typical, not mandatory—with the appropriate problem encoding, it is per-
fectly possible to apply TBL to almost any supervised classification task. In addition,
many extensions have been suggested, some of which are specifically aimed at enhanc-
ing the method’s expressive power. For instance, TBL can be rebuilt into a regressor
(with real-valued output) or a probabilistic classifier (with a probability distribution
over the entire set of classes as output); see Section 4.
With this article, our aim is to provide a self-contained but still relatively compre-
hensive introduction to TBL. It is intended to be readable without much acquaintance
with either specialized linguistic terminology or the toolbox of a computational lin-
guist. Linguistic terminology cannot be entirely avoided, however: to date, TBL has
been applied almost exclusively to natural language data, and most citations and ex-
amples must necessarily be drawn from that area. Where deemed necessary, we have
tried to explain nonelementary concepts in a phrase or two in the body text; some-
times we also provide more extensive but less crucial comments in footnotes. This is
hopefully enough to illustrate the inputs and outputs of a certain problem, but it is
almost certainly not enough to convey the rationales behind posing it in the first place.
Furthermore, in some cases, where exact understanding of the terminology might not
be needed for the understanding of the algorithmic aspects, we found that further de-
tours added more clutter than clarity. When explanations given here are insufficient,
we refer to some dedicated textbook in computational linguistics, for instance Jurafsky
and Martin [2008].
The article is organized as follows. By way of introduction, in the practically oriented
Section 2, we review the original algorithm proposed by Brill [1993b, 1995a], with an
eye on its main design choices and inherent strong points. This tutorial-styled section
contains many forward references to later, more detailed descriptions, but is otherwise
intended to be readable in isolation.
The rest of the article is a survey of the most important TBL literature. Roughly,
this set can be divided into a handful of publications at least partly dealing with TBL
from a theoretical perspective, a few dozen that develop or extend the method along
different dimensions, and a larger number that report on the application of the method
to new domains (possibly requiring clever problem encodings or otherwise noteworthy
recasting considerations).9
This classification also forms the basis for the organization of the survey. The the-
oretical descriptions of the original TBL method are summarized in Section 3: its
relation to its cousin DTs and its closer relatives decision lists and, in particular, the
less well-known decision pylons [Bahl et al. 1989]. Section 4 provides an overview of
the most important developments of the original paradigm, aimed at augmenting it in
different directions. We review attempts to extend the range of possible inputs and the
expressivity and quality of the output, to relax the amount of supervision needed, to
improve efficiency, and to ease the problem description. Section 5 contains an overview,
necessarily brief, of TBL uses in practical applications—many and varied, but, as men-
tioned, almost exclusively within the realm of computational linguistics and its closest
9There are, of course, many more papers that simply apply TBL to a known problem or report on comparisons
with other systems for some particular task or dataset. These largely fall outside the scope of the article, but
many are found in the TBL bibliography (see the online Appendix).
ACM Computing Surveys, Vol. 46, No. 4, Article 50, Publication date: March 2014.
When Errors Become the Rule: Twenty Years with Transformation-Based Learning 50:7
Fig. 1. A barnyard scene, for transformation-based painters. From Samuel [1998b], reprinted with permis-
sion.
neighbors. Section 6 concludes the article and hints at some possible future directions.
An Appendix is dedicated to practical TBL resources, including brief presentations of
existing TBL implementations.
2. PLAIN VANILLA TRANSFORMATION-BASED LEARNING
2.1. A Painting Analogy
A useful TBL picture-painting analogy is offered by Samuel [1998b], who attributes it
to Terry Harvey. It generates several of the right intuitions, so we will retell it here
(slightly adapted).
Consider the barnyard picture in Figure 1 (where, for simplicity, colors have been
named rather than rendered). A painter comes by. As it happens, he is actually a
transformation-based painter, which is mostly like any other painter, except he does
not ever want to change from a smaller brush to a larger. He is also more-than-average
cavalier about making mistakes, claiming that they can always be fixed later.
Our painter finds the barnyard picture and decides to reproduce it on his own canvas,
as follows. First, he looks at the current state of his painting (a blank canvas) and
compares it to the target, or truth, represented by the figure. He notes that the most
efficient way to reduce the difference between his painting and the truth is to take the
largest brush he has and paint the entire canvas blue. When the paint has dried, he
again compares the current state of his painting to the truth. This time, he finds that
the easiest way to increase the similarity to the target is to take a slightly smaller
brush and paint the filled outlines of a red barn. There is no need to worry about
non-red details, such as windows, doors, and roof, as these will be taken care of in later
stages, by smaller brushes.
And so our painter goes on. With each change of color, he picks a finer brush and uses
it with increasingly thin and precise strokes. Coarser brushes, used early on, cover a
large part of the picture—they add a lot of paint, but they also make many mistakes.
With later, thinner brushes, less paint will be added, but also fewer errors. The final
step might be to fill in the fine black lines with a very fine brush.
The main point of the analogy is that the painter uses a sequence of color-brush
pairs, in descending order according to how much paint they add to the canvas. Each
point of the canvas may be repainted several times; although we can be convinced that
the overall result looks increasingly like the target with each application of a brush, we
cannot be sure about the color of a specific point until all brushes have been applied.
ACM Computing Surveys, Vol. 46, No. 4, Article 50, Publication date: March 2014.
50:8 M. Uneson
Fig. 2. Data flow of TBL training, Brill’s original algorithm. Main loop in heavier stroke.
2.2. Algorithmic Overview
Transformation-based learning works in much the same way as transformation-based
painting. The method produces a sequence of rules, or transformations, ordered after
impact. Early rules are very general and may change classifications on large fractions
of the data, usually committing errors in so doing. Subsequent rules are more specific
and may correct errors introduced by earlier ones. A single transformation rule has the
general form
if CONDITION (x) then do ACTION (x)
where x is a data sample; CONDITION, sometimes referred to as “context,” is a predicate
(i.e., a boolean-valued function) on attributes of x and/or its local context; and ACTION
changes some attribute of x. The rules are induced automatically from the training
data. The actual structure of CONDITION and ACTION is delineated by user-specified
patterns known as templates; thus, templates define the transformation space.
The flow of data in the original TBL training algorithm (as presented in the seminal
works Brill [1993b, 1995a]) is shown in Figure 2, and pseudocode for a typical imple-
mentation including a few typical optimizations is given in Algorithm 1. The output of
the learning algorithm is an ordered sequence of transformation rules. New data (once
they have been initialized in the same way as the training data, see Sections 2.2.3 and
2.3.1) can now be classified by applying this sequence of learned transformations to it.
In the following, we look at the datasets, at the templates, and at the flow of control.
Finally, we return to some critical points where design choices lurk.
2.2.1. Corpora. Like most machine learning methods, TBL takes as its point of depar-
ture a dataset of a certain size. For applications dealing with natural language, such a
dataset is usually referred to as a corpus—indeed, corpora are the bread and butter of
computational linguistics.10
10A corpus, pl. corpora, is basically a sizable collection of real-world natural language data—text or speech
or something more exotic, such as video recordings of sign language. We use that term because it is the
most common for the tasks TBL has been applied to. However, it was not created by any transformation-
based deities, and the reader should feel free to replace it with “big dataset” at any time. For nonlinguistic
applications, this may roll more smoothly off the tongue.
ACM Computing Surveys, Vol. 46, No. 4, Article 50, Publication date: March 2014.
When Errors Become the Rule: Twenty Years with Transformation-Based Learning 50:9
ALGORITHM 1: “Original” TBL. Algorithm 3.3 of Florian [2002b], adapted and reprinted with
permission.
Input: set of templates T , training corpus D, score threshold θ
Output: an ordered list L of transformation rules
/* baselinetag(x) computes an initial classification of x (Sections 2.2.3 and 2.3.1) */
/* C[x] returns the current classification of sample x (or sets it) */
/* D[x] returns the true classification of sample x */
(1) foreach x ∈ D : C[x] ← baselinetag(x)
(2) i ← 0; L ← [] /*iteration 0: L is empty list*/
(3) Repeat:
(a) Ri ← ∅
(b) foreach x ∈ D such that C[x] = D[x]:
i. generate all correcting rules R(x) from the templates
ii. foreach r ∈ R(x) : increment posD(r)
iii. Ri ← Ri ∪ R(x)
(c) foreach r ∈ Ri , sorted in descending order of posD(r):
i. for all x ∈ D such that C[x] = D[x] ∧ C[r(x)] = D[x] :
A. update negD(r)
B. if scoreD(r) < scoreD(b): stop evaluating rule r (continue loop)
ii. if posD(r) < scoreD(b): stop evaluating rules Ri (break loop)
iii. if scoreD(b) < scoreD(r) : b ← r
(d) if scoreD(b) ≤ θ : return L and exit
(4) Append b to L; i ← i + 1; goto (3)
In the TBL case, this dataset is assumed to come with reference classifications, mak-
ing it a reference corpus. Thus, TBL is a supervised method: it depends on the existence
of some annotations that can be taken as truth. The annotations are usually provided
(or at least proof-read) by humans. Manually annotated corpora thus represent large
investments, sometimes enormous, and creating them from scratch for a single project
is rarely an option.
A corpus can generally be thought of as a set of items that are independent of each
other, by nature or by assumption, with respect to the task at hand. For instance, in POS
tagging (Example 2), we can be reasonably sure that the POS of the words in the current
sentence does not depend on those in the previous one. In dialogue act tagging, each
utterance (Example 3) is clearly dependent on the previous one, but the dialogue acts in
one conversation are likely independent of those in another. Partitioning the corpus into
a set of subsets for which we can assume mutual independence is highly beneficial, for
the quality of the learned representation as well as the efficiency with which it can be
learned. To tag POS, we will want our corpus split into sentence-sized chunks. For the
case of dialogue acts, the corpus will hopefully contain some delimiters distinguishing
individual dialogues (this case also illustrates that the need of relevant and reliable
annotations may go beyond individual samples). The individual classifications are
often called tags (irrespective of their being true or not, and irrespective of the task at
hand).11 In the TBL case, we will often speak of the training corpus, which is essentially
the reference corpus with the annotations deleted.12
11Somewhat confusingly, the term tag may sometimes be short for part-of-speech. However, in this article,
we will treat the latter as a special case of the former.
12Here and in Figure 2, we make the distinction between the two to emphasize the conceptual “read-only”
nature of the reference corpus; by contrast, the training corpus can conceivably be thought of as mutable
data. Some authors may use the term “training corpus” for both senses.
ACM Computing Surveys, Vol. 46, No. 4, Article 50, Publication date: March 2014.
50:10 M. Uneson
Table I. Sample Templates for POS Tagging by TBL, Original Phrasing, in μ-TBL Syntax and as Prose
Template Change A to B whenever . . .
pos:A>B <- pos:C@[-1] . . .the preceding word has pos C
pos:A>B <- pos:C@[2] . . .the word two after has pos C
pos:A>B <- pos:C@[1,2] . . .one of the two following words has pos C
pos:A>B <- wd:C@[0] . . .the current word is C
pos:A>B <- wd:C@[-1] & pos:D@[1] . . .the preceding word is C and the following has pos D
2.2.2. Templates. A TBL problem specification uses templates to describe the allowable
space of transformations, and this is the main way of encoding any a priori ideas we
may have: domain and expert knowledge, constraints, and assumptions. For many
tasks, the templates are a natural, transparent, and compact way of specifying such
assumptions.
A few sample templates for POS tagging are shown in Table I.13 As an example,
if we rephrase the top one in prose we might get “change value of feature ‘pos’ from
A to B, whenever the feature ‘pos’ one step to the left has value C.” A, B, and C
are variables implicitly ranging over the domain of their respective features—over all
parts-of-speech, in this case.
Templates are a core component of TBL systems. From the perspective of machine-
learning theory, the template specification carries (a large part of) the inductive bias
[Mitchell 1997] of TBL. For instance, if we only use the single template just described,
we are actually disregarding all dependencies of neighbors except the one immediately
to the left (which clearly is a strong and simplistic assumption). Similarly, if we wish
to stipulate that all decisions be made only from left context, perhaps because the
system is to be used in real-time word-for-word processing, then this assumption can
be enforced by choosing the appropriate templates.
The templates in Table I are a subset of the 26 proposed in Brill [1995a]. In real-world
scenarios, depending on the task, this number may be a magnitude less [Brill 1995b] or
greater [Carberry et al. 2001]. A large number of templates may be difficult or tedious
to specify by hand, especially for domains we may not understand completely. We will
later see several developments addressing this and other template-related problems
(see Sections 2.4.4, 4.1.2, 4.1.3, and 4.6).
2.2.3. Flow of Control. With these pieces of declarative knowledge in place, the control
flow is as follows (Figure 2, Algorithm 1). In an initialization step, all samples of the
training corpus are classified (annotated, tagged) according to some simple baseline
algorithm, perhaps giving each word its most common tag according to a lexicon or
database we have, or which we extract from the reference corpus. The result is prelim-
inarily annotated data, the current corpus ci = c0.
In the main loop of the algorithm (heavier stroke in Figure 2, item 3 in Algorithm 1),
the current corpus ci is compared to the truth, probably uncovering some errors. For
each error, we use the templates to derive rules that will correct it. Conceptually, each
of the rules derived is then scored: the rule, call it r, is tentatively applied to (a fresh
copy of) the current corpus. In reality, there are several optimizations to reduce the
number of scorings, some of which can be seen in Algorithm 1; and there are several
later, alternative algorithms that we will meet in Section 4.3. The result is compared
to the truth, which yields some number of corrected errors (good applications, gr) and
13The syntax used for example templates and rules, here and throughout this article, is borrowed from
the μ-TBL system [Lager 1999b]. The templates with prose descriptions in Table I and the derived rules
in Table IV should be self-explaining and varied enough to exemplify all constructions we will meet, but
otherwise manuals are available at http://www.ling.gu.se/∼lager/Mutbl/manuals.html.
ACM Computing Surveys, Vol. 46, No. 4, Article 50, Publication date: March 2014.
When Errors Become the Rule: Twenty Years with Transformation-Based Learning 50:11
Table II. Sample Templates for POS Tagging by TBL
As in Table I but generalized: the precondition of the current tag is moved from ACTION to CONDITION.
Template (ACTION <- CONDITION) Change the current pos into B whenever . . .
pos:>B <- pos:A@[0] & pos:C@[-1] . . .the current word has pos A and the preceding has pos C
pos:>B <- pos:A@[0] & pos:C@[2] . . .the current word has pos A and the word two after has pos C
pos:>B <- pos:A@[0] & pos:C@[1,2] . . .the current word has pos A and one of the two following has pos C
pos:>B <- pos:A@[-1] . . .the preceding word has pos A
pos:>B <- wd:W@[0] . . .the current word is W
some number of newly introduced errors (bad applications, br). The score of the rule fr
is normally a function of gr and br—often simply f (gr, br) = gr − br.
One of the rules b receives the highest score (with respect to f ). It is selected, added
to the list of learned rules, and applied to ci, returning the next current corpus ci+1.
The main loop repeats until some termination criterion is fulfilled—for instance, when
there are no more rules with a score above a predefined threshold.
2.3. Notes on Design Choices
In this section, we take the same stroll again, but with more attention to details
previously left out. Unfortunately, these are often incompletely specified in system
descriptions.
2.3.1. Baseline Annotation. The initial baseline annotation can actually be even simpler
than suggested: TBL does not really care where the first current corpus comes from.
Thus, we could assign random tags or the most common tag overall to all the samples,
or even just a placeholder.
If we do, we deliberately avoid incorporating some useful information, and we may
have to pay with somewhat lower performance (at the very least, we will need more
rules, and they will take longer to learn). However, as we illustrate later, our main
interest sometimes is knowledge rather than performance: the rules themselves. If
so, we might prefer that rules encode everything we are able to induce from data,
not just what we can add to some task-and-language-specific performing baseline,
however simple.14 Dumber baselines may form a background against which the learned
knowledge more clearly stands out.
On the other end of the scale, the initial annotation might well be sophisticated,
perhaps the output of another classifier. In this case, TBL acts as a postprocessing
step, specialized in correcting the errors of others.
2.3.2. Template Format. The templates suggested by Brill and repeated in most descrip-
tions of TBL have the form “change A to B whenever condition C holds,” as exemplified
in Table I. For some tasks, this phrasing facilitates a certain optimization of the train-
ing process (most useful for POS tagging of English and in any case obsoleted by
later developments; see Section 4.3.1). A more expressive formulation of the templates
[Samuel 1998b; Ngai and Florian 2001a] moves the tag=A part from the ACTION to
the CONDITION. Table II rephrases the first three templates of Table I in this way. In
addition, it gives two templates to learn useful rules such as “change any tag into B
when preceded by tag A” or “change any tag into B for word W,” which were not possible
to express in the original formalism.
2.3.3. Rule Candidate Generation. The generation of rule candidates is generally best
driven by examining the E existing errors in the N-sized corpus and using the |T |
templates to derive rules that correct them. The O(E|T |) rules thus derived are known
14For a comparison, the very simple baseline for POS tagging previously described—just assign each word
its most common tag—often reaches 90% accuracy for inflection-poor languages such as English.
ACM Computing Surveys, Vol. 46, No. 4, Article 50, Publication date: March 2014.
50:12 M. Uneson
Table III. Handling Templates with Out-of-Bound (OOB) Accesses
Five example strategies. From the perspective of a template, a sequence w1 . . . wn may behave (a) as if surrounded
by nothing, or (b) as if preceded and followed by an infinite number of OOB tokens (here ), or (c) as if it had a
special left boundary marker (here 
) at position 0 and a special right boundary marker (here ) at position n + 1,
or (d) a combination of b and c, or (e) as if surrounded by an infinite number of position-unique tokens (here 
,


, etc.; , , etc.) in both directions. See also Section 2.3.3.
. . . − 2 − 1 0 1 . . . n n + 1 n + 2 . . . Position/Strategy
. . . ∅ ∅ ∅ w1 . . . wn ∅ ∅ . . . a) Template not applicable OOB
. . .    w1 . . . wn   . . . b) Single OOB token
. . . ∅ ∅ 
 w1 . . . wn  ∅ . . . c) Boundary markers only
. . .   
 w1 . . . wn   . . . d) Boundary markers w/ OOB token
. . . 


 

 
 w1 . . . wn   . . . e) Position-unique token
to be at least somewhat helpful: no time will be wasted with rules that will never
correct any errors, or (worse) rules that will never be triggered by the training data at
all.15
We return to training efficiency issues (Section 4.3.1); here, we only note that most
rules that were candidates for the current corpus ci also will be so for the next ci+1.
Thus, much of what we record about the rules could be recorded once and then cached
and minimally updated between iterations. This observation underlies several of the
faster techniques we will see later.
A point of ambiguity in deriving rules, unfortunately not often specified in the de-
scription of practical implementations, is how to handle templates that refer to nonex-
isting positions in the sequence. For instance, consider the following single-sequence,
two-sample corpus, while again learning from the tag:A>B <- tag:C@[-1] template:
(6) truth a b
current corpus x b
One answer is to stipulate that the template does not apply if it refers to any posi-
tion outside the sequence at hand; in the example, learning would thus immediately
terminate. Another way, suggested by Curran and Wong [1999] but probably used
by many, is to extend the vocabulary with special tokens—perhaps a single special
symbol for any access outside the bounds, or just one marker for the left boundary
and one for the right, or a combination of the two, or a unique token for each position
where access was attempted. Table III spells out these possibilities; there are many
variations. Any of them except the first would allow us to learn a rule that corrects the
last error in Example 6, for instance, as tag:x>a <- 
@[-1]. For a corpus of mostly
long sequences—or for a corpus in which we only have a single sequence, perhaps
because we haven’t bothered to split the data into mutually independent sequences
in the first place—the difference is negligible. With many short ones, as is common in
natural language processing, it may be of importance.
For certain applications, we might have a particular interest in high-accuracy rules.
An often-used approach is to compute the accuracy of a rule candidate, for a typical
classification task simply acc(gr, br) = gr/(gr + br), and provide an accuracy threshold
a, 0.5 < a ≤ 1.0, which the rule must surpass to be further considered.
2.3.4. Rule Scoring. The scoring of a rule candidate r employs some user-defined idea
of goodness f . Ideally (and, for TBL, usually), this measure corresponds directly to the
final evaluation function e by which the system is ultimately to be judged. For instance,
in many classification tasks, an obvious candidate for e is simply the accuracy—the
15Conceptually, though, we note that we could arrive at the same set of good candidates and innumerably
many more useless ones without looking at the data, by blindly instantiating all templates with all possible
values of the (discrete) feature domains.
ACM Computing Surveys, Vol. 46, No. 4, Article 50, Publication date: March 2014.
When Errors Become the Rule: Twenty Years with Transformation-Based Learning 50:13
percentage of correct classifications. For those cases, f can be a simple function of
the number of good gr and bad br changes that r will bring about if applied. The
straightforward f (g, b) = g−b given earlier is commonly used for f . We note, however,
that, as far as the basic TBL algorithm is concerned, any f that fulfills f (g, b) >
0 iff g > b is good enough. Put into words, the only requirement is that any rule
with a positive score will decrease the total error count (and thus the algorithm must
terminate), and any rule that decreases the total error count will have a positive score
(and thus all positive rules may be learned). With this observation, it is conceivable
for f to introduce a bias; for instance, we might simply generalize the previous scoring
function to fα(g, b) = gα − bα. With α = 1, we retrieve the original function. With
α < 1, the scoring function will reward high-accuracy rules (for instance, f0.9(100, 10) >
f (200, 100)). Similarly (but probably less useful), with α > 1, it will reward rules with
large impact on the corpus (for instance, f1.1(200, 100) > f (120, 10)). So far, we have
ignored the number of neutral applications nr, where r just changes one error into
another. It is also mostly ignored in the literature or found to be of little importance
when mentioned. For example, in experiments described by Lager [1999b], the rules
learned by the two different scoring functions fα=1 = gr −br and f ′α=1 = gr −br −nr were
not significantly different. Nevertheless, the task at hand may dictate valid reasons for
letting f depend also on nr; perhaps the main interest lies with the rules learned, rather
than in the number of reduced errors, and we prefer low-impact rules to high-impact
ones with the same or even higher gr − br.16
Some tasks do not use accuracy but rather slightly more involved forms of e. This
is true in particular when classification is not the goal in itself but rather an artifact
of the particular problem encoding (cf. the discussion and the encoding examples in
Section 5.1). For example, in named entity recognition, the goal is to identify substrings
of words referring to specific locations, people, places, organizations, and the like.
Here, evaluation is based on the measures precision and recall, well-known in pattern
recognition and information retrieval. The two measures are often combined into their
harmonic mean, the F1 measure.17 Similarly, in parsing, where the goal is to assign
a tree structure to an input sentence, there are a large number of ways to evaluate
system output against the gold standard, as defined by human annotators [Carroll
et al. 1998].
Of course, more generally speaking, any scoring function that reflects our ideas of the
task at hand by quantifying the gain of applying a candidate rule could be used. Such a
function could well take additional inputs—say, sequence length, current correctness,
or estimated classification probability (cf. Section 4.2.1). For instance, we might be
more interested in maximizing the number of correctly tagged sequences rather than
the number of correctly tagged sequence elements. If so, we will weight rule applications
in almost-correct sequences, whether they correct or introduce errors, differently from
those in sequences with more errors.
More elaborate scoring schemes seem to be little explored (but see Section 4.2.3). One
should note that some of the faster training algorithms we meet later (Section 4.3.1)
assume simple scoring rules and will not work with more complex variants. More impor-
tantly, the scoring function is used not only to rank the rules, but also in the stopping
criterion; thus, with exotic scorings the user may need to assume the responsibility
16It should be noted that most reports on the empirical behavior of TBL describe the special case of POS
tagging on English, which has several properties not necessarily shared by other tasks (such as initial
accuracy on the order of 90%, a tagset size on the order of 100, and few dependencies on distance larger than
2 or 3). Other questions, perhaps yet unasked, may have 1% or 99% as initial accuracy, tagset sizes of 2 or
2,000, and much wider sequential dependencies.
17http://en.wikipedia.org/wiki/F1_score.
ACM Computing Surveys, Vol. 46, No. 4, Article 50, Publication date: March 2014.
50:14 M. Uneson
of defining a terminating process. If a user-defined scoring function cannot be used
directly, a simple way to do so is to exploit the fact that the rules are ranked af-
ter their estimated usefulness and just predefine some maximum number of rules
learned.
See also the discussion on scoring in a multidimensional learning setting,
Section 4.1.1.
2.3.5. Rule Selection. The selection of the best rule uses greedy search: whenever a
choice needs to be made, the locally optimal one (here, the highest-scoring rule) is
picked, without worrying about its impact on future choices. This simplification rep-
resents a major pruning of the search space, which is indeed immense. Somewhat
simplifying an example from Curran and Wong [2000], if we consider only rules where
conditions and actions read the same attribute and conditions all refer to all positions
in a fixed window of width C, then with a tag vocabulary of size |Vt| and templates of
the type exemplified in Table II, we get |Vt|C+1 different transformation candidates for
each rule. Learning P of these in the optimal order involves P! |Vt|(C+1)P possibilities.
Pruning is clearly necessary for other than toy-sized examples.
The highest scoring rule is not guaranteed to be the optimal one, however. It is not
difficult to find problem instances in which greedy rule selection will fail to produce the
globally best solution. For instance, consider the single-sequence, six-sample corpus of
Example 7 (for clarity, with only attribute “tag” shown) and learning with the same
single template as before (tag:A>B <- tag:C@[-1]):
(7) truth a b a c a d
current corpus a b d b d d
In this case, the greedy algorithm will learn the single rule d>a <- tag:b@[-1], which
will correct two out of the three errors and then terminate. The optimal solution,
however, is to start with b>c <- tag:d@[-1], which only corrects a single error but
allows two other rules, d>a <- tag:b@[-1] and d>a <- tag:c@[-1] (in any order), to
take care of the remaining two errors.
The top-scoring rule thus only approximates the optimal one. However, it is at least
likely to be reasonable (more so if the scores are summed from many and short se-
quences, rather than few and long ones). In practice, the greedy approach seems to
work well over a wide range of applications, and apparently (probably due to the
already problematic training times) nothing else has been tried. We also note that,
although a complete scrambling of the learned rule list certainly will hurt, TBL is not
generally very sensitive to minor reorderings (for the common case study of English
POS tagging, see Curran and Wong [2000]). Clearly, rules that do not alter each others’
context are independent and can be reordered without consequence. Generally speak-
ing, later rules are more often independent, if nothing else because they have fewer
application sites and apply in more specific conditions. Similarly, a better baseline will
produce fewer and more independent rules.
2.3.6. Rule Application. The application of the best rule, once it is selected, could happen
in several ways, and, as pointed out in Brill [1995a], the difference is crucial for rules
in which the action happens to influence the condition. Either we can first identify
all the places where the rule is to fire and then apply it to them all at once (delayed
application), or we may allow earlier changes to influence later: first check the first
position to see if the condition holds, apply the rule if it does, and repeat at the next
position (immediate application). In the latter case, there is no particular reason why
first and next should be taken in left-to-right order: we could just as well start from
ACM Computing Surveys, Vol. 46, No. 4, Article 50, Publication date: March 2014.
When Errors Become the Rule: Twenty Years with Transformation-Based Learning 50:15
the right (or, to be sure, in any other of the N! ways, like left-to-right but odd positions
before even; but let us restrict discussion to the less far-fetched options).
Brill [1995a] points out the differences but takes no obvious stand. In our view, the
intelligibility and declarativity of the rule representation may suffer badly with im-
mediate application. For instance, modifying an example of Brill’s, suppose we have
the rule tag:a>b <- tag:a@[-1] & tag:a@[1] and a nine-sample corpus, a single se-
quence of nine “a” tags. After applying the rule in the three different ways, we get
a a a a a a a a a (current corpus)
a b b b b b b b a (delayed application)
a b a b a b a b a (immediate application, left-to-right)
a b a b a b a b a (immediate application, right-to-left)
Now consider the application of the same rule, but to a single-sequence, ten-sample
corpus instead:
a a a a a a a a a a (current corpus)
a b b b b b b b b a (delayed application)
a b a b a b a b a a (immediate application, left-to-right)
a a b a b a b a b a (immediate application, right-to-left)
That is, if our corpus contains an odd number of samples, we get 100% coincidence
between left-to-right and right-to-left application of the same rule; but if it contains
an even number, we get 0%. This is hardly the way to achieve clear, declarative rule
semantics. In our view, immediate application is a bad idea for much the same rea-
son that self-modifying code is, and delayed application is the only option if we are
interested in interpretability of the induced knowledge.
2.3.7. Stopping Condition. The stopping condition is normally a score threshold; when
there are no more rules reaching this threshold, training is terminated. However, since
the rules are ordered in terms of impact, we will get a meaningful result also if training
is interrupted after, say, k rules or h hours. A well-chosen score threshold will minimize
learning of spurious rules (and training time) without compromising performance. See
also the discussion in Section 2.4.4.
2.4. TBL Strong Points
Albeit simple, the variant of TBL as we have seen so far exhibits several desirable
properties. Here, we expand on these. The list is roughly ordered by how unusual the
respective feature is in the general machine learning menagerie.
The shortcomings of original TBL and some attempts to remedy them is the topic of
Section 4, as well as extensions to ease its restrictions on input and output.
2.4.1. Interpretability of Learned Representation. As already hinted at, sometimes our main
interest may lie in the learned representation itself: the declarative knowledge that a
classifier induces rather than its actual performance when this knowledge is applied.
Statistical representations, essentially black boxes of numbers, are generally not very
informative in this respect.
By contrast, the interpretability of the representation is high for rule learning al-
gorithms and even more so if they can provide some kind of relevance ranking of
the learned rules. TBL does this very efficiently by outputting its rules ordered after
expected impact.
We are not, of course, claiming that interpretability amounts to cognitive or psycho-
logical validity; whatever human processes are employed in sequential classification,
they are unlikely to employ hundreds of rules (or, even more unlikely, millions of
ACM Computing Surveys, Vol. 46, No. 4, Article 50, Publication date: March 2014.
50:16 M. Uneson
Table IV. Sample POS Tagging Transformations
The first few transformations learned from English text (60,000 words of financial news from Wall Street Journal),
with example phrases from the same source. Templates as in Brill [1995a]. For clarity, the examples only show the
tags matching the corresponding rule. Tags appearing: VBP: verb, present, not 3rd person singular; VB: verb base
form (infinitive); MD: modal verb (e.g., can, will , should, may); TO: the word to; NNS: plural noun; NN: singular or
mass noun; DT: determiner; NP: proper noun; IN: preposition or subordinating conjunction; WDT: wh-determiner
(e.g., relative which).
ID Score Acc Rule Example
1 98 0.99 pos:’VBP’>’VB’ <-
pos:’MD’@[−1,−2,−3]
government will/MD decide/VBP>VB
on rates
2 51 1.00 pos:’VBP’>’VB’ <- pos:’TO’@[−1] compelled to/TO serve/VBP>VB the
interests
3 42 0.82 pos:’VB’>’VBP’ <- pos:’NNS’@[−1] interest rates/NNS continue/VB>VBP
to undermine
4 42 1.00 pos:’NN’>’VB’ <- pos:’MD’@[−1] would/MD cost/NN>VB the Treasury
far more
5 41 0.81 pos:’VB’>’NN’ <- pos:’DT’@[−1,−2] a/DT leading force/VB>NN in the field
6 41 0.67 pos:’IN’>’WDT’ <- wd:that@[0] &
pos:’NNS’@[−1]
alternative fuels/NNS that/IN>WDT
don’t pollute
7 38 0.97 pos:’VBN’>’VBD’ <- pos:’NP’@[−1] Dexter/NP reduced/VBN>VBD its
interest in 1987
8 28 0.60 pos:’NN’>’VB’ <- pos:’TO’@[−1] a contract to/TO supply/NN>VB
equipment
conditional probabilities or other statistical parameters). But the sequence of rules is
understandable enough that it might encourage inspection, modification, experimen-
tation, and occasionally give a new insight.
To illustrate with the POS tagging task,18 Table IV gives the first few learned trans-
formation rules from the Wall Street Journal, a widely used English corpus, and pro-
vides examples from the same text. All of the rules cited are general enough that they
could be suggested as rules of thumb for human POS taggers (at least inexperienced
ones, still trying to internalize and abstract the definitions). The accuracy figures si-
multaneously hint at the reliability of the rules thus learned (however, the actual
scores of the rules are unimportant, because they depend on the size of the training
corpus and the specific tagset chosen). For instance, rules 4–5 may in such a setting be
paraphrased
“Uncertain if you look at a noun or a verb? Well, here is some help. If the previous word is a modal, such
as can, should, may, then what you see is almost certainly a verb. If one of the two previous words is a
determiner (of which the most common cases are articles, such as a, an, the), then you probably see a
noun.”
The two things to note here are that these rules make perfect sense and that they
were extracted automatically.
For ease of exposition, we have used the same 26 templates as in Brill [1995a]. Note,
however, that the alternative templates mentioned in Section 2.3.1 might permit even
18We have chosen POS tagging (Example 2) as a recurring example not because we believe it is the only
thing TBL is good for, but rather because it is a practical, basic, and well-defined task, often needed as a
preprocessing step, and frequently treated in the literature.
We also observe that very often the challenges of any task for a particular language are decided by its
typological properties – how many, how regular, how complicated are the inflection patterns, how rigid the
word order, how well-defined the word boundaries, etc. POS tagging of Russian is very different from POS
tagging of Chinese, and both are different again from POS tagging of English. In this way, the typological
variety of the thousands of languages of the world brings to light a wide range of interesting subchallenges.
POS tagging, being one of the most basic tasks, has inspired (or at least been used to illustrate) several
extensions of popular machine learning algorithms, including TBL.
ACM Computing Surveys, Vol. 46, No. 4, Article 50, Publication date: March 2014.
When Errors Become the Rule: Twenty Years with Transformation-Based Learning 50:17
Table V. Sample Stress and Word Accent Rules for Swedish
The first few transformations learned from 33,390 syllables in 12,396 noncompound, noninflected entries of a
Swedish pronunciation dictionary [Hedelin et al. 1987]. 35 templates with the following features: relative syl-
lable position (left/right); word length (redundantly) both in numeric and syllabic representation (#syll[able]s,
mono/bi/tri+); pref[ix]/suff[ix] of length 1. . .6. Accuracy threshold = 0.96. Positive indices count syllables from the
beginning of the word, negative from the end. For instance, rule 7 says that bisyllabic words ending in -ig, such
as konstig ‘strange,’ stenig ‘stony,’ will have accent II on the next-to-last syllable. See also Section 2.4.1.
ID Score Acc Accent@Syll <- Condition Example
1 1453 1.00 accI@[-1] <- len:mono bil, jobb
2 891 0.99 accII@[-2] <- suff:a & len:bi väska, springa
3 707 0.99 accI@[-2] <- suff:isk mystisk, arabisk
4 551 0.96 accI@[-1] <- suff:t & len:tri+ desperat, kolorit
5 482 0.99 accI@[-2] <- suff:ra parera, konstruera
6 373 0.99 accI@[-2] <- suff:ing etablering, parkering
7 351 1.00 accII@[-2] <- suff:ig & len:bi konstig, stenig
8 320 0.98 accII@[-3] <- suff:are hammare, visare
9 234 0.96 accI@[-1] <- suff:on & len:tri+ konvention, reklamation
10 197 0.99 accII@[1] <- suff:de & sylls:3 yttrande, leende
11 195 1.00 accI@[-1] <- suff:sm kubism, kataklysm
12 185 0.97 accI@[-2] <- suff:er & len:bi vacker, smicker
13 176 0.97 accI@[-1] <- suff:i & sylls:4 pedanteri, fotografi
14 166 0.98 accI@[2] <- pref:f ör f örvisa, f örmå
stronger generalizations: rules 1 and 4 may possibly be merged, and the same goes for
rules 2 and 8.
Although of great practical value, the main point of POS tagging rules is seldom to
provide insights we did not have before. An example with different priorities is lexical
stress and word accent prediction for Swedish. Somewhat simplified, each Swedish
noncompound, noninflected word has a particular syllable that bears (main) stress. The
stressed syllable is associated with one out of two possible word accents, corresponding
to pitch contours of the voice. Precisely on what syllable stress is placed and which of
the two accents (conventionally named Accent I or Accent II) the syllable will have
is mostly predictable from orthography, but certainly not trivially so. These are good
circumstances for automatic detection of interesting rules. Due to reasons of space, we
cannot be very detailed, but the main setup and results of a minimal such study are
summarized in Table V. Note the high value of the accuracy threshold, typical where
the contents of rules are more interesting than their scores.
The words in the study were all noncompound and noninflected, and several rules
would have looked different otherwise (for instance, rule 4 is not true for common in-
flected forms such as dragit ‘pulled,’ talat ‘spoken,’ and huset ‘the house’). Also, in some
cases, the choice between counting from left or from right when placing stress is arbi-
trary; most obviously, the first and the last syllable in a monosyllabic word are identical.
However, the results are already enough to reject the popular misunderstanding19 that
Swedish basically has stress on the first syllable. As can be seen, almost all of the rules
are conditioned on suffix, not on prefix, and stress placement is more reliably predicted
from the end (this fact is reflected as negative indices in the accent/stressed syllable
column).
2.4.2. Compactness of Learned Representation. A major aspect of interpretability, but also
of great help in practical implementations (cf. Section 4.3.2), is the fact that the learned
19Just to be clear: this is a popular misunderstanding, not a professional one. The massive influence from
stress-final French in the seventeenth and eighteenth centuries did not only affect the vocabulary but also,
so as to accommodate the new stress patterns, induced changes in the prosodic system—until that time
typically Germanic, stress-initial. See, e.g., Zonneveld et al. [1999].
ACM Computing Surveys, Vol. 46, No. 4, Article 50, Publication date: March 2014.
50:18 M. Uneson
representation is very compact. As an extreme example, Brill [1994] presents a TBL
system for unknown-word-guessing (i.e., assigning the most likely POS to words not in
the system lexicon; for instance, reviving the examples from Section 1.2, guessing that
staycation is a noun and defriend a verb). He quotes comparable performance for his
148-rule system and an existing statistical unknown-word-guesser with 100,000,000
parameters.
2.4.3. Real-World Objective Function. TBL is an example of error-driven learning: the
function we use to evaluate and choose between different solution candidates (the ob-
jective function, in optimization problem terminology) is typically (a monotonic function
of) the (current) number of errors in the corpus. Crucially, however, this number is in-
teresting not only for the system, but also for its end user: it is the primary criterion
for judging the system’s success. Thus, our way of evaluating competing rules or rule
sets directly optimizes a measure in which we have a practical, real-world interest,
and differences in this measure correspond to differences in performance. By contrast,
most classifiers use some less straightforward objective function that is only indirectly
related to the classifier performance. Although the correlation is, of course, designed to
be strong, it is not necessarily perfect.
The real-world relevance of the objective function also allows TBL training to be
recast as an optimization problem. This view permits the application of typical opti-
mization techniques to the task. For instance, Wilson and Heywood [2005] use genetic
algorithms to minimize the error function between reference and current corpus (see
further Section 4.1.3).
2.4.4. Resistance to Overtraining. For most machine learning algorithms, a major prob-
lem is overtraining: the learned representation describes random error or noise and
thus fails to generalize outside the training data. For instance, it is very easy to detri-
mentally overfit DTs, and careful measures must be taken to avoid it (e.g., by grow-
ing the trees to completion and then back-pruning, or by performing some statistical
analysis before deciding that a node should be further split [Mitchell 1997]). TBL, by
contrast, comes with an implicit ranking of the learned rules: they are automatically
ordered after expected impact. This fact is the main reason for the method’s remarkable
insensitivity to overtraining.
To be clear, TBL does overtrain; that is, if left to train until conclusion with a very
low score threshold, it will learn a large number of spurious, low-impact rules with no
prospects of generalization (most of which will apply to a single site in the training
corpus). But this trail of irrelevant rules does not significantly influence overall perfor-
mance (in either direction). In case we prefer not seeing them anyway (perhaps because
our main interest is the relevant rules only and not overall classifier performance), an
efficient filter is just to raise the score threshold.20 More sophisticated approaches are
also conceivable (see more on classifier combination, discussed in Sections 5.2 and 5.3).
Perhaps more disturbingly, irrelevant rules may conceivably emanate from unfortu-
nate choices of templates. Ramshaw and Marcus [1994] briefly investigate this issue.
They report on experiments with training in the presence of a template that can safely
be assumed to be irrelevant (such as “the POS of the word 37 positions to the left of
the current”). When used in isolation, such a template naturally yields a large number
of spurious rules; but when combined with relevant templates, its influence is largely
neutralized. Their conclusion is that the presence of irrelevant templates will have
20Exactly where to put it will depend on task, intention, corpus, tagset size, etc., and may need some
experimentation, but it need not be very high. As a comparison, Brill recommends a score threshold of 2 for
his POS tagger designed for English (typical tagset sizes 50–150), on the corpus sizes of the mid-90s (105
words).
ACM Computing Surveys, Vol. 46, No. 4, Article 50, Publication date: March 2014.
When Errors Become the Rule: Twenty Years with Transformation-Based Learning 50:19
little impact, if only they are mixed with relevant ones. This is useful knowledge,
particularly for cases where we are uncertain on what templates best catch the de-
pendencies of the problem—disregarding practical matters such as training time and
memory usage, there is little risk in specifying all possible templates we can think of
(see also Section 4.1.2).
We note that Ramshaw and Marcus [1994] are brief on their results, and in any case,
more investigation into TBL overtraining behavior would be welcome, for differently
sized data and tagsets and for other templates and tasks. In the words of Manning and
Schütze [2001], it appears to be more of an empirical result than a theoretical one.
2.4.5. Search during Training Rather Than Application. In TBL learning, new rules are com-
puted on the current, updated version of the training corpus; the learner may thus
exploit all knowledge gained so far in the process (including that encoded in previ-
ously learned rules). Moreover, as Florian [2002b, p. 21; p. 33] observes, this recursive
search takes place at training time rather than test time, when greedily selecting the
most efficient error-correcting rule sequence. Differently from many machine learning
algorithms, no search is involved during application; the rules are simply applied in
the order they were learned. This is of course preferable; normally, a system is applied
much more often than it is trained.
2.4.6. Integration of Heterogeneous Features. Like many but not all machine-learning
methods, TBL can seamlessly integrate heterogeneous features, especially symbolic-
ranged, boolean-valued ones. For instance, without any explicit modeling of indepen-
dence relations, POS tagging could easily draw from information sources such as “is
the previous word capitalized?,” “does the next word contain a number?,” or even “does
this sentence have eight words?”
This flexibility becomes particularly powerful when combined with an expressive
way of specifying templates, perhaps in the form of a little programming language of
its own. See Section 4.6 and Figure 10 for examples.
2.4.7. Competitive Performance. The rules induced by TBL may be interesting or at least
interpretable to humans. Sometimes, however, we don’t really care about such fringe
benefits, but only about classification performance. As a stand-alone classifier, TBL
generally reaches competitive results on a wide range of tasks and state of the art on
some. For other tasks, it lags somewhat behind the best statistical classifiers (at the
time of writing, often Support Vector Machines). However, the trend in recent years is
that the best overall results are reached not by single, stand-alone classifiers, but by
combining several of these into ensemble learners. Such systems generally gain from
diversity in their constituents, and indeed TBL often contributes diversity. We return
briefly to classifier combination later (Section 5.2).
2.5. TBL vs. Decision Trees
As several authors have noted [Ramshaw and Marcus 1994; Brill 1995a; Manning
and Schütze 2001; Florian 2002b], TBL has several commonalities with decision trees
[Breiman et al. 1984; Quinlan 1993]. Decision trees (DTs) are a widely used and well-
researched machine learning method, often one of the first to fall out of the discrete
classifier compartment when opening the machine learner’s toolbox. Given their pop-
ularity, we close this practically oriented section with a brief comparison between
decision trees and TBL from a pragmatic point of view. In Section 3, we revisit the
comparison from a more theoretical perspective.
A prototypical DT outputs a set of yes/no questions that can be asked about a sample
to get at its classification, similar to the context part of a transformation rule. Just as
for TBL, the questions may refer to attributes of the sample being classified, but also to
ACM Computing Surveys, Vol. 46, No. 4, Article 50, Publication date: March 2014.
50:20 M. Uneson
those of other elements on which it may depend; and, just as for TBL, the learner easily
and seamlessly integrates information of different types without any need of explicit
modeling of dependencies.
The differences are just as important, however. First, DTs automatically synthesize
complex questions on any subset of the available attributes, whereas vanilla TBL
requires the format of the rules to be specified by the templates.21
Second, DTs have no (easy) way of saving away current hypotheses; thus, the ques-
tions cannot refer to intermediate predictions. Thus, in DTs (as in most other ML
classification schemes), classification is performed once and never changed. By con-
trast, TBL makes several passes through the data and earlier predictions may be
changed later. As Ramshaw and Marcus [1994] put it: “[D]ecision trees are applied
to a population of non-interacting problems that are solved independently, while rule
sequence learning is applied to a sequence of interrelated problems that are solved in
parallel, by applying rules to the entire corpus.”22
At least for such interrelated problems, then, we should not be surprised to find real-
world cases in which a solution with transformations is much more concise and natural
than an equivalent DT. Brill exemplifies with tagging a word whose left neighbor is to,
which may be either an infinitival marker (to eat) or a preposition (to Scotland). In the
first case, to is an excellent cue for verbs, in the second a good one for nouns. However,
it may well be that the tagging of to itself into these two classes is unreliable. If so,
TBL can automatically delay the exploitation of this cue until it has been more reliably
established by other intermediate rules, working on current predictions. By contrast,
a DT that exploits the same information is quite complicated and will likely contain
duplicate subtrees. This property is at the heart of what Florian [2002b, p. 21] terms
the dynamic behavior of TBL: bidirectional dependencies can be exploited at training
time without penalty. In effect, the rules can first find and classify islands of relative
certainty at any position in the sequence, then leverage the knowledge gained in the
slightly deeper waters on either side.
This fundamental difference between DTs and TBL can also be viewed as one between
stateless and stateful classification. State is a mixed blessing, the discussions on which
fill many a book in an average computer science library.23 Here, we only note that a DT
at work generally has no notion of order or time, whereas learned TBL rule sets are
strictly ordered, with current predictions representing state.24
State aside, several (although not all) of the points mentioned in the previous
section (2.4) apply also to the comparison between DTs and TBL. We here only note the
transparency of the objective function in TBL (which in DTs correspond to some rather
indirect measure of node impurity, as estimated by, for instance, information gain or
Gini coefficient [Breiman et al. 1984]) and the much better resistance to problems with
sparse data and overtraining. As for the latter, a TBL rule has access to the entire
training data when being evaluated; by contrast, at each node, a DT recursively splits
its training data into smaller subsets that, from that point on, know nothing about each
21However, see also Section 4.1.3 and 4.1.2.
22Brill [1995a] proves by induction that for any DT there is a TBL rule list that returns the same classification.
Furthermore, he shows that ordinary TBL rules are strictly more expressive than DTs: precisely due to the
possibility of leveraging intermediate results, there are classification tasks that can be solved by TBL but
not by DTs. However, as pointed out by Florian [2002b, p. 85], the proof makes the unrealistic assumption
that the number of target classes is unbounded. With predicates containing at most k conjuncts and a fixed
number c of allowed classifications, there are DTs (for instance, of depth k(c−1)+1) that cannot be expressed
as a TBL rule list.
23The classic Abelson and Sussman [1996] is but one of them.
24Brill [1995a] describes a transformation list, when applied, as a processor rather than a classifier.
ACM Computing Surveys, Vol. 46, No. 4, Article 50, Publication date: March 2014.
When Errors Become the Rule: Twenty Years with Transformation-Based Learning 50:21
Fig. 3. Decision topologies recognizing the boolean formula (A∧ B) ∨ (C ∧ D). The edge widths are intended
to indicate the relative number of samples going down each branch (assuming, for the sake of example,
uniform distribution and conditional independence, p(X) = p(X|Y ) = 0.5; X, Y ∈ {A, B, C, D}). Decision
trees (a) repeatedly ask partial queries, following every sample from root to leaf. They can express arbitrary
conjunctions but often fragment the data; also, disjunctions may lead to subtree duplication (see the subtrees
under the two C nodes). Decision lists (b) look for complex queries that can perform the classification in a
single step. They thus tend to concentrate on the exceptional cases early, then more general rules later.
Decision pylons (c) may apply any number of queries to an input sample, depending on its present state
(branch). During classification, samples may be moved back and forward between the branches; to a pylonic
classifier, all paths leading to a given state are equivalent, and all samples in that state are indistinguishable.
As a result, predicates cannot exploit classification history, but training suffers less from data fragmentation.
other—the lower the nodes are placed in the tree, the more fragmented data they learn
from. In addition to overtraining, this fragmentation often leads to inefficient repre-
sentations in form of subtree duplication: once subsets of the training data have been
divided along different branches, they are independent and may well induce identical
subtrees [Oliver 1992] (see also Figure 3(a)).
ACM Computing Surveys, Vol. 46, No. 4, Article 50, Publication date: March 2014.
50:22 M. Uneson
3. TBL THEORY
In this section, after some brief preliminaries, we survey the theoretical work on TBL,
especially in terms of its relation to similar, more well-known classifiers. Much work
remains to be done on TBL theory; only a handful of publications on TBL include any
theoretical considerations at all. Notable exceptions are Ramshaw and Marcus [1994]
and, in particular, Florian [2002b, Ch. 3], on which most of this section is based. For
reasons of space, we must necessarily omit much detail and especially the longer proofs;
we refer to the original when the full picture is needed.
We have noted (Section 2.5) the practical similarities between TBL and DTs. From a
more theoretical perspective, they both belong to the larger family of decision topologies:
the classifier poses a series of questions on the item to be classified, the answers of which
will effectively steer the evaluation through a graph. Some nodes are leaves (terminals,
exits) and output a classification; others contain further questions.
The class of decision topologies includes several other members, the properties of the
underlying graph being their most important defining trait. Florian bases a theoretical
account of TBL on two of them: decision lists [Rivest 1987], less familiar than DTs
but still tried on many problems, and decision pylons [Bahl et al. 1989], rather more
exotic.25 The relevance of decision pylons, put into a single sentence, is that they are just
another representation of TBL rule sequences and that they can be shown to subsume
the class of decision lists, thus enabling the reuse of several known theoretical results
for the latter class.
The subsequent sections, after providing some background, expand on these claims.
The following notation is used, taken from Florian [2002b] and Rivest [1987].
—true and false are represented by 1 and 0, respectively.
—Xn = {0, 1}n is the n-dimensional Boolean space (or, equivalently, the set of binary
strings of length n). We assume, in a particular discrete learning situation, that
the samples are represented by n boolean-valued attributes and that we have some
predefined encoding of these attributes (for instance, the ith bit set representing that
attribute i is true). Then, an element x ∈ Xn describes an object, or more formally,
represents the truth value for that object of a specific predicate on the feature space
– for instance, “is the object edible?” or “is the current word ‘that’ and the POS of the
previous word ‘NNS’? ”
—F ⊆ Xn is the space of samples; Fn is the space of dimensionality n.
—C is the set of target classifications (the classifier range); for a binary classifier,
|C| = 2 and, by convention, C = {0, 1}.
We also write cl(x) for the class that classifier cl assigns to sample x.
3.1. Decision Lists
Decision trees are well-known players on the ML stage. Briefly, a DT, once learned, is a
rooted tree in which each node ni holds a question qi regarding the input. At evaluation
time, the answer of the question qi decides which of the children of ni, c(ni), to visit
next. The number of possible answers to qi equals the number of branches bi = |c(ni)|.
When reaching a leaf, an answer is given—usually as the top-candidate classification
or as a probability distribution over all classes. If all nodes have bi = 2, then the tree
is binary, and the questions generally have the form of predicates.
The less well-known decision lists [Rivest 1987] are a special case of a binary DT, in
which one of the branches (conventionally the “yes” branch) points to a leaf: that is, it
25To the extent that relative familiarity can be operationalized as ratio of hit counts in a popular search
engine, “decision trees” are roughly 25 times as familiar as “decision lists,” which in turn are 1,000 times as
familiar as “decision pylons” (July 2013).
ACM Computing Surveys, Vol. 46, No. 4, Article 50, Publication date: March 2014.
When Errors Become the Rule: Twenty Years with Transformation-Based Learning 50:23
immediately yields an answer. More importantly, in contrast to (most variants of) DTs,
where each qi queries only a single feature of the data, the questions (predicates) of
a decision list often are complex conjuncts that can refer to many different features.
When building a decision list, the predicate space is searched for predicates pi such
that the subset Ti of the examples for which pi is true have zero or close to zero entropy;
that is, for which (almost) all members of Ti have the same classification. In practice,
this tends to result in lists with specialized predicates (high precision but possibly low
coverage) early on and more general (higher coverage but probably lower precision)
rules later.
The number of allowed conjuncts k in the predicates of a decision list is the main
determinative factor of its expressive power (as well as the flip side of expressive power,
data sparsity). Another relevant characteristic of a decision list is the number of nodes
it contains, which we term its length; or, when we prefer the tree view, its depth. More
formally, a k-decision list is a sequence of pairs
(p1, v1), . . . , (pr, vr), (8)
where pi is a predicate term with at most k conjuncts; vi ∈ C; and r is the length of
the list. pr is always true, and vr is called the default value. The set of k-decision lists
with predicates taken from Xn is denoted by k-DL(n). The algorithmic interpretation of
Equation 8 is given in Algorithm 2. An example of a simple boolean formula expressed
as a DT and a decision list is shown in Figures 3(a) and 3(b)
ALGORITHM 2: Evaluating a decision list
if p1 then
return v1
else if p2 then
return v2
else if . . . then
. . .
else
return vr
end if
3.2. Decision Pylons
Decision pylons were introduced in Bahl et al. [1989] in order to make the construction
of a tree-based language model with composite questions tractable. They fit excellently
as an abstract, foundational model for TBL. Indeed, Florian [2002b, p. 17] succinctly
but aptly summarizes TBL as a “hill-climbing strategy of learning decision pylons.”
A decision pylon starts out with a default classification of each sample. Then, it
applies a list of queries in order. For each query, which is a pair of predicate p (defined
on the feature space and the current class) and new class c, it identifies the subset of
samples for which p is true and changes the classification of that subset of samples to c.
Crucially, decision pylons are not trees: there may be more than one path to a given
node. They keep no record of classification history and thus operate with a simpler
predicate space; but, on the other hand, they suffer less from data fragmentation.
As for decision lists, the most important characteristic of a pylon is the maximum
number of conjuncts k in its queries. Pylons also have what we term a depth, the number
of questions, and a width, the number of output classes. More formally, a k-decision
pylon is a sequence of pairs
(v0), (p1, c1 → v1), (p2, c2 → v2), . . . , (pr, cr → vr) (9)
ACM Computing Surveys, Vol. 46, No. 4, Article 50, Publication date: March 2014.
50:24 M. Uneson
Fig. 4. Example of decision pylon of class 1-DP(n) that can correctly classify the number of set bits problem,
not solvable by any k-decision list if k < n. Figure 3.2 of Florian [2002b], reprinted with permission.
where pi is a predicate term with at most k conjuncts; vi ∈ C; i = 1 . . . r; and v0 is called
the default value of the decision list. The set of k-decision pylons with predicates taken
from Xn is denoted by k-DP(n).
Examples of decision pylons are shown in Figures 3(c) and 4. The algorithmic inter-
pretation of Equation 9 is given in Algorithm 3.
ALGORITHM 3: Evaluating a decision pylon
c ← v0 {c holds current classification}
if p1 and c = c1 then
c ← v1
else if p2 and c = c2 then
c ← v2
else if . . . then
. . .
else if pr and c = cr then
c ← vr
end if
return c
3.3. Lists to Pylons and Back
Decision lists and decision pylons have some obvious differences and similarities. The
main operational difference is that, for decision lists, the evaluation process is short-
circuiting—once a single predicate returns true, evaluation terminates and a classi-
fication is output. By contrast, decision pylons take no shortcuts—all predicates are
tested and applied, irrespective of the applicability of previous rules.
In terms of representational power, Florian [2002b, p. 57] shows that, for the general
classification case, with independent samples26
(1) k-DL(n) is a proper subset of the class k-DP(n).
(2) k-DP(n) is a proper subset of the class p-DL(n), where p = min(k(|C| − 1), n)
26That is, the results do not generalize to sequential classification or other types of structured prediction,
where chains of rules can exploit dependencies longer than the template width.
ACM Computing Surveys, Vol. 46, No. 4, Article 50, Publication date: March 2014.
When Errors Become the Rule: Twenty Years with Transformation-Based Learning 50:25
Here, we give Florian’s constructive proofs of these two crucial properties (slightly
adapted and shortened). The two proofs follow and generalize earlier work given in
Roth [1998], where the claim k-DP(n) ⊆ k-DL(n) is proven for binary classification.
(1) For every decision list dl ∈ k-DL(n), there is an equivalent decision pylon dp ∈
k-DP(n) such that dl(x) = dp(x), ∀x ∈ Xn. To show this, we introduce the nota-
tion (pk, ∗ → vk) as a shorthand for (pk, c1 → vk), (pk, c2 → vk), . . . , (pk, cm → vk)
where C = c1 . . . cm is an enumeration of the classification space. First, consider the
decision list
dl = (p1, v1), . . . , (pk, vk).
Let x ∈ Xn be an arbitrary sample, and let j be the first index such that pj(x) = true.
The classification assigned by the decision list dl(x) is then v j . Now, construct the
decision pylon
dp = (vk), (pk, ∗ → vk), (pk−1, ∗ → vk−1), . . . , (p1, ∗ → v1)
and apply it to x. At some point, the evaluation reaches . . . , (pj, ∗ → v j). After that,
no more predicates pj−1, pj−2, . . . , p1 will be true, and thus dp(x) is again v j .
To show that the inclusion is strict, that there are k-DP(n) that cannot be rep-
resented by any k-DL(n), consider the simple problem of counting the number of
set bits in a bit string of length n. Because decision lists are stateless and lack
memory, a correct solution needs to inspect all the n predicates at once. Therefore,
any k-DP(n), k < n, cannot solve this problem. By contrast, a 1-DP(n) solution is
given in Figure 4.
(2) For every decision pylon dp ∈ k-DP(n), there is an equivalent decision list dl ∈
k-DL(n) such that dp(x) = dl(x), ∀x ∈ Fn. To see this, we consider the k-decision
pylon
dp = (v0), ( f1, c1 → v1), . . . , ( fm, cm → vm).
and build an equivalent decision list dl by exhaustive construction, as follows.
(a) For every possible sample x ∈ Fn:
i. Trace the series of states  that x will pass through when evaluated by dp:
 = v0, γ1, γ2, . . . , γm
ii. Construct the predicate P = true ∧ fi . . . , ∀i ∈ 1 . . . m − 1 where γi = γi+1
(that is, for every state change, add the triggering predicate to P)
iii. Add to dl (P, γm), if not already present.
(b) Finally, add v0 as the default value of dl.
To see that any predicate P in dl needs at most min(n, k · (c − 1)) terms, we first
note the trivial case that a term of size n always is sufficient (all variables can
be inspected at once). For the more interesting case, form the directed graph G =
(V, E), where V = C and (α, β) ∈ E whenever α = γi and β = γi+1 for some
i ∈ 1 . . . m− 1 (that is, there is an edge between state α and β whenever β follows
α at some point in ). Also, associate the predicate fi to that edge, if it had no
previous association.
Clearly, a path through G from v0 to γm must exist (because the graph contains
the path implicitly described by ). Now, because a decision pylon immediately
forgets a sample’s classification history once the sample is in a given state, any
path through G from v0 to γm is equivalent. In particular, the shortest path S will
do. If a shortest path exists, it can be no longer than |C| − 1, because it can at most
visit all |C| nodes. Thus, a P formed by the conjunction of all predicates associated
with the edges of S, each being at most k-sized, needs at most k · (c − 1) terms.
ACM Computing Surveys, Vol. 46, No. 4, Article 50, Publication date: March 2014.
50:26 M. Uneson
These are important results because several known properties of decision lists will
then also apply to decision pylons. Below, following Florian [2002b], we comment the
most relevant from a decision-pylonic point of view. For full details and proofs on
decision lists, we refer to Rivest [1987], the major theoretical contribution on the topic.
3.3.1. For Binary Classification, k-DL(n) = k-DP(n). As an immediate consequence of the
two properties just given, we note that if c = |C| = 2, then p = k, and thus k-DL(n)
= k-DP(n). That is: for the important class of binary classifiers, the class of k-decision
lists is equivalent to the class of k-decision pylons.
3.3.2. The Class k-DP(n) Contains the Classes k-CNF(n), k-DNF(n), and k-DT(n). Each of the
classes k-CNF(n) (Boolean formulae expressed in Conjunctive Normal Form, i.e., as a
conjunction of at most k disjunctions of literals); k-DNF(n) (Boolean formulae expressed
in Disjunctive Normal Form, i.e., as a disjunction of at most k conjunctions of literals);
and k-DT(n) (decision trees of depth at most k) are proper subsets of the class k-DL(n)
[Rivest 1987]. Thus, they are also contained in k-DP(n).
3.3.3. k-DP(n) is PAC Learnable. PAC learnability [Valiant 1984; Mitchell 1997] is used
to characterize a computational learning problem in terms of complexity. For a binary
classification function, PAC learnability implies that there is some algorithm that,
with as high probability 1 − δ as desired (the P of PAC, for “probably”), can construct a
classifier with an error rate  as low as desired compared to the target (the AC of PAC,
for “approximately correct”), independently of the distribution of the sample space.
Furthermore, if this algorithm is polynomial in (N, 1/δ, 1/), where N is the input size,
then the problem is said to be efficiently PAC learnable. Rivest [1987] shows that the
class k-DL(n) is efficiently PAC learnable; thus, this is true also for the class k-DP(n)
[Florian 2002b, p. 62 f.].
4. TBL EXTENSIONS AND DEVELOPMENTS
In Section 2.4, we showcased several desirable properties of the basic TBL method,
but there is certainly room enough for development, improvement, and extension. In
the following, we briefly survey the most important new ideas for TBL: extending the
domain of the learned hypothesis (Section 4.1), extending its range (i.e., the type of
its predictions; Section 4.2), increasing efficiency (Section 4.3), decreasing the need of
supervision (Section 4.5), and facilitating declarative problem specifications by domain-
specific languages (Section 4.6). For convenience, Table VI summarizes the subheadings
of this section, with selected references for each.
4.1. Extending the Hypothesis Domain
4.1.1. Multidimensional Learning. Many real-world applications involve more than one
subtask—perhaps one classifier to assign POS to each word and another one for com-
bining the result into, say, binary branching parse trees. A simple way of performing
such combined tasks is to put the appropriate classifiers in a pipeline, resulting in
a strictly feed-forward system, in which the outcome of task k cannot influence the
outcome of task k − 1. For very different and/or truly independent tasks, this may be
the best way. A variation is beam search, in which we keep n > 1 hypotheses from early
steps and defer complete disambiguation until later.
However, when two tasks A and B are somewhat dependent, we may benefit more
from multitask learning [Caruana 1997]. Intuitively, if we can expect that solutions to
A may provide useful information for solving B, and vice versa, then it would be better
not to impose an artificial ordering of the tasks on the system but rather solve them
in parallel. In that way, the easy cases (of both A and B) may provide information for
the more difficult ones (of both A and B). Two such interrelated tasks are POS tagging
and chunking—dividing sentences into larger, non-overlapping units. We look closer at
ACM Computing Surveys, Vol. 46, No. 4, Article 50, Publication date: March 2014.
When Errors Become the Rule: Twenty Years with Transformation-Based Learning 50:27
Table VI. TBL Developments and Extensions, with Selected References
Development Main references
Extending the hypothesis domain
Multidimensional learning Florian and Ngai [2001]; Florian [2002b]
Automatic template learning Curran and Wong [2000]; Milidiú et al. [2007]; dos Santos
and Milidiú [2009]; dos Santos [2009]
Template-free TBL Wilson and Heywood [2005]
Extending the hypothesis range
Predicting probabilities Florian et al. [2000]; dos Santos and Milidiú [2007]
Predicting sets Lager [1999b, 2001]
Predicting numbers Bringmann et al. [2002]
Improving efficiency
Efficiency in training Ramshaw and Marcus [1994]; Samuel [1998b]; Hepple
[2000]; Carberry et al. [2001]; Ngai and Florian [2001a];
Florian [2002b]
Efficiency in application Roche and Schabes [1995]
Improving robustness: Redundant rules Florian [2002b]
Unsupervised TBL Brill [1995b]; Aone and Hausman [1996]; Becker [1998]
DSLs and template compositionality Lager [1999b]; Lager and Zinovjeva [1999]
chunking later (Figure 12); here, we only note that there are easy cases for both the
POS and the chunking part, where the tag can be predicted directly from word forms,
and that these two sets do not overlap completely.
Indeed, multitask learning on well-chosen representations may be worthwhile even
when we are not really interested in solving all of the tasks; learning POS by jointly
learning chunks is conceivably easier than learning POS alone.
One way to set up such a joint classifier is to simply create a new derived feature
as a concatenation or tuple of the features we wish to combine. However (in analogy
with computing joint probability distributions), this strategy may cause problems of
data sparseness with many types of data. Unusual joint values will have unreliable
estimates, and joint values that do not occur in the training data will never be predicted.
A better way for many purposes is to let the tasks share a common representation
and let each classifier work on it simultaneously. TBL is well suited for joint learn-
ing in this way. Florian and Ngai [2001] point out the few changes needed to the
main algorithm, mainly small modifications to the scoring function f . As stated before
(Section 2.3.3), to guarantee termination, f needs to assign positive values only to rules
that actually decrease the current error count. This requirement is easily met also in
a multidimensional setting, by letting f take more than one field into account:
f (r) =
∑
s∈D
n∑
i=1
wi · (Si(r(s)) − Si(s))
Here, D is the set of training samples (the training corpus); r is a candidate trans-
formation rule to be scored; r(s) is the transformed sample resulting from applying r
to sample s; n is the number of fields (tasks); wi is an optional weight or priority for
task i; and Si(s) is an indicator of the current classification of sample s for task i (1 if
correct, 0 if not).
The weights wi could be used to manually assign priorities to each subtask [Florian
and Ngai 2001]. They might conceivably also be initialized from training data based on
the counts of correct and incorrect sample classifications figures for each field, as con-
stants for the entire training session, or even as per-iteration values to be recalculated
and reassigned after each rule application. Different weights for the target attributes,
whether set manually or derived from data, are apparently an unexplored option.
ACM Computing Surveys, Vol. 46, No. 4, Article 50, Publication date: March 2014.
50:28 M. Uneson
To conclude, in the TBL case, multidimensional learning can be seen as a general-
ization of one of the previous key arguments for the method: we can use intermediate
results to guide later predictions. The multidimensional addition is that such interme-
diate results may refer to and exploit more than one feature.
4.1.2. Automatic Template Learning. Templates are the main tool for embedding and en-
coding domain knowledge in TBL. Although this strategy is flexible, templates can
be tedious or difficult to produce. The tedium may to a large extent be alleviated by
automation (e.g., the template compiler described in Lager [1999a]). As we have seen,
overtraining is seldom a problem in TBL, so unnecessary templates will mostly only
affect training time. A greater concern is insufficient domain knowledge: for less well-
understood problems, it may be difficult to know where to start. Learning templates
automatically then become an attractive option.
Curran and Wong [2000] envision “evolving templates that change in size, shape and
number as the learning algorithm continues,” starting from few and simple templates
that are gradually refined into (or replaced by) more complex ones in a data-driven
fashion. They present no implementation, but they show empirically that the number of
conditions in the templates and their specificity (e.g., words rather than tags) increase
during learning—simple templates with few conditions are most efficient early on, but
later in the learning session more involved templates tend to pay off better. However,
the task providing their data is again POS tagging for English; it would be desirable
to see their claims corroborated for other tasks and datasets.
Less abstractly, Milidiú et al. [2007] implement a genetic algorithm [Mitchell 1997]
for automatic template learning. Their system shows impressive performance, but their
reported setup suffers from slow training; with the TBL algorithm as the fitness func-
tion, training must happen for each individual of the population, for each generation.
For anything but the smallest feature sets, this rapidly becomes intractable.
A more recent approach [dos Santos and Milidiú 2009; dos Santos 2009], dubbed
“Entropy-based Transformation Learning” (ETL), instead constructs templates from a
DT trained on the task at hand. A DT [Mitchell 1997; Quinlan 1993] can in this context
be thought of as a series of yes/no questions asked about an object to be classified, with
the questions ordered after expected usefulness (cf. Section 2.5). The main idea behind
ETL is now that the features that are addressed by the DT-induced questions on task
X are likely to make up a good set of TBL templates for X. Each path from the root to
any internal node of the learned DT corresponds to a specific series of questions and
thus a specific set of features (Figure 5); that is, a template.
Some care needs to be taken to avoid typical DT training problems. First, the stan-
dard algorithms will strongly favor high-dimensional features—for instance, word iden-
tity rather than POS. The version of ETL described in dos Santos [2009] tries to control
this by sorting the values of a high-dimensional feature in decreasing order of infor-
mation gain (IG) and replacing all except the z top-scoring ones with a dummy value,
where z is a parameter of the algorithm. Second, as discussed in Section 2.5, DTs are
inherently stateless and have no notion of a “current” classification. For the purposes of
template generation, ETL solves this by introducing the true value of the classifications
in the context (but not for the current object).
dos Santos and Milidiú [2009] report excellent results for ETL on a number of tasks
[Fernandes et al. 2010; Milidiú et al. 2008, 2010; dos Santos et al. 2008, 2010]. The
approach has apparently so far only been used by this single group of researchers; a
more thorough evaluation will have to wait.
4.1.3. Template-Free TBL. An even more radical approach than automatic template
learning is TBL without any templates at all. Wilson and Heywood [2005] suggest a
genetic algorithm [Mitchell 1997], which (in contrast to the previous genetic approach
ACM Computing Surveys, Vol. 46, No. 4, Article 50, Publication date: March 2014.
When Errors Become the Rule: Twenty Years with Transformation-Based Learning 50:29
Fig. 5. Template extraction from decision tree [dos Santos 2009]. First, the leaves and the edge labels of the
learned tree (left, for POS tagging on Swedish; only a small fragment shown) are discarded. In the resulting
subtree, each path from the root to an internal node corresponds to a (not necessarily unique) template
(right).
Fig. 6. Bit string encoding of one (fictive) particular rule in TBL by genetic algorithm. Any subset of tags
within a window of current position ±3 can be encoded, replacing a manually specified template set. From
Wilson and Heywood [2005], adapted and redrawn with permission.
by Milidiú et al. [2007]) does away with templates entirely. Instead, an entire sequence
of rules corresponds to one individual in the population, and one individual is a collec-
tion (378, in the experiment) of rules, each represented as a bit string of fixed length
(48, in the experiment; Figure 6). Rule reordering within individuals is altered with a
crossover operator.
Although an undeniably novel approach, several critical choices are tuned to the
specifics of POS tagging (in particular, for English). Thus, the rather small tagset size
is part of the encoding assumptions: it is not clear how the method would handle a
larger target set or many-valued features (as may well be crucial in other tasks). It
is also unclear how it would respond to a lower scoring baseline. The paper does not
exemplify any rules learned; that would otherwise have made interesting comparisons
with regular TBL.
TBL without manually specified templates seems in particular well-motivated when
the choice of templates is problematic, perhaps due to poor understanding of the
domain. This is hardly the case for POS tagging in English, so template-less genetic
algorithms may have more potential for other tasks. Performance is also quite a bit
ACM Computing Surveys, Vol. 46, No. 4, Article 50, Publication date: March 2014.
50:30 M. Uneson
lower (the authors report 89.8% from a baseline of 80.3%; Brill’s original algorithm
scores 97.0% on the same corpus).
4.2. Extending the Hypothesis Range
4.2.1. Predicting Probabilities: Soft Classification. The vanilla output of a TBL classifier is
one class per sample, with no record kept of the confidence in the choice. Such output
is based on hard decisions, each committing the system to a single possibility with no
information on the certainty of the choice.
Probabilistic systems, by contrast, make soft decisions, providing confidence mea-
sures for each classification. Confidence measures are useful for many purposes and
indispensable for some. For instance, in ensemble systems (Section 5.2), the member
classifiers may disagree; it is then very useful to know how much they are willing
to insist. In active learning, the idea is to minimize manual effort; such systems use
confidence measures to identify the most uncertain (and thus, to the system, most
informative) samples and ask the annotator for their classification. For some applica-
tions, hard decisions are simply inappropriate. Larger, multicomponent systems (e.g.,
speech recognizers) are generally built from many smaller modules that all deal with
probabilistic input and output (say, as probability distributions or as ranked candidate
lists).
Two notable attempts have been made to enhance TBL with probabilistic classifica-
tion. They share the basic idea of splitting the training data into equivalence classes:
all the samples that have been tagged X for reasons Y are considered together. Then
probabilities can be estimated for each equivalence class by standard means (e.g.,
maximum likelihood estimation, probably with some smoothing).27
Florian et al. [2000] give an algorithm for transforming a learned rule list into an
equivalent DT and then taking the leaves of that tree as equivalence classes. They
note that the equivalence classes thus constructed will tend to vary a lot in size. In
particular, with a good baseline, the equivalence class of samples to which no rules
apply at all can easily make up most of the corpus, and the probability estimates of
that class will be close to those arrived at without any learning at all. To remedy that,
the learned tree is treated as a (highly accurate) prefix, whose paths are grown further
with standard DT learning methods [Quinlan 1993].
dos Santos and Milidiú [2007] instead construct equivalence classes from the baseline
classification and rule traces; for instance, all samples that had initial classification NN
and were later touched by rules 11, 57, and 88 form an equivalence class of their own.
The problem of unevenly sized classes is solved by subdividing on manually specified
auxiliary features. The authors claim a significant improvement over Florian et al.
[2000] on comparable tasks. Their method arguably adheres better to the inherently
stateful TBL paradigm (cf. Section 2.5) but seems to involve more task-specific hand-
coding (the auxiliary features), as well as more assumptions on the data (the relevance
of the initial classification).
In any case, probabilistic TBL is an interesting subject with several unexplored paths.
For instance, it is not clear what influence a dumber initial classification (forcing more
samples to be touched by some rule) or a higher accuracy threshold (inducing a bias
27Smoothing is a generic term for techniques for improving probability estimations of stochastic events
drawn from so large event spaces that they may rarely or (more often) never have been seen before. Human
language offers many such event spaces. For instance, the sentence “please wait for a while before swallowing
the headphone” probably never has occurred before, and in any corpus which does not include this paper
it will have zero occurrences. Yet, the sentence clearly has some non-zero probability. Smoothing helps
estimating that probability from existing non-zero counts. It is also an entire research field of its own.
ACM Computing Surveys, Vol. 46, No. 4, Article 50, Publication date: March 2014.
When Errors Become the Rule: Twenty Years with Transformation-Based Learning 50:31
for more accurate rules) would have on the quality of the estimates learned with one
method or the other.
4.2.2. Predicting Sets: Constraint Grammar. We have already mentioned (Section 1.3) that
one of the most successful POS taggers for English is the originally handwritten rule
system EngCG [Karlsson et al. 1995]. CG stands for Constraint Grammar. Very briefly
and somewhat simplified, such a system starts with initializing a candidate set of
tags for each word to the set of all possible tags for that word (as found in a lexical
lookup or in a reference corpus). Then it traverses a list of rules, each formalizing some
requirement, or constraint, on the solution.28 Tags not fulfilling a certain constraint
can be removed from the candidate set (the last tag should not be removed, however).
Ideally, only one correct tag will remain when all rules have been applied. In reality,
of course, some samples will have had the correct tag removed in the process, and others
will still have more than one possibility left when training ends. Thus, the evaluation
measure of a CG-like system needs to be adapted accordingly. Just like for named entity
recognition (Section 2.3.4), it is usually calculated on the precision/recall/F1 measures.
The set representation of CG-like systems allows them to exploit negative informa-
tion, say, “a determiner is never followed by a verb,” just as easily as positive. This is a
major advantage compared to ordinary TBL, and indeed a favorable contrast to most
symbolic classifiers.
On the other hand, a substantial drawback is their reliance on every single rule
being correct: it is not possible to let later rules correct the mistakes of earlier ones. If
the correct tag is erroneously removed from a sample, there is no magic black hat from
which it could be reproduced. Thus, this style of learning will strongly emphasize rules
with perfect or almost perfect accuracy. Another potential disadvantage of CG-style
systems is that they output sets; using a CG tagger as a component in a larger system
thus presupposes that the interfaces of later links are prepared to handle unranked sets
as input. In an increasingly probabilistic view on knowledge induction and transfer,
this assumption may be problematic.
CG rules were originally specified by hand, but naturally, several attempts have
been made to extract them automatically from a corpus (e.g., Samuelsson et al. [1996];
Lindberg and Eineborg [1998]). The question of interest here is how to do so by TBL.
Lager [1999b] shows that this can be achieved by conceiving of transformations not as
replacement rules but rather as set operations. He incorporates this style of learning
in his μ-TBL system (see also Appendix). Especially important are reduction rules,
which remove an element from a set if it is not the last (otherwise, they do nothing).
In English, such a reduction rule, corresponding to the CG rule example given earlier,
may read “given a word w and its current tag set T , if the word immediately to
the left of w is uniquely tagged as DT, then reduce T with tag VB (that is, remove
VB from T if it is not the only tag in T ).” In the μ-TBL formalism, this rule would
read pos:red vb <- unique pos:dt@[-1], and it would probably be derived from the
template pos:red A <- unique pos:B@[-1]. Lager [2001] explores this learning style
for the case of POS tagging. The results are promising, but prohibitively slow for
scaling up: from a modest-sized 240kW corpus with 15 templates, the TBL Constraint
Grammar learner described in Lager [2001] learns 4,866 rules in three weeks (!). A more
efficient algorithm for CG-style TBL is clearly crucial if the method is to gain wider
usage.
4.2.3. Predicting Numbers: Transformation-Based Regression (TBR). An innovative extension
of the prediction range from discrete classes to real numbers is suggested by Bringmann
28The “Grammar” part of the term is less informative and reflects the early uses of the approach: it is
certainly conceivable to use it on problems outside grammar or POS tagging.
ACM Computing Surveys, Vol. 46, No. 4, Article 50, Publication date: March 2014.
50:32 M. Uneson
Fig. 7. Applying a TBR rule r = if c(si) then yi(t + 1) ← a + b ∗ yi(t). The subset of the data (gray) for which
the condition part c of r is true undergoes a linear transformation y(t + 1) = a + b ∗ y(t) toward the (correct)
diagonal. From Bringmann et al. [2002], reprinted with permission.
et al. [2002]: TBR, short for Transformation-Based Regression. The end product looks
somewhat like a regression tree [Breiman et al. 1984; Quinlan 1993], but, as we have
seen (and the authors point out), TBL gains additional expressivity by making multiple
passes through the data. Thus, intermediate results can be leveraged, and predictions
can be improved based on current predictions, rather than set once and for all.
TBR deals with continuous transformations. A TBR rule has the following form:
if c(si) then yi(t + 1) ← a + b ∗ yi(t).
Here, si = (xi, yi) is the ith sample in the dataset; xi is a vector of attribute values
describing the ith sample; yi is a numerical value to be predicted for the ith sample;
c(si) is a predicate on the sample si and/or its neighbors (as before); yi(t) denotes the
value yi at iteration t; and a, b are parameters chosen to minimize the error after the
transformation.
A rule only applies to (i.e., transforms linearly) the subset of the data for which c
holds (Figure 7). The algorithm for rule instantiation and scoring needs some modifi-
cation for TBR, and the stopping criterion is handled somewhat differently; we refer to
Bringmann et al. [2002] for details.
Two drawbacks are difficult to avoid when leaving the world of discrete classifiers and
entering the continuous regression domain. First, overtraining becomes a much more
pressing issue. This is a standard problem, with several common solutions. The one
proposed by Bringmann et al. [2002] is similar to that often used to avoid overfitting
in DTs: partition the available data into a training set, used to form a hypothesis
as described earlier, and a validation set, used to evaluate the effect of pruning or
simplifying this hypothesis. In the TBR case, the learned TBR rule list can thus be
truncated to the smallest one within a certain error—that is, one standard error—from
the best result on the validation set.
Second, arguably a greater loss, TBR sacrifices a large part of rule interpretability
(cf. Section 2.4.1): the meaning of rules in the form just presented is difficult to grasp.
Although the top scoring rules may make some intuitive sense, rule effect is generally
cumulative, and later rules that linearly transform the output of earlier ones make no
more sense to humans than does any other black-box method.
It should be noted, however, that these downsides are no worse for TBR than for
other regressors. Performance-wise, the authors report TBR to be competitive with
state-of-the-art regression algorithms on difficult sequential regression tasks (predict-
ing segment durations from phonological features and predicting three main musical
parameters of expression in piano playing).
ACM Computing Surveys, Vol. 46, No. 4, Article 50, Publication date: March 2014.
When Errors Become the Rule: Twenty Years with Transformation-Based Learning 50:33
4.3. Improving Efficiency
4.3.1. Efficiency in Training. In Section 3, we mentioned that TBL is PAC learnable in
polynomial time. This is not the same as practical usefulness, of course, and indeed,
long training times have been (and still are, for some uses) a major impediment. Brill’s
original implementation (in C) was generously made available on the web early on
and was often used for POS tagging as a preprocessing step in other applications.
However, although it contained clever optimizations, it commonly needed several days
of training time and that on corpora that are considered small by the standards of
the time of writing. More complex tasks are rather worse, and the situation is further
exacerbated when a TBL system is used as one of several base learners in iterating
ensemble learners (cf. Section 5.2).
In this section, we summarize the time complexity of the training phase of the
original TBL algorithm and describe several later attempts to improve on it. We use
the following notation: T for the template set, |T | for the size of the template set,
m for the number of learned rules (the depth of the decision pylon), and N for the
size of the training data. We also write gr, br for good and bad count for rule r; nR for
the total number of rules with at least one positive application; w for the maximum
template width; E, e for the initial error count and the initial error rate (E = eN),
respectively; and Ai for the number of application sites of rule ri. Any other notation
will be introduced as necessary.
Without making assumptions on the input distribution, it is not so easy to put useful
bounds on estimates of TBL training complexity. The worst-case analysis is clear, but
rather absurd: a corpus of size N wherein all initial tags are incorrect and the correct
tags are all unique. Thus, no possible rule will ever correct more than a single error.
Given |T | templates, we get O(N|T |) rules to choose from in each iteration,29 and
because there are N errors to correct and no rule corrects more than one of them, we
will have to learn O(N) rules. Thus, training takes O(N2|T |) time, and no indexing
schemes will improve the situation. Of course, nobody would want to try machine
learning on such a dataset, but it at least establishes an upper bound. We could also
consider O(N) a best-case lower bound—we won’t need the templates to find out that
there are no errors to correct, but we clearly need to inspect the entire corpus.
The much more difficult average case analysis has, to our knowledge, only been
attempted in Florian [2002b, app. B]. The author assumes a Zipfian distribution [Zipf
1949] of the errors. That is, the frequency f of an element of rank k is f ∝ 1ks , with s
close to 1.30 Assuming this distribution has proven reasonable for its original domain—
word frequencies—as well as for a range of other linguistic data, and also for unrelated
properties such as population ranks or earthquake magnitudes. But, of course, it need
not generalize to all areas.
With this assumption, Florian bounds the time complexity of the original TBL
algorithm (Section 2.2) to 	(|T |mN + mNnR), which is 	(mN2) if nR = 	(n) and |T | is
considered constant. The full proof occupies well over 10 pages and is involved enough
that the author additionally offers a handwaving argument; here, we repeat this light
version without modification and refer to the original for detail.
The main loop of the original TBL learning algorithm (see Algorithm 1, p. 8) has two
disjoint parts, interleaved in execution:
29Templates may contain alternative positions, such as pos:A@[1,2], meaning some pos tag A at position 1
or 2. Given a specific sample with erroneous classification, such a template will usually generate more than
one rule that corrects it. We disregard that in the discussion here. At any rate, considering the maximum
context width a constant, O(|T |) templates will still generate O(|T |) rules per application site.
30In a population of N elements, the proportionality constant c is the inverse of the Nth generalized harmonic
number HN,s; c = 1/
∑N
n=1(1/ns).
ACM Computing Surveys, Vol. 46, No. 4, Article 50, Publication date: March 2014.
50:34 M. Uneson
Fig. 8. Data flow of TBL training, extended with state (dashed boxes). State content tried includes an index
to application sites for each rule and vice versa [Ramshaw and Marcus 1994] or only good/bad scores for each
rule [Ngai and Florian 2001b].
(1) A candidate generation part (3b), which generates rules with at least one positive
application, requiring 	(|T | · m · N) time (at each of the m iterations, each of the
N samples need to be examined, with each of the incorrectly classified samples
generating (|T |) rules).
(2) A candidate test loop (3c), which has to compute the negative applications for each
rule that could not be discarded by any of the optimizations (e.g., 3c-i-B and 3c-ii
in the figure). Each such count has to examine the entire corpus in 	(N), which
is very expensive. Early on, only a small (constant) number of rules need to be
checked before a winner is found, but when the score of the best rule decreases,
a larger amount of rules will need checking, until, at score 1, all of them will.
Florian’s handwaving argument now states that a constant number of rules needs
to be checked in the beginning and all nR at the end, yielding an average of nR/2
rules checked per iteration. Thus, the running time for (3), which dominates the
total running time, is O(m· N · nR) This is, in fact, conservative, because the rule at
percentile (say) 50 in nR still is very low-scoring—there are many more rules with
small counts than with large.
The slow training is indeed a major practical disadvantage: for any sizeable corpus,
N2 quickly becomes infeasible. An early attempt to speed up training is represented by
Ramshaw and Marcus [1994]. Their general idea is to use state to avoid recalculation.
In the TBL case, this involves generating all potentially helpful rules once, computing
some amount of information for each, and saving that information as part of global
state. Then, after each rule application, it is enough to update state minimally, in-
stead of regenerating new rule candidates from scratch. Figure 8 shows the system
architecture augmented with state (cf. Figure 2).
The state of Ramshaw and Marcus [1994] implements an elaborate indexing scheme.
In an initial phase, the corpus is traversed linearly twice, once to identify the O(eN|T |)
positive application sites and their associated rules (one rule for each of the E = eN
errors and each of the |T | templates), once to identify the negative application sites of
those rules. As a result, each rule in effect has a list of (pointers to) the samples it may
be applied to, and each sample has a list of (pointers to) the rules that can be applied
to it. The list of rules is scored and sorted, and the top scoring rule is selected. When
ACM Computing Surveys, Vol. 46, No. 4, Article 50, Publication date: March 2014.
When Errors Become the Rule: Twenty Years with Transformation-Based Learning 50:35
applied, each of its sites of application can be accessed in constant time, and all the
rules affected by the change of corpus state can in turn be efficiently updated.
The initial phase takes O(nR + nR · N), then the updating after application of each
of the m rules starts at O(A1|T |w) for the first one and approaches O(S|T |w) as the
number of samples changed Ar (at most 2 · gr) approaches the constant score threshold
S. Because
∑
Ai = O(E), the total running time is dominated by the initial phase.
This reasoning disregards the rather unpredictable case when the updating process
needs to introduce a new rule that has not previously had any positive effect; this
will warrant a linear scan.31 With that exception, however, once the index is built, no
linear searches need to be performed in the corpus. Unfortunately, even with judicious
filtering of rules that have higher negative than positive score, this method consumes
O(E · |T |) memory, which makes it infeasible for most real-world tasks with more than
a handful of templates.
Ngai and Florian [2001b] propose what is essentially a more memory-friendly sim-
plification of the algorithm of Ramshaw and Marcus [1994]. Instead of storing O(Ar)
pointers for each rule, their state only stores a good/bad count (gr, br); gr + br = Ar. At
the cost of one linear search per iteration (excepting, again, rules that previously have
had no positive effect but may need to be introduced after an update), the rule-to-score
list then becomes much smaller and the sample-to-rule list superfluous.
Thus, after building and indexing the initial rule list, the top scoring rule can be found
in constant time, applied in O(N), and the state updated in O(N + A|T |w). Toward the
end of training, this approaches O(N + ST w), where S is the score threshold.
Florian [2002b, p. 216 ff.] shows that the running time of this algorithm is 	(|T |mN+
mN), given the same Zipfian assumption on the error distribution as earlier, where N,
as before, is the size of the training data (in types rather than tokens), m is the number
of rules learned (i.e., the depth of the decision pylon), and T is the template set.
Similarly to the complexity of original TBL, the proof is rather involved; we refer to
the original publication.
Another approach that drastically improves time and space behavior is the explicitly
stated assumptions proposed by Hepple [2000]: independence (no rule interaction with
respect to the condition part of a rule—all rules are learned in the context of the
initial annotation) and commitment (at most one change per sample). Hepple describes
two algorithms based on them (ICA and ICP: Independence, Commitment, and either
Append or Prepend, depending on whether the rules are applied in the learned order
or in reverse). With the IC assumptions, the transformation rule lists actually become
equivalent to decision lists (Section 3.1). Of course, the validity of the assumptions
depends very much on the problem. Some problems are not sequential; others are
sequential, but can reasonably be treated as if they weren’t; yet others are sequential
and ignoring this property implies significant information loss. Furthermore, the error
rate of the baseline is crucial. Fewer errors, of course, mean fewer and lower scoring
rules to learn; thus, rule application sites are generally far between and even the rules
of an optimal sequence O will not interfere with each other. Poorer baselines, on the
other hand, mean more rule interaction in O, which the IC algorithms will not be able to
capture. As we have noted, POS tagging, which is what ICA and ICP were designed for,
generally starts from a very good baseline, especially for English: then, the performance
loss is slight (and probably outweighed by the possibility of training on much larger
datasets and/or lower thresholds). For several other tasks, IC is significantly worse
[Ngai and Florian 2001b].
31Or, at least, may warrant a linear scan. Many rule candidates correct more or less the same set of errors (cf.
Section 4.4), and for some tasks it may be possible to ignore the introduction of new rules at little performance
cost. If so, training becomes even faster.
ACM Computing Surveys, Vol. 46, No. 4, Article 50, Publication date: March 2014.
50:36 M. Uneson
The main benefit of the IC assumptions is that the training data can conceptually be
partitioned according to the initial tagging. For instance, in the POS tagging case, the
rules that apply to all words initially tagged as verbs (whether correctly or not) can be
learned before (and independently of, so also, for instance, in parallel with) the rules
that apply to those initially tagged as nouns. This permits ambitious indexing schemes,
like that of Ramshaw and Marcus [1995], but in a much more memory-friendly fashion.
Hepple [2000] does not provide any complexity figures, and such estimates will depend
on many details such as the baseline accuracy and the size of the classification set
|C|. At any rate, the examples provided show impressive speedups (several orders of
magnitude).
Yet another approach to improving training efficiency, entirely different, is Monte
Carlo (MC) sampling of the rule space [Samuel 1998a; Carberry et al. 2001]. Much
like Brill’s original implementation, in each iteration 1 . . . m, this method first builds a
confusion pair index Ierr and a per-tag-index of correctly tagged locations Icorr, in O(N).
Next, the entries of Ierr are dealt with in descending order of error location count,
and for each error location, correcting rules will be suggested from the templates and
evaluated until a winner has been chosen. However, rather than using all templates,
we are now only using an R-sized sample of them, where R is some small integer (R = 1
has been successfully applied to several tasks). The reasoning is that good rules will
(by definition) be suggested for several error locations, and sampling is unlikely to miss
them all.
In effect, we get an unbiased sample of (the currently interesting part of) the rule
space. Usually the highest scoring rule will be the best one and almost always one of
the best. As we have mentioned (p. 14), small reorderings of good rules are usually not
crucial: when several rules have about the same efficiency, either they correct the same
error, in which case we could pick any of them, or they correct different errors, in which
case they are mostly independent and their local ordering relative to each other is of
little consequence.
The main characteristic of MC TBL learning is that running time does not depend on
the number of templates (an upper bound is O(R·m· N+m· N ·nR) = O(m· N ·nR); this is
still O(mN2) if nR = 	(N), but with much lower constants). Thus, MC sampling is par-
ticularly useful where the number of templates is high—for instance, when the feature-
space is inherently high-dimensional and/or the domain is not (yet) completely under-
stood. Samuel [1998a] shows impressive speedups on dialogue act tagging (two orders
of magnitude in training time and memory usage). But even for a well-researched task
such as POS tagging for English, where a good template set is known, Carberry et al.
[2001] report good speedups (roughly |T |/R) with no concomitant drop in performance.
MC sampling may be suboptimal when rule semantics rather than performance is
the main interest (cf. Section 2.4.1). Also, task specifics may impact performance more
than in the reported cases. For instance, low-scoring rules yield smaller samples and
thus are scored with less certainty than high-scoring rules, for well-known statistical
reasons; also, if there are very many rules with approximately the same score, true
ordering requirements among them are more likely to be violated. Thus, the approach
may be more problematic when the bulk of the error correction is catered for by a large
number of low-scoring rules. At any rate, MC sampling deserves consideration when
nondeterminism is acceptable.
4.3.2. Efficiency in Application. We have already cited Florian [2002b] (Section 2.4.5),
pointing out that the TBL algorithm performs its (computationally expensive) search
during training time. No search is necessary when the rules are applied, and thus that
operation is comparatively cheap. Nevertheless, it requires some thought to go from
comparatively cheap to blazingly fast.
ACM Computing Surveys, Vol. 46, No. 4, Article 50, Publication date: March 2014.
When Errors Become the Rule: Twenty Years with Transformation-Based Learning 50:37
The naive application of a set of learned rules to new data is by sequential substitu-
tion, one rule at a time. Thus, it takes O(mKN) time to apply m rules requiring context
of size K to a input of size N. This may be good enough for some purposes, but when the
rules, for instance, describe a preprocessing step for data-intensive applications (say,
information extraction), it is too slow.
Roche and Schabes [1995] apply finite-state algebra to the rule sequences. They show
that a single learned TBL rule ri may be regarded as a nondeterministic transducer
ti. It follows that the entire set of rules corresponds to the composition of such trans-
ducers, which in itself is a nondeterministic transducer T . Not all nondeterministic
transducers can be determinized. However, the authors show that T , resulting from
composing fixed-width context TBL rule sequences, actually can (except perhaps for
practical limitations). This yields a transducer with O(n) performance in the size of the
input, with very low constants—in fact, the run time of their POS tagger is dominated
by the data transfer time from disk.
4.4. Improving Robustness: Redundant Rules
Florian [2002b, p. 128 f.] points out an artifact of the TBL algorithm: it learns no
redundant rules. That is, in each iteration, only the top-scoring rule is picked, and
other rules that may correct more or less the same group of errors are simply discarded.
Although perhaps only marginally worse than the retained one, the discarded ones are
thus shadowed by it, unlikely to be reactivated. When applied to unseen data, however,
it is possible that the retained rule will never trigger, although one of the discarded
ones would have.
Florian [2002b] gives an example from word sense disambiguation (WSD), where
the goal is to identify the particular sense of a word given its context (out of a set of
possible senses in isolation). An often-cited example in English is “bass,” which denotes
a fish; it also refers to a musical instrument (and has several related musical meanings,
which we gloss over here). The fish sense is very much more likely to occur with context
words such as boat, sea, ocean, salmon, halibut, net, wind, salt, sauce. . .; whereas typical
neighbors of the musical sense are conductor, soprano, jazz, note, groove, scene, concert,
bow, recording . . . . When trained on such data, TBL will choose the best disambiguator
in the training set according to its templates, say “‘salmon’ within a window of width
10” for the fish sense, and discard the others. But, of course, in some new dataset,
“boat” may well be present and “salmon” may not.
Accordingly, to improve TBL robustness by redundant rules, Florian [2002b] suggests
a simple change to the TBL learner: all redundant, discarded rules with at least k
applications and p accuracy (he uses p = 1) are appended to the learned rule sequence.
The addition, notably, happens in ascending order of number of application sites, so that
rules with higher positive counts apply last. The extended rule list will not leave fewer
errors in the original training corpus than the originally learned one alone. However,
on unseen data, one might possibly see an improvement. Indeed, on the WSD task, but
with different datasets, he reports a relative error reduction ranging from a modest
(although statistically significant) 1% to 2% to a very impressive 30%. Word sense
disambiguation represents context with large feature spaces, and the kind of problems
described easily turns up; redundant rule application has apparently not been tried on
other tasks.
4.5. Widening the Bottleneck: Unsupervised TBL
Classification tasks are by definition supervised: they depend on annotations provided
by humans beforehand. For instance, the original TBL algorithm presupposes for the
learning step a reference corpus in which each element in each of the sequences,
whatever they might represent, has been tagged with its true class. Such datasets are
ACM Computing Surveys, Vol. 46, No. 4, Article 50, Publication date: March 2014.
50:38 M. Uneson
generally costly to construct. If one can get away with less expensive resources, it is
of course useful. Brill [1995b] proposes an extension to TBL POS-tagging, where the
baseline annotation of a word is created by simply listing all of its possible tags in some
fixed but arbitrary order, disregarding context. For instance, the baseline annotation
of the will likely be the singleton DT; but export will perhaps list VB|VBP|NN (infinitive
verb, as in Cubans want to export more sugar; present-tense, non-3rd person singular
verb, as in Cubans export sugar; or noun, as in Cuban export of sugar will increase.
The system treats this three-item set as a single tag like any other, except that it can
be decomposed for scoring. The templates look just like before, for instance, A > B <-
tag:@[-1].
Rule scoring has to be adapted: because we do not have access to truth, we cannot
simply count errors corrected. We can, however, score rules according to how efficiently
we expect them to reduce uncertainty. This can be done in many ways. Brill [1995b]
uses the following formula (although differently presented) to score rules derived from
the template A > a <- C (change A to a in context C, where A is an ambiguous tag
and a is one of its possibilities):
score(A, a, C) = U (a)
[
UC(a)
U (a)
− maxY
(
UC(Y )
U (Y )
)]
.
Here, U (X) is the total number of words in the corpus uniquely tagged X, UC(X) is
the number of words uniquely tagged X in the context C, and Y is a variable ranging
over the ambiguous tags of A except a.
In this example, with the specific context c = dt@[-1] (determiner one step to the
left), this amounts to
score(VB|VBP|NN, VB, c) = U (VB)
[
Uc(VB)
U (VB)
− max
(
Uc(VBP)
U (VBP)
,
Uc(NN)
U (NN)
)]
score(VB|VBP|NN, VBP, c) = U (VBP)
[
Uc(VBP)
U (VBP)
− max
(
Uc(VB)
U (VB)
,
Uc(NN)
U (NN)
)]
score(VB|VBP|NN, NN, c) = U (NN)
[
Uc(NN)
U (NN)
− max
(
Uc(VB)
U (VB)
,
Uc(VBP)
U (VBP)
)]
.
Terms of the form UC (X)U (X) range from 0 to 1, and thus the second factor ranges from −1
to 1. Given the context of the example, any real-world corpus will find it close to −1 for
the first two cases and close to 1 for the third.
With only four templates, Brill [1995b] reports rather impressive 96% accuracy (from
a baseline of 90%). However, he simplistically assumes a complete lexicon. Aone and
Hausman [1996] extend the method for Spanish to cope with unseen words, and Becker
[1998] generalizes and parameterizes the approach, allowing compositional templates
and different scoring schemes. For instance, the scoring scheme just presented disre-
gards all ambiguous contexts, which presupposes that there are enough of the non-
ambiguous ones; but one might also want to include ambiguous tagging in the scoring
(and then probably weighted according to degree of uncertainty).
We may extrapolate this idea to CG-style rules (Section 4.2.2) by considering all tags
as sets, possibly singletons, and have the transformations implement set operations
rather than replacements (e.g., reduction: remove a tag from the possibilities). This
allows much easier exploitation of negative information and might be a way to explore
unsupervised transformation-based learning also for the CG paradigm. The search
space is very large, though, and the problematic training time of TBL-CG needs to be
addressed first.
ACM Computing Surveys, Vol. 46, No. 4, Article 50, Publication date: March 2014.
When Errors Become the Rule: Twenty Years with Transformation-Based Learning 50:39
Fig. 9. Chunking the Portuguese sentence O aluno esqueceu o caderno de caligrafia amarelo em casa ‘The
student left the yellow calligraphy notebook at home.’ Example from dos Santos and Oliveira [2005].
4.6. Abstracting the Problem: DSLs and Template Compositionality
dos Santos and Oliveira [2005] apply TBL to chunking (see Section 5.1 for a definition
of the task) in Portuguese, but find that traditional templates are too inflexible to
catch important lexical relations. As a case in point, they mention the relation between
preposition em ‘on’ and verb esqueceu ‘left’ (Figure 9). With the verb esquecer ‘leave,’ a
prepositional phrase headed by em will generally be an adverb; if so, it does not belong
to the object noun phrase. Thus, it should normally be tagged o (not i, as it would in
most other V-PP combinations).
As a remedy for this and similar error-prone cases, the authors propose a widened
“constraint atomic term,” in which templates can express things like “condition on
value x of feature X of the current word’s closest neighbor that has value y for feature
Y.” For the example, they give a template of the form (where [x;y] indicates a closed
interval from x to y, inclusive):
word[0;0](pos=prep) word[2;10]32 (pos=verb).
From the data in Figure 9, such a template would allow the system to induce the
rule
word[0;0](pos=prep)=em word[2;10](pos=verb)=esqueceu --> chunk=O.
This rule should be read “if pos[0]=prep and word[0]=em, and, for the first item in
the closed interval [−2; −10] where pos=verb, word=esqueceu; then change the value
of feature chunk to O for the target item.” The authors then suggest algorithms to
implement their new extension.
We mention this idea not because we find it a good one, but to illustrate a point.
In our view, the suggested extension is at best an acceptable solution to the wrong
problem, a problem that was phrased too specifically to begin with. Much better
than adding one ad-hoc feature or another to the syntax of TBL templates is to
make the templates themselves form a little language, with values, abstractions,
and combinators as befit the domain. Such a language is known as a domain-specific
language (DSL); in the words of van Deursen et al. [2000], a DSL is a “small, usually
declarative, language that offers expressive power focused on a particular problem
domain.” An appetizing alternative to implementing a DSL from scratch is to extend
32Starting two to the left (−2) rather than with the left neighbor (−1) leaves a spot for the direct object, as
in don’t leave keys on the table.
ACM Computing Surveys, Vol. 46, No. 4, Article 50, Publication date: March 2014.
50:40 M. Uneson
Fig. 10. Dialogue act tagging in μ-TBL: data representation, auxiliary predicates, annotated templates.
Adapted excerpt from Lager and Zinovjeva [1999]. Abbreviations: s: speaker, u: utterance, da: dialogue act,
mem: member. The templates are nothing but Prolog clauses with a handful of system-defined operators (for
the convenience of the reader, the ones occurring in the examples are shown at the bottom, with their informal
reading). For instance, the template da:A>B <- s:C@[0] & u_mem:W@[0]) roughly reads “For attribute da,
value A should be retagged to value B whenever attribute s has value C at position 0, and attribute u_mem has
value W at position 0 (that is, when W is one of the words of the utterance at 0).”
some suitable base language with domain-specific constructs. In this way, all features
of the base language can be reused. More importantly, so can the users’ knowledge of
it. Such an extension can range in complexity from some preprocessing step up to a
complete embedding (an embedded DSL [Hudak 1996, 1998]).
To our knowledge, there is only one attempt at a DSL for TBL: the template language
of the μ-TBL system [Lager 1999b]. We look closer at general aspects of μ-TBL (and
other existing TBL systems) in the online Appendix; here, we focus on this particular
DSL. A moderately complex template description for a particular task (dialogue act
tagging) in this system is given in Figure 10. μ-TBL is implemented in the logic pro-
gramming language Prolog, and its template language is just a Prolog extension (by
preprocessing). Figure 10 (as well as the later examples) may not be very informative
for readers unfamiliar with that language, and we must anyway omit some details
ACM Computing Surveys, Vol. 46, No. 4, Article 50, Publication date: March 2014.
When Errors Become the Rule: Twenty Years with Transformation-Based Learning 50:41
Fig. 11. Templates for Portuguese chunking in a fictive DSL. Data representation (top) and auxiliary
predicates in different degrees of abstraction (see Section 4.6).
on data format, argument passing to the auxiliary predicates, and the like. Two main
points should be possible to appreciate, however. First, some features are given di-
rectly in the data representation and others are computed dynamically by auxiliary
predicates of arbitrary complexity, but the templates use both kinds and cannot tell
the difference. If we were to trade space for time by including some of the computed
features in the data representation instead, the templates wouldn’t notice. Second, the
templates themselves are compositional; they are ordinary Prolog terms and can be
assembled and disassembled by the standard mechanisms of the language.
In the following, we take the Portuguese example of Figure 9 as a minimal coding
exercise in a Prolog-styled DSL—fictive, but perfectly conceivable and not much beyond
the existing μ-TBL template language of Figure 10.
First, assuming a data representation similar to what we have seen (Figure 11,
top), we find that the example template can be readily expressed in the predicate
closest_left_verb_2_10(P, W), which will succeed when W is a verb at position P to
the left of the current word, 2 <= P <= 10 (Template 1a in Figure 11, middle).
Of course, there is little reason to hardcode the direction, the bounds, or the POS we
look for. Instead, we can and should abstract, giving them as parameters instead. If
the DSL at hand allows that we specify or instantiate only some of the arguments (i.e.,
partial application, as is typical for functional programming languages such as ML or
Haskell), then abstraction by parameterization is particularly rewarding.
ACM Computing Surveys, Vol. 46, No. 4, Article 50, Publication date: March 2014.
50:42 M. Uneson
Our fictive DSL, unencumbered by practical considerations, obediently supports
anything we can think of, including partial application.33 Thus, we introduce the
more generalized predicate closest/6 (Figure 11, bottom), with the understanding
that the partial application closest(left, verb, 2, 10) behaves just like clos-
est_left_verb_2_10/2. This allows Template 1a to be rephrased as Template 1b. More
interestingly, with closest/6 in place, our DSL now lets us replace the direction to
search in (left) and the POS tags to search for (verb and prep) with variables, as
usual to be filled in with the values that make the strongest predictions. For instance,
Template 2 in Figure 11 expresses something like “condition the chunk tag change on
the current word, its part-of-speech POS1, and the word (within a window to either
left or right) that belongs to a part-of-speech that POS1 usually has informative
associations with.”
To be sure, Template 2 in Figure 11 is not necessarily a good one. If nothing else,
it is almost certainly inefficient: depending on the size of the corpus and the tagsets,
the combinatorial explosion may render it unusable in practice. However, decisions on
which searches to perform and which to prune are better left to the user than to the
designers of the template language.
We could go on and parameterize the window edges or, even better, replace the win-
dow entirely with some predicate “earlier in the sentence.” We will stop here, however,
in the belief that we have demonstrated the expressivity and conciseness a DSL can
gain by reusing well-known concepts and constructs of the base language.
5. TBL IN PRACTICE
This section samples documented uses of TBL as a stand-alone classifier, with emphasis
on the variety of tasks it has been applied to. We also provide examples of slightly
more involved problem encodings that these tasks have prompted. Finally, we look
very briefly at classifier combinations involving TBL, whether horizontal (stacking) or
vertical (e.g., in committees).
5.1. TBL as a Stand-Alone Classifier
TBL is a flexible and adaptable method, as witnessed by the large number of tasks it has
been applied to. Table VII lists a selection of tasks and associated central references.
It is intended to be suggestive rather than exhaustive (a link to a comprehensive
bibliography is given in the online Appendix), but it should cover the most diverse
cases. The task names listed in the table may not be very informative for readers
who are not familiar with linguistic terminology. Indeed, some may be cryptic even to
those who are: not all of the tasks listed are urgent or even relevant for all languages,
due to factors such as cross-linguistic variation, differences in writing systems, and
availability of appropriate data. We refer to standard textbooks such as Jurafsky and
Martin [2008] for the interested;34 however, the details aren’t very important. The
main point is that practically any level of language, spoken or written, is replete with
symbolic classification tasks based on sequential information in the local context; and
with clever encoding, many problems can be made to fit this mold, some of them perhaps
not obviously sequential. Indeed, in recent years, the case has been made [Roth 1998;
33In most real Prolog systems, partial application is a bit uglier; without preprocessing, the exam-
ple might look something like Partial = closest(left, verb, 2, 10), call(Partial, Position, Word).
See Naish [1996] for a critical discussion of higher order programming in Prolog.
34Just to avoid misunderstandings, linguistic classification tasks such as those in Table VII do not form a
closed set. Part-of-speech tagging has been performed by humans for thousands of years, since the early
grammarians of Sanskrit and Greek, but word alignment (deciding what words should be paired together
in sentences which are translations of each other) or document format processing (e.g., extracting rules for
turning poor HTML into well-formed XML) are recent techniques to help satisfy recent needs.
ACM Computing Surveys, Vol. 46, No. 4, Article 50, Publication date: March 2014.
When Errors Become the Rule: Twenty Years with Transformation-Based Learning 50:43
Table VII. Sample TBL Applications
Task Reference
Part-of-speech (POS) tagging Brill [1993b, 1995a,b]
Unknown word guessing Brill [1994], Mikheev [1997]
Text chunking Ramshaw and Marcus [1995]
Prepositional phrase attachment Brill and Resnik [1994]
Parsing/grammar induction Brill [1996]
Morphological disambiguation Oflazer and Tür [1996]
Spelling correction Mangu and Brill [1997]
Word segmentation Palmer [1997]
Message understanding Day et al. [1997]
Dialogue act tagging Samuel et al. [1998]
Prosody prediction Fordyce [1998]
Ellipsis resolution Hardt [1998]
Word sense disambiguation Dini et al. [1998], Florian [2002b]
Document format processing Curran and Wong [1999]
Grapheme-phoneme conversion Bouma [2000]
Grammar correction Hardt [2001]
Handwritten character segmentation Kavallieratou et al. [2000]
Regression Bringmann et al. [2002]
Hyphenation Bouma [2003]
Named entity recognition Aberdeen et al. [1995], Florian et al. [2003]
Compound segmentation Park et al. [2004]
Disfluency detection Kim et al. [2004]
Semantic role labeling Williams et al. [2004]
Word alignment Ayan et al. [2005]
Information extraction Nahm [2005]
Biomedical term normalization Tsuruoka et al. [2008]
Human activity recognition Landwehr et al. [2008]
Daelemans 1995] that all useful linguistic inferences can be viewed as problems of
general classification (in linguistic terms, either as disambiguation or segmentation).
This assumption has opened new fields to several machine-learning algorithms—TBL
is just one of them.
We only give two examples here, chosen to illustrate diversity in problem encodings
(and scoring functions) in TBL. A few more can be found elsewhere in this text (e.g.,
Section 4.6) and, of course, many others in the references of Table VII.
In NP chunking, a string of words is to be divided into nonoverlapping, nonrecursive
subsequences corresponding to noun phrases (NPs).35 It is a special case, although the
most important one, of the task of text chunking, where other types of phrases also are
targeted. Figure 12 shows an example and a way (from Ramshaw and Marcus [1995])
of casting the task as a classification problem very much like POS tagging, with the
same type of rules learned. Differently from POS taggers, but like the CG learner we
encountered earlier (Section 4.2.2), NP chunkers are usually evaluated on precision
and recall and/or their combination into F1 (Section 2.3.4); the latter measure is then
also the scoring function of the TBL learner.
The more intricate problem of parsing requires the encoding of recursive structures.
The straightforward although somewhat unlinguistic solution proposed in Brill [1993a]
is shown in Figure 13. For parsing, there are several possible measures of accuracy
[Carroll et al. 1998] and several conceivable TBL scoring functions. Brill [1993a] follows
35There is no single “correct” solution to this problem; practical needs will decide how to treat modifiers
such as possessives, prepositional phrases, or relative clauses. Of course, whatever the decision, it should be
consistently held to.
ACM Computing Surveys, Vol. 46, No. 4, Article 50, Publication date: March 2014.
50:44 M. Uneson
Fig. 12. NP chunks with encodings for light the candle on the table after Ramshaw and Marcus [1995],
before and after applying the transformation I>O <- wd:the@[1]. I and O mark inside and outside an NP,
respectively. In this particular tagset, known as IOB, a third tag, B, is used for the first word in a new
chunk directly following another one, as in give/O the/I king/I his/B throne/I. Several other tagsets have been
proposed for the task, e.g. Tjong Kim Sang and Veenstra [1999].
Fig. 13. Parse trees with encodings for light the candle on the table after Brill [1993a], before and after
applying the transformation delete a left paren to the right of a determiner. Not shown are some
behind-the-scene machinations to cater for parenthesis balancing (see Brill [1993a] for details).
Pereira and Schabes [1992] in using the percentage of noncrossing constituents36 when
system output is compared to truth, as provided by a human annotator. For instance,
if we imagine that the transformation depicted in Figure 13 was never found by the
system, so that Figure 13(a) shows the final output of the parser and Figure 13(b)
shows truth, then on the table would be considered a correct constituent, but candle on
the table would not.
5.2. TBL in Company: Ensemble Learning
As with any other classifier, in addition to its stand-alone use exemplified in the previ-
ous section, TBL may be used in combination with other classifiers. We have noted (Sec-
tion 2.3) that a particularly simple way of doing this is by classifier stacking [Wolpert
1992]. That is, TBL is used as a postprocessing, error-correcting step: because the al-
gorithm assumes an initial-state annotation, but cares little where it comes from, it
could just as well take the output of any other classifier as this initial baseline (e.g.,
Ruland [2000], Wu et al. [2004]). Of course, the converse is also possible, where a TBL
classifier feeds its output into another system (e.g., Florian [2002a]).
There are more sophisticated approaches than this to ensemble learning; that is,
combining multiple classifiers into one in the hope of reaching better predictive perfor-
mance than could be obtained from any of the constituent models alone. For instance,
36A constituent is an internal node in the parse tree, or, operationally, the string of words between matching
parentheses.
ACM Computing Surveys, Vol. 46, No. 4, Article 50, Publication date: March 2014.
When Errors Become the Rule: Twenty Years with Transformation-Based Learning 50:45
Fig. 14. Induced algorithm clustering based on similarity of classification behavior in a word-sense disam-
biguation task (Senseval2 for English, [Edmonds 2002]). Similarity between two classifiers is measured as
pairwise agreement rate. Dendrogram generated by maximal-linkage hierarchical agglomerative clustering.
Figure 6.13 of Florian [2002b], reprinted with permission. See Section 5.2 for details.
a typical committee classifier might combine multiple ground-level classifiers (“com-
mittee members”) with a higher level classifier (“president”), with the idea that the
different committee members have complementary strengths and weaknesses, and
the job of the president is to learn when to trust whom. Ensemble learners generally
gain from being composed by classifiers with complementary strengths [Kuncheva and
Whitaker 2003]. To put it another way, ensemble learning is pointless if all component
classifiers make the same errors.
Classifier combination, whether stacked or committee-based, largely falls outside the
scope of this article.37 We make two points, however. First, in stacking, when used on
the output of one or more other classifiers, TBL rules may freely provide some analysis
of the errors of these (besides, hopefully, boosting overall performance). Second, TBL
is different enough from most other classifiers to make it a worthy member of many
committees.
As a single but indicative data point, Florian [2002b, Ch. 6] presents a comparative
empirical study on word sense disambiguation: five different supervised classifiers
working on the same set of features. The feature sets are the basic and commonly used
bag-of-words (i.e., the multiset of neighbors of a given word, within some window), but
also derived features describing local context (n-grams) and syntactic context (closest
modifiers). The participants are TBL, decision lists (DL; Section 3.1), Bayes ratio [Gale
et al. 1992], cosine similarity; and Naive Bayes [Manning and Schütze 2001]. The latter
is called “Feature-Enhanced Naive Bayes” (FENBayes) in the study, to distinguish it
from a sixth baseline algorithm: an even more naive Naive Bayes that has access only to
the bag-of-words. Florian divides the participating algorithms into “discriminative” (DL
and TBL), which try to identify a single or at most a few maximally informative features
in any given context, and “aggregative” (the others), which seek to weigh together all
available evidence. This division in algorithmic design is indeed reflected in observed
behavior, so that TBL and DL are rather different from the others. Figure 14 shows a
hierarchical clustering based on pairwise agreement rate of output classifications for
these six algorithms. We refer to Florian [2002b] for details.
37Examples of practical use of TBL in committees include Brill and Wu [1998], Florian et al. [2003], and Li
et al. [2006].
ACM Computing Surveys, Vol. 46, No. 4, Article 50, Publication date: March 2014.
50:46 M. Uneson
5.3. TBL in TBL Company
The aim of classifier combination of the type described in Section 5.2 is chiefly to boost
performance: combining the complementary strengths of several classifiers into a new
and better one. This can be done also with TBL as the single base learner. For instance,
a number of slightly different classifiers can be induced from repeated resamplings of
equally sized subsets of the training data (bootstrap aggregating, or bagging [Breiman
1996]). At classification time, each TBL classifier is applied independently, and its
output is taken as one vote for that class. After counting votes, the majority wins. dos
Santos et al. [2010] claim to be the first to try this approach for their ETL algorithm
(Section 4.1.2). Their resulting combined classifier is tried on text chunking, named
entity recognition, and semantic role labeling. It shows substantial improvement on
ETL alone, especially for the semantic role labeling task, and it achieves competitive
or close to state-of-the-art results for all of them.
Another idea for combining the knowledge of several TBL classifiers, apparently
unexplored, is to merge the rule sequences learned in bagging, rather than weighing
together the classifications they emit. Basically, rules that are ranked high by all of
the classifiers are highly likely to be relevant, but rules ranked low or (in particular)
learned only by a few are likely not. Combining preferences or ranked lists is a common
problem in, for instance, information retrieval (e.g., Freund et al. [2003]).
As pointed out previously, TBL may overtrain in the sense that many of its late
learned rules are spurious and irrelevant; its resistance to overtraining lies in the fact
that such rules are automatically ranked as low-impact: they will not influence perfor-
mance much in either direction. Thus, we might not expect a great performance boost
from weeding out irrelevant rules. However, if our main interest is the knowledge
distilled, that is beside the point.
6. CONCLUSION
In this article, we have aimed at a self-contained introduction to the original TBL
algorithm and a survey of the most important later developments. In the introduction,
we also expressed the hope of promoting the general interest in and usage of TBL. As
we mentioned, the method is almost unheard of outside the world of computational
linguistics. Indeed, the absence of nonlinguistic applications in Table VII is striking,
the only evident exceptions being Bringmann et al. [2002] and Landwehr et al. [2008].
We don’t see any particular reasons why this should be so. It is true that the kind of
local dependencies TBL is especially suited for exploiting are particularly common in
language, but they certainly occur elsewhere.
Practical matters are important, of course. To encourage usage, good introductions
and good toolkits are instrumental. We hope the present text may fulfill the first role,
even though we have had to sacrifice detail for overview in many places. Where the
full story is needed, we believe the pointers provided will be helpful; if not, we refer
to the previously mentioned TBL bibliography. The online Appendix provides links to
this and a few other TBL resources on the web. As for the second role, the Appendix
also briefly surveys the situation with respect to open-sourced implementations, which
perhaps can best be summarized as “contributions welcome.”
Turning to the TBL method itself, we may briefly speculate on future general devel-
opments. We have mentioned several areas that seem to deserve wider attention (e.g.,
ETL, Section 4.1.2; TBR, Section 4.2.3; probabilistic TBL, Section 4.2.1; Constraint
Grammar-TBL, Section 4.2.2) and others that apparently have been explored little or
not at all (e.g., weights in multidimensional learning, Section 4.1.1; unsupervised Con-
strained Grammar-TBL, Section 4.5; alternative scoring schemes, Section 2.3.3; TBL
ACM Computing Surveys, Vol. 46, No. 4, Article 50, Publication date: March 2014.
When Errors Become the Rule: Twenty Years with Transformation-Based Learning 50:47
rule purification in ensembles, Section 5.3). Time will tell which ones of these are worth
their salt.
Ideally, we will by then also know what modifications to TBL theory (Section 3)
the different extensions may motivate. However, judging from the current TBL theory
versus TBL practice publication ratio, this may be a bit optimistic: it is fair to say that,
with notable but few exceptions, the interest of TBL authors so far has been squarely
practical.
In a slightly larger perspective, it should be recognized that TBL is (mostly) a su-
pervised machine learning method and that, generally speaking, the trend has moved
away from most such methods. The current focus of the machine learning community
clearly lies on unsupervised learning on very large datasets. Still, for most NLP tasks,
the application of unsupervised learning to practical problems is still at an early stage,
and at any rate, not all datasets are gigantic. We expect that there will always be some
use for supervised methods in specialized and not-so-specialized domains.
ACKNOWLEDGMENT
Several people have been gracious enough to review earlier versions of this article, substantially improving
it. Specifically, I would like to thank Mats-Eeg Olofsson for proofreading an early draft; Joakim Nivre and
three anonymous reviewers for helpful corrections, suggestions, and requests for clarification; and Garnett
Wilson and Radu Florian for kindly providing original figures and other materials.
REFERENCES
Harold Abelson and Gerald J. Sussman. 1996. Structure and Interpretation of Computer Programs. MIT
Press, Cambridge.
John Aberdeen, John Burger, David Day, Lynette Hirschman, Patricia Robinson, and Marc Vilain. 1995.
MITRE: description of the Alembic system used for MUC-6. In Proceedings of the 6th Conference on
Message Understanding. Association for Computational Linguistics, 141–155.
Chinatsu Aone and Kevin Hausman. 1996. Unsupervised learning of a rule-based Spanish part of speech
tagger. In Proceedings of the 16th Conference on Computational Linguistics, Vol. 1. Association for
Computational Linguistics, 53–58.
Nezip F. Ayan, Bonnie J. Dorr, and Christof Monz. 2005. Alignment link projection using transformation-
based learning. In Proceedings of the Conference on Human Language Technology and Empirical Methods
in Natural Language Processing. Association for Computational Linguistics, 185–192.
Lalit R. Bahl, Peter F. Brown, Peter V. de Souza, and Robert L. Mercer. 1989. A tree-based statistical
language model for natural language speech recognition. Acoustics, Speech and Signal Processing, IEEE
Transactions 37, 7 (1989), 1001–1008.
Markus Becker. 1998. Unsupervised part of speech tagging with extended templates. In Proceedings of
ESSLLI 1998, Student Session.
Gosse Bouma. 2000. A finite state and data oriented method for grapheme to phoneme conversion. In
NAACL-2000. 303–310.
Gosse Bouma. 2003. Finite state methods for hyphenation. Natural Language Engineering 9 (2003), 5–20.
DOI:http://dx.doi.org/10.1017/S1351324903003073
Leo Breiman. 1996. Bagging predictors. Machine Learning 24, 2 (1996), 123–140.
Leo Breiman, Jerome Friedman, Richard Olshen, and Charles Stone. 1984. Classification and Regression
Trees. Wadsworth and Brooks, Monterrey, CA.
Eric Brill. 1993a. Automatic grammar induction and parsing free text: A transformation-based approach. In
Proceedings of the Workshop on Human Language Technology. Association for Computational Linguis-
tics, 237–242.
Eric Brill. 1993b. A Corpus-Based Approach to Language Learning. Ph.D. Dissertation. University of Penn-
sylvania, Philadelphia, PA.
Eric Brill. 1994. Some advances in transformation-based part of speech tagging. In Proceedings of the 12th
National Conference on Artificial Intelligence. Arxiv preprint cmp-lg/9406010 (1994), 722–727.
Eric Brill. 1995a. Transformation-based error-driven learning and natural language processing: A case study
in part of speech tagging. Computational Linguistics 21, 4 (1995), 543–565.
ACM Computing Surveys, Vol. 46, No. 4, Article 50, Publication date: March 2014.
50:48 M. Uneson
Eric Brill. 1995b. Unsupervised learning of disambiguation rules for part of speech tagging. In Proceedings
of the 3rd Workshop on Very Large Corpora, Vol. 30. 1–13.
Eric Brill. 1996. Learning to parse with transformations. In Recent Advances in Parsing Technology. Kluwer.
Eric Brill and Philip Resnik. 1994. A rule-based approach to prepositional phrase attachment disambigua-
tion. In Proceedings of COLING’94. 1198–1204.
Eric Brill and Jun Wu. 1998. Classifier combination for improved lexical disambiguation. In Proceedings of
the 17th International Conference on Computational Linguistics, Vol. 1. Association for Computational
Linguistics, 191–195.
Björn Bringmann, Stefan Kramer, Friedrich Neubarth, Hannes Pirker, and Gerhard Widmer. 2002.
Transformation-based regression. In Machine Learning: International Workshop then Conference. Cite-
seer, 59–66.
Sandra Carberry, K. Vijay-Shanker, Andrew Wilson, and Ken Samuel. 2001. Randomized rule selection in
transformation-based learning: A comparative study. Natural Language Engineering 7, 2 (2001), 99–116.
John Carroll, Ted Briscoe, and Antonio Sanfilippo. 1998. Parser evaluation: A survey and a new proposal. In
Proceedings of the 1st International Conference on Language Resources and Evaluation. 447–454.
Rich Caruana. 1997. Multitask learning. Machine Learning 28, 1 (1997), 41–75.
James R. Curran and Raymond K. Wong. 1999. Transformation-based learning for automatic translation
from HTML to XML. In Proceedings of the 4th Australasian Document Computing Symposium (ADCS99).
Citeseer.
James R. Curran and Raymond K. Wong. 2000. Formalization of transformation-based learning. In ACSC.
IEEE Computer Society, 51–57.
Walter Daelemans. 1995. Memory-based lexical acquisition and processing. In Machine Translation and the
Lexicon, P. Steffens (Ed.). Springer, Berlin, 85–98.
David Day, John Aberdeen, Lynette Hirschman, Robyn Kozierok, Patricia Robinson, and Marc Vilain. 1997.
Mixed-initiative development of language processing systems. In Proceedings of the Fifth Conference on
Applied Natural Language Processing. Association for Computational Linguistics, 348–355.
Luca Dini, Vittorio Di Tomaso, and Frédérique Segond. 1998. Error driven word sense disambiguation. In
Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th In-
ternational Conference on Computational Linguistics, Vol. 1. Association for Computational Linguistics,
320–324.
Cı́cero N. dos Santos. 2009. Entropy Guided Transformation Learning. Ph.D. Dissertation. Pontifı́cia Uni-
versidade Católica do Rio de Janeiro.
Cı́cero N. dos Santos and Ruy L. Milidiú. 2007. Probabilistic classifications with TBL. In Computational Lin-
guistics and Intelligent Text Processing, Alexander Gelbukh (Ed.). Lecture Notes in Computer Science,
Vol. 4394. Springer, Berlin, 196–207.
Cı́cero N. dos Santos and Ruy L. Milidiú. 2009. Entropy guided transformation learning. Foundations of
Computational Intelligence. 1, (2009), 159–184.
Cı́cero N. dos Santos, Ruy L. Milidiú, Carlos E. M. Crestana, and Eraldo R. Fernandes. 2010. ETL Ensembles
for Chunking, NER and SRL. In Computational Linguistics and Intelligent Text Processing, Alexander
Gelbukh (Ed.). Lecture Notes in Computer Science, Vol. 6008. Springer, Berlin, 100–112.
Cı́cero N. dos Santos, Ruy L. Milidiú, and Raúl Renterı́a. 2008. Portuguese part-of-speech tagging using en-
tropy guided transformation learning. In Computational Processing of the Portuguese Language, António
Teixeira, Vera de Lima, Luı́s de Oliveira, and Paulo Quaresma (Eds.). Lecture Notes in Computer Sci-
ence, Vol. 5190. Springer, Berlin, 143–152.
Cı́cero N. dos Santos and Claudia Oliveira. 2005. Constrained atomic term: Widening the reach of rule
templates in transformation based learning. In EPIA(Lecture Notes in Computer Science), Carlos Bento,
Amı́lcar Cardoso, and Gaël Dias (Eds.), Vol. 3808. Springer, 622–633. DOI:http://dx.doi.org/10.1007/
11595014_61
Philip Edmonds. 2002. SENSEVAL: The evaluation of word sense disambiguation systems. ELRA Newsletter
7, 3 (2002), 5–14.
Eraldo R. Fernandes, Cı́cero N. dos Santos, and Ruy L. Milidiú. 2010. A machine learning approach to
Portuguese clause identification. Computational Processing of the Portuguese Language (2010), 55–64.
Radu Florian. 2002a. Named entity recognition as a house of cards: Classifier stacking. In Proceedings of the
6th Conference on Natural Language Learning. 1–4.
Radu Florian. 2002b. Transformation Based Learning and Data-Driven Lexical Disambiguation. Syntactic
and Semantic Ambiguity Resolution. Ph.D. Dissertation, Johns Hopkins University.
Radu Florian, John Henderson, and Grace Ngai. 2000. Coaxing confidences from an old friend: Probabilistic
classifications from transformation rule lists. In Proceedings of the 2000 Joint SIGDAT Conference on
Empirical Methods in NLP and Very Large Corpora. Association for Computational Linguistics, 26–34.
ACM Computing Surveys, Vol. 46, No. 4, Article 50, Publication date: March 2014.
When Errors Become the Rule: Twenty Years with Transformation-Based Learning 50:49
Radu Florian, Abe Ittycheriah, Hongyan Jing, and Tong Zhang. 2003. Named entity recognition through
classifier combination. In Proceedings of the 7th Conference on Natural Language Learning at HLT-
NAACL 2003, Vol. 4. Association for Computational Linguistics, 171.
Radu Florian and Grace Ngai. 2001. Multidimensional transformation-based learning, In Proceedings of the
5th Workshop on Computational Language Learning (CoNLL-2001). CoRR cs.CL/0107021 (2001).
Cameron Fordyce. 1998. Prosody Prediction for Speech Synthesis Using Transformational Rule-Based Learn-
ing. Master’s Thesis, Boston University.
Yoav Freund, Raj Iyer, Robert E. Schapire, and Yoram Singer. 2003. An efficient boosting algorithm for
combining preferences. Journal of Machine Learning Research 4 (2003), 933–969.
William A. Gale, Kenneth W. Church, and David Yarowsky. 1992. A method for disambiguating word senses
in a large corpus. Computers and the Humanities 26, 5–6 (1992), 415–439.
Daniel Hardt. 1998. Improving ellipsis resolution with transformation-based learning. In AAAI Fall Sympo-
sium.
Daniel Hardt. 2001. Transformation-based learning of Danish grammar correction. In Proceedings of RANLP
2001, Tzigov Chark. Citeseer.
Per Hedelin, Anders Jonsson, and Per Lindblad. 1987. Svenskt uttalslexikon (3rd ed.). Technical report.
Chalmers University of Technology.
Mark Hepple. 2000. Independence and commitment: Assumptions for rapid training and execution of rule-
based POS taggers. In Proceedings of the 38th Annual Meeting on Association for Computational Lin-
guistics. Association for Computational Linguistics, 278.
Paul Hudak. 1996. Building domain-specific embedded languages. ACM Computing Surveys (CSUR) 28, 4
(1996).
Paul Hudak. 1998. Modular domain specific languages and tools. In Proceedings of the 5th International
Conference on Software Reuse, P. Devanbu and J. Poulin (Eds.). IEEE Computer Society Press, 134–142.
Daniel Jurafsky and James H. Martin. 2008. An Introduction to Natural Language Processing, Computa-
tional Linguistics, and Speech Recognition (2nd ed.). Prentice-Hall.
Fred Karlsson, Atro Voutilainen, Juha Heikkilä, and Arto Anttila (Eds.). 1995. Constraint Grammar: A
Language-Independent System for Parsing Unrestricted Text. Mouton de Gruyter.
Ergina Kavallieratou, Efstathios Stamatatos, Nikos Fakotakis, and George Kokkinakis. 2000. Handwritten
character segmentation using transformation-based learning. In Proceedings of the 15th International
Conference on Pattern Recognition (ICPR’00). 634–637.
Joungbum Kim, Sarah E. Schwarm, and Mari Ostendorf. 2004. Detecting structural metadata with decision
trees and transformation-based learning. In Proceedings of HLT-NAACL04. 137–144.
Ludmila I. Kuncheva and Christopher J. Whitaker. 2003. Measures of diversity in classifier ensembles and
their relationship with the ensemble accuracy. Machine Learning 51, 2 (2003), 181–207.
Torbjörn Lager. 1999a. μ-TBL Lite: A small, extensible transformation-based learner. In Proceedings of the
9th Conference of the European Chapter of the Association for Computational Linguistics (EACL’99).
Bergen. Poster paper.
Torbjörn Lager. 1999b. The μ-TBL system: Logic programming tools for transformation-based learning. In
Proceedings of CoNLL, Vol. 99.
Torbjörn Lager. 2001. Transformation-based learning of rules for constraint grammar tagging. In 13th Nordic
Conference in Computational Linguistics. Uppsala, Sweden, 21–22.
Torbjörn Lager and Natalia Zinovjeva. 1999. Training a dialogue act tagger with the μ-TBL System. In Pro-
ceedings of the 3rd Swedish Symposium on Multimodal Communication. Linköping University Natural
Language Processing Laboratory (NLPLAB).
Niels Landwehr, Bernd Gutmann, Ingo Thon, Luc De Raedt, and Matthai Philipose. 2008. Relational
transformation-based tagging for human activity recognition. Fundamenta Informaticae 89, 1 (2008),
111–129.
Xin Li, Xuan-Jing Huang, and Li-de Wu. 2006. Question classification by ensemble learning. IJCSNS 6, 3
(2006), 147.
Nikolaj Lindberg and Martin Eineborg. 1998. Learning constraint grammar-style disambiguation rules using
inductive logic programming. In Proceedings of the 17th International Conference on Computational
Linguistics. Association for Computational Linguistics, 775–779.
Lidia Mangu and Eric Brill. 1997. Automatic rule acquisition for spelling correction. In Machine Learning –
International Workshop then Conference. Citeseer, 187–194.
Christopher D. Manning and Hinrich Schütze. 2001. Foundations of Statistical Natural Language Processing.
MIT Press, Cambridge, MA.
ACM Computing Surveys, Vol. 46, No. 4, Article 50, Publication date: March 2014.
50:50 M. Uneson
Andrei Mikheev. 1997. Automatic rule induction for unknown-word guessing. Computational Linguistics 23,
3 (1997), 405–423.
Ruy Luiz Milidiú, C. E. M. Crestana, and Cı́cero Nogueira dos Santos. 2010. A token classification approach
to dependency parsing. In Proceedings of the 7th Brazilian Symposium on Information and Human
Language Technology (STIL’09). IEEE, 80–88.
Ruy L. Milidiú, Cı́cero N. dos Santos, and Julio C. Duarte. 2008. Phrase chunking using entropy guided
transformation learning. In Proceedings of ACL 2008. Citeseer.
Ruy L. Milidiú, Julio C. Duarte, and Cı́cero N. dos Santos. 2007. Evolutionary TBL template generation.
Journal of the Brazilian Computer Society 13(4) (2007), 39–50.
Tom Mitchell. 1997. Machine Learning. McGraw-Hill.
Un Yong Nahm. 2005. Transformation-based information extraction using learned meta-rules. Computa-
tional Linguistics and Intelligent Text Processing (2005), 535–538.
Lee Naish. 1996. Higher-order logic programming in Prolog. In Proceedings of the Workshop on Multi-
Paradigm Logic Programming, JICSLP, Vol. 96.
Grace Ngai and Radu Florian. 2001a. Transformation-based learning in the fast lane. In Proceedings of
the 2nd Meeting of the North American Chapter of the Association for Computational Linguistics on
Language Technologies 2001. Association for Computational Linguistics, 8.
Grace Ngai and Radu Florian. 2001b. Transformation Based Learning in the Fast Lane: A Generative Ap-
proach. Technical Report. Center for Speech and Language Processing, Johns Hopkins University.
Kemal Oflazer and Gökhan Tür. 1996. Combining hand-crafted rules and unsupervised learning in
constraint-based morphological disambiguation. In Proceedings of the Conference on Empirical Methods
in Natural Language Processing. 69–81.
Jonathan Oliver. 1992. Decision Graphs: An Extension of Decision Trees. Technical Report 92/173. Depart-
ment of Computer Science, Monash University.
David D. Palmer. 1997. A trainable rule-based algorithm for word segmentation. In Proceedings of the 35th
Annual Meeting of the Association for Computational Linguistics and Eighth Conference of the European
Chapter of the Association for Computational Linguistics. Association for Computational Linguistics,
321–328.
Seong-Bae Park, Jeong-Ho Chang, and Byoung-Tak Zhang. 2004. Korean compound noun decomposition
using syllabic information only. Computational Linguistics and Intelligent Text Processing (2004), 146–
157.
Fernando Pereira and Yves Schabes. 1992. Inside-outside reestimation from partially bracketed corpora. In
ACL.
J. Ross Quinlan. 1993. C4.5: Programs for Machine Learning. Morgan Kaufmann.
Lance A. Ramshaw and Mitchell P. Marcus. 1994. Exploring the statistical derivation of transformational
rule sequences for part-of-speech tagging. In Proceedings of the ACL Workshop on Combining Symbolic
and Statistical Approaches to Language. 128–135.
Lance A. Ramshaw and Mitchell P. Marcus. 1995. Text chunking using transformation-based learning. In
Proceedings of the ACL 3rd Workshop on Very Large Corpora, David Yarowsky and Kenneth W. Church
(Eds.), Vol. cmp-lg/9505040. Association of Computational Linguistics, Somerset, NJ, 82–94.
Ronald Rivest. 1987. Learning decision lists. Machine Learning 2, 3 (1987), 229–246.
Emmanuel Roche and Yves Schabes. 1995. Deterministic part-of-speech tagging with finite-state transduc-
ers. Computational Linguistics 21, 2 (1995), 227–253.
Dan Roth. 1998. Learning to resolve natural language ambiguities: A unified approach. In Proceedings of
the National Conference on Artificial Intelligence. John Wiley & Sons Ltd., 806–813.
Tobias Ruland. 2000. A context-sensitive model for probabilistic LR parsing of spoken language with
transformation-based postprocessing. In Proceedings of the 18th Conference on Computational Lin-
guistics, Vol. 2. Association for Computational Linguistics, 677–683.
Ken Samuel. 1998a. Discourse learning: Dialogue act tagging with transformation-based learning. In Pro-
ceedings of the National Conference on Artificial Intelligence. John Wiley and Sons, Ltd., 1199–1199.
Ken Samuel. 1998b. Lazy transformation-based learning. In Proceedings of the 11th International Florida
Artificial Intelligence Research Society Conference. AAAI Press, 235–239.
Ken Samuel, Sandra Carberry, and K. Vijay-Shanker. 1998. An investigation of transformation-based learn-
ing in discourse. In Machine Learning: Proceedings of the 15th International Conference.
Christer Samuelsson, Pasi Tapanainen, and Atro Voutilainen. 1996. Inducing constraint grammars. Gram-
matical Interference: Learning Syntax from Sentences (1996), 146–155.
ACM Computing Surveys, Vol. 46, No. 4, Article 50, Publication date: March 2014.
When Errors Become the Rule: Twenty Years with Transformation-Based Learning 50:51
Erik Tjong, Kim Sang, and Jorn Veenstra. 1999. Representing text chunks. In Proceedings of the 9th Con-
ference on European Chapter of the Association for Computational Linguistics. Association for Compu-
tational Linguistics, 173–179.
Yoshimasa Tsuruoka, John McNaught, and Sophia Ananiadou. 2008. Normalizing biomedical terms by
minimizing ambiguity and variability. BMC Bioinformatics 9, Suppl 3 (2008), S2.
Leslie G. Valiant. 1984. A theory of the learnable. Communication ACM 27, 11 (1984), 1134–1142.
Arie van Deursen, Paul Klint, and Joost Visser. 2000. Domain-specific languages: An annotated bibliography.
ACM SIGPLAN Notices 35, 6 (2000), 26–36.
Ken Williams, Christopher Dozier, and Andrew McCulloh. 2004. Learning transformation rules for semantic
role labeling. In Proceedings of CoNLL-2004.
Garnett Wilson and Malcolm Heywood. 2005. Use of a genetic algorithm in Brill’s transformation-based
part-of-speech tagger. In GECCO’05: Proceedings of the 2005 Conference on Genetic and Evolutionary
Computation. ACM, New York, NY, 2067–2073. DOI:http://dx.doi.org/10.1145/1068009.1068352
David Wolpert. 1992. Stacked generalization. Neural Networks 5(2) (1992), 241260.
Dekai Wu, Grace Ngai, and Marine Carpuat. 2004. Raising the bar: Stacked conservative error correction be-
yond boosting. In Proceedings of the 4th International Conference on Language Resources and Evaluation
(LREC-2004). Lisbon.
George K. Zipf. 1949. Human Behavior and the Principle of Least Effort. Addison-Wesley.
Win Zonneveld, Mieke Trommelen, Michael Jessen, Curtis Rice, Gösta Bruce, and Kristjan Arnason. 1999.
Wordstress in West-Germanic and North-Germanic languages. In Word Prosodic Systems in the Lan-
guages of Europe, Harry van der Hulst (Ed.). Walter de Gruyter, Chapter 8, 477–604.
Received October 2012; revised September 2013; accepted October 2013
ACM Computing Surveys, Vol. 46, No. 4, Article 50, Publication date: March 2014.
