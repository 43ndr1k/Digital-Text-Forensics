This article was downloaded by: [University of Aegean]
On: 03 December 2013, At: 00:25
Publisher: Routledge
Informa Ltd Registered in England and Wales Registered Number: 1072954 Registered
office: Mortimer House, 37-41 Mortimer Street, London W1T 3JH, UK
Information & Communications
Technology Law
Publication details, including instructions for authors and
subscription information:
http://www.tandfonline.com/loi/cict20
Legal evidence and advanced
computing techniques for combatting
crime: an overview
Ephraim Nissana
a Department of Computing, Goldsmiths’ College, University of
London, London, UK
Published online: 27 Nov 2013.
To cite this article: Ephraim Nissan , Information & Communications Technology Law (2013): Legal
evidence and advanced computing techniques for combatting crime: an overview, Information &
Communications Technology Law, DOI: 10.1080/13600834.2013.849438
To link to this article:  http://dx.doi.org/10.1080/13600834.2013.849438
PLEASE SCROLL DOWN FOR ARTICLE
Taylor & Francis makes every effort to ensure the accuracy of all the information (the
“Content”) contained in the publications on our platform. However, Taylor & Francis,
our agents, and our licensors make no representations or warranties whatsoever as to
the accuracy, completeness, or suitability for any purpose of the Content. Any opinions
and views expressed in this publication are the opinions and views of the authors,
and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content
should not be relied upon and should be independently verified with primary sources
of information. Taylor and Francis shall not be liable for any losses, actions, claims,
proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or
howsoever caused arising directly or indirectly in connection with, in relation to or arising
out of the use of the Content.
This article may be used for research, teaching, and private study purposes. Any
substantial or systematic reproduction, redistribution, reselling, loan, sub-licensing,
systematic supply, or distribution in any form to anyone is expressly forbidden. Terms &
Conditions of access and use can be found at http://www.tandfonline.com/page/terms-
and-conditions
Legal evidence and advanced computing techniques for combatting
crime: an overview
Ephraim Nissan*
Department of Computing, Goldsmiths’ College, University of London, London, UK
Surprisingly, evidence used to be a Cinderella until the late 1990s in ‘AI & Law’
(artificial intelligence for law), itself a burgeoning domain already in the 1980s. It
was not until the 2000s that models of reasoning about legal evidence started to
feature prominently. In so doing, it became vulnerable to the controversy about
‘probabilities in law’ among legal theorists; hence, the importance of developing
models of plausibility (i.e. ranking alternative accounts) as opposed to strong
commitment to probabilistic models of determination of guilt. Probabilistic models
however potentially have a role in helping a prosecutor decide whether to prosecute.
Moreover, they are quite useful in models supporting police investigation or helping
with police intelligence. This is especially the case of data mining, a class of
techniques which has been applied to legal databases as well as to law enforcement.
Success was achieved especially in unravelling networks and in detecting fraud. We
survey these classes of tools.
Keywords: artificial intelligence & law; legal evidence; juries; data mining and link
analysis; forensic applications; controversy about probabilities in law (reference-class
problem)
1. Introduction: how ‘AI & Law’ came to also concern itself with the evidence
Notwithstanding two seminal publications in 1989 (Kuflik, Nissan, & Puni, 1989; Thagard,
1989), it was not until the 2000s that models of reasoning about legal evidence (other than
within statistics) really took off within the discipline of artificial intelligence for law (AI &
Law), starting with a few editorial initiatives (see now Kaptein, Prakken, & Verheij, 2009;
MacCrimmon & Tillers, 2002; Martino & Nissan, 2001; Nissan & Martino, 2001, 2003,
2004). The end of the decade was soon followed by two important authored books on
the subject (Bex, 2011; Nissan, 2012). In particular, Nissan (2012) represents the culmina-
tion of an endeavour undertaken in the late 1980s, and gaining momentum following 1996.
The year 1989 was in a sense an annus mirabilis in terms of modes of reasoning about
the evidence emerging within ‘AI & Law’. It was a direction of research within ‘AI & Law’
kick-started by the earliest publication by the Canadian philosopher Paul Thagard about his
ECHO project and by my own earliest publication about my ALIBI project. ECHO uses a
neural network to emulate real-life jury verdicts (Thagard, 1989, cf. 2000a, 2000b, 2004,
2005). ALIBI, a planner from symbolic AI (prototypes were in Prolog and Lisp), imperso-
nated one accused and offered exonerating accounts or minimised liability. It came to be
© 2013 Taylor & Francis
*Email: ephraim.nissan@hotmail.co.uk
Information & Communications Technology Law, 2013
http://dx.doi.org/10.1080/13600834.2013.849438
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
0:
25
 0
3 
D
ec
em
be
r 
20
13
 
perceived as an inventor of excuses; when the charge and evidence was damning, the
excuse was contrived, and therefore lame (Fakher-Eldeen et al., 1993; Kuflik et al.,
1989). The most extensive treatment of ALIBI is in Nissan (2012, Sec. 2.2.2.).
Beyond the scope of research usually perceived from within the AI & Law research
community, two directions of research had already been active:
. Dave Schum1, a statistician often collaborating with Peter Tillers, himself a legal
scholar on evidence, has been prominent in combining statistics with diagrams organ-
ising argumentation, proposed by the prominent US legal theorist John Henry
Wigmore a century ago (Wigmore, 1913, 1937), and revived in the early 1990s by
Anderson and Twining (1991, revised as Anderson, Schum, & Twining, 2005).
Kadane and Schum’s (1996) book, modelling the evidence in the Sacco and Vanzetti
case, is the culmination of Schum’s method. Also see the books Schum (1987, 1994)
and the papers Schum (1986, 1989, 1993). Also see Schum and Tillers (1989, 1990a,
1990b, 1991).
. Jury research in North America (where indeed there is an emphasis on jury research
that elsewhere is perceived to be excessive, and not only in such countries where
there are bench trials but not trials by jury) had offered mathematical models (alge-
braic or stochastic) of how a jury’s opinion tilts over, and an appeal to cognitive mod-
elling was sounded in Nancy Pennington and Reid Hastie’s story model (see the
approaches in Hastie, 1993; Hastie, Penrod, & Pennington, 1983; and see Pennington
& Hastie, 1981, 1986, 1988, 1992, 1993).
An important aspect of operational systems resorting to data mining is that they often
combine different techniques. We therefore refrain from introducing individual techniques.
These have been discussed in detail in Nissan (2012).
2. Juries, and modelling them
Pennington and Hastie showed people a movie of a trial. They found that in order to make
sense of the wealth of detail, the participants constructed stories about what happened. In
another experiment, they found that when evidence was given in an order which made
the story easy to construct, the participants were more likely to construct the same story.
When the evidence was in story order, 78% of participants found the defendant guilty.
Yet when the evidence was out of order, only 31% voted for the guilty verdict. Emplotting
items of information into an explanatory narrative is also a subject of debate concerning
historical explanation. In the journal History and Theory, Carr (2008) exemplified how
we figure out narrative explanations (pp. 19–20).
In the words of the British legal scholar Twining (1997): ‘[P]roblems of proof, infor-
mation handling, and “evidence” arise at all stages of legal processes’ (p. 443).
There is also growing realisation that it is misleading to treat the contested trial, especially the
contested jury trial, as the only or the main or even the paradigm arena in which decisions about
questions of fact are taken, even in the United States. ‘The jury model’ still dominates much
American evidentiary discourse and has an unfortunate effect on satellite fields, such as the
agenda for psychological research into evidentiary problems. (p. 444)
A striking example is the recent empirical work on the role of stories in fact-finding, which
treats contested jury trials as the paradigm: e.g. (Bennett & Feldman, 1981); and the important
‘story model’ of Pennington and Hastie (1986, 1988, 1992, 1993), which is a by-product of jury
research (in Hastie, 1993; Hastie et al., 1983); cf. the distorting effect of the jury model on eye-
witness identification research. (Twining, 1997, p. 444, fn. 16)
2 E. Nissan
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
0:
25
 0
3 
D
ec
em
be
r 
20
13
 
In an edited volume published in Britain and entitled The jury under attack (Findlay &
Duff, 1988), McCabe felt able to ask, in the very title of an article: ‘Is Jury Research
Dead?’ (McCabe, 1988). Actually, in Britain the role of the jury has been gradually
diminishing.
In Thagard’s ECHO, the input for ECHO simulations of a trial are sets of simple prop-
ositions, and these propositions include items of evidence, prosecution hypotheses, and
defence hypotheses. Some other possible statements are identified as contradictions.
Some other statements are instances of an explains function, followed by its parameter
instances. Such statements include prosecution explanations, defence explanations, and
motives. Moreover, data are declared, within the input code, these data being testimonies
uttered by the witnesses, thus being observed by everyone inside the courtroom.
The first application of ECHO was an abductive reasoning model simulating the evol-
ving reasoning of a jury on a murder case against California Highway police officer Craig
Peyer, who was tried in San Diego for the murder of Cara Knott on 27 December 1986. The
trial ended on 27 February 1988, in a hung jury. Twenty-two young and attractive women
(who therefore were like the victim) testified that Peyer had pulled them over. And it was
known that Peyer had pulled the victim over, on the night of her death. The witnesses who
had been pulled over by Peyer also testified that Peyer talked to them longer than was
necessary for just a ticket. Moreover, they were all pulled over near the stretch of road
where the victim’s body was found.
Thagard’s theory of explanatory coherence is based on principles of symmetry, relation
between explanations, analogy, data priority, contradiction, competition, and acceptance
based on coherence. There are various algorithmic solutions available (alternative to
each other) that, along with those seven principles, can compute acceptance and rejection
of propositions, on the basis of coherence relations. In ECHO, propositions are represented
as neurons in a neural network. Explanatory coherence is dealt with as a constraint satisfac-
tion problem to be solved. Positive constraints are the coherence relations established by
explanation relations. Relations of contradiction or incompatibility between propositions
are negative constraints. Thagard (2000a) introduced greedy algorithms for carrying out
simulations by means of ECHO; this alternative algorithm does not require a neural
network. A team led by John Josephson reimplemented the Peyer case, using a different
inference engine, PEIRCE-IGTT, also being an abducer, i.e. an engine for abductive
reasoning (Fox & Josephson, 1994).
Delivery of the evidence in court conforms with dynamic uncertain inference.
‘Dynamic uncertain inference is the formation of opinions based upon evidence or argu-
ment whose availability is neither disclosed to the analyst in advance nor disclosed all at
once’ (Snow & Belis, 2002, p. 397). Dragoni and Nissan (2004) developed a model for
dealing with belief revision because of incoming information.
Belief revision is a well-researched area within artificial intelligence. It studies, in terms
of formal logic, the impact of the acquisition of incoming items of information. Let us con-
sider what it may offer for the purposes of application to the modelling of how the opinions
and beliefs of an adjudicator evolve as the trial goes on. New information and evidence inte-
grate and corroborate the cognisance of the court, but other testimonies might cause con-
flicts. In this case, it seems natural that the acquisition of the new items of evidence
should be accompanied by a reduction in the credibility of the conflicting pieces of knowl-
edge. If the juror’s corpus of evidence is not a flat set of facts but contains rules, finding such
conflicts and determining all the sentences involved in the contradictions can be hard. In
dealing with these ‘changes of mind’, if we are to adopt a formalism for belief revision,
we have to heavily rely on symbolic logic, since as much as it contributed to the history
Information & Communications Technology Law 3
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
0:
25
 0
3 
D
ec
em
be
r 
20
13
 
of ‘thinking’, logic could as well solve the problem of ‘thinking over’. Generally speaking,
AI researchers call this cognitive process belief revision.
Dragoni and Nissan (2004), being concerned with modelling the dynamics of how judi-
cial fact-finders (judges at bench trials or jurors at trials by jury) propend to either verdict,
have incorporated in the architecture they describe a component which modifies (by feed-
back) the credibility of the source from which an item of information comes, according to
how the credibility of that item of information is currently faring. Their model takes into
account how the degree of credibility of the different persons who provide different
items of information is dynamically affected by how some information they supplied
comes to be evaluated.
Upon the arrival of a new {source, information} pair into the knowledge base (i.e. a set
of extant {source, information} pairs), a truth-maintenance mechanism is applied, which
finds all minimally inconsistent subsets, finds all maximally consistent subsets, and gener-
ates a set of maximally consistent subsets.
From this set and from a set of pairs {information source, source reliability score}, a
statistical technique is applied (it may be Dempster–Shafer), and a set is generated of {infor-
mation source, source reliability score} pairs. A ranking algorithm is applied, and a ranking
of preferences is generated for the maximally consistent subsets themselves. Moreover, by
applying Bayesian conditioning, the reliability scores of the information sources are
revised. The overall schema of the multi-agent belief revision system of Dragoni and
Nissan (2004), whose control flow is shown in Figure 1, incorporates the basic ideas,
current in artificial intelligence, of:
. Assumption-based Truth Maintenance System (ATMS) to keep different scenarios.
. Bayesian probability to recalculate the a-posteriori reliability of the sources of
information.
. The Dempster–Shafer Theory of Evidence to calculate the credibility of the various
pieces of information.
Consider incoming information, β (whose source, U, is identified), further to the set of
beliefs already found in the knowledge base, namely, informations α and χ, which both
come from source W, and moreover an information being a rule (‘If α, then not β’),
which comes from source T.
The latter could, for example, be an expert witness, or then a fictitious character such as
common sense. In the parlance of Anglo-American legal evidence theory, common sense is
called ‘background generalisations’, ‘common-sense generalisations’, or ‘general
experience’.
Once past the knowledge base, in order to revise the set of beliefs with the new infor-
mation β coming from source U, two steps are undertaken. The ATMS-like mechanism is
triggered; it executes steps S1 and S2. These are dual operations, respectively, as follows:
. Find all minimally inconsistent subsets (NOGOODSs).
. Find all maximally consistent subsets (GOODSs).
Suppose then that three GOODSs have been generated; the one labelled 1 includes α, β,
and χ; the one labelled 2 includes B, χ, and the rule ‘If α, then not β’; whereas yet another
GOODS, labelled 3, includes: α, χ, and the same rule ‘If α, then not β’. Each one out of
these three GOODSs is a candidate for being the preferred new cognitive state (rather
4 E. Nissan
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
0:
25
 0
3 
D
ec
em
be
r 
20
13
 
than the only new cognitive state). The decision as to which cognitive state to select is taken
based on Dempster–Shafer. See Figure 2.
3. The controversy about probabilities in court and the emergence of the ideal of
ranking plausibility rather than probability
Information technologists coming into the field of modelling of the reasoning on legal evi-
dence are all too easily prone to embrace statistical techniques, often unaware of the fierce
controversy among legal evidence theorists, concerning whether probabilistic methods
should be allowed as a metric in guilty or not guilty determinations (e.g. Allen & Red-
mayne, 1997). In this matter, the sceptic camp’s earliest exponent was Voltaire, whereas
George Boole (1815–1864), whose Boolean algebra underlies computing, was a probabil-
ities-in-law enthusiast, but the real debate has taken place in this generation, with the New
Figure 1. Control flow in the approach to distributed belief revision described in Dragoni and
Nissan (2004).
Information & Communications Technology Law 5
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
0:
25
 0
3 
D
ec
em
be
r 
20
13
 
Evidence school. This is something distinct from the DNA wars as well as from the new
scepticism concerning fingerprints: not only radical challenges to fingerprint uniqueness,
but also challenges to too few sampled points in fingerprints. This was the crux in Scot-
land’s recent Fingerprint Inquiry (www.thefingerprintinquiryscotland.org.uk).
Pragmatically, AI cannot afford to antagonise either camp of legal scholars; if it comes
out as within the statisticians’ exclusive remit, acceptance of models for legal evidence
would be curtailed. It is of the utmost importance that artificial intelligence applications
in such modelling do not inextricably commit themselves to the statisticians’ camp,
because this would impede acceptance. In some contexts, probabilistic methods would
meet less resistance: for civil cases, rather than criminal cases, the balance of probabilities
as the standard of proof is acceptable. In criminal cases, where the Anglo-American stan-
dard is ‘beyond reasonable doubt’, as prosecutors have the discretion whether to prosecute,
offer a plea bargain, or dismiss the case, a prosecutor could conceivably use a probabilistic
tool for assisting in that decision (or ask an astrologer); this would not be as objectionable as
probability-based guilt determination in court.
Figure 2. The complete process of belief revision according to the approach proposed in Dragoni
and Nissan (2004).
6 E. Nissan
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
0:
25
 0
3 
D
ec
em
be
r 
20
13
 
Ron Allen, perhaps the most authoritative among the so-called Bayesio-sceptics at law
schools, has long wished for models of plausibility which would evaluate legal narratives
and rank possible narrative scenarios, as an alternative to probabilistic quantification, which
is marred by the reference-class problem: with respect to what do you estimate frequency?
Apart from the fact that crimes about which a court is called to decide are unique, so the
reasoning is not properly in terms of frequency and sample spaces.
In the early 1980s, there was among scholars much interest in the nature of juridical
proof. Bennett and Feldman (1981) brought stories/scripts à la Rumelhart (1980a,
1980b) to people’s attention within legal scholarship, and Pennington and Hastie (1986,
1988, 1992, 1993) produced some empiricism as for how jurors reason consistent with
the Rumelhart approach. However neither had a conceptualisation of not guilty (in criminal
cases) or not liable (in civil cases). In addition, the dominant view was that some form of
conventional probability underlay the structure.
In his scholarly output from that period, Ron Allen answered the first question by
demonstrating that it was not one but two stories that mattered. Second, he showed that
the appraisal of those stories had to be ordinal, not cardinal. It has to be relative. Implicit
in this (see, for example, the discussion of scripts, and so on) was the theory of evidence
that a 1994 paper by Allen made explicit.
In the context of that debate, the theory of anchored narratives of Wagenaar, van
Koppen, and Crombag (1993) made its appearance. The anchored narratives approach
(sometimes informally referred to as AN for short) was built on the prior work mentioned
earlier, but it has a conceptual limit: there is no rule of decision, nothing that says when a
narrative is good enough for either a civil or criminal case. It has the same conceptual limit
that Pennington and Hastie’s approach has, which is there is no operationalisation of ‘not
guilty’ or ‘not liable’. For example, Ron Allen offered that critique, amid those legal scho-
lars who offered a critical response to the theory of anchored narratives.
The central idea of the theory of anchored narratives is that juridical proof is organised
around plausible narratives where ‘plausibility’ is determined by the relationship between
the story offered at trial and the background knowledge/common sense of the decision-
maker. In Allen’s opinion, this conflates two separate issues: the macro structure of
proof, and the micro analysis of evidence, but nonetheless it is more or less accurate.
These two themes – the macro structure of proof and the micro analysis of evidence –
were precisely the themes of earlier work by Allen himself.
For example, in a paper entitled ‘The nature of juridical proof’, Allen had claimed (Allen,
1991): ‘The central question is often whether a richly textured human episode occurred at
trial, and if so, its nature. Answering these questions requires finding an interpretation
that best explains a complex set of interrelated data’ (Allen, 1991, p. 393). ‘Indeed, often
the “facts” are indistinguishable from the interpretation’ (Allen, 1991, p. 395). Allen
(1991, p. 396), throughout a section that as per its title, deals with ‘the Tension Between
the Official Epistemology and Juror Reasoning’, discussed the earlier work by Bennett &
Feldman and Pennington & Hastie. Then in Allen (1991, p. 406), Sec. 3, ‘The Equally
Well Specified Cases Proposal and its Criminal Counterpart’ is essentially a generalisation
of the theory that what occurs at trial is the comparison of the plausibility of the plaintiff’s
and defendant’s cases, and in criminal cases a determination that there is a plausible story of
guilty and no plausible story of innocence. Plausibility, in turn, is determined, as Allen
(1991, Sec. 2) argues, by references to the background knowledge/common sense of the
decision-maker. This approach by Allen quite resembles (and anticipates) the anchored nar-
ratives approach, with one exception: AN has no operationalisation of ‘not guilty’, whereas
Allen’s theory does. Ron Allen’s first article discussing this was Allen (1986).
Information & Communications Technology Law 7
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
0:
25
 0
3 
D
ec
em
be
r 
20
13
 
Allen (1986) was an early effort to give a normative account of juridical proof that
involved stories. It apparently was the paper, or one of the papers, that inspired the label
‘the new evidence scholarship’. Allen (1986) announced a new research programme in
the nature of juridical proof that was exploring the limits of formal reasoning and proposed
as a solution a version of a story model. This research programme has stimulated a large
literature especially in the USA.
The central thesis of Allen (1991) was summarised in Allen’s paper ‘Explanationism all
the way down’ (2008a, p. 325) as:
A more promising approach to understanding juridical proof is that it is a form of inference to
the best explanation. Conceiving of cases as involving the relative plausibility of the parties’
claims (normally provided in story or narrative form) substantially resolves all the paradoxes
and difficulties […].
In Allen (2008b), the relationship between juridical proof and inference to the best expla-
nation was thoroughly examined. Allen (2008a, p. 325) claimed that, if one accepts to con-
ceive of cases as involving the relative plausibility of the parties’ claims,
the complexity problem is purely a function of the choices made by the parties. Indeed, this
points out a subtle but important clarification of the role of the parties. They are sometimes
thought of as ambiguity generators, but in reality they are ambiguity discarders. Rather than
litigate on an infinite number of ways in which the universe might have been, they focus on
a few and ask fact finders to decide which is the most plausible.
‘In criminal cases, this amounts to determining whether there is a plausible story of guilt,
and if so, in addition a plausible story of innocence’ (Allen, 2008a, p. 328, note 3). Allen
(2008a, pp. 325–326) continued:
To be sure, ‘plausible’may mean ‘more likely’ but this does not mean comparative plausibility
reduces to probability. Rather, what is ‘plausible’ is a function of the explanation, its coherence,
consistency, coverage, consilience, and how it fits into the background knowledge possessed
by the fact finder.
Coverage is how much of the evidence is explained. Consilience is how unified one’s
explanations are. Consilience is the idea of the unity of knowledge. Are there underlying
principles or laws in various fields that unite them in one fashion or another? See
Wilson’s (1998) Consilience: The unity of knowledge. Wilson is known also for his work
on socio-biology and biodiversity. To say it with the blurb of the book, he calls consilience
the composition of the principles governing every branch of learning. Wilson sets out to show
how our explosive rise in intellectual mastery of the truths of our universe has its roots in the
ancient Greek concept of an intrinsic orderliness that governs the cosmos. This vision found its
apogee in the Age of Enlightenment, but then gradually was lost in the increasing fragmenta-
tion and specialization of knowledge in the last two centuries. He argues that the goals of the
original Enlightenment are surging back to life, that they are reappearing at the frontiers of
science and humanistic scholarship, and that they are beginning to sketch themselves as the
blueprint of our world.
Ron Allen is a consistently articulate critic of the application of probability theory to
juridical proof. Among the other things, Allen and Pardo (2007a, p. 109)2 find that scholar-
ship pursuing that line of inquiry
8 E. Nissan
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
0:
25
 0
3 
D
ec
em
be
r 
20
13
 
suffers from a deep conceptual problem that makes ambiguous the lessons that can be drawn
from it – the problem of reference classes. The implications of this problem are considerable.
To illustrate the problem, consider the famous blue bus hypothetical. Suppose a witness saw a
bus strike a car but cannot recall the color of the bus; assume further that the Blue Company
owns 75 percent of the buses in the town and the Red Company owns the remaining 25
percent. The most prevalent view in the legal literature of the probative value of the witness’s
report is that it would be determined by the ratio of the Blue Company buses to Red Company
buses, whether this is thought of as or plays the role of a likelihood ratio or determines infor-
mation gain (including an assessment of a prior probability) […] But suppose the Red
Company owns 75 percent (and Blue the other 25 percent) of the buses in the county. Now
the ratio reverses. And it would do so again if Blue owned 75 percent in the state. Or in the
opposite direction: it would reverse if Red owned 75 percent running in the street where the
accident occurred (or on that side of the street) and so on. Or maybe the proper reference
class has to do with safety standards and protocols for reporting accidents. Each of the reference
classes leads to a different inference about which company is more likely liable, and nothing
determines the correct class, save one: the very event under discussion, which has a likelihood
of one and which we are trying to discover.
‘The blue bus hypothetical with which we began this paper exemplifies the general
implications of reference classes, and those implications would hold for practically any
attempt to quantify a priori the probative value of evidence’ (Allen & Pardo, 2007a,
p. 113). ‘The reference-class problem […] is an epistemological limitation on attempts to
establish the probative value of particular items of legal evidence’ (p. 115, citing Pardo,
2005, pp. 374–383). This is so ‘because different classes may point in opposite directions
and nothing, other than the event itself, necessarily privileges one over another’ (Allen &
Pardo, 2007a, p. 115).
Allen and Pardo (2007a) claimed their paper was making three contributions: ‘it is a
further demonstration of the problematic relationship between algorithmic tools and
aspects of legal decision making’ (p. 110); ‘it points out serious pitfalls to be avoided for
analytical or empirical studies of juridical proof’, and ‘it indicates when algorithmic
tools may be more or less useful in the evidentiary process’ (p. 110). They enumerated
and exemplified ‘limitations on attempts to mathematically model the value of legal evi-
dence’ (p. 115). Namely:
First, and most important, the probative value of legal evidence cannot be equated with the
probabilities flowing from any given reference class for which base-rate data are available.
Related to this point, probative value likewise cannot be equated with the difference
between prior and posterior probabilities on the basis of such data, nor is it sensible simply
to translate directly an available statistic into a prior probability. Second, the above problem
regarding establishing probative value cannot be solved by merely specifying the relevant
classes with more detailed, complex, or ‘realistic’ characteristics. Third, while switching
from objective to subjective probability assessments better accommodates unstable probative
values of evidence, it nevertheless still illustrates the pervasiveness of the reference-class
problem because of its presence even when evaluating such subjective assessments. Finally,
the reference-class problem is so pervasive that it arises whenever one assesses the probative
value of evidence, even when one is not trying to fix a specific numeric value to particular items
of evidence – for example, when assessing whether evidence satisfies a standard of proof. […]
The limitations that are discussed, we contend, undermine the strong conclusions that are
drawn from such models. (pp. 115–116)
4. AI models reasoning about alternative crime scenarios: Bex’s approaches
There is a sense in which ‘AI & Law’ is in a Catch-22 situation, trying to please both the
Bayesian enthusiasts and the Bayesio-sceptics among the legal scholars. Nevertheless, even
Information & Communications Technology Law 9
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
0:
25
 0
3 
D
ec
em
be
r 
20
13
 
though there are and there will be AI & Law tools to serve (and to please) the statisticians, a
class of tools is emerging that may please the Bayesio-sceptics, by ranking scenarios
without resorting to probabilities. It is important to stress that as far as police investigations,
or the reasoning of prosecutors having to decide whether to prosecute, or foreseeing the
costs of litigation, are concerned, misgivings about probabilities are not a major
problem. It is courtroom use of probabilities that is.
The good news is that such tools which rank scenarios are now being developed, and
Floris Bex’s3 is leading the way (Bex, 2011), but, ironically, without any explicit recog-
nition of the legal theorists’ controversy about ‘probabilities in law’. Jeroen Keppens4
and Burkhard Schafer’s crime scenario modelling applies advanced logic to hypothesis gen-
eration and scenario ranking. In Aberystwyth, Wales, Keppens’s former pupil Qiang Shen,
along with Xin Fu and Tossapon Boongoen (Fu, Boongoen, & Shen, 2010), is taking scen-
ario generation further, but does so by more heavily committing himself to probabilistic
models. What Fu et al. (2010) mean by plausibility is not what Ron Allen does.
Bex’s model instead is free of probability models, even though it may incorporate them
as well. An interesting project in AI & Law that tries to combine stories and arguments in
sense-making software for crime investigation was reported about in Bex et al. (2007). Bex
et al. (2007, p. 145) pointed out:
A formal model is proposed that combines AI formalisms for abductive inference to the best
explanation and for defeasible argumentation. Stories about what might have happened in a
case are represented as causal networks and possible hypotheses can be inferred by abductive
reasoning. Links between stories and the available evidence are expressed with evidential gen-
eralisations that express how observations can be inferred from evidential sources with defea-
sible argumentation. It is argued that this approach unifies two well-known accounts of
reasoning about evidence, namely, anchored narratives theory and new evidence theory.
After the reasoning model is defined, a design is presented for sense-making software that
allows crime investigators to visualise their thinking about a case in terms of the reasoning
model.
The visualisation component of the architecture envisaged by Bex et al. (2007) is called
AVERs, and was ‘implemented as a web front-end to an SQL database. A case can be rep-
resented visually through multiple views; in this paper we will focus on the two graphical
views, that is, the evidence view and the story view’ (Bex et al., 2007, Sec. 6). Ideally, they
wanted to design a more sophisticated tool than such investigative analysis software for
organising and visualising the evidence in the practice of crime investigation, as the
British tool HOLMES 2 (short forHome Office Large Major Enquiry System), and Analyst’s
Notebook, this other tool offered by the British firm i2, as well as like the experimental tool
from the Netherlands, BRAINS, reported about by van der Schoor (2004). Moreover, Bex
et al. (2007) were building upon the remarkable record of some of the authors in research
into argumentation within AI & Law, and they also wanted to relate their project to
approaches from legal scholarship to legal narratives, and they specifically considered
the new evidence theory (citing Anderson & Twining, 1991; Schum & Tillers, 1991) and
the anchored narrative theory (citing Wagenaar et al., 1993).
There is no evidence in the paper of awareness of qualms among some scholars about
Bayesianism in models of legal evidence. Nevertheless, Bex et al. (2007) propose a model
likely to be well received by both the Bayesians and the sceptics in the controversy about
probability in law. This is because the model of Bex et al. (2007) is based on causal net-
works and on logic, without resorting to probabilities (even though arguably these could
be added, should one wish to). All in all, unsurprisingly, the model of Bex et al. (2007)
10 E. Nissan
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
0:
25
 0
3 
D
ec
em
be
r 
20
13
 
is very close to the kind of modelling of argumentation as being applied to narratives, from
the respective (and sometimes joint) oeuvre of Henry Prakken, Floris Bex, Bart Verheij, and
Susan van den Braak.
Bex et al. (2007) concede that both explanation and prediction, both of them familiar
tasks from artificial intelligence, are important in crime investigation, but admittedly, in
the given paper, they confined themselves to only model explanation, as far as stories are
concerned. They envisaged also addressing prediction in future research. They considered
physical causation, mental causation, and the defeasibility of reasoning with causal
information.
Their own approach was to combine reasoning from cause to effect and reasoning from
effect to cause. They combined abductive reasoning and modus-ponens-style reasoning:
‘while the construction of stories to explain the available evidence is modelled as abductive
reasoning with networks of causal generalisations, source-based reasoning about evidence
is modelled as modus-ponens-style reasoning with evidential generalisations’ (Sec. 3 in
Bex et al., 2007).
Bex et al. (2007, Sec. 4) adopted a causation network graphic approach along with an
example from Bex, Prakken, and Verheij (2006), in which different diagrams represent
the prosecution’s story and the defence’s story about a case of burglary. Arrows stand
for causation or sequence inside a rectangular contour with rounded angles. From
outside the contour, arrows (standing for support or for refutation, e.g. from testimony,
or from an argument about the testimony) may enter the contour and point to this or
that box, which in turn stands for a narrative element. For example, based on testimony
from various witnesses about what they heard, the defence claims that the witnesses did
not hear a loud bang, and this provides refutation for the causal expectation that there
should have been a loud bang, had the prosecution’s story be true. Bex et al. (2007,
Sec. 4) explained:
The reader may find some of the causal or evidential generalisations in this example weak or
far-fetched. However, this is not a problem for our approach. The very idea of our sense-
making system (which it shares with, for example, Wigmore’s charting method) is that it is
the user of the system who is responsible for carefully testing the quality of his stories and argu-
ments. The software should support the user in this critical process; it should not itself auto-
matically generate sensible stories and arguments.
The most important part of the approach developed in Bex et al. (2007) is the formalism.
General knowledge is in our approach expressed with two sets GC and GE of causal and evi-
dential generalisations. Logically, we formalise both types of generalisations in the same way,
with a special conditional connective ⇒ which only satisfies the modus ponens inference rule.
(Bex et al., 2007, Sec. 5.1)
Out of many formal and computational accounts of abductive reasoning that are available in
artificial intelligence, Bex et al. (2007, Sec. 5.2) proposed a simple one. They defined an
abductive framework as a tuple AC = (GC, O, F, X) where GC is the causal theory, and is
a set of causal generalisations. O stand for the observations, is a set of ground first-order
literals, and does not have to be consistent. F is either a subset of O, or the entire set O.
F is the set of the explananda, and is a consistent set of first-order literals. ‘They are the
observations which have to be explained,’ whereas ‘the observations not in F do not strictly
have to be explained but explaining them does make an explanation better’. As to X, it is the
set of the explanantia; that is to say, X ‘s the set of all ground literals occurring in the
Information & Communications Technology Law 11
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
0:
25
 0
3 
D
ec
em
be
r 
20
13
 
antecedent of some causal generalisation in GC and instantiated with some term in’ the
union set of GC and O. Let ⊢ stand for logical implication according to the set of all deduc-
tive inference rules extended with modus ponens for⇒. An explanation in terms of AC is a
set H ⊆ X of hypotheses such that for each explanandum f ∈ F it holds that the explanation
implies that explanandum (that is, that H ⊢ f) and that the explanation does not imply some-
thing false (that is, that H ⊬ ⊥).
Admittedly, as the approach in Bex et al. (2007) is purely qualitative, in order to
compare alternative explanations resorting to probability distributions over H is not appli-
cable (as stated in Sec. 5.2). Therefore, Bex et al. (2007) adopted a simple ordering on
explanations:
if H′ is better than H on the observations explained and not worse on the observations contra-
dicted, or if H′ is better on the observations contradicted and not worse on the observations
explained, then H′ is better than H. If they are equal on both criteria, then they are equally
good overall. In all other cases they are incomparable. (Sec. 5.2; they also expressed this order-
ing in formulae)
Moreover, ‘combining abduction with argumentation allows a refinement of this preference
relation’ (Bex et al., 2007). Bex et al. (2007, Sec. 5.3) defined a logic for defeasible argu-
mentation, the application being as a ‘logic for reasoning with evidential generalisations.
Since such generalisations allow for exceptions, this logic must be nonmonotonic’. Their
choice was as follows (Bex et al., 2007, Sec. 5.3):
In our case the classical inference rules are those of standard first-order logic while the
only defeasible inference rule is the modus ponens rule for the ⇒ connective. Undercutters
to this defeasible version of modus ponens are formalised as arguments for the conclusion
¬ valid(g) where g is the name of the generalisation to which modus ponensis applied.
The symbol¬ stands for negation of the proposition it precedes. The qualitativeness of the
approach, from which its avoidance of a probabilistic representation follows, would argu-
ably make the method quite interesting for the Bayesian sceptics among legal scholars,
without however antagonising the legal Bayesians (as the latter could possibly insert a prob-
abilistic component in the argumentation module). Another formalism that the Bayesian
sceptics ought to look into, as they are likely to come to like it very much, is the model
for arguments and critical questions concerning legal narratives, as proposed by Bex,
Bench-Capon, and Atkinson (2009).
Bex et al.’s (2009) paper ‘Did he jump or was he pushed? Abductive practical reason-
ing’ adopts (p. 83) Atkinson and Bench-Capon’s (2007) formal model underlying the gen-
eration of arguments and critical questions, a model itself based upon Wooldridge and van
der Hoek’s (2005) Action-based alternating transition system (AATS). As explained in Bex
et al. (2009, p. 83):
Essentially, an AATS consists of a set of states and transitions between them, with the tran-
sitions labelled with joint actions, that is, actions comprising an action of each of the agents
concerned. To represent the fact that the outcome of actions is sometimes uncertain, in the scen-
ario we use in this paper we will add a third ‘gent’which will determine whether the actions had
the desired or the undesired effect. The transitions will be labeled with motivations, corre-
sponding to the values of Bench-Capon (2003), encouraging or discouraging movement
from one state to the next. […] We use a transition system which is a simplified version of
the AATS used in Atkinson and Bench-Capon (2007) to ground the practical reasoning
12 E. Nissan
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
0:
25
 0
3 
D
ec
em
be
r 
20
13
 
argumentation scheme, but this will still allow us to hypothesise the reasoning concerning the
events that may have taken place.
A story is a chain of arcs inside the graph (Bex et al., 2009, p. 83):
Given an AATS and a number of arguments generated from the AATS, a story (a sequence of
events) is a path through the AATS. An argument explains why that path was followed, and so
gives coherence and hence plausibility to the story. For example, ‘John wrote a paper, John
went to Florence’ is a story, but it has more coherence expressed as ‘John went to Florence
because he had to present the paper he hadwritten.’
The story Bex et al. (2009) used throughout their paper is as follows (p. 83):
Picture two people on a bridge. The bridge is not a safe place: the footpath is narrow, the safety
barriers are low, there is a long drop into a river, and a tramline with frequent traffic passing
quite close to the footpath. One of the persons, call him Ishmael, is standing still, whereas
the other, Ahab, is running. As Ahab reaches Ishmael, Ishmael falls into the river. Did he
jump or was he pushed? To answer this we will need a story explaining either why Ahab
chose to push Ishmael, or why Ishmael chose to jump to his doom. If Ahab is on trial, the
story we believe will be crucial: if Ahab intended Ishmael’s death it will be murder, if there
is a less damning explanation for the push it may be manslaughter, and if Ishmael jumped,
Ahab is completely innocent. We illustrate the critical questions by reference to this
example scenario.
Given that here ‘“explanation” stands for “the performance of joint action A in previous
circumstances R”’ (Bex et al., 2009, p. 84), by which ‘we mean physical explanation, how
performing an action in R caused the new state of affairs S, as opposed to a mental expla-
nation, what motivated an agent to do a particular action’, critical questions for choice of
explanation that Bex et al. (2009) enumerate are the following (p. 84):
CQ1 ‘Are there alternative ways of explaining the current circumstances S?’, subdivided into
(a) ‘Could the preceding state R have been different?’, and (b) ‘Could the action B have been
different?’
CQ2 ‘Assuming the explanation, is there something which takes away the motivation?’
CQ3 ‘Assuming the explanation, is there another motivation which is a deterrent for doing the
action?’
CQ4 ‘Can the current explanation be induced by some other motivation?’
CQ5 ‘Assuming the previous circumstances R, was one of the participants in the joint action
trying to reach a different state?’
For example, the answer they provide (Bex et al., 2009, p. 84) for CQ5 is as follows:
Answer: in R, even though one agent performed his part of Awith motivation M, the joint action
was actually A′ which led to S′, where A′≠ A and S′≠ S
‘Ahab wanted to push Ishmael out of the way of the tram to get him out of danger, but nature
did not cooperate (and Ishmael fell off the bridge)’
Next, Bex et al. (2009, p. 85) enumerated critical questions for problem formulation, for
example: ‘Assuming the previous circumstances, would the action have any conse-
quences?’ The argument scheme and all those critical questions were then expressed for-
mally, by adopting a notation in terms of an AATS (Sec. 3.2). A state transition diagram
was drawn (p. 90) for the scenario proposed by the scenario explaining the
Information & Communications Technology Law 13
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
0:
25
 0
3 
D
ec
em
be
r 
20
13
 
circumstances of the Ahab and Ishmael narrative. Then, by adopting Bench-Capon’s
(2003) Value-based argumentation framework, a diagram was drawn (Bex et al.,
2009, p. 92) showing arguments, objections, and rebuttals. Different orderings of
values result in a number of competing explanations. The most preferred value is impor-
tant for providing an ordering of the motivations of Ahab and Ishmael. Alternatives for
Ahab’s motivation are: murder, arguable manslaughter, he did not push, or mercy
killing. Alternatives for Ishmael’s motivation are: suicide, sacrifice to let Ahab pass,
or he did not jump (pp. 92–93). Bex et al. (2009, p. 94) acknowledged that the most rel-
evant related work is Walton and Schafer (2006).
5. AI models reasoning about alternative crime scenarios: Keppens and Schafer’s
approach
Studies of crime scene investigation conducted in a cognitivist vein include Schraagen and
Leijenhorst (2001) and Ormerod, Barrett, and Taylor (2008). In an article entitled ‘Distrib-
uted cognition at the crime scene’, Baber (2010) from the University of Birmingham dis-
cussed in the journal AI & Society a conceptualisation of crime scene examination, in
terms of distributed cognition, a concept for which see Dror and Hamard (2009). In
Baber (2010), ‘distribution is defined by the number of agents involved in the criminal
justice process, and in terms of the relationship between a Crime Scene Examiner and
the environment being searched’ (from the abstract). Baber’s approach combines cognition
and ergonomics.
Prakken, Reed, and Walton (2003) discussed appropriate argument structures for
reasoning about evidence in relation to hypothesising crime scenarios. It was a paper on
using argumentation schemes for reasoning on legal evidence, mainly by way of an explora-
tion of applying Araucaria, the argument visualisation system from the University of
Dundee in Scotland,5 to an analysis in the style of Wigmore Charts. Case-based reasoning
was applied by Toland and Rees (2005) to the task of recalling similar instances of volume
crime, when confronted with a crime being investigated: the task was the identification of
crimes with similar modus operandi, and the reasoning involved potential repeat offenders.
Ribaux and Margot (1999) applied case-based reasoning to the categorisation of cases of
burglary, with the retrieval of cases with similar profiles. The work reported about in
Oatley, Zeleznikow, and Ewart (2004) is concerned with assisting the police in detecting
the perpetrators of burglary from homes, which is a high-volume crime with low detection
rates; that project made use of a variety of data mining techniques, including: classification
and association rules, neural network clustering, survival analysis and Bayesian belief nets,
case-based reasoning, as well as ontologies and logic programming.
A team that was initially led in Edinburgh by John Zeleznikow, in the early 2000s,
worked on projects whose aim was to produce software tools assisting in the assessment
of evidence in given limited, specialist domains. Jeroen Keppens and Burkhard Schafer
were members of that team. Eventually, as various persons moved around to other affilia-
tions, sequel projects emerged at different locations. The present section is concerned with
one of those lines of research.
Keppens and Zeleznikow (2002, 2003) and Keppens and Schafer (2003a, 2003b, 2004)
have reported about a project whose application is in post-mortem inquests, with the goal of
determining whether death occurred through natural causes, homicide, or suicide. In their
Dead Bodies Project, a so-called assumption-based truth maintenance system, or ATMS
(a well-known AI approach to consistency) is resorted to, in order to maintain a space of
‘possible worlds’ which correspond to hypothetical scenarios.
14 E. Nissan
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
0:
25
 0
3 
D
ec
em
be
r 
20
13
 
The project resorts to neither conventional expert systems nor case-based reasoning. Any
case is potentially unique. Crime investigation is very difficult to proceduralise. The design
solution adopted for this project was to develop a model-based reasoning system, i.e. such
a system that given a problem instance, a model of the problem is constructed, and a
problem-independent technique is applied. In the same project, dynamic preference orderings
are assigned to uncertain events. Default orderings may be overruled by inferred orderings.
An article by Keppens and Schafer (2005, abstract)
characterises an important class of scenarios, containing ‘alternative suspects’ or ‘hidden
objects’, which cannot be synthesised robustly using conventional abductive inference mech-
anisms. The work is then extended further by proposing a novel inference mechanism that
enables the generation of such scenarios.
Keppens and Schafer (2006) reported about a more advanced state of the same project
applying artificial intelligence to crime scenario modelling. The prototype of a decision-
support system was presented, for crime scenario construction. It is component events,
rather than entire scenarios, that are stored. (By scenario, a description of a combination
of events and situations is meant.) The component events are composed into useful scen-
arios by an algorithm. The input is a description of the available evidence. A network of
plausible scenarios is then generated. Those scenarios in turn can be analysed, with the
goal of devising effective evidence collection strategies. The algorithm was allegedly
highly adaptable to unanticipated cases, by allowing a major crime being investigated to
be matched by component events in several different ways. One advantage hoped for
was the avoidance of such pitfalls of human reasoning as premature case theories, or
rather premature convergence, such that police investigators tend to focus on the more
likely suspects they had identified early on.
Therefore, that project belongs to a category of software tools known as compositional
modellers, and introduced by Falkenhainer and Forbus (1991) in their paper ‘Compo-
sitional modeling: finding the right model for the job’. Compositional modelling was
also discussed by Keppens and Shen (2001). In compositional modellers, small, generic,
and reusable rules called model fragments capture a domain’s first principles. These are
fundamental theories describing the behaviours and mechanisms that occur in the domain of
interest […]. The compositional modelling paradigm is adapted to the crime investigation
domain by employing causal rules describing how combinations of assumed states and
events lead to new states and events in plausible crime scenarios. (Keppens & Schafer, 2006)
Another category in which the system described by Keppens and Schafer (2006) is abduc-
tive diagnosers. In abductive diagnosis (Console & Torasso, 1991), what the conditions are
of a physical system under investigation are determined by comparing observations as pre-
dicted by models, to such observations that are extracted from the real world. The gener-
ation of models, in an abductive diagnoser, is done by resorting to a knowledge base of
first principles about the given domain. First principles are general rules, independent
from the decision procedure, and in this they differ from the heuristic rules (i.e. rules of
thumb) found in rule-based expert systems. In the project of Keppens and Schafer (2006),
the first principles are expressed by means of causal rules describing how some states and events
are triggered by other known or assumed states and events. The possible causes of a given set of
available evidence are inferred by means of an abductive inference procedure. These causes form
the hypothetical scenarios describing plausible crimes. Potential additional evidence that may
Information & Communications Technology Law 15
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
0:
25
 0
3 
D
ec
em
be
r 
20
13
 
confirm or contradict these scenarios is then deduced using the same causal rules. This abductive,
first-principles based approach recognises that while the individual scenarios encountered in a
major crime investigation may be virtually unique and vary widely, the underlying domain
knowledge on evidence and the types of events that create it are not. It also encourages a prin-
cipled hypothetico-deductive investigative methodology because it hypotheses all (known) poss-
ible causes of the available evidence, composes these causes into plausible scenarios and deduces
additional evidence from the plausible scenario. This promotes consideration of many scenarios,
instead of individual ones, in deciding on future investigative actions. Finally, the approach also
allows making expert domain knowledge available to less experienced investigators.
In the architecture of the decision-support system described by Keppens and Schafer
(2006) – see Figure 3 – an ATMS is the central inference mechanism. A scenario space
Figure 3. The architecture of the decision-support system described by Keppens and Schafer (2006),
redrawn and rearranged from Figure 2. Data structures are shown in this figure as rectangles, whereas
ellipses correspond to the inference mechanism.
16 E. Nissan
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
0:
25
 0
3 
D
ec
em
be
r 
20
13
 
is maintained by means of the ATMS. ‘All’ possible scenarios that explain the available evi-
dence are storied in the scenario space. The scenarios are represented as logic predicates;
these predicates denote events and states, or causal relations between events and states.
Causal relations between assumptions, states, and events are represented as scenario
fragments, each of these being a tuple comprising a set of variables, a set of relations
called preconditions, a set of relations called postconditions, and a set of relations called
assumptions. There also is a representation of inconsistencies, e.g. ‘a person can not kill
himself both with such an intention (i.e. in a suicide) and without this intention (i.e. in
an accidental self-killing)’ (Keppens & Schafer, 2006, Sec. 4.4). The knowledge base com-
prises property definitions, a set of scenario fragments, and a set of inconsistencies. ‘Prop-
erty definitions describe which types of predicate correspond to a symptom, fact, hypothesis
or investigative action’ (Keppens & Schafer, 2006, Sec. 4.5). An example of scenario frag-
ment is this one (Keppens & Schafer, 2006, Sec. 4.3):
if a person P suffers from ailment or injury C, C is the cause of death of P, and there is a medical
examiner E, and assuming that E determines the cause of death of P and makes the correct diag-
nosis, then there will be a piece of evidence in the form of a cause of death report indicating that
according to E, the cause of death of P is C.
Keppens and Schafer (2006, Sec. 3.2) explained:
Once constructed, the scenario space is analysed through a series of queries. Queries are ques-
tions about the scenario space. Their answers are computed by extracting relevant parts from
the scenario space and reported back in an understandable format. To interface between the
human and scenario space, a query analyser translates standard types of user queries into a spe-
cification of ATMS nodes of interest, and a report generator provides the means to represent a
partial scenario space back to the user.
What the scenario space is made to initially contain is based on the initial set of given
facts and evidence, and is constructed by means of a knowledge base. For example, these
five pieces of evidence appear in an example from Keppens and Schafer (2006): n1: ‘A
hanging corpse of a person identified as johndoe has been found’; n11: ‘A report by a psy-
chologist identified as frasier (n15) stating that johndoemay have been suicidal prior to his
death’; n14: ‘The observation of suicide trial marks on the body of johndoe’; n16: ‘The body
of johndoe exhibits signs of petechiae’ (i.e. small red to purple spots on the eyes or skin,
caused by either disease or asphyxiation); n20: ‘A report by a medical examiner identified as
quincy (n7) stating that the cause of death of johndoe was asphyxiation.’
One possible scenario based on this evidence is suicide by hanging. For example,
The hanging corpse (n1) and the summed cause of death (n20) are the consequents of johndoe’s
hanging (n5), which he was unable (unwilling) to end (n4). The petechiae is caused by asphyx-
iation (n15) resulting from the hanging. johndoe’s suicide by hanging requires that johndoe is
suicidal (n7) and the last two pieces of evidence are a consequence of his suicidal state.
(Keppens & Schafer, 2006, Sec. 4.1)
Each scenario was represented as a causal hypergraph. A hypergraph is a generalisation of
a graph, such that an edge may appear not just between two nodes, but between a set of
nodes including two or more nodes. But scenarios were represented as directed acyclic
hypergraph, whose nodes are events or states, whereas the edges are directed hyperarcs,
each one from a set of at least one event or state, towards one and only one event or
state. The scenario of suicide by hanging was shown in Keppens and Schafer
Information & Communications Technology Law 17
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
0:
25
 0
3 
D
ec
em
be
r 
20
13
 
(2006, Figure 3), but we find it more convenient to translate here that hypergraph into a
ruleset and a list of propositions. The ruleset is shown here in Table 1 (where /\ stands
for and); each rule corresponds to one of the hyperarcs of the causal hypergraph of the scen-
ario of suicide by hanging. The correspondence between node identifiers and particular
propositions is listed in Table 2. In Table 1, each row stands for a directed hyperarc of
the causal hypergraph, and here in the order we chose to reflect the arrangement in
Keppens and Schafer’s (2006) original diagram of Figure 3, from top to bottom. The
meaning of the nodes is defined in Table 2.
Keppens and Schafer (2006, Sec. 4.2) classify information by distinguishing facts
(‘pieces of inexplicable, certain information’) from evidence (‘information that is certain
and explicable’), by distinguishing three kinds of ‘uncertain and explicable’ information
(uncertain states, uncertain events, and hypotheses), and by distinguishing three types of
assumptions, i.e. of ‘uncertain and inexplicable information’:
. Default assumptions describe information that is normally presumed to be true. In
theory, the number of plausible scenarios that explain a set of available evidence is
virtually infinite, but many of these scenarios are based on very unlikely presump-
tions. Default assumptions aid in the differentiation between such scenarios by
expressing the most likely features of events and states in a scenario. A typical
example of a default assumption is the presumption that a doctor’s diagnosis of the
cause of death of person is correct (e.g. n18).
. Conjectures are the unknown causes of certain feasible scenarios (e.g. n7). Unlike
default assumptions, conjectures are not employed to differentiate between the rela-
tive likelihood of scenarios.
. Uncommitted investigative actions, i.e. possible but not yet performed activities
aimed at collecting additional evidence, are also treated as assumptions. At any
given stage in the investigation, it is uncertain which of the remaining uncommitted
investigative actions will be performed. The reasoning required to perform such an
action involves looking at its consequences instead of its causes, and therefore
they are not (causally) explicable. As such, investigative actions assume a similar
role as default assumptions and conjectures: i.e. they are employed to speculate
about the plausible (observable) consequences of a hypothetical scenario.
The scenario-space builder instantiates scenario fragments as well as inconsistencies
into an ATMS. In the initialisation phase, an ATMS is generated that contains one node
Table 1. The hyperarcs of the scenario of suicide by
hanging (from Figure 3 in Keppens & Schafer, 2006).
n1 ← n4 ∧ n5
n16 ← n15
n20 ← n15 ∧ n17 ∧ n3 ∧ n18 ∧ n19
n15 ← n5
n17 ← n4 ∧ n5
n4 ← n6
n5 ← n6
n21 ← n6
n15 ← n2 ∧ n10 ∧ n9 ∧ n7
n6 ← n7 ∧ n8
n14 ← n7∧ n3 ∧ n13 ∧ n12
18 E. Nissan
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
0:
25
 0
3 
D
ec
em
be
r 
20
13
 
per piece of available evidence. Next, a backward chaining phase is executed. All plausible
causes of the available evidence are added to the ATMS. A process is repeated, until
exhausting all possible unifications6 of individual consequents of a scenario fragment
with a node already in the ATMS. That process does the following for each possible uni-
fication: it instantiates the antecedents and assumptions of that scenario fragment; the
process adds a node to the ATMS for antecedent instance that does not already have a
node; it adds an assumption node to the ATMS for each assumption instance that does
not already have a node; and the process adds to the ATMS a justification (i.e. a rule like
the rows in Table 1, but also added nodes such as assumption nodes can be included)
‘from the nodes corresponding to the antecedent and the assumption nodes corresponding
to the assumptions, to the node corresponding to the consequent’ (Keppens & Schafer,
2006, Sec. 5.2.1).
Once the backward chaining phase is exhausted because action as described was
taken for each possible unification, execution enters the forward chaining phase.
What this phase does is adding to the ATMS all possible consequences of the plausible
scenarios. Whereas the backward chaining phase repeated its process until exhausting
all possible unifications of individual consequents of a scenario fragment with a node
already in the ATMS, by contrast the forward chaining phase carries out the following
process for each possible unification of the set of antecedents of a scenario fragment
with a set of nodes already in the ATMS. That process instantiates the assumptions
and consequents of that scenario fragment; the process adds an assumption node to
the ATMS for each assumption instance that does not already have a node; the
process adds to the ATMS a node for each consequent instance that does not already
have a node; and the process adds to the ATMS a justification for each consequent
instance, ‘from the nodes corresponding to the antecedent and the assumption nodes cor-
responding to the assumptions, to the node corresponding to the consequent instance’
(Keppens & Schafer, 2006, Sec. 5.2.1).
Table 2. Which event or state the nodes stand for.
n1: observe(hanging-dead-body(johndoe))
n2: psychologist(frasier)
n3: medical-examiner(quincy)
n4: impossible(end(hanging(johndoe)))
n5: hanging(johndoe)
n6: suicide(johndoe, hanging)
n7: suicidal(johndoe)
n8: suicide-action(hanging, johndoe)
n9: psychological-examination(frasier, state-of-mind(johndoe))
n10: correct-diagnosis(frasier, state-of-mind(johndoe))
n11: psychological-evaluation(frasier, state-of-mind(johndoe), suicidal)
n12: medical-examination(quincy, body(johndoe))
n13: correct-diagnosis(quincy, body(johndoe))
n14: medical-report(quincy, body(johndoe), suicide-trial-marks)
n15: suffers(johndoe, asphyxiation)
n16: observe(eyes(johndoe), petechiae)
n17: cause-of-death(johndoe, asphyxiation)
n18: correct-diagnosis(quincy, cause-of-death(johndoe))
n19: medical-examination(quincy, cause-of-death(johndoe))
n20: medical-report(quincy, cause-of-death(johndoe), asphyxiation)
n21: suicidal-death(johndoe)
Information & Communications Technology Law 19
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
0:
25
 0
3 
D
ec
em
be
r 
20
13
 
The forward chaining process is repeated until exhausting all unifications of scenario
fragment antecedents with sets of nodes in the ATMS. And finally, the consistency phase
is carried out: ‘inconsistent combination of states and events are denoted as nogoods.
This involves instantiating the inconsistencies from the knowledge base based on infor-
mation in the ATMS and marking them as justifications for the nogood node’ (Keppens
& Schafer, 2006). In the terminology of ATMS, a nogood is such a justification that has
led to an inconsistency, that is to say, from its node there is an arc →⊥ and this implies
that one of the propositions conjoined by and in the nogood must be false. With an
ATMS, one has to find all minimally inconsistent subsets (NOGOODSs), and to find all
maximally consistent subsets (GOODSs).
Keppens and Schafer (2004, Sec. 3) pointed out similarities between what the ATMS
does in their Dead Bodies project, and what a defence solicitor would do:
In developing alternative scenarios consistent with the evidence, the ATMS performs some of
the scrutiny a good defence solicitor would subject the prosecution case to. A defence soli-
citor has broadly speaking two strategies available to him. First, he can question the factual
correctness or the legal admissibility of evidence presented by the prosecution. Second, he
can accept the evidence at face value and argue that alternative explanations for their presence
are possible that do not incriminate his client. We are concerned here primarily with this
second strategy. However, it is here that we encounter a certain ambiguity, an ambiguity
explicitly recognised by the Scots law of evidence. The defence has in fact again two strat-
egies available to it. The first can be dubbed the ‘Perry Mason Stratagem’. Like the fictitious
advocate, the defence can pursue its own investigation and ‘point to the real culprit’. In Scots
law, this is known as the special defence of incrimination [Field & Raitt, 1996], recently used
(unsuccessfully) in the Lockerbie trial
for an atrocity ascribed to an act of terror: an PanAm passenger aircraft exploded while
flying over Scotland in 1988 because of a bomb on board.
Keppens, Shen, and Lee (2005) described an extension of the scenario-space gener-
ation, resorting to Bayesian modelling:
this paper shows a compositional modelling approach to synthesise and efficiently store a space
of plausible scenarios within a Bayesian Network (BN) […]. Furthermore, it presents an appli-
cation of the maximum entropy reduction technique to determine which investigative actions
are most likely to reduce doubt. (Sec. 1)
In this extension of the work described earlier in the present section, scenario fragments also
incorporate a set of probability distributions, one for each combination of the antecedent
and assumption variables.
They discussed minimal entropy-based evidence (EPE) collection in their Sec. 3.3. They
conceded however (Keppens, Shen,&Lee, 2005, in their Sec. 3.4) thatwhereas that ‘technique
guarantees to return an effective investigative action, it does not ensure globally optimal evi-
dence collection.’They proposed a remedy, in order to reduce the ‘likelihood of obtaining poor
quality locally optimal evidence collection strategies’. This is done by only ‘considering the
EPEs after performing a sequence of actions’, although this incurs computation overheads.
They then proposed a simplified equation for that remedy. Next, they turned to discussing
how to allow multiple evidence sets, or multiple hypothesis sets instead of just one.
In Figure 4, I redraw from Keppens Shen, and Lee (2005, Sec. 3, Figure 2) the archi-
tecture of their system, as extended with Bayesian networks in the knowledge represen-
tation, and with evidence collection strategies in the output.
20 E. Nissan
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
0:
25
 0
3 
D
ec
em
be
r 
20
13
 
6. Data mining, social networks and link analysis
Data mining has found application to knowledge discovery in legal databases (Stranieri &
Zeleznikow, 2005). Data mining has also been variously applied to security and criminal
detection (Mena, 2003).
Figure 4. The extended architecture, redrawn from Keppens, Shen, and Schafer (2005, Sec. 3,
Figure 2): Bayesian networks appear in the knowledge representation, and evidence collection strat-
egies appear in the output.
Information & Communications Technology Law 21
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
0:
25
 0
3 
D
ec
em
be
r 
20
13
 
Henceforth, this paper surveys applications of data mining to intelligence and investi-
gative tasks within law enforcement. This is a separate subject, just as the use of computing
in various forensic sciences has followed trajectories different from legal computing and AI
& Law. A current rapprochement between AI & Law and data mining for crime investi-
gation is ongoing, and holds the promise of synergism hopefully enhancing the concrete
usefulness of the broad overarching domain.
Social network analysis (SNA) is the branch of sociology that deals with the quantitative
evaluation of an individual’s role in a group or community, by analysing the network of
connections between that individual and others. See Breiger’s (2004) overview, Freeman’s
(2007) 4-volume set, Aggarwal (2011), Newman (2010). Exploratory visualisation of social
networks is the subject of, e.g. Brandes, Raab, andWagner (2001), but within an application
to decision-making research in a real-case study: they applied some SNA techniques for the
study the pattern of decision-making itself. They represented how the process of decision-
making can be represented as the network of interactions between the actors involved in the
process.
SNA has been applied, among other things, to organised crime. US Air Force’s Major
Jonathan T. Hamill has discussed a PhD dissertation at the Air Force Institute of Technology
in Ohio, Analysis of Layered Social Networks, application to counterterrorism being the
motivation for the project. Hamill’s thesis (2006) is concerned with prevention of near-
term terrorist attacks.
Wherever individuals are organised, we can map their links by making their social
network explicit, for example in order to better realise which advantage (i.e. which
social capital) the individual derives from the network. Link analysis is a technique in
which visualisation plays a central role, and which has become quite important for
crime intelligence and crime investigation (e.g. Leary, 2012). Historically, it emerged
within ergonomics (Fitts, Jones, & Milton, 1950; Gilbreth & Gilbreth, 1917; cf.
Harper & Harris, 1975, p. 158). By contrast, the visualisation of social networks was
apparently inaugurated by Jacob Levy Moreno (1889–1974) when he showed socio-
metric charts at the 1933 convention of the Medical Society of the State of New York
(Moreno, 1953, p. xiii), something which in retrospect, he considered to have been
the breakthrough of the sociometric movement. Moreno (Marineau, 1989) is considered
to have been the father of sociometry as well as of psychodrama and of group
psychotherapy.
Link analysis is an interactive technique, visualising – in charts or maps or diagrams –
networks of entity-to-event associations (e.g. tying a victim to a crime) as well as entity-
to-entity (e.g. blood relative, or spouse, or place of birth, or owner of a firm), and
event-to-event (e.g. tying emails to each other). Link analysis can benefit from SNA, bor-
rowing from the latter, and applying, this or that formal device. Users watching on the
screen the results returned by link analysis tools see those results, not the mathematics of
the underlying concepts from SNA. Quite possibly, the best known project in link analysis
for law enforcement is Coplink (see Sec. 7 below).
Harper and Harris (1975, p. 159) explained:
The link analysis procedure centers round the production of an association matrix and a link
diagram. The association matrix provides an array of the relationships among any set of indi-
viduals; the notation in any cell of the matrix indicates the nature of the link – strong, weak, or
none – between two individuals. The link diagram, the end product of the analysis, presents a
graphic illustration of the relationships among the set of individuals. If some of the individuals
are members of identifiable organizations, these organizations can also be incorporated into the
diagram. The analysis is completed by [a] six-step approach.
22 E. Nissan
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
0:
25
 0
3 
D
ec
em
be
r 
20
13
 
The association matrix is triangular, and the same names identify the rows and the
columns. The six steps as listed by Harper and Harris were:
(1) Assemble the available information;
(2) Abstract information relevant to individual relationships and affiliations;
(3) Prepare an association matrix;
(4) Develop a preliminary link diagram;
(5) Incorporate organisations in the diagram;
(6) Refine the link diagram.
There exist on the market such link analysis tools that are specifically tailored for assist-
ing in criminal investigation. Mena (2003, Sec. 3.11, pp. 88–104) provided a good survey
of these. Crime Workbench is an intelligence management software product for criminal
and fraud investigation;7 there is a scaled down version, Crime Workbench Web, accessible
from everywhere, and making it possible to amend, for ‘the intelligence analyst and law
enforcement investigator on the move’ (Mena, 2003, p. 100). Daisy is a link analysis
tool supporting a circular layout of nodes: these are connected by lines inside the circle,
and are possibly surmounted by histograms outside the circle.8 By contrast, the main
layout of displays generated by NETMAP – a link analysis tool9 used by several govern-
ment agencies in the USA – are a wagonwheel format, while also supporting other layouts.
Mena noted that a unique feature of another tool, Crime Link,10 ‘is its ability to generate
a two-dimensional association matrix that basically shows who knows whom, who has done
what, who has been where, etc.’ (2003, p. 97). This is a triangular table, with one-line
textual explanations (such as personal names, with their variants) shown perpendicularly
to its diagonal, thus identifying the rows columns of the matrix. Those personal names
are preceded by a bullet, if the row or column includes a bullet in at least one case. This
enables to see who knows whom.
The ORIONInvestigations criminal data organiser can be integrated with the ORION-
Link link analysis tool.11
A special feature ofORIONLink is its what-ifmode, which allows objects and their connections
to be hidden or restored on the fly, allowing for the viewing of their impact on the total organ-
ization, such as a terrorist cell or criminal gang. (Mena, 2003, p. 103)
7. Coplink
Coplink is a tool for criminal intelligence analysis which finds links in databases among
such entities.12 Developed by a team at the University of Arizona in collaboration with
the Tucson police, Coplink performs data integration, pooling together the various infor-
mation sources available (Chen, Schroeder, et al., 2003; Chen, Zeng, Atabakhsh, Wyzga,
& Schroeder, 2003; Hauck, Atabakhsh, Ongvasith, Gupta, & Chen, 2002). It ‘evolved
into a real-time system being used in everyday police work’ (Hauck et al., 2002, p. 30).
Drawing upon experience gained with the Coplink project, Chen et al. (2004) presented
a general framework for crime data mining. Next, Xiang, Chau, Atabakhsh, and Chen
(2005) described a prototype system called the COPLINK Criminal Relationship Visuali-
zer; they contrasted the use of two views, namely, a hyperbolic tree view and a hierarchical
list view. Also see Schroeder, Xu, Chen, and Chau (2007).
At the Tucson Police Department, records at the time consisted of about 1.5 million
criminal case reports, containing details from criminal events spanning the period from
1986 to 1999 (Hauck et al., 2002, p. 31). Investigators were able, before Coplink
Information & Communications Technology Law 23
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
0:
25
 0
3 
D
ec
em
be
r 
20
13
 
became available, to access the Records Management System (RMS) to tie together infor-
mation, but when it came to finding relationships inside the records they had to manually
search the RMS data (Hauck et al., 2002, p. 31).
Coplink’s underlying structure is the concept space, or automatic thesaurus, a statistics-based,
algorithmic technique used to identify relationships between objects of interest. A concept
space consists of a network of terms and weighted associations that assist in concept-based
information retrieval within an underlying information space. (Hauck et al., 2002, p. 31)
Chen et al. (2004) used a concept-space approach, in order to extract criminal relations from
the incident summaries, and create a likely network of suspects. When building a domain-
specific concept space, the first step consists of identifying document collections in the
specific subject domain, and for the Tucson police, the collection was the case reports in
the existing database. Each piece of information in the case reports database was cate-
gorised and stored in well-organised structures. In the second step, the terms were filtered
and indexed (Hauck et al., 2002, pp. 31–32).
In order to carry out named-entity extraction, Hsinchun Chen’s team used a modified
version of the AI Entity Extractor system. In order to identify the names of persons,
locations, and organisations in a document, that tool performs a three-step process. The
first step consists of identifying noun phrases according to linguistic rules. In the second
step, ‘the system calculates a set of feature scores for each phrase based on pattern matching
and lexical lookup. Third, it uses a feedforward/back-propagation neural network to predict
the most likely entity type for each phrase’ (Hauck et al., 2002, pp. 31–32).
The second data mining task for Coplink reported about in Chen et al. (2004)
involved automatically detecting deceptive criminal identities from the Tucson Police Depart-
ment’s database, which contains information such as name, gender, address, ID number, and
physical description. Our detective consultant manually identified 120 deceptive criminal
records involving 44 suspects from the database.
Efficient algorithms for searching graphs, and using Coplink’s concept space, were dis-
cussed by Xu and Chen (2004), who used as a data-set one year’s worth of crime reports
from the Phoenix Police Department. Their algorithms compute the shortest paths
between two nodes in a graph, based on weighted links.
The Coplink project does not use entity extraction techniques: they drew the data from a
structured database system. Yet, it is often the case that police records systems contain large
collections of unstructured text and structured case reports.
8. Mining the Enron’s email database
Following the US Federal Energy Regulatory Commission’s investigation of Enron, a large
corpus of emails from Enron was put into the public domain, and this was an opportunity
for research (e.g. Gray & Debreceny, 2006). Various techniques were resorted to.
Probably the most discussed continuous email monitoring is the Carnivore system developed
by the FBI to scan emails in the United States. The CIA and NSA are assumed to have
similar systems to monitor email traffic outside of the U.S. The difference being that the
FBI needs a court order before it can monitor a specific person’s email traffic in the U.S. By
the way, companies do not need a court order to monitor employee emails. (Gray & Debreceny,
2006)
24 E. Nissan
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
0:
25
 0
3 
D
ec
em
be
r 
20
13
 
One strand of research was intended to discover structures within the organisation. Wilson
and Banzhaf (2009) applied a genetic algorithm to the discovery of social networks within
the Enron-email database.
One of the tools presented by Goldberg et al. (2008) is SIGHTS, ‘designed for the dis-
covery, analysis, and knowledge visualization of social coalition in communication net-
works by analyzing communication patterns’. The algorithms of SIGHTS ‘extract groups
and track their evolution in Enron-email data-set and in Blog data. The goal of SIGHTS
is to assist an analyst in identifying relevant information’ (Goldberg et al., 2008). Goldberg
et al. (2008) also described a complementary set of tools, using Recursive Data Mining
(RDM). Those tools’ task is ‘to identify frequent patterns in communication content such
as email, blog or chat-room sessions’.
SIGHTS was ‘designed for the discovery, analysis, and knowledge visualization of social
coalition in communication networks by analyzing communication patterns’. The algorithms
of SIGHTS ‘extract groups and track their evolution in Enron-email data-set and in Blog data.
The goal of SIGHTS is to assist an analyst in identifying relevant information’ (Goldberg
et al., 2008). SIGHTS has three main modules: Data Collection/Storage, Data Learning
and Analysis, and Knowledge Extraction/Visualisation. The data sources are email data, or
blogs, and it was envisaged to also include a link to chat rooms at a later stage. From the
data sources, data are collected, and a semantic graph and metadata are stored in a database.
The Data Collection Modules operate on semantic graphs. The graphs are constructed by
adding a node for each social network actor and a directed edge from sender’s node to a reci-
pient’s node. The edges are marked with the time of the communication and, possibly, other
labels. Some edge labels are only appropriate for specified types of graphs. (from Sec. 2 of
Goldberg et al., 2008)
Blog Collector is a module accessing blogs, one of the data sources. The algorithm modules
of SIGHTS interact with the database, retrieving from it the semantic graph and metadata,
and storing in it derived data. The algorithm modules include: Real-Time Clustering,
Leader Identification, Topic Identification, Cycle Group Analysis, Stream Group Analysis.
Interactive visualisation, through which users access the output of the algorithm, accounts
for: Size vs. Density plot, Graph of Clusters plot, Group Persistence view, Leaders and
Group Evolution view, interactive Graph of Overlapping Clusters.
In SIGHTS, there is an Opposition Identification Module. Its task is to identify ‘the
positive and negative sentiments between pairs of bloggers based on the length and
average size of the messages in the conversations that took place among them’ (from
Sec. 2 of Goldberg et al., 2008). From LiveJournal.com, SIGHTS splits threads of com-
ments into conversations between pairs of bloggers.
The module employs the Support Vector Machine classifier13 that was trained using a data set
that was manually created to determine the oppositions between bloggers using the length of
the conversation and the average length of the message in the conversation to determine
whether bloggers opposed each other in a given conversation. (Goldberg et al., 2008)
Bennett and Campbell (2000) provided a good introduction to support vector machines
(SVMs). Steinwart and Christmann (2008) published a book on SVMs. So did Campbell
and Ying (2011), whose abstract states, among the other things:
Support Vectors Machines have become a well established tool within machine learning. They
work well in practice and have now been used across a wide range of applications from
Information & Communications Technology Law 25
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
0:
25
 0
3 
D
ec
em
be
r 
20
13
 
recognizing hand-written digits, to face identification, text categorisation, bioinformatics, and
database marketing. In this book we give an introductory overview of this subject. We start with
a simple Support Vector Machine for performing binary classification before considering multi-
class classification and learning in the presence of noise.
An early definition of the concept was as follows:
The support-vector network is a new learning machine for two-group classification problems.
The machine conceptually implements the following idea: input vectors are non-linearly
mapped to a very high-dimension feature space. In this feature space a linear decision
surface is constructed. Special properties of the decision surface ensures [sic] high generaliz-
ation ability of the learning machine. […] High generalization ability of support-vector net-
works utilizing polynomial input transformations is demonstrated. (Cortes & Vapnik, 1995,
p. 273)
RDM is
a text mining approach that discovers patterns at varying degrees of abstraction in a hierarchical
fashion. The approach allows for certain degree of approximation in matching patterns, which
is necessary to capture non-trivial features in realistic datasets. Due to its nature, we call this
approach Recursive Data Mining (RDM). (Chaoji, Hoonlor, & Szymanski, 2010)
Goldberg et al. (2008) also described a complementary set of tools, using RDM. Those
tools’ task is ‘to identify frequent patterns in communication content such as email, blog
or chat-room sessions’. The approach of Goldberg et al. (2008) ‘enables discovery of pat-
terns at varying degrees of abstraction, in a hierarchical fashion, and in language indepen-
dent way’. They
use RDM to distinguish among different roles played by communicators in social networks
(e.g., distinguishing between leaders and members). Experiments on the Enron dataset,
which categorize members into organizational roles demonstrate that use of the RDM dominant
patterns improves role detection. (from the abstract)
Szymanski and Zhang (2004) and Coull and Szymanski (2008) resorted to RDM for mas-
querade detection (within intrusion detection affecting computer resources) and author
identification. These are subjects investigated using various techniques by other authors
(e.g. Elsayed & Oard, 2006; de Vel, Anderson, Corney, & Mohay, 2001).
Goldberg et al. (2008)
used Recursive Data Mining (RDM) for distinguishing the roles of the communicators in a
social group. In general, RDM discovers, in a recursive manner, statistically significant patterns
in a stream of data. The key properties of the pattern discovery in RDM include: (i) no restric-
tion of the size of gaps between patterns, (ii) recursive mining in which discovered patterns are
replaced by the new token and the mining is repeated on the newly created string, (iii) tolerance
to imperfect matching. (from Sec. 1 of Goldberg et al., 2008)
9. Auction fraud: NetProbe
In NetProbe, a tool for detecting fraud at online auction sites (Chau, Pandit, & Faloutsos,
2006; Pandit, Chau, Wang, & Faloutsos, 2007), users and transactions were modelled as a
Markov random field (MRF), tuned for the detection of suspicious patterns generated by
fraudsters. A belief propagation mechanism was resorted to, in order to infer the
26 E. Nissan
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
0:
25
 0
3 
D
ec
em
be
r 
20
13
 
maximum likelihood state probabilities of nodes in the MRF, given a propagation matrix
and possibly a prior state assignment for some of the nodes. Each node can be in one
out of three states, namely, fraud, accomplice, or honest, or then its state may be undeter-
mined, in the sense that NetProbe does not assign a state to that node. NetProbe uses the
propagation matrix in order to detect bipartite cores in the graph.
In order for the users to get answers to their queries in real time when using NetProbe,
the Carnegie Mellon team which developed NetProbe, also developed Incremental Net-
Probe, a version which allows approximation. This avoids wasteful recomputation from
scratch of node beliefs. Incremental NetProbe incrementally updates node beliefs as
small changes occur in the graph.
In NetProbe, the
key idea is to infer properties for a user based on properties of other related users. In particular,
given a graph representing interactions between auction users, the likelihood of a user being a
fraudster is inferred by looking at the behaviour of its immediate neighbors. (Pandit et al., 2007,
p. 203)
This is why trust propagation and authority propagation research are akin to the method
of NetProbe. Some ‘non-trivial design and implementation decisions’ were made by the
Carnegie Mellon team
while developing NetProbe. In particular, we discuss the following contributions: (a) a paral-
lelizable crawler that can efficiently crawl data from auction sites, (b) a centralized queuing
mechanism that avoids redundant crawling, (c) fast, efficient data structures to speed up our
fraud detection algorithm, and (d) a user interface that visually demonstrates the suspicious be-
havior of potential fraudsters to the end user. (Pandit et al., 2007, p. 202)
MRFs are suitable for such problem-solving about inference that there is uncertainty in
observed data. An MRF is a probabilistic model defined by local conditional probabilities.
The concept is useful for devising contextual models with prior information: MRF theory is
typically resorted to in order to model context-dependent entities (such as, in image proces-
sing within computer science, image pixels). Basically, an MRF is an undirected graph, that
is to say, the edges between pairs of nodes are not arrows. Each node in an MRF can be in
any of a finite number of states. The state of a node statistically depends upon each of its
neighbours (i.e. those nodes to which the given node is connected by an edge), and upon
no other node in the graph. A propagation matrix, symbolised as ψ, represents the depen-
dency between a node and its neighbours in the given MRF. Each case ψ(i, j) in the matrix
has a value which is equal to the probability of a node i being in state j given that it has a
neighbour in state i. If an assignment of states to the nodes in an MRF is given, then by
using the propagation matrix it is possible to compute a likelihood of observing that assign-
ment. The problem of inferring the maximum likelihood assignment of states to nodes,
where the correct states for some of the nodes are possibly known beforehand, is solved
by those using MRFs by resorting to heuristic techniques (this is so because enumerating
all states would be exponential in time, and because of the lack of any known theoretical
method that would solve this problem for a general MRF), and an especially powerful heur-
istic method to do that is the iterative message passing scheme of the belief propagation
algorithm.
In order to detect likely fraudsters, a belief propagation mechanism was resorted to in
NetProbe, that algorithm being generally used in order to infer the maximum likelihood
state probabilities of nodes in the MRF, given a propagation matrix and possibly a prior
Information & Communications Technology Law 27
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
0:
25
 0
3 
D
ec
em
be
r 
20
13
 
state assignment for some of the nodes. In the MRF of NetProbe, each node stands for a
user. Each edge stands for one or more transactions between pairs of users. If there is an
edge between two nodes, this indicates that the two users for whom the two nodes stand
have transacted at least once. Each node can be in one out of three states, namely, fraud,
accomplice, or honest, or then its state may be undetermined, in the sense that NetProbe
does not assign a state to that node. Pandit et al. (2007, p. 204) claimed they
uncovered a different modus operandi for fraudsters in auction networks, which leads to the
formation of near bipartite cores. Fraudsters create two types of identities and arbitrarily
split them into two categories – fraud and accomplice. The fraud identities are the ones used
eventually to carry out the actual fraud, while the accomplices exist only to help the fraudsters
carry out their job by boosting their feedback rating. Accomplices themselves behave like per-
fectly legitimate users and interact with other honest users to achieve high feedback ratings. On
the other hand, they also interact with the fraud identities to form near bipartite cores, which
helps the fraud identities gain a high feedback rating. Once the fraud is carried out, the
fraud identities get voided by the auction site, but the accomplice identities linger around
and can be reused to facilitate the next fraud.
NetProbe uses the propagation matrix in order to detect bipartite cores in the graph. In
NetProbe, a particular propagation matrix was devised, so that the belief propagation mech-
anism would suit the behaviour of fraudsters and their accomplices. The intuition (Pandit
et al., 2007, pp. 204–205) was that a fraudster would avoid linking to another fraudster.
Rather, a fraudster would link heavily to accomplices. An accomplice, instead, would
link to both honest nodes and fraudsters, but the accomplice has a higher affinity for fraud-
sters. As to honest nodes (i.e. innocent users), they link to honest nodes as well as to accom-
plices, because the honest user believes the accomplice to be honest.
What is meant by near bipartite cores is that inside the graph which represents the online
auction site, one expects to find such subsets of the nodes (i.e. such subsets of the users) that
the given subset is a complete bipartite graph. That is to say, the given subset could be
divided into two subsubsets, and each node in either subsubset has edges linking it to all
nodes in the other subsubset. If we replace ‘all’ with ‘one or more of the’, then we would
have a bipartite graph that is not a complete bipartite graph. This is also a possibility that
is relevant for detecting fraudsters and their accomplices at sites like eBay.
10. Polonium: tera-scale graph mining for malware detection
Polonium is a tera-scale graph-mining tool for malware detection (Chau, Nachenberg,
Wilhelm, Wright, & Faloutsos, 2010). The reputation-based approach adopted is a Syman-
tec protection model that, for every application that users may encounter, computes a repu-
tation score (considered as belief in belief propagation), and protects them from files whose
score is poor. Various attributes contribute to reputation: whether an application comes from
known publishers, whether it already has many users, and so forth. ‘Good files typically
appear on many machines and bad files appear on few machines’ (Chau et al., 2010).
Another intuition is what was called homophilic machine–file relationships:
We expect that good files are more likely to appear on machines with good reputation and bad
files more likely to appear on machines with low reputation. In other words, the machine-file
relationships can be assumed to follow homophily. (Chau et al., 2010)
An undirected, unweighted bipartite graph of files and machines was generated
28 E. Nissan
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
0:
25
 0
3 
D
ec
em
be
r 
20
13
 
from the raw data, with almost 1 billion nodes and 37 billion edges (37,378,365,220). 48
million of the nodes are machine nodes, and 903 million are file nodes. An (undirected)
edge connects a file to a machine that has the file. All edges are unweighted; at most one
edge connects a file and a machine. The graph is stored on disk as a binary file using the adja-
cency list format. (Chau et al., 2010)
‘[W]e want to label a file node as good or bad, along with a measure of the confidence in
that disposition’ (Chau et al., 2010). In Polonium, there is a tradeoff concerning false posi-
tives (FPs), expressed in how the belief propagation algorithm is made to stop.
Chau et al. (2010) claimed: ‘We evaluated it with the largest anonymised file sub-
missions data-set ever published, which spans over 60 terabytes of disk space’ (emphasis
in the original), with over 900 million files described in the raw data, from a total of
47,840,574 machines. Polonium resorts to graph mining. Like NetProbe, it also resorts
to the belief propagation algorithm. ‘We adapted the algorithm for our problem. This adap-
tation was non-trivial, as various components used in the algorithm had to be fine tuned;
more importantly, […] modification to the algorithm was needed to induce iterative
improvement in file classification’ (Chau et al., 2010).
A reputation-based approach was adopted. In a nutshell,
the key idea of the Polonium algorithm is that it infers a file’s goodness by looking at its associ-
ated machines’ reputations iteratively. It uses all files’ current goodness to adjust the reputation
of machines associated with those files; this adjusted machine reputation, in turn, is used for re-
inferring the files’ goodness. (Chau et al., 2010)
‘Symantec has computed a reputation score for each machine based on a proprietary
formula that takes into account multiple anonymous aspects of the machine’s usage and be-
haviour. The score is a value between 0 and 1’ (Chau et al., 2010).
Computing reputation in a credible manner was made possible by the worldwide
Norton Community Watch programme, with millions of users contributing data anon-
ymously. This is a huge file submission data-set. The raw data undergo processing at
Symantec, and then are fed into Polonium, which mines the data statistically, and
machine learning is applied. ‘Each contributing machine is identified by an anonymized
machine ID, and each file by a file ID which is generated based on a cryptographically-
secure hashing function’ (Chau et al., 2010). An undirected, unweighted bipartite graph
of files and machines was generated
from the raw data, with almost 1 billion nodes and 37 billion edges (37,378,365,220). 48
million of the nodes are machine nodes, and 903 million are file nodes. An (undirected)
edge connects a file to a machine that has the file. All edges are unweighted; at most one
edge connects a file and a machine. The graph is stored on disk as a binary file using the adja-
cency list format. (Chau et al., 2010)
Polonium computes the reputation for a given application, and is used in concert with
other Symantec malware detection technologies. In the belief propagation algorithm as used
in Polonium, belief corresponds to reputation. The Polonium team treated each file as a
random variable X, whose value is
. either xg (this being the ‘good’ label),
. or xb (this being the ‘bad’ label).
. The probability P(xg) is the file goodness,
. whereas P(xb) is the file badness,
Information & Communications Technology Law 29
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
0:
25
 0
3 
D
ec
em
be
r 
20
13
 
and the sum of the two probabilities is 1. Therefore, by knowing the value of one, one also
knows the other. For each file i, the goal is to find the marginal probability P(Xi = xg), that is
the goodness of that file. Domain knowledge helps infer label assignments.
Terminology with the respective definitions include: file ground truth for ‘file label,
good or bad, assigned by human security experts’ (here, by ‘file’ an executable file is
meant); known-good file for ‘file with good ground truth’; known-bad file for ‘file with
bad ground truth’; and unknown file for ‘file with unknown ground truth’.
Symantec maintains a ground truth database that contains large number of known-good and
known-bad files, some of which exist in our graph. We can leverage the labels of these files
to infer those of the unknowns. The ground truth files influence their associated machines
which indirectly transfer that influence to the unknown files. (Chau et al., 2010)
Moreover, the possibility of errors is recognised: in the case of Polonium, True Positive (TP)
stands for ‘malware instance correctly identified as bad’, as opposed to FP for ‘a good file
incorrectly identified as bad’. FPs are a price to pay that comes with some successful
malware detection tools: Tesauro, Kephart, and Sorkin (1996), who applied neural networks,
were able to detect ‘boot sector viruses’ with over 90% TP rate in identifying those viruses,
but on the other hand this came at a 15–20% FP rate. In Polonium, there is a tradeoff concern-
ing FPs that is expressed in how the belief propagation algorithm is made to stop.
Virus signatures are virus profiles or virus definitions. Malware detection comes in two
major categories: anomaly-based detection, based on some presumed ‘normal’ behaviour
from which malware deviates, and signature-based detection, in which malware instances
are detected because they fit some profiles (Chau et al., 2010; Idika &Mathur, 2007). It was
Kephart and Arnold (1994) who first used data mining techniques to automatically extract
virus signatures. Schultz, Eskin, Zadok, and Stolfo (2001) were among those who pio-
neered the application of machine learning algorithms (in their case, Naive Bayes and
Multi-Naive Bayes) to classify malware.
11. Conclusions
Artificial intelligence for legal applications has only since around 2000 begun to model
reasoning on legal evidence in a sustained manner. In this overview, we considered both
early and recent tools, and pointed out the importance of the controversy concerning Baye-
sianism among legal scholars, for the likelihood of acceptance of AI applications by legal
scholars. We paid special attention to the work by Bex and Keppens on crime scenario mod-
elling. Bex’s work is especially promising, as it goes in the direction of providing plausi-
bility ranking models that are not based on probabilities, and are therefore palatable to
the Bayesio-sceptic camp within the Bayesian controversy among legal scholars. This is
also the case of the non-probabilistic version of the model by Keppens and Schafer. We con-
sidered Thagard’s ECHO as a tool for jury modelling as well as the belief revision system of
Dragoni and Nissan (2004), accounting for evolving impressions of incoming information
on judges at bench trials or on jurors at trials by jury.
We also considered a few representative systems of current application of data mining to
the fight against crime. Data mining for fraud detection is a major breakthrough (cf. Kou, Lu,
Sirwongwattana, & Huang, 2004; Phua, Lee, Smith-Miles, & Gayler, 2005; Weatherford,
2002). Coplink has relatively been a very visible project, because of its own merits
making it an archetype, as well as because of where its publications appeared. Link analysis
however is widespread, including in law enforcement, and there even exist commercial
30 E. Nissan
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
0:
25
 0
3 
D
ec
em
be
r 
20
13
 
software products, based on that kind of technique, for the police or detectives. Text mining
found a forensic application in the analysis of the Enron database, and this application in turn
became relatively visible because of the notoriety of the Enron scandal. We only considered
an example of email mining as applied to the Enron database. Malware, the application
domain of the Polonium data mining project, is an area practically as big as computer security.
This suggests that we are going to see much more research into the application of data mining
in this area, and our coverage was by no means exhaustive: we could not do justice to com-
puter or network security in the short compass of this overview.
A major direction of research that has emerged is the analysis of suspected complicity in
networks, in relation to various kinds of fraud. Online auction fraud is a major area of appli-
cation, because of how sore the problem is. Fraud involving stock brokers is another area in
which there has been investment in information technology, evolving from old-fashioned
rule-based expert systems, to the current emphasis on data mining. Fiscal fraud is an econ-
omically important sector within fraud, one that (like in the case of the Pisa SNIPER
project, described in Basta, Giannotti, Manco, Pedreschi, & Spisanti, 2009) is quite relevant
for enforcing governmental fiscal policies.
Notes
1. By Dave Schum, see Kadane & Schum (1996), and the several publications Schum (1986),
Schum (2001), and Schum and Martin (1982).
2. By those same authors, also see Allen and Pardo (2007b, 2008). Apart from Ron Allen’s papers
already cited, cf. Allen (1992, 1994, 1997, 2000, 2001a, 2001b, 2003).
3. Floris Bex has authored Bex (2011), Bex et al. (2009), Bex, van Koppen, Prakken, and Verheij
(2010), Bex et al. (2007), Bex, Prakken, Reed, and Walton (2003), Bex et al. (2006), Bex and
Walton (2010).
4. It is worthwhile to mention here several studies by Keppens (2007, 2009), Keppens and Zelez-
nikow (2002, 2003), Keppens and Schafer (2003a, 2003b, 2004, 2005, 2006), Keppens and
Shen (2001, 2004), Keppens, Shen, and Lee (2005), Keppens, Shen, and Schafer (2005).
5. Araucaria is available for free at http://www.computing.dundee.ac.uk/staff/creed/araucaria.
6. Take for example the syllogism “All men are mortal, and Socrates is a man; therefore
Socrates is mortal”. In predicate calculus, the three expressions
A
X(man(X)⇒mortal(X)).
man(socrates).
man(socrates)⇒mortal(socrates).
respectively stand for “All men are mortal”, “Socrates is a man”, and “Socrates is a man,
therefore Socrates is mortal”. Unification is an algorithm that an automated problem solver
can use in order to determine that socrates may be substituted for X.
7. http://www.memex.com/cwbover.html.
8. http://www.daisy.co.uk/daisy.html.
9. http://www.altaanalytics/com/.
10. http://www.crimelink.com/.
11. http://www.oriosci.com/productinfo/Magic.html.
12. http://ai.bpa.arizona.edu/coplink.
13. Support vector machines are the subject of Sec. 6.1.9.3 in the book Nissan (2012).
References
Aggarwal, C. C. (2011). Social network data analysis. Berlin: Springer.
Allen, R. J. (1986). A reconceptualization of civil trials. Boston University Law Review, 66, 401–437.
Allen, R. J. (1991). The nature of juridical proof. Cardozo Law Review, 13, 373–422.
Allen, R. J. (1992). The hearsay rule as a rule of admission. Minnesota Law Review, 76, 797–812.
Information & Communications Technology Law 31
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
0:
25
 0
3 
D
ec
em
be
r 
20
13
 
Allen, R. J. (1994). Factual ambiguity and a theory of evidence. Northwestern University Law Review,
88, 604–640.
Allen, R. J. (1997). Rationality, algorithms and juridical proof: A preliminary inquiry. International
Journal of Evidence and Proof, 1, 254–275.
Allen, R. J. (2000). Clarifying the burden of persuasion and Bayesian decision rules: A response to
Professor Kaye. International Journal of Evidence and Proof, 4, 246–259.
Allen, R. J. (2001a). Artificial intelligence and the evidentiary process: The challenges of formalism
and computation. Artificial Intelligence and Law, 9(2/3), 99–114.
Allen, R. J. (2001b). Clarifying the burden of persuasion and Bayesian decision rules: A response to
Professor Kaye. International Journal of Evidence and Proof, 4, 246–259.
Allen, R. J. (2003). The error of expected loss minimization. Law, Probability & Risk, 2, 1–7.
Allen, R. J. (2008a). Explanationism all the way down. Episteme, 3(5), 320–328.
Allen, R. J. (2008b). Juridical proof and the best explanation. Law & Philosophy, 27, 223–268.
Allen, R. J., & Lively, S. (2003 [2004]). Burdens of persuasion in civil cases: Algorithms
v. explanations. MSU Law Review, 2003, 893–944.
Allen, R. J., & Pardo, M. S. (2007a). The problematic value of mathematical models of evidence.
Journal of Legal Studies, 36, 107–140.
Allen, R. J., & Pardo, M. S. (2007b). Probability, explanation and inference: A reply. International
Journal of Evidence and Proof, 11, 307–317.
Allen, R. J., & Pardo, M. S. (2008). Juridical proof and the best explanation. Law & Philosophy, 27,
223–268.
Allen, R., & Redmayne, M. (Eds.). (1997). Bayesianism and juridical proof [Special issue]. The
International Journal of Evidence and Proof, 1, 253–360. London: Blackstone.
Anderson, T., Schum, D., & Twining, W. (2005). Analysis of evidence: How to do things with facts.
Based on Wigmore’s science of judicial proof. Cambridge: Cambridge University Press.
Anderson, T., & Twining, W. (1991). Analysis of evidence: How to do things with facts. (With a tea-
cher’s manual.) London: Weidenfeld & Nicolson, 1991; Boston, MA: Little, Brown, 1991;
Evanston, IL: Northwestern University Press, 1998. [The 2nd edn. (extensively revised) is
Anderson et al. (2005)].
Atkinson, K., & Bench-Capon, T. J. M. (2007). Practical reasoning as presumptive argumentation
using action based alternating transition systems. Artificial Intelligence, 171(10–15), 855–874.
Baber, C. (2010). Distributed cognition at the crime scene. AI & Society, 25, 423–432.
Basta, S., Giannotti, F., Manco, G., Pedreschi, D., & Spisanti, L. (2009). SNIPER: A data mining
methodology for fiscal fraud detection. In Mathematics for finance and economy, special issue
of ERCIM News, 78 (July), 27–28. Retrieved from http://ercim-news.ercim.org/
Bench-Capon, T. J. M. (2003). Persuasion in practical argument using value based argumentation fra-
meworks. Journal of Logic and Computation, 13(3), 429–448. Retrieved from http://www.csc.liv.
ac.uk/∼tbc/publications/jcl03.pdf
Bennett, K. P., & Campbell, C. (2000). Support vector machines: Hype or Hallelujah? SIGKDD
Explorations, 2(2), 1–13. New York: ACM Press.
Bennett, W. L., & Feldman, M. S. (1981). Reconstructing reality in the courtroom: Justice and judg-
ment in American culture. New Brunswick, NJ: Rutgers University Press.
Bex, F. (2011). Arguments, stories and criminal evidence: A formal hybrid theory (Vol. 92).
Dordrecht: Springer Law and Philosophy Library, Springer.
Bex, F., Bench-Capon, T., & Atkinson, K. (2009). Did he jump or was he pushed? Abductive practical
reasoning. Artificial Intelligence and Law, 17(2), 79–99. Retrieved from http://www.computing.
dundee.ac.uk/staff/florisbex/Papers/AILaw09.pdf
Bex, F. J., van den Braak, S. W., van Oostendorp, H., Prakken, H., Verheij, H. B., & Vreeswijk,
G. A. W. (2007). Sense-making software for crime investigation: How to combine stories and
arguments? Law, Probability & Risk, 6, 145–168. Retrieved from http://www.computing.
dundee.ac.uk/staff/florisbex/Papers/LPR07.pdf. The same article with diagrams in colour:
http://www.cs.uu.nl/research/projects/evidence/publications/lpr07submitted.pdf
Bex, F. J., van Koppen, P. J., Prakken, H., & Verheij, B. (2010). A hybrid formal theory of arguments,
stories and criminal evidence. Artificial Intelligence and Law, 18(2), 123–152. Retrieved from
http://www.cs.uu.nl/groups/IS/archive/henry/Bexetal10.pdf; http://www.computing.dundee.ac.
uk/staff/florisbex/Papers/AILaw10.pdf
Bex, F. J., Prakken, H., Reed, C., & Walton, D. N. (2003). Towards a formal account of reasoning
about evidence: Argumentation schemes and generalisations. Artificial Intelligence and Law,
32 E. Nissan
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
0:
25
 0
3 
D
ec
em
be
r 
20
13
 
12, 125–165. Retrieved from http://www.computing.dundee.ac.uk/staff/florisbex/Papers/
AILaw03.pdf
Bex, F. J., Prakken, H., & Verheij, B. (2006). Anchored narratives in reasoning about evidence. In
T. M. van Engers (Ed.), Legal knowledge and information systems – JURIX 2006: The nineteenth
annual conference on legal knowledge and information systems, Paris, France (pp. 11–20).
Amsterdam: IOS Press.
Bex, F. J., & Walton, D. (2010, December 16–17). Burdens and standards of proof for inference to the
best explanation. In R. Winkels (Ed.), Legal knowledge and information systems. JURIX 2010:
The 23rd international conference on legal knowledge and information systems. University of
Liverpool, UK. Frontiers in Artificial Intelligence and Applications (Vol. 223, pp. 37–46).
Amsterdam: IOS press.
Brandes, U., Raab, J., & Wagner, D. (2001, October 19). Exploratory network visualization:
Simultaneous display of actor status and connections. Journal of Social Structure, 2(4).
Retrieved from http://www.cmu.edu/joss/content/articles/volume2/BrandesRaabWagner.html
Breiger, R. L. (2004). The analysis of social networks. In M. Hardy & A. Bryman (Eds.),Handbook of
data analysis (pp. 505–526). London: SAGE.
Campbell, C., & Ying, Y. (2011). Learning with support vector machines. Synthesis Lectures on
Artificial Intelligence and Machine Learning, 5(1), 1–95.
Carr, D. (2008). Narrative explanation and its malcontents. History and Theory, 47, 19–30.
Chaoji, V., Hoonlor, A., & Szymanski, B. K. (2010). Recursive data mining for role identification in
electronic communications. International Journal of Hybrid Information Systems, 7(3), 89–100.
Retrieved from http://www.cs.rpi.edu/~szymansk/papers/ijhis.09.pdf
Chau, D. H., Nachenberg, C., Wilhelm, J., Wright, A., & Faloutsos, C. (2010, July 25). Polonium:
Tera-scale graph mining for malware detection. Proceedings of the second workshop on large-
scale data mining: Theory and applications (LDMTA 2010), Washington, DC. Retrieved from
http://www.ml.cmu.edu/current_students/DAP_chau.pdf
Chau, D. H., Pandit, S., & Faloutsos, C. (2006, September 18–22). Detecting fraudulent personalities
in networks of online auctioneers. Proceedings of the European conference on machine learning
(ECM) and principles and practice of knowledge discovery in databases (PKDD) 2006, Berlin,
pp. 103–114.
Chen, H., Chung, W., Xu, J. J., Wang, G., Qin, Y., & Chau, M. (2004). Crime data mining. IEEE
Computer, 37(4), 50–56.
Chen, H., Schroeder, J., Hauck, R., Ridgeway, L., Atabakhsh, H., Gupta, H., Boarman, C.,
Rasmussen, K., & Clements, A. (2003). COPLINK Connect: Information and knowledge man-
agement for law enforcement. Decision Support Systems, 34(3), 271–285.
Chen, H., Zeng, D., Atabakhsh, H., Wyzga, W., & Schroeder, J. (2003). COPLINK managing law
enforcement data and knowledge. Communications of the ACM, 46(1), 28–34.
Console, L., & Torasso, P. (1991). A spectrum of logical definitions of model-based diagnosis.
Computational Intelligence, 7(3), 133–141.
Cortes, C., & Vapnik, V. (1995). Support-vector networks. Machine Learning, 20, 273–297.
Coull, S., & Szymanski, B. K. (2008). Sequence alignment for masquerade detection. Computational
Statistics and Data Analysis, 52(8), 4116–4131.
Dragoni, A. F., & Nissan, E. (2004). Salvaging the spirit of the meter-models tradition: A model of
belief revision by way of an abstract idealization of response to incoming evidence delivery
during the construction of proof in court. Applied Artificial Intelligence, 18(3/4), 277–303.
Dror, I., & Hamard, S. (2009). Cognition distributed: How cognitive technology extends our minds.
Amsterdam: Benjamins.
Elsayed, T., & Oard, D. W. (2006, July 27–28). Modeling identity in archival collections of email: A
preliminary study. Third conference on email and anti-spam, CEAS 2006, Mountain View, CA.
Fakher-Eldeen, F., Kuflik, T., Nissan, E., Puni, G., Salfati, R., Shaul, Y., & Spanioli, A. (1993).
Interpretation of imputed behaviour in ALIBI (1 to 3) and SKILL. Informatica e Diritto
(Florence), Year 19, 2nd Series, 2(1/2), 213–242.
Falkenhainer, B., & Forbus, K. (1991). Compositional modeling: Finding the right model for the job.
Artificial Intelligence, 51, 95–143.
Field, D., & Raitt, F. (1996). Evidence. Edinburgh: W. Green.
Findlay, M., & Duff, P. (Eds.). (1988). The jury under attack. London: Butterworths.
Fitts, P. M., Jones, R. E., & Milton, J. L. (1950). Eye movements of aircraft pilots during instrument-
landing approaches. Aeronautical Engineering Review, 9(2), 1–6.
Information & Communications Technology Law 33
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
0:
25
 0
3 
D
ec
em
be
r 
20
13
 
Fox, R., & Josephson, J. R. (1994). Software: PEIRCE-IGTT. In J. R. Josephson & S. G. Josephson
(Eds.), Abductive inference: Computation, philosophy, technology (pp. 215–223). Cambridge:
Cambridge University Press.
Freeman, L. C. (2007). Social network analysis. 4 vols. London: SAGE.
Fu, X., Boongoen, T., & Shen, Q. (2010). Evidence directed generation of plausible crime scenarios
with identity resolution. Applied Artificial Intelligence, 24(4), 253–276.
Gilbreth, F. B., & Gilbreth, L. M. (1917). Applied motion study. New York: Sturgis and Walton.
Goldberg, M., Hayvanovych, M., Hoonlor, A., Kelley, S., Magdon-Ismail, M., Mertsalov, K.,
Szymanski, B., & Wallace, W. (2008, May). Discovery, analysis and monitoring of hidden
social networks and their evolution. IEEE homeland security technologies conference, Boston,
MA, pp. 1–6.
Gray, G. L., & Debreceny, R. (2006). Continuous assurance using text mining. 12th world continuous
auditing & reporting symposium. Retrieved from http://raw.rutgers.edu/docs/wcars/12wcars/
Continuous_Assurance_Text_Mining.pdf
Hamill, J. T. (2006). Analysis of layered social networks (PhD dissertation). Report AFIT/DS/ENS/06
03. Graduate School of Engineering and Management, Air Force Institute of Technology (Air
University). Retrieved from http://www.afit.edu/en/docs/ENS/dissertations/Hamill.pdf and
http://www.au.af.mil/au/awc/awcgate/afit/hamill_layered_social_networks.pdf
Harper, W. R., & Harris, D. H. (1975). The application of link analysis to police intelligence. Human
Factors, 17(2), 157–164.
Hastie, R. (Ed.). (1993). Inside the juror: The psychology of juror decision making (Cambridge Series
on Judgment and Decision Making). Cambridge: Cambridge University Press, 1993 (hard cover),
1994 (paperback).
Hastie, R., Penrod, S. D., & Pennington, N. (1983). Inside the jury. Cambridge, MA: Harvard
University Press.
Hauck, R. V., Atabakhsh, H., Ongvasith, P., Gupta, H., & Chen, H. (2002). COPLINK concept space:
An application for criminal intelligence analysis. IEEE Computer, 35(3), 30–37.
Idika, N., & Mathur, A P. (2007). A survey of malware detection techniques. Technical report.
Department of Computer Science. West Lafayette, IN: Purdue University.
Kadane, J., & Schum, D. (1996). A probabilistic analysis of the Sacco and Vanzetti evidence.
New York: John Wiley.
Kaptein, H., Prakken, H., & Verheij, B. (Eds.). (2009). Legal evidence and proof: Statistics, stories,
logic. Applied Legal Philosophy Series. Surrey: Ashgate.
Kephart, J., & Arnold, W. (1994). Automatic extraction of computer virus signatures. Proceedings of
the 4th Virus Bulletin International Conference (pp. 178–184). Abingdon, England.
Keppens, J. (2007, June 4–8). Towards qualitative approaches to Bayesian evidential reasoning.
Proceedings of the 11th International Conference on Artificial Intelligence and Law, Stanford,
California, 2007. ICAIL ’07 (pp. 17–25). New York: ACM.
Keppens, J. (2009). Conceptions of vagueness in subjective probability for evidential reasoning.
Proceedings of the 22nd Annual Conference on Legal Knowledge and Information Systems
(JURIX 2009) (pp. 89–99). Rotterdam, The Netherlands.
Keppens, J., & Schafer, B. (2003a). Using the box to think outside it: Creative skepticism and com-
puter decision support in criminal investigations. Proceedings of the IVR 21st World Congress
special workshop on artificial intelligence in the law: Creativity in legal problem solving,
Lund, Sweden. Internationale Vereinigung für Rechts- und Sozialphilosophie (IVR) =
International Association for Philosophy of Law and Social Philosophy (http://www.ivr2003.
net/). Retrieved from http://www.meijigakuin.ac.jp/∼yoshino/documents/ivr2003/keppens-
schafer.pdf
Keppens, J., & Schafer, B. (2003b). Assumption based Peg unification for crime scenario modelling.
In D. Bourcier (Ed.), Legal knowledge and information systems: JURIX 2003 – The sixteenth
annual conference, held at the University of Utrecht, The Netherlands. Amsterdam: IOS Press.
Retrieved from http://www.jurix.nl/pdf/j05-07.pdf
Keppens, J., & Schafer, B. (2004). “Murdered by persons unknown” – Speculative reasoning in law
and logic. In T .F. Gordon (Ed.), Legal knowledge and information systems. Jurix 2004: The
seventeenth annual conference (pp. 109–118). Amsterdam: IOS Press.
Keppens, J., & Schafer, B. (2005). Assumption based Peg unification for crime scenario modelling. In
M.-F. Moens & P. Spyns (Eds.), Proceedings of the 2005 Conference on Legal Knowledge and
34 E. Nissan
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
0:
25
 0
3 
D
ec
em
be
r 
20
13
 
Information Systems; JURIX 2005: The eighteenth annual conference, Frontiers in Artificial
Intelligence and Applications (Vol. 134, pp. 49–58). Amsterdam: IOS Press.
Keppens, J., & Schafer, B. (2006). Knowledge based crime scenario modelling. Expert Systems with
Applications, 30(2), 203–222.
Keppens, J., & Shen, Q. (2001). On compositional modelling. Knowledge Engineering Review, 16(2),
157–200.
Keppens, J., & Shen, Q. (2004). Compositional model repositories via dynamic constraint satisfaction
with order-of-magnitude preferences. Journal of Artificial Intelligence Research, 21, 499–550.
Keppens, J., Shen, Q., & Lee, M. (2005, May 18–20). Compositional Bayesian modelling and its
application to decision support in crime investigation. Proceedings of the 19th International
Workshop on Qualitative Reasoning (QR2005) (pp. 138–148). Graz University of Technology,
Graz, Austria.
Keppens, J., Shen, Q., & Price, C. (2011, August). Compositional Bayesian modelling for compu-
tation of evidence collection strategies. Applied Intelligence, 35(1), 134–161.
Keppens, J., Shen, Q., & Shafer, B. (2005). Probabilistic abductive computation of evidence collec-
tion strategies in crime investigation. Proceedings of the 10th International Conference on
Artificial Intelligence and Law (ICAIL 2005), Bologna, Italy (pp. 215–224). New York: ACM
Press.
Keppens, J., & Zeleznikow, J. (2002, November 6–8). On the role of model-based reasoning in
decision support in crime investigation. The IASTED International Conference on Law and
Technology (pp. 77–83). LawTech 2002, Cambridge, MA, USA. Anaheim: ACTA Press.
Keppens, J., & Zeleznikow, J. (2003, June 24–28). A model based reasoning approach for generating
plausible crime scene scenarios from evidence. In G. Sartor (Ed.), Proceedings of the ninth inter-
national conference on artificial intelligence and law (ICAIL 2003), Edinburgh, Scotland
(pp. 51–59). New York: ACM Press.
Kou, Y., Lu, C. T., Sirwongwattana, S., & Huang, Y. P. (2004). Survey of fraud detection techniques.
Proceedings of the 2004 International Conference on Networking, Sensing, and Control
(pp. 749–754). Taipei, Taiwan.
Kuflik, Ts., Nissan, E., & Puni, G. (1989, September). Finding excuses with ALIBI: Alternative plans
that are deontically more defensible. Proceedings of International Symposium on
Communication, Meaning and Knowledge vs. Information Technology, Lisbon. Also in
Computers and Artificial Intelligence, 10(4), 1991, 297–325, and in J. Lopes Alves (Ed.),
Information Technology & Society: Theory, Uses, Impacts, APDC & SPF, Lisbon, 1992,
pp. 484–510.
Leary, R. (2012). FLINTS, a tool for police investigation and intelligence analysis. Ch. 7 in Vol. 2 of
Nissan E. (2012). Computer applications for handling legal evidence, police investigation, and
case argumentation. Dordrecht: Springer.
MacCrimmon, M., & Tillers, P. (Eds.). (2002). The dynamics of judicial proof: Computation, logic,
and common sense. Studies in Fuzziness and Soft Computing series, Vol. 94. Heidelberg:
Physica-Verlag (of Springer).
Marineau, R. F. (1989). Jacob Levy Moreno, 1889–1974: Father of psychodrama, sociometry, and
group psychotherapy. London: Routledge.
Martino, A. A., & Nissan, E. (Eds.). (2001). Formal approaches to legal evidence. Artificial
Intelligence and Law, 9(2/3), 85–224.
McCabe, S. (1988). Is jury research dead? In M. Findlay & P. Duff (Eds.), The jury under attack
(pp. 27–39). London: Butterworths.
Mena, J. (2003). Investigative data mining for security and criminal detection. Boston, MA:
Butterworth.
Moreno, J. L. (1953).Who shall survive: Foundations of sociometry, group psychotherapy, and socio-
drama. Boston, MA: Beacon House. Originally published in 1934 and later in 1953 and 1978.
Newman, M. E. (2010). Networks: An introduction. Oxford: Oxford University Press.
Nissan, E. (2012). Computer applications for handling legal evidence, police investigation, and case
argumentation. 2 vols. Dordrecht: Springer.
Nissan, E., & Martino, A. A. (Eds.). (2001). Software, formal models, and artificial intelligence for
legal evidence. Special issue of Computing and Informatics, 20(6), 509–656.
Nissan, E., &Martino, A. A. (Eds.). (2003). Building blocks for an artificial intelligence framework in
the field of legal evidence. Special issue (two parts), Cybernetics and Systems, 34(4/5), 233–411,
34(6/7), 413–583.
Information & Communications Technology Law 35
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
0:
25
 0
3 
D
ec
em
be
r 
20
13
 
Nissan, E., & Martino, A. A. (Eds.). (2004). The construction of judicial proof: A challenge for arti-
ficial intelligence modelling. Special issue, Applied Artificial Intelligence, 18(3/4), 183–393.
Oatley, G., Zeleznikow, J., & Ewart, B. (2004). Matching and predicting crimes. In A. Macintosh, R.
Ellis, & T. Allen (Eds.), Applications and innovations in intelligent systems XII. Proceedings of
AI2004, The twenty-fourth SGAI international conference on innovative techniques and appli-
cations of artificial intelligence. AI-2004. Queens’ College, Cambridge, UK (pp. 19–32).
Berlin: Springer-Verlag.
Ormerod, T. C., Barrett, E. C., & Taylor, P. J. (2008). Investigating sensemaking in criminal contexts.
In J. M. Schraagen, L. G. Militello, T. C. Ormerod, & R. Lipshitz (Eds.), Naturalistic decision
making and macrocognition (pp. 81–102). Surrey: Ashgate.
Pandit, S., Chau, D. H., Wang, S., & Faloutsos, C. (2007). NetProbe: A fast and scalable system for
fraud detection in online auction networks. InWWW 2007: Proceedings of the 16th International
Conference on World Wide Web, Banff, Alberta, Canada, Track: Data Mining, Session: Mining in
Social Networks (pp. 201–210). New York: ACM.
Pardo, M. S. (2005). The field of evidence and the field of knowledge. Law and Philosophy, 24,
321–391.
Pennington, N., & Hastie, R. (1981). Juror decision-making models: The generalization gap.
Psychological Bulletin, 89, 146–287.
Pennington, N., & Hastie, R. (1986). Evidence evaluation in complex decision making. Journal of
Personality and Social Psychology, 51, 242–258.
Pennington, N., & Hastie, R. (1988). Explanation-based decision making: Effects of memory structure
on judgment. Journal of Experimental Psychology: Learning, Memory and Cognition, 14,
521–533.
Pennington, N., & Hastie, R. (1992). Explaining the evidence: Tests of the story model for juror
decision making. Journal of Personality and Social Psychology, 62, 189–206.
Pennington, N., & Hastie, R. (1993). The story model for juror decision making. In R. Hastie (Ed.),
Inside the juror: The psychology of juror decision making (pp. 192–221). Cambridge: Cambridge
University Press.
Phua, C., Lee, V., Smith-Miles, K., & Gayler, R. (2005). A comprehensive survey of data-mining-
based fraud detection research. Clayton School of Information Technology, Monash
University, Clayton, Victoria, Australia. Retrieved from http://clifton.phua.googlepages.com/
Prakken, H., Reed, C., & Walton, D. N. (2003, June 24–28). Argumentation schemes and generalis-
ations in reasoning about evidence. In G. Sartor (Ed.), Proceedings of the Ninth International
Conference on Artificial Intelligence and Law (ICAIL 2003), Edinburgh, Scotland (pp. 32–41).
New York: ACM Press.
Ribaux, O., &Margot, P. (1999). Inference structures for crime analysis and intelligence: The example
of burglary using forensic science data. Forensic Science International, 100, 193–210.
Rumelhart, D. E. (1980a). Schemata: The building blocks of cognition. In R. J. Spiro, B. C. Bruce, &
W. F. Brewer (Eds.), Theoretical issues in reading comprehension (pp. 38–58). Hillsdale, NJ:
Erlbaum.
Rumelhart, D. E. (1980b). On evaluating story grammars. Cognitive Science, 4, 313–316.
van der Schoor, J. (2004). Brains voor de recherche [In Dutch]. Justitiële Verkenningen, 30, 96–99.
Schraagen, J. M., & Leijenhorst, H. (2001). Searching for evidence: Knowledge and search strategies
used by forensic scientists. In E. Salas & G. Klein (Eds.), Linking expertise and naturalistic
decision making (pp. 263–274). Mahwah, NJ: LEA.
Schroeder, J., Xu, J., Chen, H., & Chau, M. (2007). Automated criminal link analysis based on
domain knowledge. Journal of the American Society for Information Science and Technology,
58(6), 842–855.
Schultz, M., Eskin, E., Zadok, E., & Stolfo, S. (2001, May 14–16). Data mining methods for detection
of new malicious executables. 2001 IEEE Symposium on Security and Privacy (pp. 38–49).
Oakland, California, USA. IEEE Computer Society.
Schum, D. (2001). Evidence marshaling for imaginative fact investigation. Artificial Intelligence and
Law, 9(2/3), 165–188.
Schum, D., & Tillers, P. (1989).Marshalling evidence throughout the process of fact investigation: A
simulation (Report Nos. 89-01 through 89-04, supported by NSF Grant No. SES 8704377).
New York: Cardozo School of Law.
36 E. Nissan
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
0:
25
 0
3 
D
ec
em
be
r 
20
13
 
Schum, D., & Tillers, P. (1990a). A technical note on Computer-assisted Wigmorean argument struc-
turing (Report No. 90-01 (Jan. 15, 1990), supported by NSF Grant No. SES 8704377). New York:
Cardozo School of Law.
Schum, D., & Tillers, P. (1990b).Marshalling thought and evidence about witness credibility (Report
(March 15, 1990), supported by NSF Grants Nos. SES 8704377 and 9007693). New York:
Cardozo School of Law.
Schum, D., & Tillers, P. (1991). Marshalling evidence for choice and inference in litigation. Cardozo
Law Review, 13, 657–704. Also Report 91-03 (March 18, 1991), supported by NSF Grant Nos.
SES 8704377 and 9007693. New York: Cardozo School of Law.
Schum, D. A. (1986). Probability and the processes of discovery, proof, and choice. Boston University
Law Review, 66, 825–876.
Schum, D. A. (1987). Evidence and inference for the intelligence analyst. 2 vols. Lanham, MD:
University Press of America.
Schum, D. A. (1989). Knowledge, credibility, and probability. Journal of Behavioural Decision
Making, 2, 39–62.
Schum, D. A. (1993). Argument structuring and evidence evaluation. In R. Hastie (Ed.), Inside the
juror: The psychology of juror decision making (pp. 175–191). Cambridge: Cambridge
University Press.
Schum, D. A. (1994). The evidential foundations of probabilistic reasoning (Wiley Series in Systems
Engineering). New York: John Wiley. Reprinted, Evanston, IL: Northwestern University Press,
2001.
Schum, D. A., & Martin, A. W. (1982). Formal and empirical research on cascaded inference in jur-
isprudence. Law and Society Review, 17, 105–151.
Snow, P., & Belis, M. (2002). Structured deliberation for dynamic uncertain inference. In
M. MacCrimmon & P. Tillers (Eds.), The dynamics of judicial proof: Computation, logic, and
common sense. Studies in Fuzziness and Soft Computing (Vol. 94, pp. 397–416). Heidelberg:
Physica-Verlag.
Steinwart, I., & Christmann, A. (2008). Support vector machines. New York: Springer-Verlag.
Stranieri, A., & Zeleznikow, J. (2005). Knowledge discovery from legal databases. Springer Law and
Philosophy Library, Vol. 69. Dordrecht: Springer.
Szymanski, B., & Zhang, Y. (2004, June). Recursive data mining for masquerade detection and author
identification. Proceedings of the fifth IEEE System, Man and Cybernetics Information Assurance
(SMCIA) Workshop (pp. 424–431). West Point, NY.
Tesauro, G., Kephart, J., & Sorkin, G. (1996). Neural networks for computer virus recognition. IEEE
Expert, 11(4), 5–6.
Thagard, P. (1989). Explanatory coherence. Behavioural and Brain Sciences, 12(3), 435–467.
Commentaries and riposte up to p. 502.
Thagard, P. (2000a). Coherence in thought and action. Cambridge, MA: The MIT Press.
Thagard, P. (2000b). Probabilistic networks and explanatory coherence. Cognitive Science Quarterly,
1, 91–114.
Thagard, P. (2004). Causal inference in legal decision making: Explanatory coherence vs. Bayesian
networks. Applied Artificial Intelligence, 18(3/4), 231–249.
Thagard, P. (2005). Testimony, credibility and explanatory coherence. Erkenntnis, 63, 295–316.
Toland, J., & Rees, B. (2005). Applying case-based reasoning to law enforcement. International
Association of Law Enforcement Intelligence Analysts Journal, 15(1), 106–125.
Twining, W. (1997). Freedom of proof and the reform of criminal evidence. In E. Harnon & A. Stein
(Eds.), Rights of the accused, crime control and protection of victims, special volume of the Israel
Law Review, 31(1–3), pp. 439–463.
de Vel, O., Anderson, A., Corney, M., & Mohay, G. (2001). Mining E-mail content for author identi-
fication forensics. ACM SIGMOD Record, 30(4), 55–64.
Wagenaar, W. A., van Koppen, P. J., & Crombag, H. F. M. (1993). Anchored narratives: The psychol-
ogy of criminal evidence. Hemel Hempstead, England: Harvester Wheatsheaf and St. Martin’s
Press.
Walton, D., & Schafer, B. (2006). Arthur, George and the mystery of the missing motive: Towards a
theory of evidentiary reasoning about motives. International Commentary on Evidence, 4(2),
1–47.
Weatherford, M. (2002). Mining for fraud. IEEE Intelligent Systems, 17, 4–6.
Wigmore, J. H. (1913). The problem of proof. Illinois Law Review, 8(2), 77–103.
Information & Communications Technology Law 37
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
0:
25
 0
3 
D
ec
em
be
r 
20
13
 
Wigmore, J. H. (1937). The science of judicial proof as given by logic, psychology, and general
experience, and illustrated judicial trials (3rd ed.). Boston, MA: Little, Brown & Co.
Previously: The principles of judicial proof; or, the process of proof as given by logic, psychology,
and general experience, and illustrated judicial trials. Boston, MA, 1931, 1934, 2nd ed.; The
principles of judicial proof: As given by logic, psychology, and general experience, and illus-
trated judicial trials. Boston, MA, 1913, 1st ed.
Wilson, E. O. (1998). Consilience: The unity of knowledge. London: Little, Brown & Knopf (distrib.
Random House).
Wilson, G., & Banzhaf, W. (2009, May 18–21). Discovery of email communication networks from the
Enron corpus with a genetic algorithm using social network analysis. Proceedings of the Eleventh
Conference on Evolutionary Computation (pp. 3256–3263). Trondheim, Norway.
Wooldridge, M., & van der Hoek, W. (2005). On obligations and normative ability: Towards a logical
analysis of the social contract. Journal of Applied Logic, 3, 396–420.
Xiang, Y., Chau, M., Atabakhsh, H., & Chen, H. (2005). Visualizing criminal relationships:
Comparisons of a hyperbolic tree and a hierarchical list. Decision Support System, 41, 69–83.
Xu, J. J., & Chen, H. (2004). Fighting organized crimes: Using shortest-path algorithms to identify
associations in criminal networks. Decision Support Systems, 38, 473–487.
38 E. Nissan
D
ow
nl
oa
de
d 
by
 [
U
ni
ve
rs
ity
 o
f 
A
eg
ea
n]
 a
t 0
0:
25
 0
3 
D
ec
em
be
r 
20
13
 
