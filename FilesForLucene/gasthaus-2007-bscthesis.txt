Institute of Cognitive Science
Bachelor‚Äôs Thesis
Prototype-Based Relevance Learning
for Genre Classification
Jan A. Gasthaus
jgasthau@uni-osnabrueck.de
September 11th, 2007
First supervisor: Prof. Dr. Stefan Evert
Second supervisor: Dr. Martin Lauer

Summary
In this bachelor‚Äôs thesis several prototype-based supervised learning algorithms from the
Learning Vector Quantization (LVQ) family are evaluated with respect to their suitability for
text classification tasks in computational linguistics. The algorithms under investigation are LVQ
[Kohonen, 1986], GLVQ [Sato and Yamada, 1996], and SNG [Hammer et al., 2005], as well as
their extensions to relevance learning GRLVQ [Hammer and Villmann, 2002] and SRNG [Hammer
et al., 2005]. Genre classification in the British National Corpus is used as the benchmark text
classification problem.
The algorithms are evaluated in terms of performance on three distinct genre classification
tasks, each combined with two different sets of features. The performance is analyzed with respect
to different parameter settings and model complexities, and the influences of the parameters on
the classification accuaracy as well as the learning behavior are examined. The performance is
also compared to that of the well known support vector machine (SVM) classifier. In addition,
a qualitative analysis of the additional information provided by these algorithms (relevance
information, prototypes) is performed.
The algorithms are found to achieve high accuracies comparable to those achieved by the
SVM classifier on all classification tasks and data sets.

Contents
1 Introduction 1
1.1 Objectives and General Outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
1.2 Classification Problems in Computational Linguistics . . . . . . . . . . . . . . . . 1
1.3 Genre Classification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
1.4 Prototype-Based Relevance Learning . . . . . . . . . . . . . . . . . . . . . . . . . 3
2 Prototype-Based Relevance Learning 5
2.1 The Learning Task . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
2.2 Prototype-Based Classifiers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
2.2.1 Codebook-Based Classifiers . . . . . . . . . . . . . . . . . . . . . . . . . . 6
2.2.2 Structure of the Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . 8
2.2.3 Learning Vector Quantization (LVQ) . . . . . . . . . . . . . . . . . . . . . 8
2.2.4 Generalized Learning Vector Quantization (GLVQ) . . . . . . . . . . . . . 9
2.2.5 Supervised Neural Gas (SNG) . . . . . . . . . . . . . . . . . . . . . . . . . 11
2.3 Relevance Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
2.3.1 Generalized Relevance Learning Vector Quantization (GRLVQ) . . . . . . 12
2.3.2 Supervised Relevance Neural Gas (SRNG) . . . . . . . . . . . . . . . . . . 13
2.4 Practical Considerations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
2.4.1 libgrlvq ‚Äì Java Software Library . . . . . . . . . . . . . . . . . . . . . . . 13
2.4.2 Time and Space Complexity . . . . . . . . . . . . . . . . . . . . . . . . . . 14
2.4.3 Prototype Initialization . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
2.4.4 Learning Rates and Rate Decay . . . . . . . . . . . . . . . . . . . . . . . . 15
2.4.5 Stopping Criterion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
3 Classification Tasks / Data Sets 17
3.1 Categorizations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
3.1.1 Genre . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
3.1.2 Santini . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
3.1.3 Gender . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
3.2 Data Sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
3.2.1 High-Level Data Set . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
3.2.2 Low-Level POS Data Set . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
3.2.3 Data Postprocessing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
v
4 Empirical Evaluation 25
4.1 Quantitative Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
4.1.1 Update Rule Comparison and Parameter Evaluation . . . . . . . . . . . . 25
4.1.2 Performance by Update Rule and Data Set . . . . . . . . . . . . . . . . . 29
4.1.3 Learning Curve Comparison . . . . . . . . . . . . . . . . . . . . . . . . . . 33
4.1.4 Relevances for Feature Selection . . . . . . . . . . . . . . . . . . . . . . . 36
4.2 Qualitative Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
4.2.1 Clusters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
4.2.2 Relevances . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
5 Conclusions 45
5.1 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
5.2 Outlook and Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
A Data Set Descriptions and Statistics 51
B Results: Additional Figures 57
C libgrlvq README 63
vi
Chapter 1
Introduction
1.1 Objectives and General Outline
This bachelor‚Äôs thesis has two main objectives: The first objective is to evaluate several proto-
type-based supervised learning algorithms from the Learning Vector Quantization (LVQ) family
with respect to their suitability for text classification tasks in computational linguistics. Not only
the performance in terms of classification accuracy is to be assessed, but also the usefulness of the
additional information these algorithms provide (relevance information, interpretable prototypes)
for further linguistic investigation.
The second objective is to provide a firm basis for further experiments using these algorithms
by providing a flexible implementation as a Java library, and by exploring the influence of the
algorithms‚Äô parameters on classification performance and learning behavior.
The rest of this thesis is organized as follows: This chapter briefly introduces the main concepts,
namely classification problems in computational linguistics in general and genre classification in
particular, as well as the general ideas behind prototype-based learning. Chapter 2 is devoted
to the discussion of the learning algorithms. The general principles underlying the algorithms
are discussed first, followed by a review of the learning rules within a common mathematical
framework and a discussion of issues related to the practical application of the algorithms.
Chapter 3 describes the classification tasks (Section 3.1), and discusses the features that were
used as input to the algorithms (Section 3.2). The experiments that were conducted, as well as
their results are presented and discussed in Chapter 4, followed by a summary, some concluding
remarks and an outlook on possible future work in Chapter 5.
1.2 Classification Problems in Computational Linguistics
Many tasks in computational linguistics can be thought of as classification problems, i.e. problems
in which a label from a finite set of labels should be assigned to each entity from a (possibly
infinite) set of entities. The most obvious examples are part-of-speech tagging (where the entities
are words in context and the labels are part of speech tags), word-sense disambiguation (where the
entities are contexts in which a word occurs and the labels are word senses) and text classification
(where the entities are documents and the labels are categories), but there are many more
1
2 CHAPTER 1. INTRODUCTION
problems from all levels of linguistic analysis that can be cast in this way. If each entity is to
be assigned exactly one label1, the classification can be described formally as a total function
f : X ‚Üí Y mapping entities x ‚àà X to their labels y ‚àà Y. Solving classification problems thus
amounts to specifying the assignment function f in some way. One possibility to accomplish
this is to specify the function ‚Äúby hand‚Äù, e.g. by writing simple if-then rules ‚Äì this was the
approach employed by early taggers and text classification systems. However, the real appeal
of phrasing a problem as a classification problem lies in the fact that it allows methods for the
automatic induction of f from a set of labeled training examples to be applied. A vast variety of
such methods has been developed by research in machine learning (ML) over the last decades,
and many of these methods have been applied very successfully to problems in computational
linguistics. In this thesis one specific classification problem, namely genre classification, and one
specific class of such learning methods, which can be referred to as prototype-based (relevance)
learning, will be considered.
1.3 Genre Classification
The term genre can be defined as
[. . . ] a category assigned on the basis of external criteria such as intended
audience, purpose, and activity type, that is, it refers to a conventional, culturally
recognized grouping of texts based on properties other than lexical or grammatical
(co-)occurrence features [. . . ] [Lee, 2001, p. 38]
While according to Lee [2001] there is some controversy about the precise meaning of this
term in contrast to related terms such as register and style, it seems to be generally agreed upon
that both genre and register refer to groupings of text with respect to external, non-linguistic
criteria.2
Many studies have been carried out (most prominently by Douglas Biber [1988, 1994]) to
confirm the intuition that language use differs between genres, e.g. that newspaper articles differ
in their use of certain syntactic constructions from novels or texts found in academic journals.
Genre classification can be described as the task of determining the genre of a text based on
only the text itself (or certain features extracted from it). There are several contexts in which
genre classification can be useful: In the endeavor of using the web as a large linguistic corpus,
an automatic system for genre classification can help build register diversified corpora, which is
desirable for several reasons (see [Biber, 1995]). A summary of the work done in this direction can
be found in Santini [2007]. Genre classification can also be used for model selection to improve
the performance of statistical parsers and other methods based on language models. However, in
this thesis, genre classification will be mainly viewed as a linguistic benchmark problem for the
learning algorithms.
1This is not a strong restriction as classification tasks where some other number of labels should be assigned
can be decomposed into several single-label tasks.
2While some distinction can be made between genre and register this is not necessary for the discussion at hand,
where both will be ‚Äúoperationalized‚Äù to mean ‚Äúgroups of texts collected and compiled for corpora or corpus-based
studies‚Äù[Lee, 2001].
Prototype-Based Relevance Learning
for Genre Classification
1.4. PROTOTYPE-BASED RELEVANCE LEARNING 3
1.4 Prototype-Based Relevance Learning
The term prototype-based (relevance) learning is used here to collectively refer to several extensions
of the Learning Vector Quantization (LVQ) algorithm [Kohonen, 1986]. In particular, these
are Generalized LVQ (GLVQ) [Sato and Yamada, 1996, Hammer and Villmann, 2002], and
Supervised Neural Gas (SNG) [Hammer et al., 2005], as well as their extensions to relevance
learning GRLVQ [Hammer and Villmann, 2002] and SRNG [Hammer et al., 2005] (where ‚ÄúR‚Äù
stands for ‚Äúrelevance‚Äù in both cases). All of these can be referred to as prototype-based learning
algorithms because of their common operation scheme, which can be roughly characterized as
follows: The training data (usually vectors in Rn)3 is used to construct ‚Äúprototypical examples‚Äù
for each class in the data set. To determine the class of a new instance, the instance is compared
to all prototypes and then assigned the class of the prototype that is most similar to it (according
to some similarity/distance measure). Relevance learning refers to the ability of the classifiers to
determine the importance (for classification) of certain features in the input during training, and
to adapt the learning process accordingly.
GLVQ is an extension of LVQ with more stable learning behavior which can be interpreted
as gradient descent on sensible error function. GRLVQ extends this algorithm with relevance
learning, by also updating the weights of an adaptive metric according to the gradient of the error
function. SRNG analogously extends the supervised variant of the Neural Gas (NG) algorithm
[Martinetz et al., 1993] (which incorporates data-driven ‚Äúneighborhood cooperation‚Äù similar to
the well-known self-organizing map [Kohonen, 1995]) in a similar way.
The main advantage of these methods over other commonly used classifiers such as support
vector machines for applications in computational linguistics is that their operation scheme has a
very intuitive appeal, and that their internal structure can easily be examined and lends itself to
further interpretation. As the prototypes are vectors in the same space as the training examples
(usually Rn), they can be interpreted directly, thus making the classification operation very
transparent. Furthermore, the prototypes can be used group the training data into clusters (by
assigning each document to the nearest prototype) which can be examined and compared to gain
further insights into the data. The determined relevances might not only improve classification
and generalization ability of the classifier, but can also be interpreted directly to gain information
about the discriminative ability of the features.
Additionally, as the methods operate iteratively, they can be used with a large number
of training examples (if available) and can be updated online (i.e. during the operation as a
classifier). These methods have been shown to perform well on several benchmark problems ‚Äì
even with very sparse models.
3Numeric features thus have to be extracted from the linguistic entities beforehand, see Section 3.2.
Prototype-Based Relevance Learning
for Genre Classification

Chapter 2
Prototype-Based Relevance Learning
This chapter describes the prototype-based learning algorithms in a common mathematical
framework. The mathematical notation used is summarized in Table 2.1.
X The input space of the classifier; X ‚äÜ Rn
Y The output of the classifer; a finite set of classes {c1, . . . , cC}
f The (unknown) total mapping of inputs to correct outputs; f : X ‚Üí Y
fÃÇ The approximation of f induced by the classifier; fÃÇ : X ‚Üí Y
Xtrain A finite subset of the input space for which the correct output is known;
{x1, . . . , xN} = Xtrain ‚äÇ X
ftrain The restriction of f to Xtrain; ftrain : Xtrain ‚Üí Y
xi, yi The i-th training input xi = (Œæi1, . . . , Œæ
i
n) and its corresponding output yi = ftrain(xi)
P A finite set of prototypes {p1, . . . , pP } ‚äÇ Rn
œÄ A function assigning a class to each prototype; œÄ : P ‚Üí Y
pj A particular prototype (depending on the context)
p+, p‚àí Nearest correct (p+) and nearest incorrect (p‚àí) prototype
d A distance measure (metric) defined on the input space; d : X √ó X ‚Üí R
d2 The squared euclidean metric d2((Œæi1, . . . , Œæ
i
n), (Œæ
j
1, . . . , Œæ
j
n)) =
‚àën
k=1(Œæ
i
k ‚àí Œæ
j
k)
2
dl
The weighted squared euclidean metric dl((Œæi1, . . . , Œæ
i
n), (Œæ
j
1, . . . , Œæ
j
n))
=
‚àën
k=1 Œªk(Œæ
i
k ‚àí Œæ
j
k)
2 for vector l = (Œª1, . . . , Œªn) of weights.
Table 2.1: Summary of the notation used in this chapter.
2.1 The Learning Task
As mentioned in the introduction, a classification problem can be described by means of a total
function f : X ‚Üí Y, where X is a set of entities and Y = {c1, . . . , cC} is a finite set of possible
5
6 CHAPTER 2. PROTOTYPE-BASED RELEVANCE LEARNING
classes. In other words, the function f maps each entity to the class it belongs to.
For most practical applications, however, this mapping is unknown and cannot easily be
described explicitly. The task then is to approximate the correct classification for all entities,
given only a subset of the entities and their corresponding correct labels.
In the machine learning literature, this kind of task is known as a classification task, which is
a type of supervised learning. The general form of a classification task is the approximation of
an unknown total function f : X ‚Üí Y by means of another total function fÃÇ : X ‚Üí Y (which is
called the hypothesis, model, or classifier), from only a finite number of training examples, i.e.
pairs (xi, yi), where xi ‚àà Xtrain (with Xtrain = {x1, . . . , xN} ‚äÇ X ) and yi = f(xi).
Given a sequence [(xi, yi)]Ni=1 of training examples, a supervised learning algorithm aims at
inducing a hypothesis fÃÇ , such that fÃÇ(x) coincides with f(x) for all x ‚àà X , i.e. a hypothesis
that not only makes correct predictions for the training examples, but also for new unseen
examples. The ability of a classifier to make correct predictions for unseen data is known as
its generalization ability, and depends on the strategy the classifier uses to generalize from the
encountered training examples (know as the inductive bias of the classifier).
A wide variety of such algorithms have been proposed ‚Äì for an overview refer to any book
about machine learning / pattern classification (e.g. [Webb, 2002] or [Mitchell, 1997]).
2.2 Prototype-Based Classifiers
2.2.1 Codebook-Based Classifiers
A broad class of learning methods can be characterized by a simple operation scheme: During
the training phase, some finite subset of the input space P = {p1, . . . , pP } ‚äÇ X , together with a
total function œÄ : P ‚Üí Y, is stored. Taken together, P and œÄ are referred to as the codebook.1
After training, classification of a new input x is performed by the rule
fÃÇ : x 7‚Üí œÄ(pi) such that d(x, pi) is minimal (2.1)
where d : X √óX ‚Üí R is a metric defined on X . In other words, the new input is compared to all
entries in the codebook, and then the class label of the entry with the smallest distance to the
input is returned. The models fÃÇ produced by methods based on this scheme are thus completely
characterized by the codebook (P, œÄ) together with the distance measure d.
The pi together with the metric d split the input space X into P regions
Ri : {x ‚àà X | ‚àÄpj d(x, pi) ‚â§ d(x, pj)} i ‚àà {1, . . . , P} (2.2)
which are called the receptive fields of the pi.2 In mathematics, such a decomposition of a metric
space by a finite set of points, illustrated in Figure 2.1, is called a Voronoi tessellation or Voronoi
decomposition after the Russian mathematician Georgy Voronoi.
1The term codebook is sometimes also used to refer to P alone.
2Note that, by this definition, an element x ‚àà X can belong to more than one receptive field Ri if the distances
to the respective pi are equal. Such ties will rarely occur in practice and can be broken arbitrarily.
Prototype-Based Relevance Learning
for Genre Classification
2.2. PROTOTYPE-BASED CLASSIFIERS 7
x
y
1 2 3 4 5
1
2
3
4
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè‚óè ‚óè ‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
Figure 2.1: Illustration of a Voronoi decomposition of an artificial two-dimensional two-class data set by
six prototypes (shown in red).
The general classification rule given in (2.1) gives rise to a wide range of learning algorithms3
which differ in the way the codebook (P, œÄ) is constructed from the training examples [(xi, yi)]Ni=1.
One of the simplest classifiers that can be conceived based on this rule is the so called nearest
neighbor classifier [Cover and Hart, 1967]: During the training phase, this model simply stores
all training examples [(xi, yi)]Ni=1 in the codebook, i.e. pi = xi and œÄ(pi) = yi for i ‚àà {1, . . . , N}.
All computation is deferred until classification ‚Äì a behavior referred to as lazy learning.
This simple classifier has two main problems: First, while it makes only correct predictions
on the training data, its generalization ability is relatively poor, because no abstraction from the
training examples takes place. Second, the time required to classify a new instance is proportional
to the number of training examples.4
Starting from this simple classifier, there are several ways to improve its performance both
in terms of generalization ability and time complexity. One idea is to be selective about which
training examples are stored in the codebook based on some criterion; this is the strategy
employed by instance-based learning (IBL) [Aha et al., 1991].
The idea on which the classification algorithms, which are the topic of this thesis, are based,
is to not store the training examples directly, but to abstract from them a small number of
prototypical examples for each class ‚Äì hence the name prototype-based learning.
3An even broader range of learning algorithms can described if the label is allowed to be determined by some
function of the labels of the k (for some k ‚àà N) nearest codebook entries.
4In the naÃàƒ±ve implementation the time complexity of classifying a new instance is O(n√óN) if d can be computed
in O(n). This bound can be improved by storing the codebook in a more advanced data structure (with the
drawback of a higher space complexity) ‚Äì but this is beyond the scope of this overview.
Prototype-Based Relevance Learning
for Genre Classification
8 CHAPTER 2. PROTOTYPE-BASED RELEVANCE LEARNING
2.2.2 Structure of the Algorithms
The main concern of the prototype-based algorithms discussed next is thus to extract a set of
prototypes from the training examples that, when combined with the nearest neighbor classification
rule, yield a hypothesis fÃÇ that performs well for seen and unseen inputs.
All learning methods described here try to achieve this goal by iteratively improving some
initial set of prototypes P0 (keeping œÄ fixed) with respect to some objective function. The general
operation scheme of these algorithms can be described as follows:
1. Initialize the prototypes
2. For each training example (xi, yi), determine the subset of the prototypes that needs to be
updated on the basis of this example. For LVQ, only the prototype nearest to xi is updated;
GLVQ updates the nearest prototype from the class yi as well as the nearest prototype not
from this class, and SNG updates all prototypes from class yi and the nearest prototype
not from this class.
3. Update the prototypes determined in the previous step with respect to the gradient of an
error function. Roughly speaking, prototypes from the same class as the training example
are drawn towards the training example, while prototypes with a different class are pushed
away.
4. Repeat from step 2 until the prototypes converge or some other stopping criterion is met.
The methods differ only in steps 3 and 4, and in the following the term update rule will refer to
how these steps are carried out. Also, the terms iteration and epoch will be used interchangeably
to refer to steps 2 and 3 performed once for every training example.
In the following description of the update rules of LVQ, GLVQ and SNG, the derivations will
be omitted ‚Äì for these the reader is referred to the references given.
2.2.3 Learning Vector Quantization (LVQ)
The Learning Vector Quantization (LVQ) update rule first proposed by Kohonen [Kohonen,
1986, 1995] is based on the ideas of Vector Quantization (VQ) ‚Äì a classical technique from signal
processing for modeling probability distributions using prototypes, which is still widely used (e.g.
in video and audio compression) today. In fact, there are several variants of the LVQ algorithm ‚Äì
the variant described here (and used for the experiments) is the simplest one (called LVQ1 by
Kohonen). Given a training example xi, LVQ1 updates only the nearest prototype pj (i.e. the
prototype pj such that ‚àÄpi d(xi, pj) ‚â§ d(xi, pi)) according to the following update rule:
pj ‚Üê
{
pj + (xi ‚àí pj) if yi = œÄ(pj)
pj ‚àí (xi ‚àí pj) otherwise
(2.3)
where  ‚àà (0, 1) is called the learning rate.5 By this update rule, the closest prototype is pulled
in the direction of the example if it has the correct class, and is pushed away if the classes do
5Kohonen [1995] states that the learning rate should be chosen rather small (below 0.1), and that it should
decrease over time.
Prototype-Based Relevance Learning
for Genre Classification
2.2. PROTOTYPE-BASED CLASSIFIERS 9
not match in each update step. While intuitively this makes sense (and works quite well in
practice), this update rule is not well-founded mathematically and thus can only be seen as a
(well-chosen and justified) heuristic [Seo and Obermayer, 2003]. However, attempts have been
made to either justify the LVQ update rule directly [Crammer et al., 2002], or to derive some
mathematically-well founded variant of it.
2.2.4 Generalized Learning Vector Quantization (GLVQ)
Generalized Learning Vector Quantization [Sato and Yamada, 1996] refers to a class of algorithms
that try to find good prototypes by minimizing a cost function of the form
S =
N‚àë
i=1
g(¬µfÃÇ (xi)) (2.4)
where g : R ‚Üí R is some monotonically increasing function and ¬µfÃÇ : Xtrain ‚Üí R is a function
that is negative iff xi is classified correctly by the current hypothesis (fÃÇ(xi) = yi) and positive
iff xi is classified incorrectly (fÃÇ(xi) 6= yi), by stochastic gradient descent. The standard LVQ
update rule emerges from this framework by choosing g as the identity and
¬µfÃÇ (xi) =
{
1¬∑ d2(xi, pj) if yi = œÄ(pj)
‚àí1¬∑ d2(xi, pj) if yi 6= œÄ(pj)
(2.5)
where d2 is the squared Euclidean distance and pj is the nearest prototype (according to d2)
[Sato and Yamada, 1996, Hammer and Villmann, 2002, Hammer et al., 2005]. Choosing ¬µfÃÇ in
this way violates the restrictions placed on it, and thus may lead to instable behavior of the
gradient descent procedure.
Sato and Yamada [1996] propose an alternative6, where g is chosen as the sigmoid function
sgd(x) = (1 + e‚àíx)‚àí1, and ¬µfÃÇ is chosen as
¬µfÃÇ (xi) =
d(xi, p+)‚àí d(xi, p‚àí)
d(xi, p+) + d(xi, p‚àí)
. (2.6)
where p+ is the closest correct prototype7, and p‚àí is the closest incorrect prototype. The shape
of this cost function for a single training example is shown in Figure 2.2. Taking the derivative of
the error function defined in this way with respect to the prototypes yields the following update
rules for gradient descent
p+ ‚Üê p+ ‚àí + ¬∑ sgd‚Ä≤(¬µfÃÇ (xi)) ¬∑
2 ¬∑ d(xi, p‚àí)
(d(xi, p+) + d(xi, p‚àí))2
¬∑ ‚àÇd(xi, p+)
‚àÇp+
(2.7)
p‚àí ‚Üê p‚àí + ‚àí ¬∑ sgd‚Ä≤(¬µfÃÇ (xi)) ¬∑
2 ¬∑ d(xi, p+)
(d(xi, p+) + d(xi, p‚àí))2
¬∑ ‚àÇd(xi, p‚àí)
‚àÇp‚àí
(2.8)
6While GLVQ is actually a whole class of algorithms, the term is usually used to refer to this particular
instantiation. The version of GLVQ presented here is the one extended to GRLVQ in [Hammer and Villmann,
2002], which differs slightly from the original version proposed in Sato and Yamada [1996] in that it does not
include the training time t in the exponent of the sigmoid function.
7The prototype such that yi = œÄ(p+) and for all other pj with œÄ(pj) = yi it holds that d(xi, p+) ‚â§ d(xi, pj).
The closest incorrect prototype is defined analogously.
Prototype-Based Relevance Learning
for Genre Classification
10 CHAPTER 2. PROTOTYPE-BASED RELEVANCE LEARNING
where +, ‚àí ‚àà (0, 1) are learning rates, sgd‚Ä≤ is the derivative of the sigmoid function given
by sgd‚Ä≤(x) = sgd(x) ¬∑ (1 ‚àí sgd(x)), and ‚àÇd(xi,p+)‚àÇp+ ,
‚àÇd(xi,p‚àí)
‚àÇp‚àí
refer to the gradient of the distance
measure with respect to the nearest correct and incorrect prototype respectively.8
2
4
6
8
10
Distance Correct
2
4
6
8
10
Distance Incorrect
0.3
0.4
0.5
0.6
0.7
Error
Figure 2.2: Plot of the error GLVQ sgd(¬µfÃÇ (xi)) for a single training example xi depending on the
distances to the nearest correct and the nearest incorrect prototype.
This update rule has been shown to have a more robust behavior than LVQ2.19, as the
prototypes do not diverge over time [Sato and Yamada, 1996]. LVQ2.1 has been subjected to
margin analysis and it has been shown that the update rule maximizes as quantity known as the
hypothesis margin, which is related to generalization ability [Crammer et al., 2002]. According to
Hammer et al. [2005], this also true for GLVQ.
However, the performance of GLVQ depends on the initialization of the prototypes, because
as a stochastic gradient descent method on a possibly multimodal function it can get stuck in
local optima [Hammer et al., 2005] ‚Äì a problem that the SNG algorithm presented next tries to
overcome.
8If d is the squared Euclidean distance, d(x,p)
‚àÇp
= 2 ¬∑ (p‚àí x)
9LVQ2.1 is a modification of LVQ1 also proposed by Kohonen [1995] that updates two prototypes ‚Äì the closest
correct and incorrect one ‚Äì in every update step.
Prototype-Based Relevance Learning
for Genre Classification
2.3. RELEVANCE LEARNING 11
2.2.5 Supervised Neural Gas (SNG)
Supervised Neural Gas (SNG) as proposed in Hammer et al. [2005] is an extension of the
unsupervised Neural Gas (NG) algorithm [Martinetz et al., 1993] to supervised learning within
the GLVQ framework.
Denote by r(xi, pj) the rank of the prototype pj among all prototypes such that œÄ(pj) = yi
(i.e. all prototypes from the same class as xi) with respect to the distance d(xi, pj) (for some
distance measure d).10 Then for some positive constant Œ≥ (called the neighborhood range), the
function hŒ≥(xi, pj) = exp(‚àír(xi, pj) ¬∑ Œ≥‚àí1) decreases from 1 to 0 as the rank increases. The cost
function minimized by SNG is given by
SSNG =
N‚àë
i=1
P‚àë
j=1
Œ¥yi,œÄ(pj)
weightÔ∏∑ Ô∏∏Ô∏∏ Ô∏∑
hŒ≥(xi, pj)
C(Œ≥, P+)
¬∑
GLVQ errorÔ∏∑ Ô∏∏Ô∏∏ Ô∏∑
sgd
(
d(xi, pj)‚àí d(xi, p‚àí)
d(xi, pj) + d(xi, p‚àí)
)
. (2.9)
where Œ¥i,j is the Kronecker delta that is 1 if i = j and 0 otherwise11, p‚àí is the closest incorrect
prototype as in GLVQ, and C(Œ≥, P+) is a normalization constant depending on Œ≥ and the number
of prototypes in the same class as xi (denoted by P+) ensuring that the weights add up to 1.
As indicated, the error term can thus be seen as a weighted sum over the GLVQ errors of all
prototypes from the correct class, where the weight decreases with the rank of the prototype.
Iteratively minimizing the cost term in (2.9) yields the following updates for all prototypes
pj such that œÄ(pj) = yi (i.e. all correct prototypes), and the closest incorrect prototype p‚àí:
pj ‚Üê pj ‚àí + ¬∑
hŒ≥(xi, pj)
C(Œ≥, P+)
¬∑ sgd‚Ä≤(¬µfÃÇ (xi)) ¬∑
2 ¬∑ d(xi, p‚àí)
(d(xi, pj) + d(xi, p‚àí))2
¬∑ ‚àÇd(xi, pj)
‚àÇpj
(2.10)
p‚àí ‚Üê p‚àí + ‚àí ¬∑
‚àë
pj
¬∑hŒ≥(xi, pj)
C(Œ≥, P+)
¬∑ sgd‚Ä≤(¬µfÃÇ (xi)) ¬∑
2 ¬∑ d(xi, pj)
(d(xi, pj) + d(xi, p‚àí))2
¬∑ ‚àÇd(xi, p‚àí)
‚àÇp‚àí
(2.11)
where ¬µfÃÇ (xi) is defined as for GLVQ (2.6), with p+ replaced by pj .
12
SNG thus mainly differs from GLVQ in that all correct prototypes are updated for each
training example, where the magnitude of the update is determined by the rank of the prototype
among all correct prototypes. Hammer et al. [2005] argue that combining NG and GLVQ in this
way avoids the dependency of GLVQ on good initial prototypes, as ‚Äú[t]he NG-dynamics aim at
spreading all prototypes with a specific class label faithfully among the respective data‚Äù while
‚Äú[t]he simultaneous GLVQ dynamics make sure that those class borders are found which yield a
good classification.‚Äù [Hammer et al., 2005, p. 26]
2.3 Relevance Learning
All codebook-based methods rely crucially on the appropriateness of the employed distance
measure d for the data. It should be clear from the above discussion that within the GLVQ
10More formally: r(xi, pj) = |{pk |œÄ(pk) = œÄ(pj) = yi ‚àß d(xi, pk) ‚â§ d(xi, pj)}|.
11The inner summation thus is really only over the prototypes of the same class as xi.
12Here,
PP
j=1 Œ¥yi,œÄ(pj) has been shortened to
P
pj
to aid readability.
Prototype-Based Relevance Learning
for Genre Classification
12 CHAPTER 2. PROTOTYPE-BASED RELEVANCE LEARNING
framework (including SNG), any such distance measure can be used, as long as it is differentiable
with respect to its arguments. The most commonly used choice for d is the (squared) euclidean
metric13, here denoted by d2
d2((Œæi1, . . . , Œæ
i
n), (Œæ
j
1, . . . , Œæ
j
n)) =
n‚àë
k=1
(Œæik ‚àí Œæ
j
k)
2 (2.12)
‚àÇd2((Œæi1, . . . , Œæ
i
n), (Œæ
j
1, . . . , Œæ
j
n))
‚àÇŒæjk
= 2 ¬∑ (Œæjk ‚àí Œæ
i
k) (2.13)
In this metric all components of the vectors thus contribute equally to the overall distance.
The idea behind relevance learning is that in a given data set certain dimensions might be
more relevant (for correct classification) than others, and that it should be possible to infer the
relevance of each dimension from the data. A simple extension of d2 that allows to assign a
weight to each dimension can be defined as
dl2((Œæ
i
1, . . . , Œæ
i
n), (Œæ
j
1, . . . , Œæ
j
n)) =
n‚àë
k=1
Œªk ¬∑ (Œæik ‚àí Œæ
j
k)
2 (2.14)
‚àÇdl2((Œæ
i
1, . . . , Œæ
i
n), (Œæ
j
1, . . . , Œæ
j
n))
‚àÇŒæjk
= 2 ¬∑ Œªk ¬∑ (Œæjk ‚àí Œæ
i
k) (2.15)
where l = (Œª1, . . . , Œªn) is a vector of weights (relevances) with ||l|| = 1 and Œªi ‚â• 0.14
Now, as has been shown by Bojer et al. [2001], Hammer and Villmann [2002], and Hammer
et al. [2005], any parameterized metric dl that is also differentiable with respect to its parameters
l, can be adapted during GLVQ/SNG training according to the gradient of the respective error
term (2.4)/(2.9) with respect to l.
2.3.1 Generalized Relevance Learning Vector Quantization (GRLVQ)
The update rule for the parameters of the distance measure in GLVQ is given by
l ‚Üê l ‚àí l ¬∑ sgd‚Ä≤(¬µfÃÇ (xi)) ¬∑
(
e+ ¬∑ ‚àÇd(xi, p+)
‚àÇp+
‚àí e‚àí ¬∑ ‚àÇd(xi, p‚àí)
‚àÇp‚àí
)
(2.16)
where p+ and p‚àí are again the closest correct and incorrect prototype respectively, ¬µfÃÇ is defined
as in (2.6), l ‚àà (0, 1) is a learning rate, and e+ and e‚àí are defined as
e+ =
2 ¬∑ d(xi, p‚àí)
(d(xi, p+) + d(xi, p‚àí))2
e‚àí =
2 ¬∑ d(xi, p+)
(d(xi, p+) + d(xi, p‚àí))2
(2.17)
The standard GLVQ update (2.8) together with the update rule for the weights of the distance
measure (2.16) is referred to Generalized Relevance Learning Vector Quantization (GRLVQ)
[Hammer and Villmann, 2002].
13As in all these methods only relative distances are important, using the squared euclidean distance instead of
the vanilla euclidean distance does not make much difference. However, it does simplify the formulas and speeds
up computations because no square roots need to be computed.
14Hammer and Villmann [2002] suggest that the constraints on the weights should be enforced by renormalization
after each update step.
Prototype-Based Relevance Learning
for Genre Classification
2.4. PRACTICAL CONSIDERATIONS 13
2.3.2 Supervised Relevance Neural Gas (SRNG)
The SNG algorithm can be extended analogously [Hammer et al., 2005], with the additional
update rule for l given by
l ‚Üê l ‚àí l ¬∑
‚àë
pj
hŒ≥(xi, pj)
C(Œ≥, P+)
¬∑ sgd‚Ä≤(¬µfÃÇ (xi)) ¬∑
(
e+ ¬∑ ‚àÇd(xi, pj)
‚àÇpj
‚àí e‚àí ¬∑ ‚àÇd(xi, p‚àí)
‚àÇp‚àí
)
(2.18)
where the terms are as defined in (2.11)15.
2.4 Practical Considerations
2.4.1 libgrlvq ‚Äì Java Software Library
This thesis is accompanied by an implementation of the algorithms presented in this chapter as a
Java software library called libgrlvq16, which was used to carry out all experiments described in
Chapter 4. In this section only the general architecture of this library will be outlined. For further
information refer to the README file of the command line interface Experimenter (reprinted as
Appendix C) and the JavaDoc documentation that comes with the software.
As at the time of writing there was no other freely available implementation of these
algorithms17, the idea behind libgrlvq was to provide a flexible framework for testing and using
prototype-based learning algorithms in general, and not just an ad-hoc implementation for this
thesis. The library was designed with modularity in mind: Most core components are described
by interfaces, such that the actual implementation can easily be changed. The core interfaces of
the library are:
DistanceMeasure This interface represents a distance measure and includes function prototypes
to calculate the distance between two vectors, or evaluate the derivative with respect to
the arguments or the parameters at some point. It currently has three implementations:
EuclideanDistance, PDistance, and WeightedDistance.
VecStore Interface of a data structure that stores vectors and their associated labels and supports
basic operations for storing and retrieving as well as operations for returning the/all closest
prototype(s) to a given vector (with the same or a different class label). Currently there is
only the ‚ÄúnaÃàƒ±ve‚Äù implementation in ArrayVecStore, which stores the vectors in an array
and supports the search operations in time linear in the number of stored vectors. However,
more elaborate data structures, such as the one proposed by Maire et al. [2004], should be
easy to plug in using this interface.
UpdateRule Given a VecStore of training examples, a VecStore of prototypes, and a Distance-
Measure, the update method of an UpdateRule performs one full update step, i.e. updates
15The relevance update rule given by Hammer et al. [2005] is incorrect (wrong sign of the update). The correct
version given in Villmann and Hammer [2003] and Villmann et al. [2006] is shown here.
16Licensed under the GPL and available from http://www.ikw.uos.de/‚àºCL/theses/gasthaus2007/
17There is, however, an implementation of the LVQ variants proposed by Kohonen available in LVQ PAK
[Kohonen et al., 1996] and in the R [R Development Core Team, 2007] package class.
Prototype-Based Relevance Learning
for Genre Classification
14 CHAPTER 2. PROTOTYPE-BASED RELEVANCE LEARNING
the prototypes according to some rule using all training examples. Currently implemented
are: LVQUpdateRule, GLVQUpdateRule, GRLVQUpdateRule, SNGUpdateRule and SRNGUp-
dateRule.
Initialization This interface encapsulates a prototype initialization method. Currently im-
plemented are FromDatapointsInitialization, KMeansInitialization, and Agglomer-
ativeClusteringInitialization (not fully functional).
The Experimenter command line interface can be used to carry out a wide range of experi-
ments by allowing most parameters to be set through options and flags. It can be run in batch
mode to facilitate grid-search experiments, and produces various output files that can be used
easily for further analyzes.
2.4.2 Time and Space Complexity
In practical applications, not only the performance of a classifier in terms of accuracy might be
important, but also its time and space requirements for building the model and classifying new
instances.
After the model of a prototype-based classifier has been built, classifying an instance amounts
to finding the nearest prototype in the codebook. If the distance between a prototype and an
instance can be computed in time linear in the number of dimensions of the input space n, and
the search is done in a naÃàƒ±ve way by comparing the instance to every prototype, this requires
O(P √ó n) steps. The space needed to store the model is simply the space needed to store the
codebook ‚Äì also O(P √ón) ‚Äì plus the space needed to store the parameters of the distance measure
(if any).18
Training the model can be quite time-intensive, however: For each of the N training examples,
the distance to each of the P prototypes has to be computed, and this process has to be repeated
until the prototypes converge, say I times. If it is again assumed that the distance between two
vectors can be computed in time linear in the number of dimensions n, then the total time needed
to train the model is O(I √ó N √ó P √ó n). While this asymptotic time complexity is the same
for all the algorithms, the hidden constant is much bigger for S(R)NG, because all prototypes
from the same class as the example have to be ranked and updated in every step. Also, the
updating and normalizing of the relevance terms makes GRLVQ and SRNG significantly slower
than GLVQ and SNG. On a data set with 65 dimensions and 2014 training examples, training
with GLVQ for 1000 epochs with 40 prototypes takes about 14 seconds, while it takes over 6
minutes with SNG, and about 9 minutes with SRNG.19 Using twice as many prototypes roughly
doubles the time needed as expected. The memory needed to store the prototypes and training
data can be neglected (less than 2 MB for this data set).
2.4.3 Prototype Initialization
All of the algorithms presented here keep the number of prototypes P and their assignment
to classes œÄ fixed during training, so these have to be decided upon beforehand. While there
18In libgrlvq the training examples are also stored in memory, but they could easily be kept on disc or in a
database as well.
19These timings were determined using an AMD Opteron CPU with 2.6 GHz.
Prototype-Based Relevance Learning
for Genre Classification
2.4. PRACTICAL CONSIDERATIONS 15
are several extensions of LVQ that adapt the number of prototypes during training (e.g. by
Qin and Suganthan [2004]), these will not be considered here. Also, in the present study (as in
[Hammer and Villmann, 2002] and [Hammer et al., 2005] where the algorithms were introduced)
the number of prototypes in each class is kept equal, and the overall number of prototypes is
considered a free parameter, i.e. no methods to determine an approximately optimal number
of prototypes per class (with respect to performance on unseen data) are employed.20 Once
the number of prototypes per class is fixed, the question arises how the prototypes should be
initialized. Two different methods to initially place the prototypes within the data were used
and compared.
The first method simply selects random training examples from the correct class as initial
prototypes. While this ensures that the prototypes are placed in the vicinity of some (at least
one) data points from the correct class, it does not guarantee that the prototypes are spread well
throughout the data, and outliers may be selected as initial values.
The second method takes random data points as initial cluster centroids, performs k-means
clustering [Manning and SchuÃàtze, 1999, pp. 515‚Äì518] for each class separately until convergence,
and then initializes the prototypes to the determined cluster centroids. The k-means algorithm
is known to converge very fast while at the same time producing clusters with small intra-cluster
distances, thus making it well-suited for quickly producing good initial prototype guesses.
There are obviously other possible ways (both simpler and more complex) to initialize the
prototypes (e.g. simply with random small values as done by Hammer et al. [2005]), but only
these two are considered here.
2.4.4 Learning Rates and Rate Decay
The learning rates are the main free parameters of the update rules. As their optimal values
depend on the data, they must be determined through experiments. Kohonen [1995] suggests that
the learning rate  for LVQ1 should be rather small (below 0.1), and the experiments described
in the papers that introduced G(R)LVQ and S(R)NG used learning rates + and ‚àí between 0.1
and 0.001, l between 0.01 and 0.0001, and Œ≥ between 100 and 0.5.
Kohonen [1995] also states that the learning rate  should decrease with the number of
iterations performed, but claims that the exact method how this is done is not crucial. Villmann
and Hammer [2003] and Hammer et al. [2005], multiply the neighborhood range Œ≥ is with 0.995
after each iteration, but all other learning rates are kept constant. Sato and Yamada [1996]
and Hammer and Villmann [2002] also keep the learning rate constant during training. In the
experiments described in Chapter 4, the learning rates + and ‚àí were either kept constant or
decayed linearly to a pre-specified target value (namely 0.01). l was set to 0 in the initial phase
of training (as suggested by Hammer and Villmann [2002]), and afterwards kept constant, and Œ≥
was always multiplied with 0.995 after each iteration.
2.4.5 Stopping Criterion
When to stop training is another important aspect that has to be considered when practically
applying the algorithms. As the update rules perform stochastic gradient descent on an error
20Some suggestions for further research in this direction are outlined in Section 5.2.
Prototype-Based Relevance Learning
for Genre Classification
16 CHAPTER 2. PROTOTYPE-BASED RELEVANCE LEARNING
function, they should reach a (local) minimum at some point and training can be stopped. A
threshold on the average change in prototype positions can thus be used as the basic stopping
criterion. However, this requires the determination of a sensible threshold value. Additionally,
if the learning rate is decayed over time, this artificially decreases the change in prototypes,
making the criterion dependent on the chosen decay method. The approach taken here (as in
the literature mentioned above) is to simply train for a fixed (large enough) number of epochs.
However, if training time is critical, this is a sub-optimal solution that can be improved (though
this is not done here, see Section 5).
Prototype-Based Relevance Learning
for Genre Classification
Chapter 3
Classification Tasks / Data Sets
Genres can be defined at different levels of granularity. For the documents1 in the British
National Corpus (BNC) [Burnard, 2007] ‚Äì a collection of roughly 4000 documents from various
sources, amounting to a total of about 100 million words of English text ‚Äì Lee [2001] has
produced a hierarchical classification consisting of super-genres such as academic prose or texts
from broadsheet national newspapers as well as more fine-grained distinctions such as academic
writings in natural science vs. social science. In total, there are 46 written and 24 spoken
genres in his classification. Additional meta-information about the texts such as author gender,
publication type and date, and so on is also available, though sometimes incomplete.
3.1 Categorizations
Categorization Characteristics
Genre 4 classes; documents per class: 505, 463, 518, 528; 3 to 15 sub-genres
in each class
Santini 10 classes; 15 documents per class; no sub-genres
Gender 2 classes; documents per class: 920 (male), 414 (female); 21 sub-
genres that occur in both classes (see Table A.2)
Table 3.1: Summary of the characteristics of the three genre categorizations.
Based on Lee‚Äôs genre classification and the available meta-information, three groupings of
the BNC documents into categories were constructed: One consisting of four relatively broad
super-genres (denoted Genre), one consisting of ten more fine-grained genres (called Santini
after a study by Santini [2004]), and one consisting of two ‚Äúgenres‚Äù defined by the gender of
1The term document is used here to refer to the text contained in the 4048 separate files of the BNC, i.e. to
the objects for which meta-information is available. These documents usually contain more than one text, e.g. all
articles from a newspaper issue or academic journal. However, since the documents are viewed as a long string of
words here and all internal structure is disregarded, the term text is used interchangeably with document.
17
18 CHAPTER 3. CLASSIFICATION TASKS / DATA SETS
the author2 (called Gender). The characteristics of these categorizations are summarized in
Table 3.1, and the details will be described in the next paragraphs. Each of these categorizations
was then combined with two different sets of features extracted for all documents in the BNC
(described in Section 3.2) to yield a total of six classification tasks with distinct characteristics.
3.1.1 Genre
For the Genre categorization, four of the super-genres proposed by Lee [2001], namely ‚ÄúAcademic
prose‚Äù, ‚ÄúFiction‚Äù, ‚ÄúNewspapers‚Äù (comprising national, regional & local, as well as tabloid
newspapers), and ‚ÄúNon-academic prose (non-fiction)‚Äù (referring to non-fiction texts written for
a general, non-university-level audience) were selected. Each of these super-genres comprises
several (sub-)genres, as shown in Table 3.2.
Academic Prose Fiction Newspaper Non-academic Prose
W:ac: 505 W:fict: 463 W:newsp: 518 W:non ac: 528
humanities arts 87 drama 2 brdsht nat:arts 51 humanities arts 110
medicine 24 poetry 30 brdsht nat:commerce 44 medicine 17
nat science 43 prose 431 brdsht nat:editorial 12 nat science 62
polit law edu 186 brdsht nat:misc 95 polit law edu 93
soc science 142 brdsht nat:report 49 soc science 123
tech engin 23 brdsht nat:science 29 tech engin 123
brdsht nat:social 36
brdsht nat:sports 24
other:arts 15
other:commerce 17
other:report 39
other:science 23
other:social 37
other:sports 9
tabloid 6
W:news script 32
Table 3.2: Breakdown of the four super-genres included in the Genre classification task by (sub-)genres.
The identifiers (e.g. W:non ac:medicine) are the ones created by Lee [2001], and the numbers are the
number of documents in each genre. Note that W:news script is also included in the ‚ÄúNewspaper‚Äù genre.
3.1.2 Santini
The Santini classification is named after a study of Santini [2004]. In this study, the task was
to distinguish ten (four spoken and six written) genres, namely ‚ÄúConversation‚Äù, ‚ÄúInterview‚Äù,
‚ÄúPublic Debate‚Äù, and ‚ÄúPlanned Speech‚Äù (spoken), and ‚ÄúAcademic Prose (Technical Engineering)‚Äù,
2Though the definition of genre given in the introduction allows viewing this distinction as genres, this is
somewhat against common usage.
Prototype-Based Relevance Learning
for Genre Classification
3.2. DATA SETS 19
‚ÄúAdvert‚Äù, ‚ÄúBiography‚Äù, ‚ÄúInstructional‚Äù, ‚ÄúPopular Lore‚Äù, and ‚ÄúReportage‚Äù (written).3 Santini
only used 15 documents per class (which is the number of documents available in the smallest
class), so in order to make the results comparable (at least to some extend), the class sizes were
also reduced to 15 by random sampling.4
3.1.3 Gender
The Gender categorization consists only of two classes: Texts written by female authors and
texts written by male authors. While these distinction is typically not associated with the term
genre (though it can be according to the broad definition given in the introduction), it is included
here to create a relatively hard two-class classification problem with a skewed class distribution
(there are 920 male-authored texts, but only 414 written by female authors; for all the other
texts the authorship is either mixed or unknown.). Additionally, the results obtained can be
compared to those of previous studies that tried to distinguish texts by author gender based
on syntactic features (e.g. [Koppel et al., 2002, Argamon et al., 2003]). While in those studies
the corpus was balanced for genre in each class, the distribution of genres between both classes
is highly uneven here, because all documents in the BNC for which author sex information is
available are used: While there is about an equal number of fiction texts written by male and
female authors in the corpus (212 and 218 documents respectively), most of the non-fiction texts
were written by men (‚âà 78%). A breakdown of the documents in each genre by author sex can
be found in Table A.2 in the appendix.
3.2 Data Sets
As mentioned in the introduction, the prototype-based learning methods discussed here operate
on vectors from Rn. The BNC documents thus have to be represented as such vectors, and the
representation has to be ‚Äúrich‚Äù enough for the learning algorithms to be able to discriminate the
classes.
Two different sets of feature were extracted for all documents in the BNC. Both are based
on the assumption that the frequencies of certain grammatical constructions differ between the
varieties of English used in the genres. The features of both data sets were extracted solely
from the word, lemma, and POS information available in the BNC; all other available structural
information was not used, and no deeper linguistic analysis (e.g. parsing) was performed. Both
data sets are not only interesting for the classification tasks presented above, but may also be
used for other linguistic investigations, for example to study variation between the texts with
unsupervised methods.
3The corresponding genre identifiers in Lee‚Äôs classification are: S:conv, S:interview, S:pub debate,
S:speech scripted, W:ac:tech engin, W:advert, W:biography, W:instructional, W:pop lore,
W:newsp brdsht nat:reportage.
4Santini [2004] does not state which exact documents were used in each class, so the documents chosen here by
random sampling most certainly differ from the ones used in her study. Which documents were used here is shown
in Table A.1 (in the appendix).
Prototype-Based Relevance Learning
for Genre Classification
20 CHAPTER 3. CLASSIFICATION TASKS / DATA SETS
3.2.1 High-Level Data Set
For the first data set, the idea was to extract syntactic features from the texts that have been
shown to differ across genres/registers in previous studies, i.e. to employ domain knowledge to
extract a relatively small set of high-quality, high-level features.
Biber [1988] set out to construct such a data set in order to use it to identify common
directions of variation across texts using factor analysis. He counted the occurrence of 65
lexical and grammatical patterns (e.g. passives, gerunds, different pronouns, etc.) in 481 texts,
which were first annotated with word classes using a rule-based POS tagger. The patterns were
described using rules similar to regular expressions over words and POS tags. A typical example
of such a rule is the one used to extract one form of ‚Äúby passives‚Äù (feature 18): BE + (ADV) +
(ADV) + VBN + by [Biber, 1988, p. 228].
The rules he used (described in detail in [Biber, 1988, Appendix II]) were taken as the basis
for the rules used to extract the features for this data set, which will consequently be referred to
as the Biber data set.
Regular expressions over sequences of word/POS pairs were used to count the relevant
patterns in the documents of the BNC.5 Most rules had to be adjusted for the different tag set
(which allowed for some improvements), and two features had to be left out (subordinator-that
deletion (60) and independent clause coordination (65)) because they could not be extracted
reliably without post-editing by hand (which was possible for Biber‚Äôs relatively small data set). A
list of features used is given in Table 3.3; for a full description and justification of these features
refer to [Biber, 1988, Appendix II]. The script used to extract these features is available together
with the resulting data set from http://www.ikw.uos.de/‚àºCL/theses/gasthaus2007/.
It should be pointed out that in addition to the occurrence frequencies of the 63 syntactic
patterns (per 1000 words of text), the data set (this one as well as Biber‚Äôs) contains two additional
features: the type/token ratio6 (43) and the average word length (44).
3.2.2 Low-Level POS Data Set
The second data set is based on relative frequency counts of sequences of part-of-speech tags
(referred to as POS n-grams). Sequences of length 1, 2, and 3 are used, which are commonly
referred to as unigrams, bigrams and trigrams respectively. It was created using almost no
domain knowledge, making it a prototypical example of a quantity approach to feature extraction
where a large number of possibly redundant, low-quality features are used.
The assumption behind using frequencies of POS sequences as features is that the texts differ
in their use of certain syntactic constructions, and that at least the local syntactic structure is
captured adequately by sequences of POS tags.7
5Some of the features (e.g. passives, complements in certain positions) possibly could have been extracted more
reliably if a richer linguistic analysis in the form of syntactic parsing would have been performed first. However,
this was deemed beyond the scope of this investigation.
6This was calculated as the number of tokens divided by the number of different lemmatized forms in the
whole document. However, this is not a reliable estimate, because a) the documents vary in length, and b) the
documents sometimes contain several independent texts (especially in the newspaper documents).
7The good performance of POS taggers based on n-gram language models can be taken as a justification for
this second assumption.
Prototype-Based Relevance Learning
for Genre Classification
3.2. DATA SETS 21
01 past tense
02 perfect aspect
03 present tense
04 place adverbials
05 time adverbials
06 first person pronouns
07 second person pronouns
08 third person pronouns
09 pronoun it
10 demonstrative pronouns
11 indefinite pronouns
12 pro-verb do
13 direct WH-questions
14 nominalizations
15 gerunds
16 other nouns
17 agentless passives
18 by passives
19 be main verb
20 existential there
21 that verb complements
22 that adjective complements
23 WH-clauses
24 infinitives
25 present participial clauses
26 past participial clauses
27 past participial WHIZ deletion relatives
28 present participial WHIZ deletion relatives
29 that relative clauses on subject position
30 that relative clauses on object position
31 WH relative clauses on subject position
32 WH relative clauses on subject position
33 pied-piping relative clauses
34 sentence relatives
35 because
36 though / although
37 if / unless
38 other adverbial subordinators (not 35-37)
39 total prepositional phrases
40 attributive adjectives
41 predicative adjectives
42 total adverbs
43 type/token ratio
44 word length
45 conjuncts
46 downtoners
47 hedges
48 amplifiers
49 emphatics
50 discourse particles
51 demonstratives
52 possibility modals
53 necessity modals
54 predictive modals
55 public verbs
56 private verbs
57 suasive verbs
58 seem / appear
59 contractions
60 ‚Äì
61 stranded prepositions
62 split infinitves
63 split auxiliary
64 phrasal coordination
65 ‚Äì
66 synthetic negation
67 analytic negation
Table 3.3: Features in the Biber data set
Prototype-Based Relevance Learning
for Genre Classification
22 CHAPTER 3. CLASSIFICATION TASKS / DATA SETS
Obviously, the complexity of the syntactic constructions that can be captured increases with
the length of the POS sequences considered. However, the number of possible sequences increases
exponentially with the length, so already for sequences of length n = 3 the number of features
may be too large to work with.
Each token in the BNC is assigned a tag from the C5 tagset [Burnard, 2007, Section 6.5],
which consists of 61 grammatical tags (including 4 punctuation tags).8 There are thus 612 = 3721
possible bigrams, and 613 = 226, 981 possible trigrams ‚Äì too many features to be feasible, even
as input to most feature selection algorithms.
While only roughly half of the possible trigrams actually occur, this number still has to
be significantly reduced by some selection method to make further computations feasible.9 A
problem related to the high number of possible trigrams is their extremely sparse distribution:
Over 50% of the trigrams occur less than 15 times in the whole corpus, and there is no trigram
that occurs in all documents.
Thus, some computationally inexpensive method for selecting ‚Äúgood‚Äù n-grams10 is required,
What amounts to a ‚Äúgood n-gram‚Äù depends on what the objective is: If only the discrimination
ability of the features with respect to a known classification is important, the class labels can
be used in the selection process. Possibilities include selecting the n-grams that maximize the
difference of the within-class means, or that have the highest mutual information with the class
label.
If the target classification is not to be used in the selection process (because the data set
should be useable for several classification tasks and/or unsupervised methods), ‚Äúgood n-grams‚Äù
must be characterized in a different way: Selecting the n-grams with the highest overall frequency
is possibly the simplest approach, but seems to work relatively well (e.g. used by Santini [2004]
and Koppel et al. [2002]). However, it can be argued that the most common syntactic patterns
in a language are not the ones that differ most between groups of texts.11 Another heuristic is
to select the trigrams whose frequencies have the largest variance12 across documents, as this
may hint at good discrimination ability. As high variance can simply be due to a large number
of small random variations, it can be expected not to be a very good estimator of discrimination
ability by itself.13
In search of a better heuristic, several other statistics of the distributions of trigrams across
documents were explored, including measures of skewness and kurtosis defined as the third and
fourth standardized central moments (see below), as well as Pearson‚Äôs second skewness coefficient
8In addition to the 61 basic tags, there are so called ‚Äúambiguity tags‚Äù consisting of two basic tags (ordered by
probability), which are used by the tagger if it cannot reliably disambiguate a tag. In this study, these ambiguity
tags were always treated as if they were the more probable of the two tags.
9Even storing the trigrams frequencies for the 4048 documents as 64 bit floating point numbers in a non-sparse
way requires 107, 206√ó 4048√ó 8 bytes ‚âà 3.5 GB of space.
10The length n is considered fixed here.
11As illustrated by Figure A.1 in the appendix, the most common patterns have roughly normal distributions
across documents, likely due to random ‚Äì not systematic ‚Äì variation.
12Trigram frequencies should be normalized by text length; If they are not, trigrams that occur often will also
have a high variance due to the variation in text length (which is large in the BNC).
13Figure A.3 shows the distributions of the 10 trigrams with the highest overall variance across documents.
Note that several trigrams that have the highest overall frequency are also among the ones that have the highest
variance.
Prototype-Based Relevance Learning
for Genre Classification
3.2. DATA SETS 23
defined as 3 ¬∑ (mean‚àímedian) ¬∑ œÉ‚àí1.
skewness =
‚àëN
i=1(xi ‚àí x)3
(N ‚àí 1)œÉ3
kurtosis =
‚àëN
i=1(xi ‚àí x)4
(N ‚àí 1)œÉ4
(3.1)
In experiments, the distributions of trigrams which have the highest skewness/kurtosis-ratio
SKR =
skewness
kurtosis
=
‚àëN
i=1(xi ‚àí x)3 ¬∑ œÉ‚àëN
i=1(xi ‚àí x)4
(3.2)
were found to have several desirable properties, including non-sparsity and high variance due to a
small number of larger deviations.14 While this criterion might be hard to justify mathematically15,
it was chosen as the n-gram selection heuristic for this data set because initial results seemed
promising.
Classification Highest Freq. Highest Var. Highest SKR
Genre 83.5 (¬± 2.3) 82.7 (¬± 2.1) 82.8 (¬± 2.2)
Gender 77.1 (¬± 3.2) 77.1 (¬± 3.0) 76.0 (¬± 2.2)
Table 3.4: Comparison of the n-gram selection criteria using GLVQ with 10 prototypes per class
(5-fold cross-validation). None of the differences between the methods is significant on the 5% level.
For comparison: Using 100 randomly chosen trigrams yields an accuracy around 50% on the Genre
classification task.
To confirm the initial results, a pre-experiment comparing the three selection methods
described above (highest overall frequency, highest variance, highest skewness/kurtosis-ratio)
was conducted using the 100 highest-scoring trigrams on the Genre and Gender classification
task. GLVQ with 10 prototypes per class was used as classifier.16 The results are shown in Table
3.4. While in this experiment no significant difference between the selection methods could be
demonstrated17, the SKR criterion was still given precedence over the other methods for the
selection of the n-grams used in this data set.
The features in the final POS data set are the relative frequencies of all 61 unigrams, the 100
highest SKR bigrams, and the 200 highest SKR trigrams.
3.2.3 Data Postprocessing
After feature extraction, both data sets were normalized so that each feature has a zero mean
and a standard deviation of 1. For the experiments using the LIBSVM support vector machine
14Histograms of the distributions of the 10 trigrams with the highest skewness/kurtosis-ratio are shown in Figure
A.2 in the appendix.
15Kurtosis is usually only considered for symmetric distributions, i.e. distributions which have a skewness close
to zero. Measures of kurtosis that are independent of skewness can be defined (e.g. [Blest, 2003]), and might be an
interesting direction for further research.
16Some further sample experiments using a different number of prototypes and the SNG update rule, as well as
experiments using the LIBSVM [Chang and Lin, 2001] support vector machine classifier showed similar results.
17Significance was determined using a paired-samples t-test on the cross-validation results.
Prototype-Based Relevance Learning
for Genre Classification
24 CHAPTER 3. CLASSIFICATION TASKS / DATA SETS
classifier, each feature was additionally scaled to range between ‚àí1 and 1 (as suggested in the
documentation [Chang and Lin, 2001]).
Prototype-Based Relevance Learning
for Genre Classification
Chapter 4
Empirical Evaluation
In this chapter the evaluation procedures and their results will be discussed. The evaluation can
be split into two main components: A quantitative component (Section 4.1), which amounts to
the evaluation of the performance of the different update rules (in terms of accuracy) with respect
to different parameter values (e.g. learning rates, number of iterations, number and initialization
of the prototypes), and a qualitative component (Section 4.2), in which the usefulness of the
additional information provided by these learning methods (i.e. prototypes and relevances) is
assessed.
4.1 Quantitative Evaluation
4.1.1 Update Rule Comparison and Parameter Evaluation
Experimental Design
In order to initially assess the performance of the update rules in relation to each other, and
to estimate the influence of the free parameters on the performance, a grid search over 3756
parameter/update rule combinations was performed on the Genre-Biber dataset. The following
parameters were varied in this experiment:
1. Update rule (None1, LVQ, GLVQ, GRLVQ, SNG, or SRNG)
2. Number of prototypes per class (1, 2, 3, 5, 10, or 20)2
3. Initialization method (Data ‚Äì random data points (from the correct class), or KMeans ‚Äì
class centroids determined by the k-means algorithm run for each class separately)
4. Number of performed iterations (10, 100, 1000, 3000)
1Included to compare the baseline performance after prototype initialization.
2As the data set has four classes, this amounts to 4, 8, 12, 20, 40, or 80 prototypes total.
25
26 CHAPTER 4. EMPIRICAL EVALUATION
5. Learning rate decay (none, or linear)3
6. Initial learning rates + and ‚àí ((1, 1), (0.1, 0.1), (0.1, 0.05)), for SRNG and GRLVQ the
learning rate l was either l = 0.1 ¬∑ + or l = 0.1 ¬∑ + 4, and for SNG and SRNG initial
values for Œ≥ were (10, 1, and 0.1)5
For each possible combination of these parameters, the classification accuracy was determined
using 5-fold cross-validation (with pre-defined, random splits).6
Results
One objective of this experiment was to evaluate the performance of the different update rules
against each other. To this end, the runs with the highest (average) classification accuracy were
compared, grouped by number of prototypes per class. The results are summarized in Table (4.1)
and Figure (4.1).
Update Accuracy / # Prototypes
Rule 1 2 3 5 10 20
Data 51.2 (¬± 11.0) 60.1 (¬± 8.6) 62.7 (¬± 8.0) 70.0 (¬± 4.9) 72.9 (¬± 5.5) 76.2 (¬± 3.0)
KMeans 78.5 (¬± 2.3) 82.0 (¬± 1.2) 84.4 (¬± 1.1) 83.7 (¬± 0.8) 85.0 (¬± 1.3) 85.4 (¬± 1.2)
LVQ 78.4 (¬± 2.6) 81.3 (¬± 3.4) 82.6 (¬± 2.3) 85.5 (¬± 1.4) 86.3 (¬± 2.0) 87.3 (¬± 1.4)
GLVQ 79.5 (¬± 2.7) 83.8 (¬± 1.9) 84.9 (¬± 1.6) 85.1 (¬± 1.8) 87.1 (¬± 1.0) 87.2 (¬± 1.0)
GRLVQ 79.5 (¬± 2.6) 83.9 (¬± 2.4) 84.9 (¬± 1.3) 86.3 (¬± 1.5) 87.6 (¬± 1.4) 88.2 (¬± 1.5)
SNG 79.5 (¬± 2.7) 83.8 (¬± 1.9) 84.9 (¬± 1.6) 86.0 (¬± 1.2) 87.3 (¬± 1.4) 87.6 (¬± 1.2)
SRNG 79.3 (¬± 2.7) 84.1 (¬± 2.2) 84.6 (¬± 1.7) 86.1 (¬± 1.3) 87.3 (¬± 1.2) 88.0 (¬± 1.5)
Table 4.1: Maximum average classification accuracy (¬± standard deviation) in percent by update rule
and number of prototypes per class. The averages and standard deviations are computed over the five
cross-validation runs. These results are shown graphically in Figure 4.1.
The first thing to note about these results is that classification accuracies for all update
rules increase with the number of prototypes used, so even with 20 prototypes per class no
overfitting seems to take place. When using only few (1, 2, or 3) prototypes, none of the update
rules is able to improve the results significantly7 over those obtained by placing the prototypes
using the k-means clustering algorithm in each class ‚Äì with LVQ (initialized with k-means) the
results are even slightly worse (not significant). With 5 or more prototypes the picture changes:
3As the method of learning rate decay was expected to make only a small difference (if any at all), the
exponential learning rate decay method was not used in this comparison. Also, only the learning rate decay
method of the prototype learning rates + and ‚àí was changed. With linear decay, both learning rates were
linearly reduced to 0.01.
4For the first 200 iterations, l was set to 0, afterwards set to the said value and then kept constant.
5These values were multiplied with 0.995 after each iteration.
6Using 10 2.4 GHz computers, this took approximately 10 days; Especially the roughly 1700 SRNG experiments
required a lot of time.
7The results for each cross-validation run were compared using a paired sample t-test.
Prototype-Based Relevance Learning
for Genre Classification
4.1. QUANTITATIVE EVALUATION 27
k‚àíMeans LVQ GLVQ GRLVQ SNG SRNG
M
ax
im
um
 A
cc
ur
ac
y
0.78
0.80
0.82
0.84
0.86
0.88
0.90
1 2 3 5 10 20 1 2 3 5 10 20 1 2 3 5 10 20 1 2 3 5 10 20 1 2 3 5 10 20 1 2 3 5 10 20
Figure 4.1: Maximum achieved mean accuracy (over cross-validation runs) by update rule and number
of prototypes (per class) on the Genre-Biber data set, as shown in Table 4.1.
With 5 prototypes, GRLVQ performs significantly better (p < 0.05) than just k-means, with 10
prototypes GLVQ, GRLVQ, and SNG yield significant improvements, and with 20 prototypes all
update rules yield an improvement over the k-means baseline (all p < 0.05).
The influence of the parameters on the accuracy was determined by fitting several linear
models with the mean accuracy as response.8 This was done for each update rule separately,
but the results are quite similar and can be summarized as follows: As could be expected, the
number of prototypes is the main predictor for the resulting accuracy. On average, around 60%
of the overall variation in accuracy can be explained by this factor alone.
The second main predictor is the number of training iterations, and its interaction with the
initialization method (together explaining an additional 15% of the variance): Increasing the
number of learning steps improves performance, but the effect is much stronger when the initial
prototypes are not very good. This effect is illustrated in Figure 4.2. Somewhat surprisingly,
8The linear regressors used were the applicable learning rates, the logarithm of number of prototypes and the
logarithm of the number of update steps (taking the logarithm of these values significantly improved the fits of the
models), the initialization method and the decay method (as contrasts), as well as all interactions.
Prototype-Based Relevance Learning
for Genre Classification
28 CHAPTER 4. EMPIRICAL EVALUATION
M
ea
n 
ac
cu
ra
cy
0.65
0.70
0.75
0.80
0.85
da
ta
.1
0
km
ea
ns
.1
0
da
ta
.1
00
km
ea
ns
.1
00
da
ta
.1
00
0
km
ea
ns
.1
00
0
da
ta
.3
00
0
km
ea
ns
.3
00
0
‚óè
‚óè
‚óè
‚óè ‚óè ‚óè ‚óè ‚óè
glvq
da
ta
.1
0
km
ea
ns
.1
0
da
ta
.1
00
km
ea
ns
.1
00
da
ta
.1
00
0
km
ea
ns
.1
00
0
da
ta
.3
00
0
km
ea
ns
.3
00
0
‚óè
‚óè
‚óè
‚óè ‚óè ‚óè ‚óè ‚óè
grlvq
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè ‚óè ‚óè
sng
0.65
0.70
0.75
0.80
0.85
‚óè
‚óè
‚óè
‚óè ‚óè ‚óè ‚óè ‚óè
srng
Figure 4.2: Mean accuracy (over all experiments) by initialization method and number of performed
updates for each update rule.
the influence of the learning rates was not significant in this analysis.9 However, the rate
decay method shows a similar pattern of interaction with the number of learning steps as the
initialization method: Not decaying the learning rate works better if only few iterations are
performed, but in the long run it does not make a significant difference which method is used.
Overall, the learning algorithms are quite robust with respect to the exact parameter settings
and achieve good performance without the need for excessive parameter tuning.10 These effects
will be analyzed again in terms of learning behavior in Section 4.1.3.
9In fact, this is not too surprising: First of all, the relationship between the learning rates and the accuracy is
most likely not linear, but more complex. Second, the performance was only sampled at three unique combinations
of + and ‚àí, two values of l and three values of Œ≥.
10LVQ is an exception: The results may deteriorate if the learning rate is chosen too high. In the experiments
where the learning rates + and ‚àí were stated at 1 and then decayed to 0.01, accuracy dropped below 25%.
Prototype-Based Relevance Learning
for Genre Classification
4.1. QUANTITATIVE EVALUATION 29
4.1.2 Performance by Update Rule and Data Set
Each of the update rules was evaluated on all of the data sets described in Chapter 3 using 5-fold
cross-validation. For comparison, a support vector machine classifier (LIBSVM [Chang and Lin,
2001]) was trained on each data set, also using 5-fold cross-validation.
The parameters of the update rules were set to a combination that resulted in high accuracies
in the grid search experiment described in the previous section.11 No further tuning of the
parameters to the data sets was performed. The only additional parameter that was varied was
the number of prototypes per class, as this is dominant factor influencing the performance by
controlling the complexity of the model.
The support vector machine classifiers used RBF kernels in all experiments. For each
data set, good values for the kernel parameter Œ≥ and the regularization parameter C were
determined in a grid parameter search using the values Œ≥ ‚àà {2‚àí5, 2‚àí3, 2‚àí1, 21, . . . 215} and
C ‚àà {2‚àí15, 2‚àí13, 2‚àí11, . . . , 23} as suggested by Hsu et al. [2007].12 Though SVMs are designed
for two-class problems, LIBSVM allows for multi-class classification using a ‚Äúone-against-one‚Äù
approach which constructs one SVM classifier for each pair of classes and then decides the class
of an instance by majority vote [Chang and Lin, 2001].13
The results for the Genre-Biber, Genre-POS, Gender-Biber, and Gender-POS data
sets are summarized in Table 4.2, the results for the Santini classification are shown in Table
4.3 and the results obtained with LIBSVM are presented in Table 4.4.
Among the prototype-based methods, the highest accuracies are obtained by GRLVQ on all
data sets (Genre-Biber: 88.3%, Genre-POS: 87.9%, Gender-Biber: 79.4%, Gender-POS:
79.3%, Santini-Biber: 89.5%, Santini-POS: 89.7%). Except for on the Santini-Biber task,
the LIBSVM classifier achieves accuracies that are 1% to 3% higher. However, it must be noted
that the prototype-based classifiers achieve high accuracy even with extremely sparse models
using only a few prototypes per class, while the SVMs use around 850 support vectors on the
Genre and Gender tasks, and around 100 for the Santini task (with either feature set).
Both feature sets (Biber and POS) yield very similar results on all classification tasks.
However, the prototype-based methods achieve the highest results with the Biber feature set,
while the SVMs perform better with the POS features.
The results on the Santini classification task can be compared to those obtained by Santini
[2004]: Using the 1033 trigrams with the highest overall frequency and a NaÃàƒ±ve Bayes classifier,
she obtained a cross-validation accuracy of 81.1% (87.0% using 65 selected trigrams). The results
obtained here a clearly better ‚Äì the best being 89.7% using GRLVQ, 91.3% using SVMs, both
using the POS features ‚Äì, but it must be noted that these improvements can be due to a different
selection of documents (cf. Section 3.1.2).
In addition to the means and standard deviations, paired-sample t-tests were calculated in
order to assess whether the means really differ. Not all the results can be discussed in detail here,
but one question is of special interest: Does relevance learning significantly improve the results,
11The exact settings were: + = 0.1 (linearly decayed to 0.01), ‚àí = 0.05, l = 0.001 (set to 0 for the first 200
epochs), Œ≥ = 1, k-means initialization, training was performed for 1000 epochs.
12The grid search was performed using the grid.py script supplied with LIBSVM, and its default parameters
were used.
13Thus for the Genre task with four classes six SVMs are needed, and the Santini task with ten classes requires
45 SVMs.
Prototype-Based Relevance Learning
for Genre Classification
30 CHAPTER 4. EMPIRICAL EVALUATION
Algorithm # Protot. Genre-Biber Genre-POS Gender-Biber Gender-POS
k-Means 1 78.5 (¬± 2.3) 77.9 (¬± 2.5) 68.7 (¬± 2.0) 69.3 (¬± 1.6)
5 83.7 (¬± 0.8) 84.9 (¬± 1.9) 67.1 (¬± 5.7) 68.0 (¬± 4.1)
10 85.0 (¬± 1.3) 85.8 (¬± 1.4) 67.8 (¬± 3.0) 69.2 (¬± 2.3)
20 85.4 (¬± 1.7) 84.9 (¬± 1.3) 73.1 (¬± 2.2) 69.5 (¬± 2.7)
50 86.1 (¬± 1.5) 85.2 (¬± 1.3) 73.5 (¬± 2.9) 73.2 (¬± 2.0)
LVQ 1 77.0 (¬± 2.1) 74.0 (¬± 2.7) 71.1 (¬± 2.1) 70.2 (¬± 2.4)
5 83.7 (¬± 2.3) 83.2 (¬± 1.8) 71.8 (¬± 1.0) 76.3 (¬± 3.2)
10 85.4 (¬± 1.3) 84.6 (¬± 1.5) 72.5 (¬± 2.4) 75.7 (¬± 3.7)
20 86.9 (¬± 1.9) 85.8 (¬± 3.1) 75.9 (¬± 2.2) 75.3 (¬± 3.2)
50 86.6 (¬± 1.2) 86.6 (¬± 2.4) 75.6 (¬± 3.6) 76.3 (¬± 3.1)
GLVQ 1 78.1 (¬± 2.2) 74.9 (¬± 1.7) 72.9 (¬± 1.0) 70.7 (¬± 2.4)
5 85.1 (¬± 1.8) 85.1 (¬± 2.6) 77.0 (¬± 2.1) 77.5 (¬± 3.2)
10 86.7 (¬± 1.4) 85.9 (¬± 1.5) 78.3 (¬± 3.2) 77.1 (¬± 2.5)
20 87.0 (¬± 1.3) 86.8 (¬± 2.3) 78.2 (¬± 3.1) 78.2 (¬± 2.9)
50 87.4 (¬± 0.6) 87.7 (¬± 2.0) 79.3 (¬± 4.0) 78.9 (¬± 3.4)
GRLVQ 1 79.2 (¬± 2.6) 76.6 (¬± 2.1) 72.6 (¬± 1.8) 70.6 (¬± 2.2)
5 86.3 (¬± 1.6) 85.1 (¬± 1.9) 78.0 (¬± 2.1) 78.0 (¬± 3.5)
10 87.6 (¬± 1.8) 86.3 (¬± 1.6) 79.4 (¬± 2.2) 77.7 (¬± 2.4)
20 88.2 (¬± 1.5) 86.5 (¬± 2.3) 79.3 (¬± 2.8) 78.8 (¬± 3.3)
50 88.3 (¬± 0.4) 87.9 (¬± 1.7) 79.4 (¬± 3.5) 79.3 (¬± 3.6)
SNG 1 78.1 (¬± 2.2) 74.9 (¬± 1.7) 72.9 (¬± 1.0) 70.7 (¬± 2.4)
5 85.6 (¬± 1.4) 84.8 (¬± 2.1) 77.7 (¬± 2.5) 77.7 (¬± 3.0)
10 86.8 (¬± 1.2) 86.0 (¬± 1.7) 78.2 (¬± 3.2) 77.2 (¬± 2.7)
20 86.8 (¬± 1.2) 87.1 (¬± 2.1) 78.9 (¬± 2.5) 78.3 (¬± 3.1)
50 87.4 (¬± 0.7) 87.7 (¬± 2.1) 78.9 (¬± 3.6) 78.9 (¬± 3.3)
SRNG 1 79.2 (¬± 2.3) 75.9 (¬± 2.3) 72.6 (¬± 2.0) 70.7 (¬± 2.3)
5 86.0 (¬± 1.5) 84.7 (¬± 2.0) 78.5 (¬± 2.4) 77.9 (¬± 3.2)
10 87.6 (¬± 1.2) 85.9 (¬± 2.0) 78.3 (¬± 3.1) 77.4 (¬± 2.0)
20 88.0 (¬± 1.9) 86.8 (¬± 2.1) 79.3 (¬± 3.0) 78.3 (¬± 2.8)
50 88.1 (¬± 0.8) 87.8 (¬± 1.3) 79.2 (¬± 3.2) 78.9 (¬± 3.1)
Table 4.2: Average classification accuracy (¬± standard deviation) in percent on the Genre-Biber,
Genre-POS, Gender-Biber, and Gender-POS data sets by update rule and number of prototypes
per class, as determined by 5-fold cross-validation.
Prototype-Based Relevance Learning
for Genre Classification
4.1. QUANTITATIVE EVALUATION 31
Data Algo- Prototypes per Class
Set rithm 1 2 3 4 5
Biber k-Means 87.7 (¬± 0.6) 87.1 (¬± 1.0) 86.1 (¬± 3.1) 85.6 (¬± 2.0) 85.6 (¬± 0.8)
LVQ 85.2 (¬± 1.1) 84.7 (¬± 1.6) 85.3 (¬± 1.3) 84.4 (¬± 1.7) 84.8 (¬± 1.1)
GLVQ 86.3 (¬± 0.6) 87.5 (¬± 0.9) 87.3 (¬± 2.4) 87.1 (¬± 1.5) 87.1 (¬± 1.0)
GRLVQ 88.5 (¬± 1.5) 88.7 (¬± 0.8) 89.5 (¬± 1.4) 88.4 (¬± 1.5) 88.0 (¬± 2.1)
SNG 86.3 (¬± 0.6) 87.5 (¬± 0.9) 87.6 (¬± 2.3) 86.9 (¬± 2.0) 87.1 (¬± 1.0)
SRNG 88.4 (¬± 1.8) 88.9 (¬± 1.0) 89.5 (¬± 1.4) 88.4 (¬± 1.7) 88.1 (¬± 2.2)
POS k-Means 87.6 (¬± 2.4) 88.0 (¬± 1.2) 87.9 (¬± 2.0) 88.1 (¬± 1.4) 87.1 (¬± 1.9)
LVQ 82.8 (¬± 1.7) 84.5 (¬± 2.6) 86.0 (¬± 2.9) 87.2 (¬± 1.7) 87.9 (¬± 1.5)
GLVQ 88.3 (¬± 1.4) 88.7 (¬± 1.9) 88.8 (¬± 1.0) 88.8 (¬± 0.9) 88.8 (¬± 2.1)
GRLVQ 88.3 (¬± 1.4) 89.6 (¬± 1.8) 88.7 (¬± 1.2) 89.7 (¬± 0.6) 88.7 (¬± 1.2)
SNG 88.3 (¬± 1.4) 88.5 (¬± 1.7) 88.8 (¬± 1.0) 88.9 (¬± 0.8) 88.8 (¬± 2.1)
SRNG 88.1 (¬± 1.6) 89.6 (¬± 1.8) 88.8 (¬± 1.0) 89.6 (¬± 0.6) 88.7 (¬± 1.2)
Table 4.3: Average classification accuracy (¬± standard deviation) in percent on the Santini-Biber and
Santini-POS data set by update rule and number of prototypes per class. The results were determined
using 5-fold cross-validation, repeated 5 times with different random splits.
Genre Santini Gender
Biber 89.5 (Œ≥ = 2‚àí1, C = 23) 88.6 (Œ≥ = 2‚àí5, C = 21) 82.9 (Œ≥ = 2‚àí5, C = 25)
POS 90.0 (Œ≥ = 2‚àí5, C = 21) 91.3 (Œ≥ = 2‚àí7, C = 23) 82.8 (Œ≥ = 2‚àí5, C = 21)
Table 4.4: Average classification accuracies in percent for the LIBSVM classifiers on all data sets
determined by 5-fold cross-validation. Standard deviations are not available.
i.e. is there a difference in mean accuracy between GLVQ and GRLVQ, or SNG and SRNG? The
differences in mean are summarized in Table 4.5. First of all, it can be said that adapting the
distance measure at least does not hurt the performance. None of the negative differences are
significant. However, contrary to what one might expect, performance improvements are larger
on the lower-dimensional Biber data set than on the POS data set.
When G(R)LVQ is compared to S(R)NG, only one of the 40 comparisons yields a significant
difference in means: With 5 prototypes per class, the accuracy of SNG is 0.6% higher than that
of GLVQ on the Genre-Biber data set (p < 0.05).
Also, the differences in mean between 20 and 50 prototypes per class is only significant
(p < 0.05) for GRLVQ on the Genre-POS data set, but not for any other update rule or data
set.
Prototype-Based Relevance Learning
for Genre Classification
32 CHAPTER 4. EMPIRICAL EVALUATION
Genre-Biber Genre-POS Gender-Biber Gender-POS
GLVQ 1 1.09 ** 1.68 ** -0.22 -0.08
5 1.24 ** -0.05 1.05 *** 0.45
10 0.94 0.40 * 1.12 * 0.60
20 1.24 ** -0.35 1.12 ** 0.60
50 0.89 ** 0.25 0.07 0.37 *
SNG 1 1.04 *** 0.99 * -0.22 0.00
5 0.40 -0.10 0.75 0.23
10 0.79 * -0.10 0.15 0.22
20 1.19 ** -0.35 0.38 0.00
50 0.69 * 0.10 0.30 0.00
Table 4.5: Differences in mean performance (in percent) of GLVQ compared to GRLVQ and SNG
compared to SRNG. Significant differences are in bold face; the significance codes are: *: p < 0.1; **:
p < 0.05; ***: p < 0.01.
Correct Predicted
A F N NA
Acadmic 458 0 6 41
Fiction 1 457 5 0
News 8 4 492 14
Non-Academic 135 12 37 344
Correct Predicted
male female
male 853 67
female 222 192
Table 4.6: Typical confusion matrices on the Genre-Biber (left) and Gender-Biber (right) data
set. These particular values were determined using GRLVQ with 20 prototypes per class by 5-fold
cross-validation.
Confusion matrices14 were calculated in order to determine the nature of the errors made by
the classifiers. As the confusion patterns are roughly similar across update rules and data sets
(Biber vs. POS), it can be assumed that these reflect true difficulties in distinguishing the classes
using only syntactic features. Typical results are shown in Table 4.6. In the Genre classification,
the main difficulty lies in distinguishing academic texts from non-academic texts: One quarter of
the non-academic texts are falsely labeled as academic, and about 10% of the academic texts are
classified as non-academic. This result is not too surprising, as the non-academic genre includes
for example ‚Äúpopular‚Äù science journals such as ‚ÄúNature‚Äù and ‚ÄúNew Scientist‚Äù, which can be
expected to be quite similar in language to some ‚Äúacademic‚Äù journals.
14A confusion matrix shows the number of documents in each predicted class (columns) for each correct class
(rows).
Prototype-Based Relevance Learning
for Genre Classification
4.1. QUANTITATIVE EVALUATION 33
W
:a
c:
hu
m
an
iti
es
_a
rt
s
W
:a
c:
m
ed
ic
in
e
W
:a
c:
na
t_
sc
ie
nc
e
W
:a
c:
po
lit
_l
aw
_e
du
W
:a
c:
so
c_
sc
ie
nc
e
W
:a
c:
te
ch
_e
ng
in
W
:a
dm
in
W
:b
io
gr
ap
hy
W
:c
om
m
er
ce
W
:e
ss
ay
:s
ch
oo
l
W
:e
ss
ay
:u
ni
v
W
:fi
ct
:d
ra
m
a
W
:fi
ct
:p
oe
tr
y
W
:fi
ct
:p
ro
se
W
:in
st
itu
t_
do
c
W
:in
st
ru
ct
io
na
l
W
:le
tte
rs
:p
er
so
na
l
W
:m
is
c
W
:n
on
_a
c:
hu
m
an
iti
es
_a
rt
s
W
:n
on
_a
c:
m
ed
ic
in
e
W
:n
on
_a
c:
na
t_
sc
ie
nc
e
W
:n
on
_a
c:
po
lit
_l
aw
_e
du
W
:n
on
_a
c:
so
c_
sc
ie
nc
e
W
:n
on
_a
c:
te
ch
_e
ng
in
W
:p
op
_l
or
e
W
:r
el
ig
io
n
# 
of
 d
oc
um
en
ts
0
20
40
60
80
100
120
140
correct: male, predicted: male
correct: male, predicted: female
correct: female, predicted: female
correct: female, predicted: male
Gender classification results by genre
Figure 4.3: Typical classification results for the Gender classification task broken down by genre. These
particular values were determined using GRLVQ with 20 prototypes per class by 5-fold cross-validation.
The results for the Gender classification are also not unexpected: Due to the skewed class
distribution (and the skewed distribution of genres within the classes, cf. Section 3.1.3), the
results are quite different in each class. While only about 7% of the male-authored texts are
classified incorrectly, 54% of the texts by female authors are taken to be written by the other
sex. Looking at the results by genre (shown in Figure 4.3) reveals that while for fictional prose
(where the class distributions are equal) only about 25% are misclassified, none of the texts from
the academic genre and only 16% of the texts from the non-academic (non-fiction) genre are
classified correctly.
4.1.3 Learning Curve Comparison
To evaluate the learning behavior of the update rules, the data was split into a training and
a validation set in a 2 : 1 ratio. After each iteration the accuracy on both sets was evaluated.
The learning behavior was examined with respect to the dimensionality of the data set (Genre-
Biber vs. Genre-POS), the update rule, the number of prototypes, and the learning rates.
Prototype-Based Relevance Learning
for Genre Classification
34 CHAPTER 4. EMPIRICAL EVALUATION
Data Set, Update Rule, Number of Prototypes
The learning behavior for G(R)LVQ and S(R)NG on the Genre-Biber and Genre-POS data
sets with 20 prototypes per class is shown in Figure 4.4.15 While the learning behavior of all
update rules on both data sets is very similar, an interesting effect of the metric adaptation
can be observed: At 200 iterations, when the metric adaptation starts, the performance on the
training set quickly drops, while the performance on the validation set goes up. Contrary to
what one might expect, this effect is stronger on the lower-dimensional Genre-Biber data set.
However, as can been seen in the additional figures in the appendix, this effect is not stable if
the number of prototypes per class is varied.
Initialization Method
Obviously, the initialization of the prototypes does have a large impact on the performance in
the early stages of training. However, the question is whether good initial guesses speed up the
learning process and how they influence the final performance on unseen data. Figure 4.5 shows
the learning curves of GLVQ and SNG initialized with either random data points or k-means
cluster centroids on the Genre-Biber data set.16 As expected, the performance of the k-means
prototypes is better on both sets for the first few (50-100) iterations. If training is continued,
however, the randomly initialized prototypes improve and match (sometimes even surpass) the
performance of the k-means prototypes. Interestingly, this is true for both GLVQ and SNG on
this data set.
Learning Rates
The influence of the positive learning rate + was analyzed on the Genre-Biber data set using
GLVQ and SNG with 1 and 20 prototypes. The results for GLVQ and 20 prototypes are shown
in Figure 4.6.17
As could be expected, larger learning rates lead to a faster convergence to ‚Äúgood‚Äù prototypes,
but smaller learning rates yield better performance in terms of accuracy in the long run. This can
be taken as justification to decay the learning rate over time to produce quite good prototypes
quickly, which are then ‚Äúfine-tuned‚Äù using smaller learning rates. Note that with large learning
rates the performance does not change much anymore after only 10 iterations, while for + = 0.01
the accuracy on training and testing set still increases after 1000 iterations.
15Further results are shown in the Appendix: Figure B.2 shows a the same comparison using 5 prototypes per
class; Figure B.3 contrasts the learning behavior on the Gender-Biber data set for 10 and 50 prototypes, and
Figure B.4 shows the same contrast for the Genre-POS data set.
16The results are shown for only one particular random initialization. However, the experiment was repeated
several times with different random seeds and the results were similar to the one shown in all runs.
17The very similar results for SNG can be found in the Appendix in Figure B.1.
Prototype-Based Relevance Learning
for Genre Classification
4.1. QUANTITATIVE EVALUATION 35
0 200 400 600 800 1000
0.
87
0.
88
0.
89
0.
90
0.
91
0.
92
0.
93
Iterations
A
cc
ur
ac
y
Genre‚àíBiber, 20 Prototypes
GLVQ
GRLVQ
SNG
SRNG
Accuracy on training set
Accuracy on testing set
0 200 400 600 800 1000
0.
86
0.
88
0.
90
0.
92
Iterations
A
cc
ur
ac
y
Genre‚àíPOS, 20 Prototypes
GLVQ
GRLVQ
SNG
SRNG
Accuracy on training set
Accuracy on testing set
Figure 4.4: Learning curves on the Genre-Biber (top) and Genre-POS (bottom) data set for
G(R)LVQ and S(R)NG with 20 prototypes per class. Other parameters fixed at: + = 0.1, + = 0.05,
l = 0.01, Œ≥ = 3, Initialization: k-means, Rate decay: linear.
Prototype-Based Relevance Learning
for Genre Classification
36 CHAPTER 4. EMPIRICAL EVALUATION
1 5 10 50 100 500 1000
0.
75
0.
80
0.
85
0.
90
Iterations
A
cc
ur
ac
y
GLVQ ‚àí Data
GLVQ ‚àí KMeans
SNG ‚àí Data
SNG ‚àí KMeans
Accuracy on training set
Accuracy on testing set
Learning Curve, Initialization Method
Figure 4.5: Learning curves on the Genre-Biber data set for GLVQ and SNG initialized either with
random data points or k-means cluster centroids. (10 prototypes per class, + = 0.1, ‚àí = 0.05, Œ≥ = 3,
linear learning rate decay)
4.1.4 Relevances for Feature Selection
In order to initially assess the usefulness of the relevances determined by SRNG and GRLVQ
quantitatively, their applicability as feature selection criterion was evaluated. Using GRLVQ and
SRNG with 10 prototypes per class, the 10 features with the highest relevances were selected
from the Genre-Biber and Genre-POS data set. For comparison, two other commonly used
feature selection / transformation methods were applied: 10 features from each data set were
selected using a greedy algorithm based mutual information (MI) [Battiti, 1994], and PCA
[Haykin, 1998] was used to project the data sets onto their first 10 principal components. The
results, shown in Table 4.7, are somewhat inconclusive: While on the Genre-Biber data set the
features selected by GRLVQ and SRNG perform better than those selected by the MI algorithm,
this trend cannot be seen on the Genre-POS data set. Dimensionality reduction using PCA is
quite effective on both data sets when combined with GLVQ using only 10 prototypes per class:
On the Genre-POS data set, the reduction from 361 to only 10 features does not lead to a
significant difference in classification accuracy.
However, as the determined relevance terms vary with different initial prototype choices ‚Äì
this will be discussed further in Section 4.2.2 ‚Äì the results obtained may not be representative. In
order to fully assess the usefulness of the relevance terms for feature selection further experiments
need to be made, but this is beyond the scope of this thesis.
Prototype-Based Relevance Learning
for Genre Classification
4.1. QUANTITATIVE EVALUATION 37
10 20 50 100 200 500 1000
0.
78
0.
80
0.
82
0.
84
0.
86
0.
88
0.
90
0.
92
Iterations
A
cc
ur
ac
y
Œµ+ = 1
Œµ+ = 0.8
Œµ+ = 0.6
Œµ+ = 0.4
Œµ+ = 0.2
Œµ+ = 0.1
Œµ+ = 0.01
Accuracy on training set
Accuracy on testing set
GVLQ, Genre‚àíBiber, 20 Prototypes per Class
Figure 4.6: Learning curves for GLVQ with varying learning rates + on the Genre-Biber data set
using 20 prototypes per class. The negative learning rate ‚àí was fixed at 0.05 except for + = 0.01, where
‚àí = 0.005. Prototypes were initialized from data points; no learning rate decay was used.
Learner Selection Genre-Biber Genre-POS
GLVQ None 86.7 (¬± 1.4) 85.9 (¬± 1.5)
GRLVQ 84.8 (¬± 1.6) 77.5 (¬± 3.0)
SRNG 80.5 (¬± 1.8) 81.3 (¬± 2.6)
MI 78.7 (¬± 1.3) 78.5 (¬± 2.3)
PCA 85.4 (¬± 1.4) 85.8 (¬± 0.9)
SVM None 89.5 90.0
GRLVQ 83.7 79.2
SRNG 81.9 82.9
MI 83.0 83.2
PCA 86.9 87.8
Table 4.7: Comparison of cross-validation accuracies on reduced data sets. The results for GLVQ were
determined using 10 prototypes per class.
Prototype-Based Relevance Learning
for Genre Classification
38 CHAPTER 4. EMPIRICAL EVALUATION
4.2 Qualitative Evaluation
In addition to the quantitative assessment, another aim of this thesis is to determine if and how
the models produced by the learning algorithms (comprising the prototypes and the relevance
terms) can be used for further investigations of the data sets. This is done here by an exemplary
analysis of some of the models produced for the Genre and Gender classification tasks.
As mentioned in the introduction, the simplicity of the produced models is one appeal of
prototype-based classifiers. The prototypes are simply vectors in the same space as the input
with assigned class labels, and could thus be examined directly. However, contrary to what
the use of the term prototypes might suggest, the update rules presented here do not optimize
a criterion directly related to ‚Äúprototypicality‚Äù of the codebook vectors, but rather minimize
an error function aimed at producing codebook vectors that yield a good classification and
generalization ability. Figure 4.7 illustrates that this approach may yield ‚Äúprototypical‚Äù codebook
vectors under certain circumstances, although this need not be the case.
Because of this, the prototypes should not be interpreted directly and independently of each
other. What can sensibly be analyzed, however, is the clustering of the data set induced by the
receptive fields of the prototypes. An analysis of the relative distances between the prototypes
would also be possible, but is not done here.
x
y
0 1 2 3 4 5 6
0
1
2
3
4
5
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè ‚óè
‚óè
‚óè
‚óè‚óè ‚óè ‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
x
y
0 1 2 3 4 5 6
0
1
2
3
4
5
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè ‚óè
‚óè
‚óè
‚óè‚óè ‚óè ‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
y
0 1 2 3 4 5 6
0
1
2
3
4
5
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè ‚óè
‚óè
‚óè
‚óè‚óè ‚óè ‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
y
0 1 2 3 4 5 6
0
1
2
3
4
5
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè ‚óè
‚óè
‚óè
‚óè‚óè ‚óè ‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
Figure 4.7: Voronoi decomposition of an artificial two-dimensional two-class data set by six prototypes
(top) and four prototypes (bottom). The prototypes on the left were determined by k-means clustering
performed for each class separately; the ones on the right were determined by GLVQ.
Prototype-Based Relevance Learning
for Genre Classification
4.2. QUALITATIVE EVALUATION 39
4.2.1 Clusters
The clustering of the data set induced by the prototypes was analyzed on the Genre-Biber and
Gender-Biber data sets using GLVQ with 10 prototypes per class. As for both classifications
further information about sub-genres is available, the distribution of the documents across
sub-genres can be analyzed for each prototype separately. By this, further insights into the
confusions made by the classifier might be gained. Figure 4.8 shows the distribution of documents
belonging to each sub-genre found in the receptive field of each prototype.
Genre
N
um
be
r 
of
 d
oc
um
en
ts
10
20
30
40
0 1 2 3 4 5 6 7 8 9
10 11 12 13 14 15 16 17 18
10
20
30
40
19
10
20
30
40
20 21 22 23 24 25 26 27 28 29
30 31 32 33 34 35 36 37 38
10
20
30
40
39
Figure 4.8: Distribution of sub-genres across 10 prototypes per class (determined by GLVQ) on the
Genre-Biber data set. The prototypes for each class are shown in one row (bottom to top: Academic,
Fiction, Newspaper, Non-academic). The genres on the x-axis are ordered alphabetically (as listed in
Table 3.2).
There are several things to note about this induced clustering: First, a correspondence between
sub-genres and clusters can be observed in all but the newspaper genre. This can be seen most
prominently in the non-academic genre, where clusters 30, 31, and 36 exclusively contain texts from
the non ac:tech engin sub-genre18, cluster 32 comprises most of the non ac:polit law edu sub-
genre, and cluster 33 contains mostly texts from non ac:humanities arts. The correspondence
can also be observed in the genre of academic texts. Cluster 0, for example, contains mostly social
science texts (50 academic and 20 non-academic), as well as texts from the humanities arts
and polit law edu sub-genres (17 academic and 5 non-academic each). Clusters 2 and 6 almost
exclusively contain texts related to law, but while cluster 2 contains mostly books about law,
cluster 6 consists to 90% of excerpts from ‚ÄúThe Weekly Law Reports‚Äù which covers court cases.
In the fiction genre, 80% of the poetry sub-genre is contained in cluster 15.
From Figure 4.8 it is also easy to see why the academic and non-academic genres are hard
to separate (as indicated by the confusion matrix in Table 4.6): The problem lies mainly in
distinguishing between each of the parallel sub-genres in both categories. Clusters 0 and 7 contain
18In fact, cluster 30 contains only texts from ‚ÄúComputergram International‚Äù, while clusters 31 and 36 contain
mostly samples from ‚ÄúUnigram X‚Äù.
Prototype-Based Relevance Learning
for Genre Classification
40 CHAPTER 4. EMPIRICAL EVALUATION
both academic and non-academic social science / humanities texts, while cluster 9 contains
mostly natural science texts from both genres.
Genre
N
um
be
r 
of
 d
oc
um
en
ts
20
40
60
80
100
120
0 1 2 3 4 5 6 7 8 9
10 11 12 13 14 15 16 17 18
20
40
60
80
100
120
19
Figure 4.9: Distribution of sub-genres across 10 prototypes per class (determined by GLVQ) on the
Genre-Biber data set. The prototypes for each class are shown in one row (bottom: male, top: female).
The genres on the x-axis are ordered alphabetically (as listed in Figure 4.3).
When the same analysis is performed for the Gender classification (shown in Figure 4.9),
the results are not as clear-cut. The clusters induced by the female-class prototypes 12, 13, 15,
and 19 contain mostly W:fict:prose texts, cluster 16 contains biographies, and clusters 10 and
14 are almost empty. The clusters in the male class are quite diverse, except for clusters 0, 6,
and 7 which contain mostly fictional prose, and cluster 4 which contains almost exclusively texts
from W:misc.
In summary, it can be said that at least for the Genre classification, the clustering induced
by the GLVQ prototypes seems quite sensible, and the additional information can be used to
identify possible sources of classification errors.
4.2.2 Relevances
The main question that needs to be answered in order to assess the usefulness of the produced
relevance profiles for further investigations of the data, is to what extend the relevances determined
in a particular learning run describe characteristics of the data itself, and to what extend they
are just ‚Äúartifacts‚Äù of that particular run. In other words, the ‚Äústability‚Äù of the relevance terms
across several learning runs with the same or different parameter settings needs to be analyzed.
Because the relevances are determined by gradient descent on an error function with possibly
many local minima, they can in principle by quite different for each run. However, if certain
dimensions in a data set are truly more useful for classification than others, one could hope that
this is reflected ‚Äì at least on average ‚Äì in the relevance terms. Two experiments were conducted:
One to initially assess the variability of the relevance terms due to random effects, and another
to assess the influence of the update rule (GRLVQ vs. SRNG), the number of prototypes, and
the classification task (on the same data set, i.e. Genre-Biber vs. Gender-Biber).
Prototype-Based Relevance Learning
for Genre Classification
4.2. QUALITATIVE EVALUATION 41
In the first experiment, only the initial random placement of the prototypes was varied in
order to determine the ‚Äúinherent‚Äù variability of the relevance terms. The results are shown in
Figure 4.10 for the Genre-Biber data set and in Figure 4.11 for the Genre-POS data set.
While a general trend can be observed for both data sets, the exact values of the relevances and
also their ordering varies largely.
In the second experiment, the average relevances determined by 5 runs of GRLVQ and SRNG
were compared on the Biber data set for the Genre and the Gender classification. The results
for 20 and 50 prototypes per class are shown in Figure 4.12. Interestingly, even though the
results of the first experiments showed large variation across individual runs, the trends observed
when averaging of several runs are quite stable across update rules and number of prototypes.
On the Genre-Biber task, the two most relevant features are the use of present and past
tense verbs, and corresponding POS tags can also be found among the most relevant features in
the Genre-POS task (e.g. VVB, VHZ and VVD). Interestingly, combinations of coordinating
conjunction (CJC) and past tense verb (VVD) can be found four times among the 20 most
relevant POS features. Patterns involving prepositions, which are found to be relevant among
the Biber features, also belong to the most relevant features in the POS data set.
The most relevant of the POS features are patterns involving punctuation symbols (general
separating mark (PUN), and left and right brackets (PUL and PUR)). As punctuation symbols
have been shown to improve accuracy on other genre classification tasks (e.g. by Stamatatos
et al. [2000]), this result is not unreasonable.
Different relevance profiles emerge for the Genre and Gender classification, but there are
also some trends that can be observed for both classifications: Nominalizations (X14), nouns
(X16), and prepositions (X39) are found to be relevant for both classification tasks. Second
and third person pronouns are the features that are found to be most relevant for the Gender
classification, which is in line with the results obtained by Argamon et al. [2003].
Thus, when averaged over several runs to reduce the influence of random fluctuations, the
relevance profiles can be taken as trend indicators for the importance of certain (groups of)
features. It might be sensible to only consider groups of features for the analysis of the relevance
profiles (e.g. features with low, medium, and high relevance). The shape of the relevance profile
is similar for both data sets, and lends itself to such a categorization: There are a small number
of features with very high or very low relevance, but most features have a relevance close to their
initial (uniform) value.
The relevance information is thus found to be useful not just to improve performance but
also for further investigations, though the large variability prohibits strong conclusions to be
drawn from it.
Prototype-Based Relevance Learning
for Genre Classification
42 CHAPTER 4. EMPIRICAL EVALUATION
Relevance
X03_present_tense
X44_mean_word_length
X14_nominalization
X08_third_person_pronouns
X29_that_subj
X07_second_person_pronouns
X05_time_adverbials
X54_modal_predictive
X59_contractions
X55_verb_public
X24_infinitives
X33_pied_piping
X40_adj_attr
X67_neg_analytic
X12_proverb_do
X35_because
X27_past_participle_whiz
X43_type_token
X37_if
X52_modal_possibility
X19_be_main_verb
X50_discourse_particles
X30_that_obj
X49_emphatics
X20_existential_there
X41_adj_pred
X13_wh_question
X46_downtoners
X15_gerunds
X10_demonstrative_pronoun
X36_though
X38_other_adv_sub
X22_that_adj_comp
X01_past_tense
X16_other_nouns
X45_conjuncts
X39_prepositions
X34_sentence_relatives
X31_wh_subj
X42_adverbs
X56_verb_private
X51_demonstratives
X25_present_participle
X06_first_person_pronouns
X17_agentless_passives
X61_stranded_preposition
X04_place_adverbials
X28_present_participle_whiz
X21_that_verb_comp
X02_perfect_aspect
X11_indefinite_pronoun
X53_modal_necessity
X64_phrasal_coordination
X63_split_auxiliary
X32_wh_obj
X57_verb_suasive
X48_amplifiers
X18_by_passives
X23_wh_clause
X09_pronoun_it
X66_neg_synthetic
X58_verb_seem
X62_split_infinitve
X47_hedges
X26_past_participle
0.005 0.010 0.015 0.020 0.025 0.030
Figure 4.10: Relevance profile (sorted by mean relevance) of the Genre-Biber data set determined by
GRLVQ using 20 prototypes. The black line shows the mean relevance determined on the whole data set
for 5 repetitions with different random initializations of the prototypes, the dark shaded area the standard
deviation, and the light shaded area the minimum and maximum. The grey dashed line indicates the
initial uniform distribution.
Prototype-Based Relevance Learning
for Genre Classification
4.2. QUALITATIVE EVALUATION 43
Relevance
PUR_PUN_PRP
PUL
ZZ0
VVB
VHZ
CJC_VVD
NN1_PUQ_PUN
VVD_CJC
VVD_AVP_PRP
PNQ
VBZ_VVN
NP0_PUN_AT0
VVD
NN0
VHD
NP0_NP0_PRP
NP0_NN1_PUN
PRP_NP0_PRP
NP0_NP0
PNP_VVD_AVP
PUN_NP0_NN1
NN1_PUN_ITJ
NP0_VHD
ITJ
NP0_PRP_NP0
NP0_PUN
NN1_NP0
DPS_NN1
VVD_AVP_DPS
PRP_AT0_NP0
PUN_PUQ_PUQ
NN1_VM0_VVI
AV0_AV0
VVD_PRP
VBZ
NP0_NN1
NP0_PRP
VBG
VVD_AVP_AT0
DPS_NN1_VVD
NN1_CJC_VVD
CJS_PUN_PRP
NN1_VVZ
PNP_VVD
PNP_VVD_PRP
DT0_PUN_PUQ
PUN_PUQ
VVD_AT0_NN1
AV0
CJS_PNP
PUR
PUR_PUN
NN1_PUL
PRP_NP0_NP0
VVD_CJC_VVD
CJC_VVD_AVP
AT0_NN1_PUN
AJ0_NN1_PUL
PUN_PRP_NP0
NN2_PRP_NP0
TO0
PRP_NP0
DPS_NN2
PNP_VBZ_AT0
VBZ_VVN_PRP
VVZ_TO0
DPS
VHD_VVN
AV0_DT0
VHD_VVN_PRP
VVZ_TO0_VVI
VVN
VVI
PNP_VHD
PNP_VM0_VVI
AJ0_NN1_NN1
VVD_AV0
NN1_VBZ
NN1_VVZ_AT0
VVD_PNP
CRD
PUQ_PUQ
PUN
NN1_PRF_AJ0
NN1_CJT_AJ0
PNP_VHD_VVN
NN1_PNP_VHD
PUQ_VDB_PNP
VVD_PRP_AT0
PNP
PUN_PNP_VHD
PRF_AJ0
AJC
NN1_PUN_PUQ
VVI_PUN_PUQ
PUQ_PUQ_PNP
VHB
NP0_PRP_AT0
VDI
AVP
0.002 0.003 0.004 0.005 0.006 0.007
Figure 4.11: Relevance profile of the Genre-POS data set determined by GRLVQ using 20 prototypes
(sorted by mean relevance; only 100 of 361 features are shown). The black line shows the mean relevance
determined on the whole data set for 5 repetitions with different random initializations of the prototypes,
the dark shaded area the standard deviation, and the light shaded area the minimum and maximum.
Prototype-Based Relevance Learning
for Genre Classification
44 CHAPTER 4. EMPIRICAL EVALUATION
Relevance
X01_past_tense
X03_present_tense
X05_time_adverbials
X07_second_person_pronouns
X09_pronoun_it
X11_indefinite_pronoun
X13_wh_question
X15_gerunds
X17_agentless_passives
X19_be_main_verb
X21_that_verb_comp
X23_wh_clause
X25_present_participle
X27_past_participle_whiz
X29_that_subj
X31_wh_subj
X33_pied_piping
X35_because
X37_if
X39_prepositions
X41_adj_pred
X43_type_token
X45_conjuncts
X47_hedges
X49_emphatics
X51_demonstratives
X53_modal_necessity
X55_verb_public
X57_verb_suasive
X59_contractions
X62_split_infinitve
X64_phrasal_coordination
X67_neg_analytic
X02_perfect_aspect
X04_place_adverbials
X06_first_person_pronouns
X08_third_person_pronouns
X10_demonstrative_pronoun
X12_proverb_do
X14_nominalization
X16_other_nouns
X18_by_passives
X20_existential_there
X22_that_adj_comp
X24_infinitives
X26_past_participle
X28_present_participle_whiz
X30_that_obj
X32_wh_obj
X34_sentence_relatives
X36_though
X38_other_adv_sub
X40_adj_attr
X42_adverbs
X44_mean_word_length
X46_downtoners
X48_amplifiers
X50_discourse_particles
X52_modal_possibility
X54_modal_predictive
X56_verb_private
X58_verb_seem
X61_stranded_preposition
X63_split_auxiliary
X66_neg_synthetic
0.00 0.01 0.02 0.03 0.04
G
enre, G
R
LV
Q
, 20 P
rototypes
G
enre, S
R
N
G
, 20 P
rototypes
G
enre, G
R
LV
Q
, 50 P
rototypes
G
enre, S
R
N
G
, 50 P
rototypes
G
ender, G
R
LV
Q
, 20 P
rototypes
G
ender, S
R
N
G
, 20 P
rototypes
G
ender, G
R
LV
Q
, 50 P
rototypes
G
ender, S
R
N
G
, 50 P
rototypes
Figure 4.12: Relevance profile of the Biber data set determined by averaging 5 runs of GRLVQ and
SRNG using 20 prototypes for the Genre and Gender classification.
Prototype-Based Relevance Learning
for Genre Classification
Chapter 5
Conclusions
5.1 Summary
In this thesis five prototype-based learning algorithms (LVQ, GLVQ, GRLVQ, SNG, and SRNG)
were applied to three genre classification problems (Genre, Gender, and Santini) using two
different sets of features (Biber and POS) to assess the suitability of these algorithms for text
classification tasks in computational linguistics.
The influence of several parameters on the classification performance of the algorithms was
analyzed, and it was found that for the data set used (Genre-Biber) neither the initial placement
of the prototypes nor the exact choice of the learning rates is crucial to achieve satisfying results
with G(R)LVQ and S(R)NG. This was confirmed by the comparative experiments on the other
data sets, where no further parameter tuning was performed: The support vector machines, which
were coarsely tuned to each data set, performed only 1% to 3% better than the best-performing
prototype-based classifier (GRLVQ in all cases).
In the performed comparison of the algorithms, an advantage of SNG over GLVQ could
not be demonstrated on any of the data sets, but both methods performed much better than
the k-means baseline and standard LVQ. Adapting the metric to the data with GRLVQ and
SRNG was found to significantly improve the performance by around 1% in some but not all
experiments. The results obtained here are thus not as clear-cut as those presented by Hammer
et al. [2005] where SNG was shown to outperform GLVQ and SRNG was shown to outperform
SNG on artificial data sets.
The clustering provided by the prototypes was found to be useful to identify possible sources
of classification errors and to find natural groupings of documents in the data. While the flat
shape of the relevance profiles and the large variability across runs forbid a detailed interpretation
of the determined relevances, the features that are found to be most relevant were also found to
be important in previous research on genre classification. Thus the relevance profiles are found
to be useful (with limitations) to gain insights into what (groups of) features contribute most to
correct classification.
Because of the high classification accuracies that were achieved without the need for excessive
parameter tuning, and the useful further information made available by the classifiers, the
algorithms were found to be very suitable for classification tasks in computational linguistics. In
45
46 CHAPTER 5. CONCLUSIONS
conclusion it can be said that while support vector machines were shown to be the classifier of
choice when accuracy is the only concern, the intuitive nature of the prototype-based methods,
and their ability to directly control the complexity of the model (by the number of prototypes
used) make G(R)LVQ and S(R)NG a valuable alternative especially for exploring unfamiliar
data and for applications where very sparse models are needed.
5.2 Outlook and Future Work
The initial results presented in this thesis for applying prototype-based methods to linguistic
classification problems are promising, so one major direction for further research is the application
of these methods to other classification problems, such as word-sense disambiguation or chunk
parsing. It might further be interesting to explore the use of the algorithms as ‚Äúsemi-supervised‚Äù
clustering methods.
While there are certainly many directions for further research in genre classification on the
one hand, and prototype-based methods on the other hand, I want to emphasize several aspects
that came up during the work on this thesis but could not be explored further:
First, there are several ways to possibly improve both the Biber and the POS feature sets:
The feature extraction process for the Biber data set was based on regular expressions over words
and POS tags for the detection of syntactic patterns. While with this approach the occurrences
of certain syntactic patterns can be determined with quite high precision, there are also patterns
that cannot be detected reliably at all. The quality of the extraction process could be enhanced
by deeper linguistic analysis, e.g. in the form of (chunk) parsing.
As discussed in Section 3.2.2, different methods for selection of POS n-grams as features
can be used. Further research in this direction could explore other methods for selecting ‚Äúgood
n-grams‚Äù, either based on the target classification (e.g. based on mutual information), or based
on characteristics of their distributions.
Another aspect is related to the question how to determine an optimal number of prototypes
per class for a given data set. Further research could explore whether hierarchical clustering,
combined with some stopping criterion (e.g. the one proposed by Jung et al. [2003]), can be used
as a heuristic for this task, and how this compares in performance to methods that adapt the
number of prototypes during training, such as Growing GLVQ [Qin and Suganthan, 2004].
Further work might also be done on the question of how many training epochs are really
necessary and whether techniques such as early stopping [Prechelt, 1996] can be applied. While
for GLVQ and SNG training for more epochs does not hurt the performance, training time could
be reduced significantly if a reliable stopping criterion was found. As the relatively slow training
process is one drawback of the prototype-based methods, investigations on further possibilities
for speed-up, e.g. in the form of more complex data structures or batch training schemes, are
needed. The suitability of other distance measures than the Euclidean distance for linguistic
data should be explored as well.
Overall, the application of prototype-based learning algorithms to classification problems in
computational linguistics is an interesting area for further research.
Prototype-Based Relevance Learning
for Genre Classification
Bibliography
D. Aha, D. Kibler, and M. Albert. Instance-based learning algorithms. Machine Learning, 6(1):
37‚Äì66, 1991.
S. Argamon, M. Koppel, J. Fine, and A. R. Shimoni. Gender, genre, and writing style in formal
written texts. Text & Talk, 23(3):321‚Äì346, 2003.
R. Battiti. Using mutual information for selecting features in supervised neural net learning.
IEEE Transactions on Neural Networks, 5(4):537‚Äì550, 1994.
D. Biber. Variation across speech and writing. Cambridge University Press, Cambridge, UK,
1988.
D. Biber. Using register-diversified corpora for general language studies. Computational Linguis-
tics, 19(2):219‚Äì241, 1994.
D. Biber. Dimensions of register variation. Cambridge University Press, Cambridge, UK, 1995.
D. Blest. A new measure of kurtosis adjusted for skewness. Australian & New Zealand Journal
of Statistics, 45(2):175‚Äì179, 2003.
T. Bojer, B. Hammer, D. Schunk, and K. von Toschanowitz. Relevance determination in learning
vector quantization. In ESANN 2001, 9th European Symposium on Artificial Neural Networks,
Bruges, Belgium, April 25-27, 2001, Proceedings, pages 271‚Äì276, 2001.
L. Burnard. Reference Guide for the British National Corpus (XML Edition), 2007. URL
http://www.natcorp.ox.ac.uk/XMLedition/URG/.
C.-C. Chang and C.-J. Lin. LIBSVM: a library for support vector machines, 2001. Software
available at http://www.csie.ntu.edu.tw/‚àºcjlin/libsvm.
T. Cover and P. Hart. Nearest neighbor pattern classification. IEEE Transactions on Information
Theory, 13(1):21‚Äì27, 1967.
K. Crammer, R. Gilad-Bachrach, A. Navot, and A. Tishby. Margin analysis of the LVQ algorithm.
In Advances in Neural Information Processing Systems, 2002.
B. Hammer and T. Villmann. Generalized relevance learning vector quantization. Neural
Networks, 15(8-9):1059‚Äì1068, 2002.
47
48 BIBLIOGRAPHY
B. Hammer, M. Strickert, and T. Villmann. Supervised neural gas with general similarity measure.
Neural Processing Letters, 21(1):21‚Äì44, 2005.
S. Haykin. Neural Networks: A Comprehensive Foundation, chapter Principal Components
Analysis, pages 396‚Äì404. Prentice Hall, 2nd edition, 1998.
C.-W. Hsu, C.-C. Chang, and C.-J. Lin. A practical guide to support vector classification, 2007.
Paper available at http://www.csie.ntu.edu.tw/‚àºcjlin/papers/guide/guide.pdf.
Y. Jung, H. Park, D. Du, and B. Drake. A decision criterion for the optimal number of clusters
in hierarchical clustering. Journal of Global Optimization, 25(1):91‚Äì111, 2003.
T. Kohonen. Learning vector quantization for pattern recognition. Report TKK-F-A601, Helsinki
University of Technology, Espoo, Finland, 1986.
T. Kohonen. Self-Organizing Maps, volume 30 of Springer Series in Information Sciences.
Springer, Berlin, 1995.
T. Kohonen, J. Hynninen, J. Kangas, J. Laaksonen, and K. Torkkola. LVQ PAK: The Learn-
ing Vector Quantization program package. Report A30, Helsinki University of Technology,
Laboratory of Computer and Information Science, January 1996.
M. Koppel, S. Argamon, and A. Shimoni. Automatically categorizing written texts by author
gender. Literary and Linguistic Computing, 17(4):401, 2002.
D. Lee. Genres, registers, text types, domains, and styles: Clarifying the concepts and navigating
a path through the BNC jungle. Language, Learning & Technology, 5(3), 2001. See also Lee‚Äôs
corpus resources at http://clix.to/davidlee00.
F. Maire, S. Bader, and F. Wathne. Fast indexing of codebook vectors using dynamic binary
search trees with fat decision hyperplanes. Studies in Fuzziness and Soft Computing, 152:
150‚Äì166, 2004.
C. Manning and H. SchuÃàtze. Foundations of Statistical Natural Language Processing. MIT Press,
Cambridge, MA, 1999.
T. Martinetz, S. Berkovich, and K. Schulten. ‚ÄôNeural-gas‚Äô network for vector quantization and
its application to time-series prediction. IEEE Trans. on Neural Networks, 4(4):558‚Äì569, 1993.
T. Mitchell. Machine Learning. McGraw Hill, 1997.
L. Prechelt. Early stopping‚Äìbut when? Neural Networks, pages 55‚Äì69, 1996.
A. Qin and P. Suganthan. Growing generalized learning vector quantization with local neighbor-
hood adaptation rule. In Intelligent Systems, 2004. Proceedings., volume 2, pages 524‚Äì529,
2004.
R Development Core Team. R: A Language and Environment for Statistical Computing. R
Foundation for Statistical Computing, Vienna, Austria, 2007. URL http://www.R-project.
org. ISBN 3-900051-07-0.
Prototype-Based Relevance Learning
for Genre Classification
BIBLIOGRAPHY 49
M. Santini. A shallow approach to syntactic feature extraction for genre classification. Proceedings
of the 7th Annual Colloquium for the UK Special Interest Group for Computational Linguistics,
2004.
M. Santini. Automatic Identification of Genre in Web Pages. PhD thesis, University of Brighton,
Brighton (UK), 2007.
A. Sato and K. Yamada. Generalized learning vector quantization. In David S. Touretzky,
Michael C. Mozer, and Michael E. Hasselmo, editors, Advances in Neural Information Processing
Systems, volume 8, pages 423‚Äì429. The MIT Press, 1996.
S. Seo and K. Obermayer. Soft learning vector quantization. Neural computation, 15(7):1589‚Äì1604,
2003.
E. Stamatatos, N. Fakotakis, and G. Kokkinakis. Text genre detection using common word
frequencies. Proceedings of the 17th conference on computational linguistics, 2:808‚Äì814, 2000.
T. Villmann and B. Hammer. Metric adaptation and relevance learning in learning vector
quantization. Technical report, OsnabruÃàcker Schriften zur Mathematik, Preprint, no. 247,
2003.
T. Villmann, F. Schleif, and B. Hammer. Comparison of relevance learning vector quantization
with other metric adaptive classification methods. Neural Networks, 19(5):610‚Äì622, 2006.
A. Webb. Statistical Pattern Recognition. John Wiley & Sons, 2nd edition, 2002.
Prototype-Based Relevance Learning
for Genre Classification

Appendix A
Data Set Descriptions and Statistics
Genre BNC document IDs
S:conv KB6 KBN KCA KCJ KCU KCY KDD KDS KDT KE4 KP0 KP6 KP7 KST KSV
S:interview FY7 FYG FYJ FYK FYM G4R GYL GYV HDH HE8 HEV HV8 K62 K65 KRR
S:pub debate D91 D92 FMN HVF HVG HVH HVJ HVK J9S J9T J9U J9V JAC JAD KM7
S:speech:scripted D8Y D90 FUM FUN FUR HDS HDT HDU HLU HLW HLX HLY HUC KM0 KS5
W:ac:tech engin BP2 CA4 CG7 CG9 CGA CHF FE6 FNR FPG G3N H0U H7R HGR HX9 K90
W:advert AMW B27 BNX CFM CFN EC4 ECS ED1 EEG EEJ HBT HSW HT1 HT7 J5C
W:biography A7C ANR B3H CA6 CEE CES CH8 CKR CL2 EVH FS0 FTW GT7 GT8 HRB
W:instructional A16 ABB AM5 B26 C8P CCX CCY CG5 CLG ECJ EFH HGW HH6 HKL J11
W:pop lore A0G A7D ACN ACP ARJ BNT CAD CB2 CFG CGC CHB CJC CKA CKG CR7
W:newsp:brdsht nat:report A1G A2X A3U A49 A4N A4X A57 A5R A7V A8K A9M A9V AA4 AA5 AAK
Table A.1: Document IDs of the documents selected in each genre for the Santini classification.
51
52 APPENDIX A. DATA SET DESCRIPTIONS AND STATISTICS
Genre Author Gender
female male
W:ac:humanities arts 8 65
W:ac:medicine 0 2
W:ac:nat science 1 19
W:ac:polit law edu 10 74
W:ac:soc science 22 71
W:ac:tech engin 1 15
W:admin 0 1
W:biography 41 52
W:commerce 3 54
W:essay:school 4 2
W:essay:univ 2 0
W:fict:drama 1 1
W:fict:poetry 11 17
W:fict:prose 206 203
W:institut doc 0 2
W:instructional 2 2
W:letters:personal 3 1
W:misc 41 121
W:non ac:humanities arts 17 76
W:non ac:medicine 4 3
W:non ac:nat science 1 27
W:non ac:polit law edu 4 38
W:non ac:soc science 24 45
W:non ac:tech engin 0 2
W:pop lore 2 5
W:religion 6 22
Table A.2: Number of documents in each genre by author gender. Genres that contain no documents
for which the author gender is known are not shown.
Prototype-Based Relevance Learning
for Genre Classification
53
0.00 0.01 0.02 0.03 0.04
0
40
80
12
0
PRP_AT0_NN1
Relative Frequency
# 
D
oc
um
en
ts
0.00 0.01 0.02 0.03
0
20
40
60
80
AT0_AJ0_NN1
Relative Frequency
# 
D
oc
um
en
ts
0.000 0.005 0.010 0.015 0.020 0.025 0.030 0.035
0
20
60
10
0
AT0_NN1_PRF
Relative Frequency
# 
D
oc
um
en
ts
0.000 0.005 0.010 0.015 0.020
0
50
10
0
15
0
AJ0_NN1_PUN
Relative Frequency
# 
D
oc
um
en
ts
0.000 0.005 0.010 0.015 0.020
0
50
10
0
15
0
AT0_NN1_PUN
Relative Frequency
# 
D
oc
um
en
ts
0.00 0.01 0.02 0.03 0.04
0
50
10
0
15
0
NN1_PRP_AT0
Relative Frequency
# 
D
oc
um
en
ts
0.000 0.005 0.010 0.015 0.020
0
50
10
0
15
0
PRP_AT0_AJ0
Relative Frequency
# 
D
oc
um
en
ts
0.000 0.005 0.010 0.015 0.020
0
50
15
0
25
0
AT0_NN1_PRP
Relative Frequency
# 
D
oc
um
en
ts
0.000 0.005 0.010 0.015 0.020
0
50
10
0
15
0
NN1_PRF_AT0
Relative Frequency
# 
D
oc
um
en
ts
0.000 0.005 0.010 0.015
0
50
10
0
PUN_AT0_NN1
Relative Frequency
# 
D
oc
um
en
ts
Figure A.1: Histograms of the distributions of the 10 most frequent POS trigrams across documents.
Prototype-Based Relevance Learning
for Genre Classification
54 APPENDIX A. DATA SET DESCRIPTIONS AND STATISTICS
0.0000 0.0005 0.0010 0.0015
0
40
00
80
00
VHD_VVN_PRP
Relative Frequency
# 
D
oc
um
en
ts
0.000 0.001 0.002 0.003 0.004 0.005 0.006
0
40
0
80
0
12
00
CJS_PNP_VVD
Relative Frequency
# 
D
oc
um
en
ts
0.000 0.002 0.004 0.006 0.008
0
40
0
80
0
12
00
PUN_PNP_VBD
Relative Frequency
# 
D
oc
um
en
ts
0.000 0.005 0.010 0.015 0.020 0.025
0
40
0
80
0
NN1_PUN_PUQ
Relative Frequency
# 
D
oc
um
en
ts
0.000 0.002 0.004 0.006 0.008
0
50
0
10
00
15
00
PNP_VVD_PRP
Relative Frequency
# 
D
oc
um
en
ts
0.000 0.001 0.002 0.003 0.004 0.005
0
10
00
20
00
PNP_VHD_VVN
Relative Frequency
# 
D
oc
um
en
ts
0.000 0.001 0.002 0.003 0.004 0.005 0.006 0.007
0
50
0
10
00
NN2_PRP_NP0
Relative Frequency
# 
D
oc
um
en
ts
0.000 0.001 0.002 0.003 0.004 0.005 0.006
0
10
00
20
00
NN2_PUN_PUQ
Relative Frequency
# 
D
oc
um
en
ts
0.000 0.002 0.004 0.006 0.008
0
20
0
60
0
PRF_AJ0_NN1
Relative Frequency
# 
D
oc
um
en
ts
0.0000 0.0005 0.0010 0.0015 0.0020 0.0025
0
20
00
40
00
CJS_PNP_VHD
Relative Frequency
# 
D
oc
um
en
ts
Figure A.2: Histograms of the distributions of the 10 POS trigrams with the highest skewness/kurtosis
ratio across documents.
Prototype-Based Relevance Learning
for Genre Classification
55
0.00 0.01 0.02 0.03 0.04 0.05 0.06
0
20
0
60
0
PUN_ITJ_PUN
Relative Frequency
# 
D
oc
um
en
ts
0.00 0.01 0.02 0.03
0
20
40
60
80
AT0_AJ0_NN1
Relative Frequency
# 
D
oc
um
en
ts
0.000 0.005 0.010 0.015 0.020 0.025 0.030 0.035
0
20
60
10
0
AT0_NN1_PRF
Relative Frequency
# 
D
oc
um
en
ts
0.00 0.01 0.02 0.03 0.04
0
40
80
12
0
PRP_AT0_NN1
Relative Frequency
# 
D
oc
um
en
ts
0.000 0.005 0.010 0.015 0.020 0.025
0
40
0
80
0
12
00
PUN_PUQ_PNP
Relative Frequency
# 
D
oc
um
en
ts
0.000 0.005 0.010 0.015 0.020 0.025
0
40
0
80
0
NN1_PUN_PUQ
Relative Frequency
# 
D
oc
um
en
ts
0.000 0.005 0.010 0.015 0.020
0
50
10
0
15
0
AT0_NN1_PUN
Relative Frequency
# 
D
oc
um
en
ts
0.000 0.005 0.010 0.015 0.020
0
50
10
0
15
0
AJ0_NN1_PUN
Relative Frequency
# 
D
oc
um
en
ts
0.000 0.005 0.010 0.015 0.020
0
50
10
0
15
0
NN1_PRF_AT0
Relative Frequency
# 
D
oc
um
en
ts
0.00 0.01 0.02 0.03 0.04
0
50
10
0
15
0
NN1_PRP_AT0
Relative Frequency
# 
D
oc
um
en
ts
Figure A.3: Histograms of the distributions of the 10 POS trigrams with the highest variance across
documents.
Prototype-Based Relevance Learning
for Genre Classification

Appendix B
Results: Additional Figures
10 20 50 100 200 500 1000
0.
78
0.
80
0.
82
0.
84
0.
86
0.
88
0.
90
Iterations
A
cc
ur
ac
y
Œµ+ = 1
Œµ+ = 0.8
Œµ+ = 0.6
Œµ+ = 0.4
Œµ+ = 0.2
Œµ+ = 0.1
Œµ+ = 0.01
Accuracy on training set
Accuracy on testing set
SNG, Genre‚àíBiber, 20 Prototypes per Class
Figure B.1: Learning curves for SNG with varying learning rates + on the Genre-Biber data set
using 20 prototypes per class. The negative learning rate ‚àí was fixed at 0.05 except for + = 0.01, where
‚àí = 0.005. Prototypes were initialized from data points, no learning rate decay was used.
57
58 APPENDIX B. RESULTS: ADDITIONAL FIGURES
0 200 400 600 800 1000
0.
84
5
0.
85
0
0.
85
5
0.
86
0
0.
86
5
0.
87
0
0.
87
5
Iterations
A
cc
ur
ac
y
Genre‚àíBiber, 5 Prototypes
GLVQ
GRLVQ
SNG
SRNG
Accuracy on training set
Accuracy on testing set
0 200 400 600 800 1000
0.
84
0.
85
0.
86
0.
87
Iterations
A
cc
ur
ac
y
Genre‚àíPOS, 5 Prototypes
GLVQ
GRLVQ
SNG
SRNG
Accuracy on training set
Accuracy on testing set
Figure B.2: Learning curves on the Genre-Biber (top) and Genre-POS (bottom) data set for
G(R)LVQ and S(R)NG with 5 prototypes per class.
Prototype-Based Relevance Learning
for Genre Classification
59
0 200 400 600 800 1000
0.
84
0.
85
0.
86
0.
87
0.
88
0.
89
0.
90
Iterations
A
cc
ur
ac
y
Genre‚àíBiber, 10 Prototypes
GLVQ
GRLVQ
SNG
SRNG
Accuracy on training set
Accuracy on testing set
0 200 400 600 800 1000
0.
84
0.
86
0.
88
0.
90
0.
92
0.
94
0.
96
Iterations
A
cc
ur
ac
y
Genre‚àíBiber, 50 Prototypes
GLVQ
GRLVQ
SNG
SRNG
Accuracy on training set
Accuracy on testing set
Figure B.3: Learning curves on the Genre-Biber data set for G(R)LVQ and S(R)NG with 10 (top)
and 50 (bottom) prototypes per class.
Prototype-Based Relevance Learning
for Genre Classification
60 APPENDIX B. RESULTS: ADDITIONAL FIGURES
0 200 400 600 800 1000
0.
86
0.
87
0.
88
0.
89
0.
90
Iterations
A
cc
ur
ac
y
Genre‚àíPOS, 10 Prototypes
GLVQ
GRLVQ
SNG
SRNG
Accuracy on training set
Accuracy on testing set
0 200 400 600 800 1000
0.
86
0.
88
0.
90
0.
92
0.
94
0.
96
Iterations
A
cc
ur
ac
y
Genre‚àíPOS, 50 Prototypes
GLVQ
GRLVQ
SNG
SRNG
Accuracy on training set
Accuracy on testing set
Figure B.4: Learning curves on the Genre-POS data set for G(R)LVQ and S(R)NG with 10 (top) and
50 (bottom) prototypes per class.
Prototype-Based Relevance Learning
for Genre Classification
61
Relevance
X43_type_token
X29_that_subj
X16_other_nouns
X45_conjuncts
X07_second_person_pronouns
X39_prepositions
X56_verb_private
X51_demonstratives
X67_neg_analytic
X64_phrasal_coordination
X31_wh_subj
X27_past_participle_whiz
X59_contractions
X25_present_participle
X37_if
X61_stranded_preposition
X11_indefinite_pronoun
X24_infinitives
X19_be_main_verb
X21_that_verb_comp
X49_emphatics
X13_wh_question
X34_sentence_relatives
X35_because
X46_downtoners
X20_existential_there
X26_past_participle
X23_wh_clause
X66_neg_synthetic
X58_verb_seem
X47_hedges
X38_other_adv_sub
X62_split_infinitve
X01_past_tense
X03_present_tense
X44_mean_word_length
X14_nominalization
X08_third_person_pronouns
X40_adj_attr
X12_proverb_do
X42_adverbs
X09_pronoun_it
X33_pied_piping
X05_time_adverbials
X54_modal_predictive
X55_verb_public
X52_modal_possibility
X17_agentless_passives
X06_first_person_pronouns
X57_verb_suasive
X04_place_adverbials
X02_perfect_aspect
X50_discourse_particles
X53_modal_necessity
X41_adj_pred
X32_wh_obj
X10_demonstrative_pronoun
X30_that_obj
X48_amplifiers
X15_gerunds
X18_by_passives
X63_split_auxiliary
X28_present_participle_whiz
X22_that_adj_comp
X36_though
0.000 0.005 0.010 0.015 0.020 0.025 0.030 0.035
Figure B.5: Relevance profile (sorted by mean relevance) of the Genre-Biber data set determined
by GRLVQ using 20 prototypes. The black line shows the mean relevance determined by 5-fold cross-
validation, the dark shaded area the standard deviation, and the light shaded area the minimum and
maximum. The blue dashed line shows the relevances as determined on the whole data set. The grey
dashed line indicates the initial uniform distribution.
Prototype-Based Relevance Learning
for Genre Classification

Appendix C
libgrlvq README
libgrlvq -- Generalized Relevance Learning Vector Quantization Library
=========================================================================
### Copyright (C) 2007 Jan A. Gasthaus (jgasthau@uos.de) ###
URL: http://www.ikw.uos.de/~CL/theses/gasthaus2007/
OVERVIEW
--------
libgrlvq is a Java library implementing several prototype-based machine
learning algorithms, including Learning Vector Quantization (LVQ),
Generalized (Relevance) Learning Vector Quantization (G(R)LVQ), as well
Supervised (Relevance) Neural Gas (S(R)NG).
The library consists of two parts: The library itself, which is documented
in the Javadoc comments within the source files, and a command line
interface to the library (called "Experimenter") which is documented here.
QUICK START
-----------
The library comes with a compiled and packaged version of the Experimenter
in the file experimenter.jar. Experimenter has sensible defaults for
almost all of its numerous parameters, making running a first experiment
(using the Biber-Genre data set available in datasets.tar.bz2) as easy
as calling
#> java -jar experimenter.jar -f data/biber_genre/biber.genre.tab
This will run a 5-fold cross-validation experiment, using the GLVQ update
63
64 APPENDIX C. LIBGRLVQ README
rule and 10 prototypes per class. When finished, the classification
accuracies in each of the five runs will be shown. Additionally, several
files will have been written to several folders within the current
directory. The output/ directory contains a file with the cross-validation
accuracies, the prototypes/ directory contains a file for each run
containing the prototypes, and the relavances/ directory contains a file
with the relevance terms for each run (however, since by default the
squared euclidean metric is used, no relevances will be there).
The output depends on which of the three different modes the Experimenter
is run in (controlled by the -m option):
-m cross-validate The default, runs a cross-validation experiment.
The number of cross-validations can changed using
the -k option. Writes to output/, prototypes/, and
relevances/.
-m train Trains a model on the whole data set and writes
several model statistics to files. Writes a model
to the models/ directory, and additionally writes
information about the model to the prototypes/,
ranks/, protodist/, and relevances/ directories.
-m learning-curve Splits the data in a 2:1 into a training and
testing set and evaluates the performance after
each 10th iteration. The results are written to
a file in the output/ folder.
Output files are named according to the parameter settings in order to
be able to distinguish them when multiple experiments are performed.
The available command line parameters that describe the experiment are
shown below.
PARAMETERS
----------
-f, --filename
File name of the tab-separated data file. See below for a description
of the required format. This is the only mandatory option.
-m, --mode
Experimenter mode. See the description above.
Prototype-Based Relevance Learning
for Genre Classification
65
Possible values: cross-validate, train, learning-curve
Default value: cross-validate
-a, --algorithm
Learning algorithm.
Possible values: none, lvq, glvq, grlvq, sng, srng
Default value: glvq
-i, --init
Initialization method.
Possible values: data, kmeans
Default value: kmeans
-d, --distance
Distance measure.
Possible values: euclidean, weighted
Default value: euclidean
Note: Using "weighted" is only sensible if the learning
algorithm is either GRLVQ or SRNG.
-k, --cross-validations
Number of cross-validations.
Possible values: Integers > 1
Default value: 5
Note: This parameter only makes sense with
"--mode cross-validate".
-p, --prototypes
Number of prototypes per class.
Possible values: Integers > 0
Default value: 0
Note: The number of prototypes should not exceed the number
training examples in any class. If it does, a warning
will be issued.
--max-iterations
Maximum/total number of updates to be performed.
Possible values: Integers > 0
Default value: 1000
Note: As currently no method for early stopping is
implemented, the maximal number of iterations is
always performed. Thus this parameter effectively
controls the number of performed update steps.
Prototype-Based Relevance Learning
for Genre Classification
66 APPENDIX C. LIBGRLVQ README
-ep, -en, -eg, -el
The positive, negative, gamma, and relevance learning rates.
Possible values: Real numbers > 0
Default values: The default values depend on the update rule used.
Note: See the source code of the update rules for details.
--decay
Learning rate decay.
Possible values: none, linear, exponential
Default value: linear
Note: The behavior of "linear" is controlled by
--linear-decay-endval, while "exponential" obeys the
parameter --exponential-decay-const. See below.
Learning rate decay only applies to the learning rates
-ep and -en (not -eg or -el).
--linear-decay-endval
The end value of the learning rate in linear decay mode.
Possible values: Real numbers > 0
Default value: 0.01
Note: This value should be smaller than the initial values
of -ep and -en. It specifies the end value after
--max-iterations performed updates.
--exponential-decay-const
The multiplicative constant used in exponential decay.
Possible values: Real numbers 0 < x < 1
Default value: 0.99
Note: The learning rates -ep and -en are multilpied with
this constant after every update step if
"--decay exponential" is used.
--gamma-decay-const
Factor with which the neighborhood cooperativity (gamma) is mulitplied
after each update.
Possible Values: Real numbers 0 < x < 1
Note: This only has an effect for SNG and SRNG.
--pretrain
Number of steps to train before starting to adapt the relevances.
Possible values: 0 .. max-iterations
Default value: 0.2 * max-iterations
Prototype-Based Relevance Learning
for Genre Classification
67
--batch
Flag indicating batch operation; disables all messages except final
output. This is useful when the results are gathered from standard
output by some script running multiple experiments. This only makes
sense if "--mode cross-validate", since in the other modes the results
are written to files anyway.
--seed
The seed for the random number generator.
Possible values: Integers
Default value: 2342
--idx-column, --label-column, --no-header
Options controlling the how the input file is interpreted. See below.
INPUT FILE FORMAT
-----------------
Experimenter uses input files in tab-separated format, i.e. files that
contain one instance per line, with fields seperated by a tab-character.
The first line can contain variable names (if not, "--no-header" has to be
given). By default, the first column is thought to contain a unique ID
for each instance, and the last column is tought to contain the class label.
If the ID or class label are in different columns, the options
"--idx-column" and "--label-column" can be given.
The ID column may contain arbitrary strings, as long as they don‚Äôt contain
tab-characters.
The labels have to be integers ranging from 0 to #classes-1. For a
two-class problem they thus should be 0 and 1, for a four-class
problem 0, 1, 2, and 3, and so on.
ATTENTION: Not obeying this may lead to unexpected behavior.
All the other columns should contain values that can be parsed into
floating point numbers using the Java Double.parseDouble function.
In R (which was used to prepare the data files), a file in this format can
be written by a call to
write.table(cbind(x,label),
file="filename.tab",
Prototype-Based Relevance Learning
for Genre Classification
68 APPENDIX C. LIBGRLVQ README
sep="\t",
row.names=T,
col.names=T,
quote=F)
if x is a data frame containing columns of numeric variables and row.names
set to the IDs of the instances, and label is a column vector containing
the numeric label for each instance. The resulting file will contain a
header with one field less than data columns (for the ID) -- Experimenter
will detect this and handle it correctly. The advantage of using this
format is that the R function read.delim will read it correctly without
the need to specify further options.
RECOMPILING EXPERIMENTER
------------------------
Because the library does not have any external dependencies, recompiling
it from the source it straight-forward.
A call to
#> javac -cp . libgrlvq/Experimenter.java
within the src/ directory should compile the Experimenter and all library
files it depends on into Java .class files. The Experimenter can then
either be run directly with
#> java -cp . libgrlvq.Experimenter
or can be package into a .jar file with
#> jar cfm experimenter.jar libgrlvq/Manifest.txt libgrlvq/*.class
which can be used as described above.
It is also possible to compile the Experimenter into native code using the
GNU Java Compiler (gcj). This has been tried with gcj version 4.1.2 and
can lead to significant gains in performance.
From within the src/libgrlvq/ directory, call
#> gcj -c -g *.java
Prototype-Based Relevance Learning
for Genre Classification
69
to compile the source into object code which can then be linked using
# gcj -o experimenter --main=libgrlvq.Experimenter *.o
Further options can be given to the compiler to improve perfomance. The
ones used were:
-O3 -fno-bounds-check -fno-store-check -funroll-loops
-fomit-frame-pointer -march=pentium4 -mfpmath=sse -malign-double
which creates highly optimized code for the Pentium 4 platform. With this
optimizations the resulting excutable runs about 3 times faster than the
bytecode version on the testing system. Your milage may vary, however.
TOOLS
-----
Two scripts are provided in the tools/ directory that help performing
grid-search and other batch experiments.
runExperiments.py can be used to run Experimenter with many different
parameter combinations (read from a file) on several computers.
createExperiments.py can create files with parameters combinations
that can be used by runExperiments.py.
See the source code of both scripts for details.
Prototype-Based Relevance Learning
for Genre Classification

Proclamation
Hereby I confirm that I wrote this thesis independently and that I have not made use of any
other resources or means than those indicated.
OsnabruÃàck, September 11th, 2007
(Jan A. Gasthaus)
