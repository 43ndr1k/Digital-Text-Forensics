Multimodal Interactive Transcription of Ancient
Text Images
Verónica Romero, Joan Andreu Sánchez,
Alejandro H. Toselli, and Enrique Vidal
Instituto Tecnológico de Informática (ITI),
Universidad Politécnica de Valencia, Spain
{vromero,jandreu,ahector,evidal}@iti.upv.es
Abstract. The amount of digitized legacy documents has been rising
dramatically over the last years due mainly to the increasing number of
on-line digital libraries publishing this kind of documents. On one hand,
the vast majority of these documents remain waiting to be transcribed
into a textual electronic format (such as ASCII or PDF) that would
provide historians and other researchers new ways of indexing, consult-
ing and querying these documents. On the other hand, in some cases,
adequate transcriptions of the handwritten text images are already avail-
able. This drives an increasing need to align images and transcriptions in
order to make it more comfortable the consulting of these documents. In
this work two systems are presented to deal with these issues. The first
one aims at transcribing these documents using a interactive-predictive
approach, which integrates user corrective-feedback actions in the proper
recognition process. The second one presents an alignment method based
on the Viterbi algorithm to find mappings between word images of a given
handwritten document and their respective (ASCII) words on its given
transcription.
Keywords: Handwritten text recognition, Multimodal interactive
framework, Viterbi alignment.
1 Introduction
The task of transcribing old handwritten documents is becoming an important
research topic, specially because of the increasing number of on-line digital li-
braries publishing large quantities of digitized legacy documents. The vast ma-
jority of these documents, hundreds of terabytes worth of digital image data,
remain waiting to be transcribed into a textual electronic format (such as ASCII
or PDF) that would provide researchers and general public new ways of indexing,
consulting and querying these documents.
The transcription of handwritten text in these documents is usually carried
out by experts in paleography, who are specialized in reading ancient scripts,
characterized, among other things, by different calligraphy/print styles from di-
verse places and time periods. In general, this is a slow and very expensive
activity.
C. Grana and R. Cucchiara (Eds.): MM4CH 2011, CCIS 247, pp. 63–73, 2012.
c© Springer-Verlag Berlin Heidelberg 2012
64 V. Romero et al.
Up-to-date handwritten text recognition (HTR) systems are not accurate
enough to substitute the experts in these transcription tasks. The variability of
the handwriting, the complexity of the styles and the open vocabulary, explain
most of the difficulties encountered by these recognition systems [12,17,7,3]. In
order to produce adequately good transcriptions using these systems, once the
full recognition process of one document has finished, heavy human expert revi-
sion is required to really produce a transcription of standard quality. The human
transcriber is then responsible for verifying and correcting the mistakes made
by the system. Given the high error rates involved, such a post-editing solution
is quite inefficient and uncomfortable for the human corrector.
An effective alternative to post-editing is an interactive approach called “Mul-
timodal Computer Assisted Transcription of Text Images” (MM-CATTI) pro-
posed in [15]. Here, the automatic HTR system and the human transcriber
cooperate to generate the final transcription, thereby combining the accuracy
provided by the human operator with the efficiency of the HTR system. The
implemented MM-CATTI system is presented in section 2.
While the vast majority of legacy documents are currently only available in the
form of digital images, for a significant amount of these documents (manually
produced) transcriptions are already available. In these cases, digital libraries
typically offer both the original images and the corresponding transcriptions.
Generally speaking, most documents have transcriptions aligned only at the
page level (not at the level of individual text lines or words), which renders
the visualization and consulting of these documents rather uncomfortable for
the historians and general public1. This fact has suggested the development of
automatic alignment techniques which generate a mapping between each line
and word on a document image and its respective line and word on its electronic
transcription [16,13]. These alignments can help quickly locating text images
while reading a transcription, with useful applications to editing, indexing, etc.
In the opposite direction, the alignment is useful for people trying to read the
text image directly, when arriving to complex or damaged parts of the document.
Section 3 presents an image-transcription alignment system which carries out
this task.
2 Multimodal Computer Assisted Transcription of
Handwritten Text Images
When transcribing a document image, usually a post-edition process is carried
out when error-free transcriptions are expected. An interactive on-line scenario
has been presented in previous works [15] as an alternative to post-editing. This
scenario is called “Computer Assisted Transcription of Handwritten Text Im-
ages” (CATTI) and, rather than full automation, aims at assisting the human in
the proper recognition-transcription process; that is, to facilitate and to speed
up the transcription task of handwritten texts.
1 See for example http://darwin-online.org.uk/manuscripts.html
Multimodal Interactive Transcription of Ancient Text Images 65
In the CATTI framework, the user is involved in the transcription process
since she is responsible of validating and/or correcting the Handwritten Text
Recognition (HTR) output [15]. The protocol that rules this process can be
formulated in the following steps, which are iterated until a correct transcription
is obtained (see Figure 1):
(a) The HTR system proposes a full transcription (ŝ) of an input handwritten
text line image. (b) The user validates the longest prefix of ŝ which is error-free
and enters some keystrokes (word v) to correct the first error in the suffix. (c) An
extended correct prefix (p) is produced based on the previously validated prefix
and the corrections made by the user. (d) Using this new prefix, the system
suggests a suitable continuation (ŝ) and the process goes on from (b) above.
In order to improve human transcriber productivity and to make the previ-
ous defined protocol friendlier for the user, “pure” mouse action feedback was
studied in detail in [11]. As soon as the user points to the next system error, the
system proposes a new, hopefully more correct continuation, thereby trying to
anticipate the intended user correction. This way, many explicit user corrections
are avoided, saving significant amounts of expected human effort.
Furthering the goal of making the iteration process friendlier to the user led
us to the development of Multimodal CATTI (MM-CATTI) [15,14]. Traditional
peripherals like keyboard and mouse are used in CATTI to unambiguously pro-
vide the feedback associated with the validation and correction of the successive
system predictions. Nevertheless, using more ergonomic multimodal interfaces
x
INTER-0 p
ŝ ≡ ŵ antiguos cuidadores que en el Castillo sus llamadas
p′ antiguos
INTER-1 v ciudadanos
p antiguos ciudadanos
ŝ que en el Castillo sus llamadas
p′ antiguos ciudadanos que en
INTER-2 v Castilla
p antiguos ciudadanos que en Castilla
ŝ se llamaban
FINAL v #
p ≡ t antiguos ciudadanos que en Castilla se llamaban
Fig. 1. Example of CATTI interaction to transcribe an image of the Spanish sentence
“antiguos ciudadanos que en Castilla se llamaban”. Initially the prefix p is empty, and
the system proposes a complete transcription ŝ ≡ ŵ of the input image x. In each
interaction step the user reads this transcription, accepting a prefix p′ of it. Then, he
or she types in some word, v, to correct the erroneous text that follows the validated
prefix, thereby generating a new prefix p (the accepted one p′ plus the word v added
by the user). At this point, the system suggests a suitable continuation ŝ of this prefix
p and this process is repeated until a complete and correct transcription of the input
signal is reached. In the final transcription, T , the underlined boldface words are the
words typed by the user. In this example the estimated post-editing effort is 5/7 (71%),
while the corresponding interactive estimate is 2/7 (29%). This results in an estimated
effort reduction of 59%.
66 V. Romero et al.
Fig. 2. Left: illustration of CATTI multimodal user-interaction using a touch-screen.
Right: page fragment showing a line image being processed.
should result in an easier and more comfortable human-computer interaction,
at the expense of the feedback being less deterministic to the system. This is
the idea underlying MM-CATTI, which focus on touchscreen communication,
perhaps the most natural modality to provide the required feedback in CATTI
systems. It is worth noting, however, that the use of this more ergonomic feed-
back modality comes at the cost of new, additional interaction steps needed to
correct possible feedback decoding errors. Therefore, solving the multimodal in-
teraction problem amounts to achieving a modality synergy where both main
and feedback data streams help each-other to optimize overall accuracy. Several
experiments carried out in [15] show that the number of additional interaction
steps is kept very small thanks to the MM-CATTI ability to use interaction-
derived constraints to considerably improve the on-line HTR feedback decoding
accuracy. More specifically, MM-CATTI feedback decoding accuracy increases
by around 20% with respect to using just a conventional, off-the-shelf on-line
HTR decoder for the correction steps.
Figure 2 (left) shows a user interacting with the MM-CATTI system by means
of a touchscreen. Both the original image and the system’s transcription hypothe-
ses can be easily aligned and jointly displayed on the touchscreen (Figure 2,
right). A publicly available2 web-based demonstrator of this system has recently
been presented in [9]. It provides a socket based, client-server multiuser environ-
ment, where several users across the globe can work concurrently on the same
task. In addition, the web server and the MM-CATTI engine do not need to be
physically at the same place. To work with the demonstrator, first a document
and a page to transcribe are selected. Then, the user transcribes the handwritten
text images line by line, using the keyboard and mouse to make corrections. If
an e-pen is available, the MM-CATTI engine additionally uses its on-line HTR
feedback decoder to recognize the user corrective pen-strokes. Then, taking into
account the (multimodal) user corrections, the MM-CATTI engine responds with
a suitable continuation to the prefix validated by the user. Figure 3 shows dif-
ferent MM-CATTI e-pen interaction gestures: the user can explicitly write the
correct word, make a diagonal line to delete an erroneous word, make a vertical
2 See catti.iti.upv.es
Multimodal Interactive Transcription of Ancient Text Images 67
Fig. 3. Four interaction gestures to generate and/or validate an error-free prefix. From
left to right, top to bottom: substitution, single click validation, deletion, and insertion.
line followed by text to be inserted, or make a single click to ask for another
suitable continuation.
3 Aligning Text-Images and Transcriptions
As mentioned in the introduction, several handwritten documents include both,
the handwritten material and its proper transcription (in ASCII or PDF for-
mat). This fact has motivated the development of methodologies to align these
documents and their transcriptions, i.e. to generate a mapping between each
word image on a document page with its respective ASCII word on its tran-
scription [13].
Two different levels of alignment can be defined: line level and word level.
Line alignments attempt to obtain beginning and end positions of lines in tran-
scribed pages that do not have synchronized line breaks. This information al-
lows users to easily visualize the page image documents and their corresponding
transcriptions. Moreover, using these alignments as segmentation ground truth,
large amounts of training and test data for segmentation-free cursive handwrit-
ing recognition systems become available. On the other hand, word alignments
allow users to easily find the place of a word in the manuscript when reading
the corresponding transcription. For example, one could display both the hand-
written page and the transcription and whenever the mouse is held over a word
in the transcription, the corresponding word in the handwritten image would be
outlined using a box. In a similar way, whenever the mouse is held over a word
in the handwritten image, the corresponding word in the transcription would be
highlighted (see figure 4).
68 V. Romero et al.
Fig. 4. Screen-shot of the alignment prototype interface displaying an outlined word
(using a box) in the manuscript (left) and the corresponding highlighted word in the
transcription (right)
Creating such alignments is challenging since the transcription is an ASCII
text file while the manuscript page is an image. The alignment method im-
plemented here (henceforward called Viterbi alignment), relies on the Viterbi
decoding approach to Handwritten Text Recognition (HTR) based on Hidden
Markov Models (HMMs) [1,12]. These techniques are based on methods origi-
nally introduced for speech recognition [2]. In such HTR systems, the alignment
is actually a byproduct of the proper recognition process, i.e. an implicit seg-
mentation of each text image line is obtained where each segment successively
corresponds to one recognized word. In our case, word recognition is not actually
needed, as we do already have the correct transcription. Therefore, to obtain the
segmentations for the given word sequences, the so-called “forced-recognition”
approach is employed, which consists in recognizing an imposed known sequence
of words and get in this way their underlying segmentations.
4 HTR Technology Overview
The implementation of the systems described in this work involved three common
different parts: document image preprocessing, line image feature extraction and
Hidden Markov Model training/decoding.
Multimodal Interactive Transcription of Ancient Text Images 69
It is quite common for ancient documents to suffer from degradation problems.
Among these are the presence of smear, background of big variations and uneven
illumination, spots due to the humidity or marks resulting from the ink that goes
through the paper (generally called bleed-through). In addition, other kinds of
difficulties appear in these pages as different font types and sizes in the words,
underlined and/or crossed-out words, etc. The combination of these problems
contributes to make the recognition process difficult, therefore a preprocessing
module becomes essential.
Concerning the preprocessing module used in this work, the following steps
take place: first, skew correction is carried out on each document page image;
then background removal and noise reduction is performed by applying adequate
filters [4] on the whole page image. Next, a text line extraction process based on
local minima of the horizontal projection profile of the page image, divides the
page into separate line images [8,6]. In addition, connected components are used
to solve the situations where local minima alone do not allow to obtain a clear
text line separation. Finally, slant correction and non-linear size normalization
are applied [12] on each extracted line image.
As our alignment and recognition system is based on Hidden Markov Models
(HMMs), each preprocessed text line image has to be represented as a sequence
of feature vectors. To do this, the feature extraction module applies a grid to
divide the text line image into squared cells. From each cell, three features are
calculated: normalized gray level, horizontal gray level derivative and vertical
gray level derivative. The way these three features are determined is described
in [12]. Columns of cells or frames are processed from left to right and a feature
vector is constructed for each frame by stacking the three features computed
in its constituent cells. Hence, at the end of this process, a sequence of 60-
dimensional feature vectors (20 normalized gray-level components, 20 horizontal
and 20 vertical derivatives components) is obtained. An example of feature vec-
tors sequence, representing an image of the Spanish word “Castilla” is shown in
the upper part of figure 5.
Characters are modeled by continuous density left-to-right HMMs, with 6
states and 64 Gaussian mixture components per state. This topology (number
of HMM states and Gaussian densities per state) was determined by tuning
empirically the system on several corpora. Once a HMM “topology” has been
adopted, the model parameters can be easily trained from images of continuously
handwritten text lines (without any kind of word or character segmentation) ac-
companied by the transcription of these images into the corresponding sequence
of characters. This training process is carried out using a well known instance
of the EM algorithm called forward-backward or Baum-Welch re-estimation [2].
Figure 5 (bottom) shows an example of HMM character modeling.
Each lexical word is modelled by a stochastic finite-state automaton which
represents all possible concatenations of individual characters that may compose
the word. On the other hand, text line sentences are modelled using backoff bi-
grams, with Kneser-Ney back-off smoothing [5], which are estimated using the
given transcriptions of the trained set.
70 V. Romero et al.
0.7
0.3
0.8
0.2
0.9
0.1
0.8
0.2
0.8
0.2
Fig. 5. Example of 5-states HMM modelling (sequences of feature vectors of) instances
of the character “a” within the Spanish word “Castilla”. The states are shared among
all in- stances of characters of the same class. The zones modelled by each state show
graphically subsequences of feature vectors (see details in the magnifying-glass view)
compounded by stacking the normalized grey level and its both derivatives features.
All these finite-state (HMM character, word and sentence) models can be
easily integrated into a single global model on which the search for decoding
the input feature vectors sequence into the output words sequence is performed.
This search is optimally done by using the Viterbi algorithm [2], which provides
detailed word alignment information as a byproduct.
5 Evaluation Results
MM-CATTI: Experiments were carried out on several corpora [15,14,10]. Ac-
cording to the results of these experiments, when CATTI is compared with to-
tally manual transcription, the estimated user effort reductions range from 68%
to 80%. And, if compared with post-editing the results of a totally automatic
HTR output, the expected effort savings range from 5% to 23%.
For a typical transcription task, this means that to produce 100 words of
a correct transcription, a CATTI user should have to type only less than 20
words; the remaining 80 are automatically predicted by CATTI, thereby saving
a considerable amount of the (typing and, in part thinking) effort needed to
produce all the text manually. On the other hand, when compared with post-
editing, from every 100 (non-interactive) word errors, the CATTI user should
Multimodal Interactive Transcription of Ancient Text Images 71
Fig. 6. Word alignment for 6 lines of a particularly noisy part of the corpus. The four
last words on the second line as well as the last line illustrate some of over-segmentation
and under-segmentation error types.
have to interactively correct only less than 77. The remaining 23 errors would be
automatically corrected by CATTI, thanks to the feedback information derived
from other interactive corrections [15].
Alignment System: One kind of measure adopted to evaluate the quality of
alignments is the so-called alignment error rate (AER) [13], which measures
mismatches between word-images and ASCII transcriptions (tokens), excluding
word-space tokens. Preliminary results around 7% of AER were obtained on a
set of 53 pages from the “Cristo-Salvador” [10] handwritten old-document.
The two typical alignment errors are known as over-segmentation and under-
segmentation respectively. The over-segmentation error is when one word image
is separated into two or more fragments. The under-segmentation error occurs
when two or more images are grouped together and returned as one word. Fig-
ure 6 shows some of them.
6 Conclusions
In this paper, two systems have been described: one of them aiming at assisting
in the transcription of handwriting old documents, while the other one focus-
ing on the alignment between handwritten text images and their corresponding
transcriptions.
The assisted transcription system (MM-CATTI) is based on an interactive-
predictive framework which combines the efficiency of automatic HTR systems
with the accuracy of the experts in the transcription of ancient documents. In
this case, the words corrected by the expert become part of a increasingly longer
prefixes of the final target transcription. These prefixes are used by the MM-
CATTI system to suggest new suffixes that the expert can iteratively accept
or modify until a satisfactory, correct target transcription is produced. On the
other hand, for handwritten manuscripts whose transcriptions are already avail-
able, the presented alignment system maps every line and word image on the
manuscript with its respective line and word on the electronic (ASCII or PDF)
72 V. Romero et al.
transcription. This method takes advantage of the implicit alignments made by
the Viterbi decoding used in forced text image recognition with HMMs.
For these systems, adequate demonstrators have been implemented, which
clearly show the capabilities and potential benefits of using the proposed tech-
nologies. We believe these technologies are already pretty mature and we are
currently seeking to undertake projects involving large-scale field tests that will
allow us to validate and/or further develop these technologies.
Acknowledgment. Work supported by the Spanish Government (MICINN and
“Plan E”) under the MITTRAL (TIN2009-14633-C03-01) research project and
under the research programme Consolider Ingenio 2010: MIPRCV (CSD2007-
00018) and the Generalitat Valenciana under gran Prometeo/2009/14.
References
1. Bazzi, I., Schwartz, R., Makhoul, J.: An Omnifont Open-Vocabulary OCR Sys-
tem for English and Arabic. IEEE Transactions on Pattern Analysis and Machine
Intelligence 21(6), 495–504 (1999)
2. Jelinek, F.: Statistical Methods for Speech Recognition. MIT Press (1998)
3. Kavallieratou, E., Fakotakis, N., Kokkinakis, G.: An unconstrained handwriting
recognition system. International Journal on Document Analysis and Recogni-
tion 4(4), 226–242 (2002)
4. Kavallieratou, E., Stamatatos, E.: Improving the quality of degraded document
images. In: Proceedings of the Second International Conference on Document Im-
age Analysis for Libraries (DIAL 2006), pp. 340–349. IEEE Computer Society,
Washington, DC, USA (2006)
5. Kneser, R., Ney, H.: Improved backing-off for m-gram language modeling. In:
International Conference on Acoustics, Speech, and Signal Processing (ICASSP
1995), vol. 1, pp. 181–184. IEEE Computer Society, Los Alamitos (1995)
6. Likforman-Sulem, L., Zahour, A., Taconet, B.: Text line segmentation of historical
documents: a survey. International Journal on Document Analysis and Recogni-
tion 9(2), 123–138 (2007)
7. Lorigo, L., Govindaraju, V.: Offline Arabic handwriting recognition: a survey. IEEE
Transactions on Pattern Analysis and Machine Intelligence 28(5), 712–724 (2006)
8. Marti, U.V., Bunke, H.: Using a Statistical Language Model to improve the prefor-
mance of an HMM-Based Cursive Handwriting Recognition System. Int. Journal
of Pattern Recognition and Artificial Intelligence 15(1), 65–90 (2001)
9. Romero, V., Levia, L.A., Toselli, A.H., Vidal, E.: Interactive multimodal tran-
scription of text imagse using a web-based demo system. In: Procedings of the
International Conference on Intelligent User Interfaces, Sanibel Island, Florida,
pp. 477–478 (February 2009)
10. Romero, V., Toselli, A.H., Rodŕıguez, L., Vidal, E.: Computer Assisted Transcrip-
tion for Ancient Text Images. In: Kamel, M.S., Campilho, A. (eds.) ICIAR 2007.
LNCS, vol. 4633, pp. 1182–1193. Springer, Heidelberg (2007)
11. Romero, V., Toselli, A.H., Vidal, E.: Using mouse feedback in computer assisted
transcription of handwritten text images. In: Proceedings of the 10th International
Conference on Document Analysis and Recognition (ICDAR). IEEE Computer
Society, Barcelona (2009)
Multimodal Interactive Transcription of Ancient Text Images 73
12. Toselli, A.H., Juan, A., Keysers, D., González, J., Salvador, I., Ney, H., Vidal,
E., Casacuberta, F.: Integrated Handwriting Recognition and Interpretation using
Finite-State Models. International Journal of Pattern Recognition and Artificial
Intelligence 18(4), 519–539 (2004)
13. Toselli, A.H., Romero, V., Vidal, E.: Viterbi Based alignment between Text Im-
ages and their Transcripts. In: Language Technology for Cultural Heritage Data
(LaTeCH 2007), Prague, Czech Republic, pp. 9–16 (June 2007)
14. Toselli, A.H., Romero, V., Vidal, E.: Computer Assisted Transcription of Text
Images and Multimodal Interaction. In: Popescu-Belis, A., Stiefelhagen, R. (eds.)
MLMI 2008. LNCS, vol. 5237, pp. 296–308. Springer, Heidelberg (2008)
15. Toselli, A.H., Romero, V., Pastor, M., Vidal, E.: Multimodal interactive transcrip-
tion of text images. Pattern Recognition 43(5), 1814–1825 (2009)
16. Zimmermann, M., Bunke, H.: Automatic segmentation of the iam off-line database
for handwritten english text. In: Proceedings of the 16th International Conference
on Pattern Recognition, vol. 4, pp. 35–39 (2000)
17. Zimmermann, M., Chappelier, J.C., Bunke, H.: Offline grammar-based recognition
of handwritten sentences. IEEE Trans. Pattern Anal. Mach. Intell. 28(5), 818–821
(2006)
