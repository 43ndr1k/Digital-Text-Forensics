Searching, Translating and Classifying
Information in Cyberspace
Jacques Savoy, Ljiljana Dolamic, and Olena Zubaryeva
Computer Science Department, University of Neuchatel,
Rue Emile Argand 11, 2000 Neuchâtel, Switzerland
{Jacques.Savoy,Ljiljana.Dolamic,Olena.Zubaryeva}@unine.ch
Abstract. In this paper we describe current search technologies avail-
able on the web, explain underlying difficulties and show their limits,
related to either current technologies or to the intrinsic properties of all
natural languages. We then analyze the effectiveness of freely available
machine translation services and demonstrate that under certain con-
ditions these translation systems can operate at the same performance
levels as manual translators. Searching for factual information with com-
mercial search engines also allows the retrieval of facts, user comments
and opinions on target items. In the third part we explain how the prin-
ciple machine learning strategies are able to classify short passages of
text extracted from the blogosphere as factual or opinionated and then
classify their polarity (positive, negative or mixed).
Keywords: Search technology, web, machine translation, automatic text
classification, machine learning, natural language processing (NLP).
1 Introduction
We have witnessed the apparition of the web over the previous decade, but
during the next we will most certainly be able to confirm the maturity, diversity
and necessity of the web as a communication and processing medium. In this
paper, we briefly describe and evaluate the quality of three important services
being applied on the web: search technologies [1], [2], machine translation [3] and
automatic classification systems [4]. Given their affiliation with natural language
processing (NLP) [5], they play a key role in the web’s current development and
in the near future they will lead to the development of new web-based services.
First, it is through the help of search engines that the web has grown to its
current size. Without these systems it would be nearly impossible to search for
specific information (e.g, who was the first man in space), retrieve named pages
or services (e.g., passport renewal), and find the wished web sites designed for
buying goods or services (e.g., hotel Brussels). When asked for their opinion,
users seem relatively satisfied with commercial search engines, but the questions
we would like to answer in this domain is whether or not current search technolo-
gies are still progressing and what are the foreseen limits. Section 2 will provide
some answers to these points.
G. Babin, K. Stanoevska-Slabeva, P. Kropf (Eds.): MCETECH 2011, LNBIP 78, pp. 62–75, 2011.
c© Springer-Verlag Berlin Heidelberg 2011
Searching, Translating and Classifying Information in Cyberspace 63
Second, although in its early years the web was dominated by one language,
this monolingual aspect is no more the norm. Many users would now like to
communicate in a variety of languages (e.g., in multilingual countries such as
Canada, Switzerland, or international companies, etc.) and to do so they need
to overcome certain language barriers. With current search engines for example,
they may wish write a request in one language and retrieve documents written in
others. Conversely, while some users can read documents in another language,
they would not be able to formulate a query in that language or provide the
reliable search terms needed to retrieve the documents being searched (or for
retrieving images, music, video, or statistical tables for which language is less
important). In other circumstances, monolingual users may want to retrieve doc-
uments in another language and then automatically translate the retrieved texts
into their own language (before manually translating the most important infor-
mation or confirming their own understanding of a given web page). What about
the current quality of these freely available translation services? What impor-
tant aspects still need some improvement? These questions will be addressed
in Section 3.
Third, when handling and processing huge amounts of information, we need
efficient methods for classifying it into predefined categories. Simply searching for
factual (or objective) information on a given item or issue is not always the final
objective, and web surfers may want to know more about the opinions, feelings
or comments other users might have. Given the current growth in activities
in the search community, especially in adding content to blogs, online forums,
Internet platforms, etc., the task of detecting and classifying information has
become increasingly popular. It is consequently more important that a system
be developed to process the multitude languages in use and also detect any
subjective expressions [6], [7]. The current technology and underlying difficulties
in this domain will be presented in Section 4.
Separately, the search engine technology, the machine translation service and
the opinion detection and classification system are useful per se. In conjunction
they allow the user to discover additional services or products. Moreover, the
user has now the opportunity to know previous experiences done by other people
on the target service or product. Imagine the following scenario.
You travel to Berlin and you need to book a hotel. Using a commercial search
engine, you can find various hotels in Berlin, and several of them seem to fit your
criteria (location, price, comfort, etc). However you don’t have a clear indication
about the noise in the room because you really want a quiet room. It is not
clear if the location itself is noisy or not. Exploring the blogs without a search
engine is not possible, and many comments are written in German, French,
Italian, Dutch or Spanish languages that you cannot understand. Using a new
search engine combining these three technologies, you will be able to discover
comments written about the selected hotel in Berlin. To achieve this, you write
a request and if needed, your query will be translated into other languages to
obtain a broader coverage. This new search system will also classify the retrieved
comments according to their polarity, namely positive, negative or mixed. Since
64 J. Savoy, L. Dolamic, and O. Zubaryeva
many of them are written in another languages than English, you can just click
on them to obtain an English translation.
2 Search Technologies
During the last decade, the information retrieval (IR) domain [1], [2], has been
confronted with larger volumes of information from which pertinent items (text,
web pages, images, video, music) must be retrieved, when responding to user
requests (ad hoc tasks). Given that web users tend to submit short requests
composed of only one or two words [8], effective IR models are being proposed
to meet this challenge. Moreover, users expect to find one or more appropriate
answers at the very top of the retrieved items listed, and within minimal waiting
periods (less than 0.2 sec).
For this reason commercial search engines (mainly Google, Yahoo!, and Bing)
have based their technology on matching keywords (the same or very similar
terms must appear in the query and in the retrieved web pages). In addition to
this initial evidence on their relevance, search engines must also consider informa-
tion on the links pointing to these pages. Current practices therefore accounting
for the number of incoming links to a page, thus helping to define the page’s
merit (or usefulness) or the probability of finding the desired information during
a random walk (e.g., PageRank algorithm [9]), or other variants on such link-
based popularity measures [10]. The page’s hyperlink structure is also useful to
obtain short descriptions of the target page by inspecting the anchor texts. For
example, these sequences may include “<a href = “www.microsoft.com”> Mi-
cro$oft </a>” or “<a href = “www.microsoft.com”> the Big Empire </a>”, as
well as descriptors “Micro$oft” and “the Big Empire”linking to the target page
Microsoft.com. This short text-based evidence helps in describing pages contain-
ing various formulations other than those provided by the target page’s author.
Moreover, these anchor texts are very useful in obtaining textual descriptions of
various other media types including images, graphics, photos, video, music, etc.
Current search engines also take user information into account by provid-
ing them with more personalized answers, adapted to their personal interests
or other personal data. They may for example, combine the search request
“Movie”and the user’s IP number, thus allowing the search engine to provide
a list of films showing the same day in theatres located within the user’s ge-
ographical proximity. Final search results could also be influenced by a user’s
previous queries (or previous search sessions) processed by the search engine
(thematic context). Moreover, in addition to considering user location or coun-
try (the US, for example), the search engine could crosscheck user location with
data available in a national or regional census database, and then detect useful
demographic information related to the searcher, such as age, race, revenue, etc.
Finally, through considering click-through rates, search engines may be able to
adapt their answers to this and other forms of user feedback. When users tend
to select the same retrieved item for the same query for example, this target
page will obtain better rankings in the future.
Searching, Translating and Classifying Information in Cyberspace 65
In order to develop an overview of the effectiveness of current search tech-
nologies, we reviewed various papers published in three well-known evaluation
campaigns, including TREC [11] (trec.nist.gov), NTCIR (ntcir.nii.ac.jp)
or CLEF (www.clef-campaign.org). Based on their analysis of the current
state of the art in IR technology, Armstrong et al. [12] demonstrate that the
retrieval quality of the new search strategies presented in international confer-
ences (acm-sigir or acm-cikm) has not significantly improved since 1998. This
research seems in fact to have reached a plateau, thus limiting expectations of
better search quality in the near future. In another study evaluating IR systems,
Hawking & Robertson [13] demonstrate that the greater volume of information
available will render the search task easier, at least in their ability to retrieve one
or only a few pertinent documents. Current commercial search engines are work-
ing on this context, with the goal being to provide the typical user with a single
appropriate answer. So when searching in larger volume, the search task will
be simpler. Future improvements may in fact concern the processing of higher
volumes, not really the quality of the search per se. Although at present users
are generally satisfied with commercial search engines, they do complain about
the poor quality in terms of interfaces and results when using dedicated search
engine working within a single web site [14].
The question that then arises is whether or not current search technology is
perfect, or to understand when it fails to provide the correct result. To come up
with an answer we analyzed answers provided by the best IR systems, based on
a set of 160 topics submitted during three CLEF evaluation campaigns. We thus
found three main categories of failure: 1) IR process flaws, 2) natural language
defects, and 3) incomplete and imprecise user topic formulations.
Among the processing faults found in certain search engines, we discovered
problems related to stopword removal and letter normalization (conversion of
uppercase letters to lowercase). Stopword lists are used to eliminate frequently
occurring common words (the, of, a, is, us, or it) having no specific meaning
and possibly generating noise during the IR process (e.g., pages are retrieved
because they share the terms the and are with the query). On the other hand
when processing requests including phrases such as IT engineer, vitamin A or
US citizen, the systems should not remove the forms it, a or us.
To increase the likelihood of obtaining a match between query terms and web
pages, search systems apply a stemming procedure to conflate word variants into
a common stem. For example, when a topic includes the word computer, it seems
reasonable to also retrieve documents containing the word computers, as well as
morphologically related terms such as computational and computing. Included in
our set of requests for example is the topic “Elections parlementaires européennes”
(European Parliament Elections) for which relevant documents found had
élections and européennes or Europe in common with the query. Those documents
have the noun parlement instead of the adjective form parlementaire as expressed
in the topic. Although the search system was able to conflate the form europe and
européennes under the same form, it was not able to establish a link between the
terms parlement and parlementaire, and thus missed many relevant pages.
66 J. Savoy, L. Dolamic, and O. Zubaryeva
As with all natural language processing systems, users expect a certain degree
of robustness. Thus, when a query contains a spelling error, the search engine
should either suggest a corrected version or try to provide a reasonable answer.
In the request “Innondationeurs en Hollande et en Allemagne”(Flooding in Hol-
land and Germany) for example, the system should preferably suggest the term
Innondations for the incorrect spelling Innondationeurs, rather than limiting
the query to its second part (Holland and Germany). Processing this type of
situation becomes less clear when handling spelling variants (color vs. colour),
particularly proper names (Gorbachev vs. Gorbachov, Oscar vs. Oskar), and
when both variants are present in many pages.
The second main category of search engine failures involves different problems
related to natural language expressions. Among these are similar expressions
conveying the same idea or concept using different words (synonyms) or formu-
lations. In our topics, we found this problem with the nouns film and movie, or
car and automobile. In such cases when the query uses one form and the docu-
ment the other, there would not be a match and the corresponding document is
not retreived. Different regions or countries may employ different formulations
to refer to the same object. An example would be the topic “risques du téléphone
portable”(risks with mobile phones), in which the relevant documents contain
country dependant synonyms. In Switzerland for example, a portable phone is
usually a natel, in Belgium téléphone mobile, portable in France and cellulaire in
Québec. Among the top ten documents retrieved by the IR system, one can find
documents written in France (by using the formulation téléphone portable) and
some documents about the risk of being in the mountains. Other retrieved ar-
ticles may simply concern certain aspects of mobile phones (new joint ventures,
new products, etc.).
The second main problem related to natural languages is polysemy when cer-
tain words or expressions may have several interpretations or meanings. The
noun bank for example may be related to a financial institution or a river. Thus
when one form appears in a user request and the other in relevant documents,
there would not be a proper match, and not all pertinent information would be
retrieved. Other examples include the name Jaguar, which refers to an animal,
a car or a software package; while Java refers to an island, a coffee, a dance or a
language. The use of short acronyms is also the source of many incorrect inter-
pretations (e.g., BSE could be Bombay, Beirut, Bahrain, . . . Stock Exchange,
Bovine Spongiform Encephalopathy, Basic Set Element, Breast Self-Exam, etc.).
A third main source of search system mismatch occurs when user requests con-
tain expressions that are too broad or too imprecise. To illustrate this problem
we analyzed the requests for “Trade Unions in Europe,”“Edouard Balladur” or
“World Soccer Championship”. The top ranking documents retrieved by the IR
system in all cases had not one but at least two or three terms in common with
the query. Are those perfect matches? The users judged these pages to be irrel-
evant because the real intent behind the topics was “the role and importance
of trade unions between European countries,”“the economics policies of E. Bal-
ladur” or “the result of the final”. Another case involving less evident examples
Searching, Translating and Classifying Information in Cyberspace 67
was the query “AI in Latin America”, in which the IR process must have inferred
that the acronym AI represents Amnesty International and not Artificial Intel-
ligence (a polysemy problem), and in a second step that Latin America refers to
a region containing several countries. Relevant documents would cite a country
(e.g., Mexico or Colombia) without explicitly linking the country name to the
continent or region name. The request “Wonders of Ancient World” caused the
same problem in that relevant pages did not include the expression “Wonders of
Ancient World” when describing the pyramids in Egypt.
3 Machine Translation
Retrieving the correct information is the first step, and then we need to provide
it into the appropriate language. In fact we must recognize that we are living in
a multilingual world [15], and given recent developments on the web, language
(and script) diversity is steadily increasing. Based on freely available translation
services, we may hope to cross easily these various language barriers, facilitating
communication among people speaking different languages. In Europe and India
for example, and also in large international organizations (e.g., WTO, Nestlé),
multilingual communication is a real concern. In the European Union for example
there are 23 official languages that requires 23 x 22 = 506 translation pairs.
While English is not the language spoken by the majority of people around the
world, it certainly plays a central role as an interlingua medium in transmitting
knowledge or expressing opinions. The CNN success story is just one example of
the increasing importance of this language, yet the Al Jazeera network certainly
confirms that other languages will certainly be of greater importance in a near
future. The fact remains however that English is often the first foreign language
learned in Europe, India and in Far-East countries, and thus it is still very
important to provide adequate resources for translating from other languages to
English and vice-versa.
The major commercial search engines have certainly not ignored this demand
for translation resources to and from English, and Google is certainly a case
in point, given its efforts in improve searching in pages available in English as
well as other languages on the web. Regardless of language in which queries
are written, Google has launched a translation service providing two-way online
translation services, mainly between English and more than 55 other languages
(translate.google.com). Over the last few years other free Internet translation
services have also been made available, including Yahoo! (babelfish.yahoo.com)
and Reverso (www.reverso.net). Behind these web services is a translation
strategy based on statistical machine translation approaches [3], [5], where the
most probable translation is selected according to occurrence frequencies in par-
allel corpora, and able to make adjustments based on user feedback.
While translations from/to English are freely available, the resulting trans-
lated document may not be of the highest quality. We know in fact that both
automatic and human translations can sometimes be very poor (see examples
in [16]). Quality translation may depend on the relationship between the source
68 J. Savoy, L. Dolamic, and O. Zubaryeva
and target languages, particularly those having a close relationship with English
(e.g., French, German) as opposed to more distant languages such as Japanese.
Although we do not intend to evaluate translations per se, we will analyze vari-
ous IR and translation systems in terms of their ability to retrieve items written
in English, based on the automatic translation of queries written in German,
French, Spanish, Chinese and Japanese languages. A previous partial study can
be found in [17].
Our evaluation is based mainly on 310 CLEF topics reflecting typical web
search requests, and consisting of two or three words in mean. These topics
cover various subjects such as “El Niño and the Weather,”“Chinese Currency
Devaluation,”“Eurofighter,”“Victories of Alberto Tomba,”“Computer Anima-
tion,”“Films Set in Scotland,”“Oil Prices,” or “Sex in Advertisements.” This
topic set is available in English, German (DE), French (FR), Spanish (SP), Chi-
nese (ZH) and Japanese (JA).
To evaluate the quality of Google and Yahoo! translation services, we per-
formed a search using query formulations written in English to define the base-
line (monolingual search performance at 100%). Then using the topics available
in the various other languages, we first automatically translated them using ei-
ther Google or Yahoo!, and carried out the search with the translated topics
(see Table 1 for performance differences). As expected, lower performance levels
were obtained for all query translations other than English. The best level was
for the Spanish formulation translated by Google, in which case the decrease was
around 1.4%, relative to the baseline. Compared to the monolingual search, this
level was considered non-significant by a statistical test (paired t -test, significant
level at 5%), while all others were viewed as significant.
Table 1 clearly indicates that Google’s translations were better than those
achieved by Yahoo! (at least with our search process). Upon inspecting the results
for the different languages, the translation process seems easier from French,
German or Spanish languages than from Chinese, a more remote language, while
in certain cases (Japanese), the differences were smaller than expected. Finally,
based on the absolute values, we may conclude that automatic query translation
is a feasible alternative. The performance decreases due to translation are not
that large, around 6% for German and French, 8% for Japanese and a little more
for Chinese (14%). How can we obtain better automatic translation and where
are the real difficulties?
The first source of translation difficulties was the presence of proper names in
the topic. Although in certain cases names did not change from one language to
Table 1. Comparative performances of queries translated into English using machine
translation systems and manual methods (in % compared to monolingual search)
From DE From FR From SP From ZH From JA
Using Google 93.6% 93.1% 98.6% 85.2% 91.7%
Using Yahoo! 75.3% 83.4% 73.7% 62.7% 72.8%
Searching, Translating and Classifying Information in Cyberspace 69
English (e.g., France or Haiti), some modifications usually had to be made (e.g.,
London is written Londres in French). The same problem seemed to appear for
other topics, such as in “Return of Solzhenitsyn” which was written as “le retour
de Soljénitsyne” in French, “Retorno de Solzhenitsin” in Spanish, or “Rückkehr
Solschenizyns” in German. In this case, when French or German was the query
language, Yahoo!’s translation system was not able to provide the correct English
spelling for this name.
The correct translation of a proper name was found to be more difficult
when it had a specific meaning in the source language. In the query “Schnei-
der Bankruptcy”for example, because the name Schneider also means cutter in
German, and this meaning was selected by Yahoo!’s translation system, with
the phrase being translated as “Cutter bankruptcy.”Another and more difficult
example occurred with the topic “El Niño and the Weather,” where the weather
phenomenon in Spanish was designated as the noun the boy, and thus the Yahoo!
translation returned “the boy and the time,” ignoring the fact that the topic con-
tained a specific noun. When Chinese was the query language, both Google and
Yahoo! were not able to translate the proper name, leaving the Chinese word
untouched or providing a weird expression (e.g., “Schneider Bankruptcy”became
“Shi Tejia goes bankrupt”with Yahoo!).
A second main source of translation error was semantics, especially polysemy,
meaning that a source language term can be translated into several other terms
in the target language. More precisely, in order to find the appropriate word
(or expression) in English, the translation system had to take the context into
account. As shown previously for example in the Spanish topic “El Niño y el
tiempo”, the word “tiempo” could be translated as “weather” or “time”, the
Yahoo! system selected the latter. For the query “Theft of ‘The Scream’” writ-
ten in French as “Vol”du ‘Cri’”, the French word vol could be translated by
flight or theft, and the translation produced by Google was “The Flight of the
‘Scream’” and by Yahoo! was “Flight of the ‘Cry’.”
This latter translation demonstrates another problem related to synonymy,
wherein the translation system had to handle various meanings for a given trans-
lation, such as the French word cri could be translated as either scream or cry.
This synonymy aspect was also found in various requests involving the related
terms car and automobile. In the original English version the topics, the trans-
lation service provided the term car more frequently (five times to be precise)
while it never returned the term automobile. Moreover, the semantic relationship
between two (or more) alternatives is not always that close, as illustrated by the
query “Ship Collisions”, which for the Spanish, Yahoo! returned the translation
“Naval collisions.”
We found a third translation difference within the English topics where various
forms of a given root (merger, merge or merging) representing morphological and
grammatical categories. For the original request “Merger of Japanese Banks”,
for example, the system ranked the first relevant item in the top position, yet
with the translation of “Merging of Japanese Banks” the first relevant article
appeared in the 6th position. The same problem occurred again in the query
70 J. Savoy, L. Dolamic, and O. Zubaryeva
“Golden Globes 1994” where the retrieval system returned a relevant document
in the first position, while for the translated query “Gold Globes 1994”the first
relevant item only appeared in the 6th position. In this case, with the form
golden, the IR system was able to rank a relevant item in the first position, but
not with gold. In our previous example, the classical English stemmer was not
able to conflate the forms merger and merging into the same root (merging is
transformed into the stem merg while merger is left untouched).
A fourth main source of translation problems was that compound construc-
tions, such as those occurring frequently in the German language, were not al-
ways translated into English, and thus the system simply returned the German
term. With the topic “Shark Attacks”for example Google returned the German
term Haifischangriffe while for the query term “Bronchial asthma”Yahoo! re-
turned Bronchialasthma. In both cases, the translated query performs very bad.
4 Automatic Classification in the Blogosphere
As described in Section 2, search technologies are not always perfect, although
with commercial search engines users can normally find the information they are
seeking. As explained in the previous section, if needed they can resort to ma-
chine translation to find the desired information in a given language (although
the best performance is obtained when translating from/to English). The role
played by web users has changed during the last years however; to the point
they are no longer simple information consumers. They may in fact produce
information, share their knowledge within Wikipedia-like services, comment on
the news, write their own blogs or even their own web sites to express opinions,
feelings or impressions on the latest events (politics, exhibits, sports, etc.), or
even products. Now, given the expanding number of contact opportunities avail-
able in cyberspace through several social networking sites, these exchanges are
becoming very common.
With blogs and social networking sites in particular, the nature of information
exchanged is becoming personal, subjective and opinionated rather than factual,
objective and neutral. Thus when we are looking for a product or service, com-
mercial search engines provide factual information (price, technical data, sale
conditions, schedule, etc.), but the value of comments and experiences obtained
from previous customers is sometimes considered more valuable than objective
information. Opinionated comments are not only related to physical products
(books, computers, mobile phones, etc.) but may also include services (hotel, air
companies, movies, etc.) and well-known personalities (actors, politicians [18],
etc.). Given this variety of information available, retrieving of opinionated web
pages or passages is far more complex.
Not only do search engines have to retrieve pertinent information items, they
also have to filter out any underlying garbage. Within the blogosphere in partic-
ular, we need to assume that data might contain spam or other junk material,
including lies or even propaganda written by malicious people or by persons paid
to include positive (as well as negative) comments on a target topic. Second, the
Searching, Translating and Classifying Information in Cyberspace 71
system must identify the most pertinent passages, especially those containing
comments about the targeted product, while discarding any irrelevant segments
(within a blog post, or a web page). Third, the system must detect opinionated
passages (binary classification between factual or opinionated category) and even
classify the opinion types found within them. Detecting opinionated sentence is
usually the first step however as they then have to be classified according to
their polarity (positive, negative or mixed) [6], [7].
Gathering information on the users’ opinions on a given topic is of interest
to individuals (market benchmarking, information search, curiosity), but even
more to corporations (marketing, advertisement, market analysis, competitor
surveys, detecting new opportunities, etc.) and governments (monitoring, in-
telligence gathering, protection of citizens). These classification tasks are more
complex than they appear, for several reasons, and correct attribution cannot
always be done with absolute certainty. In this case, we want to known or in-
dentify the real author. Is this a real customer? Do customer really have any
experience with the target items or are they simply reporting known rumor?
The filtering of pertinent opinions can be rendered even more difficult by
noisy data (e.g., spelling errors, incomplete sentences) or divergence from stan-
dard writing. Latter, different internet-based writing situations (e.g., e-mail, chat
groups, instant messaging, blogs, virtual worlds, etc.) may generate their own
literary register due to the specific graphical, orthographical, lexical, grammat-
ical and structural features they employ [19]. Examples of these might include
the presence of smileys (e.g., :-)), the use of commonly occurring abbreviations
such as those used in text messaging (e.g., irl for in real life), as well as certain
graphical conventions (e.g., the use of uppercase letters, spaces or stars such as
“I SAID NO!”for highlighting reasons), together with the various colors and ani-
mations used for display purposes in documents. Finally, search system accuracy
could be lower than expected due to a small number of examples from which the
system is able to learn.
The previously described problems can be illustrated by an example. To find
customer reactions and comments on a given product, we need to discriminate
between factual information (“the price of the new iPhone is $800”) and opin-
ionated passages (e.g. “the screen design of the iPhone is terrific, not the micro-
phone”). An automatic classification system would have to determine whether
this last sentence expressed an opinion concerning a given component (the screen
and the microphone) of a given product (new iPhone). And then we might be
asked to determine the opinion’s polarity (positive for the screen, negative for
the micro). Moreover, the intensity and polarity may change radically when an-
other element is added (e.g. “the iPhone is beautiful,”“the iPhone is very beau-
tiful,”and “it’s not very beautiful”). To be useful, an automated system must
learn the target of a given negative clause, sometimes by resolving anaphora (e.g.,
what object is replaced by the pronoun it). The target item could of course be
more difficult to identify in other cases (“the iPhone clone is really good”).
To evaluate the current technologies available in this domain, various interna-
tional evaluation campaigns have been conducted (see the blog track at TREC or
72 J. Savoy, L. Dolamic, and O. Zubaryeva
the MOAT track at NTCIR [20]). In text categorization tasks [4], the various text
forms (e.g., clause, sentence, passage, paragraph, document) must be represented
by a numerical vector comprising useful and relevant features. Throughout this
process we would also need to extract and select those features most useful in
identifying the style differences found within the categories. In a second stage,
we would weight them according to their importance in the underlying textual
representations and also their discriminative power. Finally, through applying
classification rules or a learning scheme, the system has to decide whether or not
to assign the new input text to a given category.
From among all these possible features, the objective is to select those that are
topic-independent but would closely reflect the style of the corresponding text-
genre. To achieve this goal, three main sources can be identified. First, at the
lexical level we could consider word occurrence frequency (or character n-grams),
hapax legomena (word occurring once), vocabulary richness [21], total number of
tokens, average word length, number of characters, letter occurrence frequency,
along with other symbols, etc. Special attention should also be paid to the use
of function words (e.g., determinants, prepositions, conjunctions, pronouns such
as an, the, in, or, we, etc.) together with certain verbal forms (is, has, will).
Although the precise composition of these word lists is questionable, different
authors have suggested a wide variety of lists [22], [23].
Secondly we can also take account for syntactic information as, for example,
the part-of-speech (POS) tags by measuring distribution, frequency, patterns or
various combinations of these items. Thirdly, some authors [24], [25] have also
suggested analyzing structural and layout features, including the total number
of lines, number of lines per paragraph, number of tokens per paragraph, pres-
ence of greetings or particular signature formats, as well as features derived from
html tags. More generally, when classifying passages according to predefined cat-
egories, a fourth feature should be considered, namely the occurrence frequency
information obtained from certain content-specific keywords (e.g., said, good, bad,
impression, etc.).
Of the four sources mentioned, those features extracted directly from words
tend to reflect the semantic aspect of the underlying text. While they may corre-
spond directly to surface forms, very frequent forms are usually ignored (stopword
removal) and remaining words are stemmed, and features having low
occurrence frequencies (e.g., less than four) are usually ignored. Finally, text rep-
resentation could be limited to the presence and absence of words or stems (set-
of-words) or each feature could be weighted (bag-of-words). More sophisticated
representations could generate by applying morphological analyzers returning the
lemma and the part-of-speech (POS) for each surface word (e.g., kings produces
king/noun). When accounting for POS information, we try to reflect intrinsic
stylistic patterns and those aspects more orthogonal to the topics, while the bag-
of-words feature tends to provide a better reflection of underlying semantics.
Although individual words tend to perform better than the POS-based fea-
tures, a combination of both should tend to improve the quality of the learning
scheme. As a third common representation, we could mention the selection of
Searching, Translating and Classifying Information in Cyberspace 73
a predefined set of function words (between 50 to 120) (e.g., because, did, each,
large, us, etc.), together with certain punctuation symbols (e.g., $ . ! %, etc.).
Finally, a final text representation may simply account for the occurrence fre-
quency of these terms.
After selecting the most appropriate features, we would then need a classifi-
cation scheme capable of distinguishing between the various possible categories
found in the input passage. Two possible learning approaches can be applied to
achieve this objective: symbolic or machine learning. The symbolic (or knowledge
engineering) school suggests using different knowledge representation paradigms
(ontology, frames, rules) and inference mechanisms (first order logic, proposition
calculus) to determine the category of a new text. Relying on the services of
experts to build these representations would be difficult and expensive, and as
such represents a real knowledge acquisition bottleneck. Tuning and verifying
the final accuracy of a system like this would take time, and in the end updating
the underlying categories or input information would usually require restarting
the process at the beginning.
The machine-learning paradigm clearly dominates the field at present, and
consists of different approaches (e.g., decision tree, neural network, näıve Bayes
model, nearest neighbour, support vector machines (SVM)). These models are
data-driven or based on an inductive process. To set them up, a set of positive
and negative instances (training set) are provided, thus allowing the classifier to
observe and determine the intrinsic characteristics of both positive and negative
examples. These features are then searched within an unseen text and used to
classify it. During this general scheme known as supervised learning, the system
knows the exact category of each item belonging to the training set. When the ap-
propriate learning scheme is selected, no further manual adjustments are needed,
at least in principle, especially when a relatively large number of instances forms
the training set. Once this resource is available, the machine-learning scheme
can be applied. Otherwise, a training set must be built, a task that is however
usually easier than creating a rule-based system. This process could be simply
matter of adding a label to existing data. The training set must however reflect
real and future items to be classified, and without this data, no classifier can be
generated. Recently, some attempts have been made to develop mixed learning
strategies, using both a machine learning model and some form of knowledge
representation (be it a general thesaurus such as WordNet, or some manually
written classification rules).
An analysis of the results of opinion detection on sentences [20] during the last
ntcir evaluation campaign shows that the success rate (F-measure) was around
40% for the best system in English language, 64% for Japanese and from 55%
to 68% for (simplified or traditional) Chinese sentences. In a similar evalua-
tion, Boiy & Moens [7] reported an accuracy level of about 80% for the English
language and 66% for the corresponding French part. From our point of view,
these performance levels are encouraging but are not sufficient to allow current
commercial applications. Given how important this topic is to individuals, firms
and governments, we expect real improvements will be made in the near future.
74 J. Savoy, L. Dolamic, and O. Zubaryeva
In this vein, hybrid learning strategies may fill the gap between current perfor-
mance levels and a level needed to obtain a successful commercial product.
5 Conclusion
Large amounts of data are freely available on the Internet, to individual users as
well as companies and they all need to extract pertinent information items from
this huge volume. For queries currently being submitted, search technologies are
able to provide at least a few good responses, which in the best cases comprise
numerous pertinent web pages. Current IR systems are mainly based on key-
word matching, but they could still improve the quality of documents returned
through making better matches between surface words appearing in the query
formulation and in the web pages. This expected progress is somewhat limited
however by the underlying features of all natural languages, particularly those
related to the synonymy and polysemy linked to all natural languages on the
one hand, and on the other, to the imprecise formulation of user’s information
need (see Section 2).
Machine translation services, the most effective of which are based on sta-
tistical models, allow searchers to retrieve the information they desire in the
English language, even when the returned items are written in another language
(German, Russian, Chinese, Japanese, etc.). The translations resulting from this
process are not perfect and various problems still remain to be solved. The cor-
rect translation of names or selecting correct meanings from among two or more
available translations as explained in Section 3 are just two examples.
The advantages of statistical approaches applied to natural language processing
are that they can handle large amounts of data. Furthermore there are also many
interesting and pertinent perspectives possible through the ability to detect opin-
ionated sentences within blogospheres, etc. And then in an additional step clas-
sification could be done according to their polarity (positive, negative or mixed).
Knowing the reactions of customers, the feelings of individuals or people general
satisfaction will clearly have a real impact in marketing studies, market surveys,
and also on public opinion follow-up. The web is here to stay and it will most cer-
tainly change the way we interact with each other. On the other hand, we really
need to explore and develop techniques able to store, manage, search, translate
and process large amount of textual information that can be found on the web.
Acknowledgments. This work was supported in part by the Swiss NSF, under
Grant #200021-124389.
References
1. Manning, C.D., Raghavan, P., Shütze, H.: Introduction to Information Retrieval.
Cambridge University Press, Cambridge (2008)
2. Boughanem, M., Savoy, J. (eds.): Recherche d’information: Etat des lieux et per-
spectives. Lavoisier, Paris (2009)
Searching, Translating and Classifying Information in Cyberspace 75
3. Koehn, P.: Statistical Machine Translation. Cambridge University Press,
Cambridge (2010)
4. Sebastiani, F.: Machine Learning in Automatic Text Categorization. ACM Com-
puting Survey 14, 1–27 (2002)
5. Indurkhya, N., Damereau, F.J. (eds.): Handbook of Natural Languages Processing,
2nd edn. Chapman & Hall/CRC, Boca Raton (2010)
6. Abassi, A., Chen, H., Salem, A.: Sentiment Analysis in Multiple Languages: Fea-
ture Selection for Opinion Classification in Web Forums. ACM-Transactions on
Information Systems 26 (2008)
7. Boiy, E., Moens, M.F.: A Machine Learning Approach to Sentiment Analysis in
Multilingual Web Texts. Information Retrieval 12, 526–558 (2009)
8. Spink, A., Wolfram, D., Jansen, M.B.J., Saracevic, T.: Searching the Web: The
Public and their Queries. Journal of the American Society for Information Science
and Technology 52, 226–234 (2001)
9. Brin, S., Page, L.: The Anatomy of a Large-Scale Hypertextual Web Search Engine.
In: Proceedings of the WWW’7, pp. 107–117 (1998)
10. Borodin, A., Roberts, G.O., Rosenthal, J.S., Tsaparas, P.: Finding Authorities
and Hubs from Link Structures on the World Wide Web. In: Proceedings of the
WWW’10, pp. 415–429 (2001)
11. Voorhees, E.M., Harman, D.K.: TREC Experiment and Evaluation in Information
Retrieval. The MIT Press, Cambridge (2005)
12. Armstrong, T.G., Moffat, A., Webber, W., Zobel, J.: Improvements that Don’t
Add Up: Ad Hoc Retrieval Results since 1998. In: Proceedings ACM-CIKM,
pp. 601–609. The ACM Press, New York (2009)
13. Hawking, D., Robertson, S.: On Collection Size and Retrieval Effectiveness. Infor-
mation Retrieval Journal 6, 99–105 (2003)
14. Nielsen, J., Loranger, H.: Prioritizing Web Usability. New Riders, Berkeley (2006)
15. Danet, B., Herring, S.C. (eds.): The Multilingual Internet. Language, Culture, and
Communication Online. Oxford University Press, Oxford (2007)
16. Crocker, C.: Lost in Translation. Misadventures of English Abroad. Michael
O’Mara Books Ltd., London (2006)
17. Savoy, J., Dolamic, L.: How Effective is Google’s Translation Service in Search?
Communications of the ACM 52, 139–143 (2009)
18. Véronis, E., Véronis, J., Voisin, N.: Les politiques mis au net. Max Milo Ed., Paris
(2007)
19. Crystal, D.: Language and Internet. Cambridge University Press, Cambridge (2006)
20. Seki, Y., Ku, L.W., Sun, L., Chen, H.H., Kando, N.: Overview of Multilingual
Opinion Analysis Task at NTCIR-8. In: Proceedings NTCIR-8, pp. 209–220. NII
publication, Tokyo (2010)
21. Hoover, D.L.: Another Perspective on Vocabulary Richness. Computers & the
Humanities 37, 151–178 (2003)
22. Burrows, J.F.: Delta: A Measure of Stylistic Difference and a Guide to Likely
Authorship. Literary & Linguistic Computing 17, 267–287 (2002)
23. Zhao, Y., Zobel, J.: Effective and Scalable Authorship Attribution using Function
Words. In: Proceedings of the Second AIRS Asian Information Retrieval Sympo-
sium, pp. 174–189 (2005)
24. Stamatatos, E., Fakotakis, N., Kokkinakis, G.: Automatic Text Categorization in
Terms of Genre and Author. Computational Linguistics 26, 471–495 (2001)
25. Zheng, R., Li, J., Chen, H., Huang, Z.: A Framework for Authorship Identification
of Online Messages: Writing-Style Features and Classification Techniques. Journal
of the American Society for Information Science & Technology 57, 378–393 (2006)
